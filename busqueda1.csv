"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Transformer-Based Direct Speech-To-Speech Translation with Transcoder","T. Kano; S. Sakti; S. Nakamura","Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan","2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","958","965","Traditional speech translation systems use a cascade manner that concatenates speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis to translate speech from one language to another language in a step-by-step manner. Unfortunately, since those components are trained separately, MT often struggles to handle ASR errors, resulting in unnatural translation results. Recently, one work attempted to construct direct speech translation in a single model. The model used a multi-task scheme that learns to predict not only the target speech spectrograms directly but also the source and target phoneme transcription as auxiliary tasks. However, that work was only evaluated Spanish-English language pairs with similar syntax and word order. With syntactically distant language pairs, speech translation requires distant word order, and thus direct speech frame-to-frame alignments become difficult. Another direction was to construct a single deep-learning framework while keeping the step-by-step translation process. However, such studies focused only on speech-to-text translation. Furthermore, all of these works were based on a recurrent neural net-work (RNN) model. In this work, we propose a step-by-step scheme to a complete end-to-end speech-to-speech translation and propose a Transformer-based speech translation using Transcoder. We compare our proposed and multi-task model using syntactically similar and distant language pairs.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383496","speech-to-speech translation;Transcoder;Transformer;sequence-to-sequence model;multitask learning","Recurrent neural networks;Speech recognition;Syntactics;Predictive models;Machine translation;Task analysis;Spectrogram","language translation;learning (artificial intelligence);natural language processing;neural nets;recurrent neural nets;speech processing;speech recognition;speech synthesis","traditional speech translation systems;cascade manner;machine translation;MT;text-to-speech synthesis;step-by-step manner;ASR errors;unnatural translation results;direct speech translation;multitask scheme;target speech spectrograms;target phoneme transcription;Spanish-English language pairs;similar syntax;syntactically distant language pairs;distant word order;speech frame-to-frame alignments;step-by-step translation process;speech-to-text translation;net-work model;step-by-step scheme;complete end-to-end speech-to-speech translation;Transformer-based speech translation;similar language pairs","","20","","20","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"End-to-End Speech Translation With Transcoding by Multi-Task Learning for Distant Language Pairs","T. Kano; S. Sakti; S. Nakamura","Nara Institute of Science and Technology, Ikoma, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma, Japan; Data Science Center and Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 May 2020","2020","28","","1342","1355","Directly translating spoken utterances from a source language to a target language is challenging because it requires a fundamental transformation in both linguistic and para/non-linguistic features. Traditional speech-to-speech translation approaches concatenate automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech synthesizer (TTS) by text information. The current state-of-the-art models for ASR, MT, and TTS have mainly been built using deep neural networks, in particular, an attention-based encoder-decoder neural network with an attention mechanism. Recently, several works have constructed end-to-end direct speech-to-text translation by combining ASR and MT into a single model. However, the usefulness of these models has only been investigated on language pairs of similar syntax and word order (e.g., English-French or English-Spanish). For syntactically distant language pairs (e.g., English-Japanese), speech translation requires distant word reordering. Furthermore, parallel texts with corresponding speech utterances that are suitable for training end-to-end speech translation are generally unavailable. Collecting such corpora is usually time-consuming and expensive. This article proposes the first attempt to build an end-to-end direct speech-to-text translation system on syntactically distant language pairs that suffer from long-distance reordering. We train the model on English (subject-verb-object (SVO) word order) and Japanese (SOV word order) language pairs. To guide the attention-based encoder-decoder model on this difficult problem, we construct end-to-end speech translation with transcoding and utilize curriculum learning (CL) strategies that gradually train the network for end-to-end speech translation tasks by adapting the decoder or encoder parts. We use TTS for data augmentation to generate corresponding speech utterances from the existing parallel text data. Our experiment results show that the proposed approach provides significant improvements compared with conventional cascade models and the direct speech translation approach that uses a single model without transcoding and CL strategies.","2329-9304","","10.1109/TASLP.2020.2986886","JSPS KAKENHI(grant numbers:JP17H06101,JP17K00237); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072502","End-to-end speech-to-text translation;automatic speech recognition;machine translation (MT);multi-task learning","Task analysis;Decoding;Speech processing;Recurrent neural networks;Training;Adaptation models","computational linguistics;decoding;language translation;learning (artificial intelligence);natural language processing;neural nets;speech recognition;text analysis","source language;target language;ASR;text-to-text machine translation;MT;text-to-speech synthesizer;TTS;text information;attention-based encoder-decoder neural network;syntactically distant language pairs;speech utterances;training end-to-end speech translation;end-to-end direct speech-to-text translation system;Japanese language pairs;attention-based encoder-decoder model;direct speech translation approach;parallel text data;subject-verb-object word order;SOV word order;curriculum learning;data augmentation;speech translation","","16","","32","IEEE","20 Apr 2020","","","IEEE","IEEE Journals"
"Speech Translation by Confusion Network Decoding","N. Bertoldi; R. Zens; M. Federico","Centro per la Ricerca Scientifica e Tecnologica, ITC IRST, Trento, Italy; Lehrstuhl f√ºr Informatik Computer Science Department, RWTH Aachen University, Aachen, Germany; Centro per la Ricerca Scientifica e Tecnologica, ITC IRST, Trento, Italy","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1297","IV-1300","This paper describes advances in the use of confusion networks as interface between automatic speech recognition and machine translation. In particular, it presents an implementation of a confusion network decoder which significantly improves both in efficiency and performance previous work along this direction. The confusion network decoder results as an extension of a state-of-the-art phrase-based text translation system. Experimental results in terms of decoding speed and translation accuracy are reported on a real-data task, namely the translation of plenary speeches at the European Parliament from Spanish to English.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218346","Machine Translation;Speech Translation;Natural Language Processing","Decoding;Speech recognition;Speech processing;Natural languages;Speech analysis;Computer science;Automatic speech recognition;Natural language processing;Information processing;Information analysis","decoding;language translation;speech coding;speech recognition","speech translation;confusion network decoding;automatic speech recognition;machine translation;state-of-the-art phrase-based text translation system;plenary speeches;European Parliament","","20","2","11","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Some insights from translating conversational telephone speech","G. Kumar; M. Post; D. Povey; S. Khudanpur","Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD, USA","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","3231","3235","We report insights from translating Spanish conversational telephone speech into English text by cascading an automatic speech recognition (ASR) system with a statistical machine translation (SMT) system. The key new insight is that the informal register of conversational speech is a greater challenge for ASR than for SMT: the BLEU score for translating the reference transcript is 64%, but drops to 32% for translating automatic transcripts, whose word error rate (WER) is 40%. Several strategies are examined to mitigate the impact of ASR errors on the SMT output: (i) providing the ASR lattice, instead of the 1-best output, as input to the SMT system, (ii) training the SMT system on Spanish ASR output paired with English text, instead of Spanish reference transcripts, and (iii) improving the core ASR system. Each leads to consistent and complementary improvements in the SMT output. Compared to translating the 1-best output of an ASR system with 40% WER using an SMT system trained on Spanish reference transcripts, translating the output lattice of a better ASR system with 35% WER using an SMT system trained on ASR output improves BLEU from 32% to 38%.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854197","Speech Recognition;Natural Language Processing;Machine Translation;Human Language Technology;Spoken Language Translation","Speech;Lattices;Training;Speech recognition;Conferences;Acoustics;Speech processing","error analysis;language translation;speech recognition","Spanish conversational telephone speech translation;English text;automatic speech recognition system;statistical machine translation system;SMT system;informal register;BLEU score;automatic transcripts;word error rate;WER;ASR errors;SMT output;ASR lattice;Spanish reference transcripts;core ASR system","","3","","20","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Integration of Speech Recognition and Machine Translation in Computer-Assisted Translation","S. Khadivi; H. Ney","Department of Computer Science, RWTH Aachen University, Aachen, Germany; Department of Computer Science, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Audio, Speech, and Language Processing","21 Oct 2008","2008","16","8","1551","1564","Parallel integration of automatic speech recognition (ASR) models and statistical machine translation (MT) models is an unexplored research area in comparison to the large amount of works done on integrating them in series, i.e., speech-to-speech translation. Parallel integration of these models is possible when we have access to the speech of a target language text and to its corresponding source language text, like a computer-assisted translation system. To our knowledge, only a few methods for integrating ASR models with MT models in parallel have been studied. In this paper, we systematically study a number of different translation models in the context of the N-best list rescoring. As an alternative to the N -best list rescoring, we use ASR word graphs in order to arrive at a tighter integration of ASR and MT models. The experiments are carried out on two tasks: English-to-German with an ASR vocabulary size of 17 K words, and Spanish-to-English with an ASR vocabulary of 58 K words. For the best method, the MT models reduce the ASR word error rate by a relative of 18% and 29% on the 17 K and the 58 K tasks, respectively.","1558-7924","","10.1109/TASL.2008.2004301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648933","Computer-assisted translation (CAT);speech recognition;statistical machine translation (MT)","Speech recognition;Automatic speech recognition;Natural languages;Vocabulary;Humans;Concurrent computing;Context modeling;Error analysis;Engines","computational linguistics;graph theory;integration;language translation;speech recognition","machine translation;computer-assisted translation;parallel integration;automatic speech recognition;statistical machine translation models;speech-to-speech translation;source language text;word graphs;English-to-German;Spanish-to-English","","16","","33","IEEE","21 Oct 2008","","","IEEE","IEEE Journals"
"Towards Fluent Translations From Disfluent Speech","E. Salesky; S. Burger; J. Niehues; A. Waibel","Carnegie Mellon University, Pittsburgh PA, U.S.A.; Carnegie Mellon University, Pittsburgh PA, U.S.A.; Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany","2018 IEEE Spoken Language Technology Workshop (SLT)","14 Feb 2019","2018","","","921","926","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.","","978-1-5386-4334-1","10.1109/SLT.2018.8639661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8639661","speech translation;disfluency removal;spoken language translation;spoken language processing","Task analysis;Hidden Markov models;Speech recognition;Training data;Speech processing;Data models;Lapping","language translation;speech recognition","conversational speech phenomena;machine translation training data;end-to-end speech translation systems;sequence-to-sequence architecture;clean translations;disfluent speech;end-to-end translation;fluent translations;speech datasets","","4","","15","IEEE","14 Feb 2019","","","IEEE","IEEE Conferences"
"A Faster Approach For Direct Speech to Speech Translation","R. T. Shankarappa; S. Tiwari","Voice Intelligence Team, Samsung R&D Institute, Bengaluru, India; Voice Intelligence Team, Samsung R&D Institute, Bengaluru, India","2022 IEEE Women in Technology Conference (WINTECHCON)","25 Jul 2022","2022","","","1","6","As the world is pacing towards globalization, the demand for automatic language translators is increasing rapidly. Traditional translation systems consist of multiple steps like speech recognition, text to text machine translation, and speech generation. Issue with these systems are, latency due to multiple steps and error propagation from first steps toward last steps. Another challenge is that many spoken languages do not have text representation, so traditional system involving speech to text and text to text translation do not work. In this paper, we are presenting a recurrent neural network (RNN) based translation system that can generate a direct waveform of target language audio. We have used the sparse coding technique for the extraction and inversion of audio features. An attention-based multi-layered sequence to sequence model is trained using a novel technique on a dataset of Spanish to English audio and no intermediate text representation is used while training or inference. We have done performance comparison of proposed approaches using latency, bilingual evaluation understudy (BLEU) score and Perceptual Evaluation of Speech Quality PESQ score analysis. The resulting system provides a very fast translation with good translation accuracy and audio quality.","","978-1-6654-8674-3","10.1109/WINTECHCON55229.2022.9832314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832314","Speech Signal Processing;Machine Learning;Translation System;Encoder-Decoder","Training;Codes;Recurrent neural networks;Speech coding;Speech recognition;Predictive models;Feature extraction","language translation;natural language processing;recurrent neural nets;speech recognition;text analysis","recurrent neural network based translation system;text translation;spoken languages;error propagation;speech generation;text machine translation;speech recognition;traditional translation systems;automatic language translators;Speech translation;direct Speech;faster approach;audio quality;good translation accuracy;fast translation;Speech Quality PESQ score analysis;intermediate text representation;English audio;attention-based multilayered sequence;audio features;sparse coding technique;target language audio;direct waveform","","1","","30","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Improving Speech-to-Speech Translation Through Unlabeled Text","X. -P. Nguyen; S. Popuri; C. Wang; Y. Tang; I. Kulikov; H. Gong","Nanyang Technological University, Singapore; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Direct speech-to-speech translation (S2ST) is among the most challenging problems in the translation paradigm due to the significant scarcity of S2ST data. While effort has been made to increase the data size from unlabeled speech by cascading pretrained speech recognition (ASR), machine translation (MT) and text-to-speech (TTS) models; unlabeled text has remained relatively under-utilized to improve S2ST. We propose an effective way to utilize the massive existing unlabeled text from different languages to create a large amount of S2ST data to improve S2ST performance by applying various acoustic effects to the generated synthetic data. Empirically our method outperforms the state of the art in Spanish-English translation by up to 2 BLEU. Significant gains by the proposed method are demonstrated in extremely low-resource settings for both Spanish-English and Russian-English translations.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095578","Speech-to-speech translation;augmentation;unlabeled text","Speech recognition;Transforms;Signal processing;Acoustics;Data models;Machine translation;Task analysis","","","","1","","36","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Investigating translation of Parliament speeches","D. Dechelotte; H. Schwenk; J. L. Gauvain; O. Galibert; L. Lamel","LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","116","120","This paper reports on recent experiments for speech to text (STT) translation of European Parliamentary speeches. A Spanish speech to English text translation system has been built using data from the TC-STAR European project. The speech recognizer is a state-of-the-art multipass system trained for the Spanish EPPS task and the statistical translation system relies on the IBM-4 model. First, MT results are compared using manual transcriptions and 1-best ASR hypotheses with different word error rates. Then, a n-best interface between the ASR and MT components is investigated to improve the STT process. Derivation of the fundamental equation for machine translation suggests that the source language model is not necessary for STT. This was investigated by using weak source language models and by n-best rescoring adding the acoustic model score only. A significant loss in the BLEU score was observed suggesting that the source language model is needed given the insufficiencies of the translation model. Adding the source language model score in the n-best rescoring process recovers the loss and slightly improves the BLEU score over the 1-best ASR hypothesis. The system achieves a BLEU score of 37.3 with an ASR word error rate of 10% and a BLEU score of 40.5 using the manual transcripts","","0-7803-9478-X","10.1109/ASRU.2005.1566514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566514","","Speech recognition;Automatic speech recognition;Natural languages;Speech synthesis;Error analysis;Statistics;Equations;Humans;Vocabulary","language translation;natural languages;speech recognition;speech synthesis","speech to text translation;European Parliamentary speeches;speech recognizer;multipass system;machine translation;source language model;BLEU score","","","","17","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Experiments in word-reordering and morphological preprocessing for transducer-based statistical machine translation","A. de Gispert; J. B. Marino","TALP Research Center, Universitat Polilt√®cnica de Catalunya, Barcelona, Spain; TALP Research Center, Universitat Polilt√®cnica de Catalunya, Barcelona, Spain","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","634","639","Statistical speech translation can be achieved by an integrated search procedure that produces speech recognition and translation at the same time. Based on finite-state transducers and Viterbi search, the approach forces the rewriting of the target language in a set of modified units (with zero, one or more original words) to preserve the monotonicity of the search over the speech signal without changing the word order in the target language. In this paper, we analyse the effect of non-monotonic word alignments in two different Spanish to English translation tasks (speech-aimed small-vocabulary Verbmobil task and text-aimed large-vocabulary European Parliament task), revealing the most frequent cross patterns and experimenting with reordering strategies to improve the transducer probabilities. In addition, some preliminary results are presented on introducing POS-tagging and lemmatization, as well as some preprocessing such as categorization, to help improving the training of the system.","","0-7803-7980-2","10.1109/ASRU.2003.1318514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318514","","Natural languages;Equations;Speech recognition;Acoustic transducers;Speech processing;Text recognition;Viterbi algorithm;Pattern analysis;Speech analysis;Vocabulary","language translation;speech recognition","word-reordering;morphological preprocessing;transducer-based statistical machine translation;integrated search procedure;integrated speech recognition/translation;finite-state transducers;Viterbi search;search monotonicity;nonmonotonic word alignments;Spanish/English translation;speech-aimed small vocabulary Verbmobil task;text-aimed large-vocabulary European Parliament task;cross patterns;POS-tagging;lemmatization;categorization","","","","14","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Efficient Speech Translation Through Confusion Network Decoding","N. Bertoldi; R. Zens; M. Federico; W. Shen","Fondazione Bruno Kessler (FBK), Povo, Italy; Computer Science Department, RWTH Aachen University, Aachen, Germany; Fondazione Bruno Kessler (FBK), Povo, Italy; Information Systems Technology Group, Massachusetts Institute of Technology, Lexington, MA, USA","IEEE Transactions on Audio, Speech, and Language Processing","21 Oct 2008","2008","16","8","1696","1705","This paper describes advances in the use of confusion networks as interface between automatic speech recognition and machine translation. In particular, it presents a decoding algorithm for confusion networks which results as an extension of a state-of-the-art phrase-based text translation decoder. The confusion network decoder significantly improves both in efficiency and performance over previous work along this direction, and outperforms the background text translation system. Experimental results in terms of translation accuracy and decoding efficiency are reported for the task of translating plenary speeches of the European Parliament from Spanish to english and from english to Spanish.","1558-7924","","10.1109/TASL.2008.2002054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637895","Confusion network decoder;speech translation","Decoding;Speech recognition;Automatic speech recognition;Natural languages;Transducers;Speech processing;Speech analysis;Computer errors;Open source software;Information processing","language translation;speech coding;speech recognition;vocoders","speech translation;confusion network decoding;automatic speech recognition;machine translation;text translation decoder","","12","","33","IEEE","3 Oct 2008","","","IEEE","IEEE Journals"
"System Combination for Machine Translation of Spoken and Written Language","E. Matusov; G. Leusch; R. E. Banchs; N. Bertoldi; D. Dechelotte; M. Federico; M. Kolss; Y. -S. Lee; J. B. Marino; M. Paulik; S. Roukos; H. Schwenk; H. Ney","RWTH Aachen University, Aachen, Germany; RWTH Aachen University, Aachen, Germany; Universitat Politecnica de Catalunya, Barcelona, Spain; Fondazione Bruno Kessler (FBK), Trento, Italy; Laboratoire d'Informatique pour la Mecanique et les Sciences del Ingenieur, Orsay, France; Fondazione Bruno Kessler (FBK), Trento, Italy; University of Karlsruhe, Karlsruhe, Germany; IBM Research, Yorktown Heights, NY, USA; Universitat Politecnica de Catalunya, Barcelona, Spain; University of Karlsruhe, Karlsruhe, Germany; IBM Research, Yorktown Heights, NY, USA; Laboratoire d'Informatique pour la Mecanique et les Sciences del Ingenieur, Orsay, France; RWTH Aachen University, Aachen, Germany","IEEE Transactions on Audio, Speech, and Language Processing","15 Aug 2008","2008","16","7","1222","1237","This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The consensus translation is computed by weighted majority voting on a confusion network, similarly to the well-established ROVER approach of Fiscus for combining speech recognition hypotheses. To create the confusion network, pairwise word alignments of the original MT hypotheses are learned using an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole corpus of automatic translations rather than a single sentence is taken into account in order to achieve high alignment quality. The confusion network is rescored with a special language model, and the consensus translation is extracted as the best path. The proposed system combination approach was evaluated in the framework of the TC-STAR speech translation project. Up to six state-of-the-art statistical phrase-based translation systems from different project partners were combined in the experiments. Significant improvements in translation quality from Spanish to English and from English to Spanish in comparison with the best of the individual MT systems were achieved under official evaluation conditions.","1558-7924","","10.1109/TASL.2008.914970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599393","machine translation;natural languages;speech processing;text processing","Natural languages;Voting;Automatic speech recognition;Computer networks;Lattices;Speech recognition;Speech analysis;Speech processing;Text processing;Iterative methods","language translation;natural languages;speech recognition","machine translation;consensus translation;ROVER approach;speech recognition hypotheses;statistical alignment algorithm;TC-STAR speech translation project;state-of-the-art statistical phrase-based translation system;natural language;spoken language;written language","","19","4","46","IEEE","15 Aug 2008","","","IEEE","IEEE Journals"
"Machine Translation of English Videos to Indian Regional Languages using Open Innovation","S. K. Pulipaka; C. K. Kasaraneni; V. N. Sandeep Vemulapalli; S. S. Mourya Kosaraju","Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Guntur, Andhra Pradesh, India; Department of Computer Engineering, San Jose State University, San Jose, CA, United States; Synopsys Inc, Hyderabad, India; Department of Computer Science, University of Birmingham, Dubai","2019 IEEE International Symposium on Technology and Society (ISTAS)","23 Dec 2019","2019","","","1","7","In spite of many languages being spoken in India, it is difficult for the people to understand foreign languages like English, Spanish, Italian, etc. The recognition and synthesis of speech are prominent emerging technologies in natural language processing and communication domains. This paper aims to leverage the open source applications of these technologies, machine translation, text-to-speech system (TTS), and speech-to-text system (STT) to convert available online resources to Indian languages. This application takes an English language video as an input and separates the audio from video. It then divides the audio file into several smaller chunks based on the timestamps. These audio chunks are then individually converted into text using IBM Watson's speech-to-text (STT) module. The obtained text chunks are then concatenated and passed to Google's machine translate API for conversion to the requested Indian language. After this translation, a TTS system is required to convert the text into the desired audio output. Not many open source TTS systems are available for Indian regional languages. One such available application is the flite engine (a lighter version of Festival engine developed by Prof. Alan Black at Carnegie Mellon University (CMU)). This flite engine is used as TTS for generating audio from translated text. The accuracy of the application developed can be as high as 91 percent for a single video and averages about 79 percent. This accuracy is verified by comparing naturality of the audio with the general spoken language. This application is beneficial to visually impaired people as well as individuals who are not capable of reading text to acquire knowledge in their native language. In future, this application aims to achieve ubiquitous communication enabling people of different regions to communicate with each other breaking the language barriers.","2158-3412","978-1-7281-5480-0","10.1109/ISTAS48451.2019.8937988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937988","machine translation;text-to-speech;speech-to-text;concatenation;open innovation","Technological innovation;Videos;Engines;Optical character recognition software;Natural language processing;Business","application program interfaces;language translation;natural language processing;public domain software;speech synthesis;text analysis","machine translation;Indian regional languages;open innovation;foreign languages;natural language processing;communication domains;open source applications;text-to-speech system;speech-to-text system;STT;Indian languages;English language video;audio file;audio chunks;IBM Watson;text chunks;Indian language;audio output;open source TTS systems;flite engine;translated text;general spoken language;native language;language barriers;speech-to-text module;English videos;machine translate API","","1","","21","IEEE","23 Dec 2019","","","IEEE","IEEE Conferences"
"An analysis of machine translation and speech synthesis in speech-to-speech translation system","K. Hashimoto; J. Yamagishi; W. Byrne; S. King; K. Tokuda","Department of Computer Science and Engineering, Nagoya Institute of Technology, Japan; Centre of Speech Technology Research, University of Edinburgh, UK; Engineering Department, University of Cambridge, UK; Centre of Speech Technology Research, University of Edinburgh, UK; Department of Computer Science and Engineering, Nagoya Institute of Technology, Japan","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5108","5111","This paper provides an analysis of the impacts of machine translation and speech synthesis on speech-to-speech translation systems. The speech-to-speech translation system consists of three components: speech recognition, machine translation and speech synthesis. Many techniques for integration of speech recognition and machine translation have been proposed. However, speech synthesis has not yet been considered. Therefore, in this paper, we focus on machine translation and speech synthesis, and report a subjective evaluation to analyze the impact of each component. The results of these analyses show that the naturalness and intelligibility of synthesized speech are strongly affected by the fluency of the translated sentences.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947506","speech synthesis;machine translation;speechto-speech translation;subjective evaluation","Speech;Speech synthesis;Correlation;Speech recognition;Natural languages;Hidden Markov models;Training data","speech recognition;speech synthesis","machine translation;speech synthesis;speech-to-speech translation system;speech recognition","","6","","21","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Tackling Data Scarcity in Speech Translation Using Zero-Shot Multilingual Machine Translation Techniques","T. A. Dinh; D. Liu; J. Niehues","Department of Data Science and Knowledge Engineering, Maastricht University, The Netherlands; Department of Data Science and Knowledge Engineering, Maastricht University, The Netherlands; Department of Data Science and Knowledge Engineering, Maastricht University, The Netherlands","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6222","6226","Recently, end-to-end speech translation (ST) has gained significant attention as it avoids error propagation. However, the approach suffers from data scarcity. It heavily depends on direct ST data and is less efficient in making use of speech transcription and text translation data, which is often more easily available. In the related field of multilingual text translation, several techniques have been proposed for zero-shot translation. A main idea is to increase the similarity of semantically similar sentences in different languages. We investigate whether these ideas can be applied to speech translation, by building ST models trained on speech transcription and text translation data. We investigate the effects of data augmentation and auxiliary loss function. The techniques were successfully applied to few-shot ST using limited ST data, with improvements of up to +12.9 BLEU points compared to direct end-to-end ST and +3.1 BLEU points compared to ST models fine-tuned from ASR model.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746815","speech translation;zero-shot;few-shot;machine translation;multi-task","Conferences;Buildings;Signal processing;Multitasking;Data models;Acoustics;Machine translation","language translation;natural language processing;speech enhancement","tackling data scarcity;zero-shot multilingual machine translation techniques;end-to-end speech translation;direct ST data;speech transcription;text translation data;multilingual text translation;zero-shot translation;ST models;data augmentation;auxiliary loss function;few-shot ST;end-to-end ST;ST;semantically similar sentences;different languages;speech translation;limited ST data;ASR;ASR model","","","","17","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"The ATR Multilingual Speech-to-Speech Translation System","S. Nakamura; K. Markov; H. Nakaiwa; G. Kikui; H. Kawai; T. Jitsuhiro; J. . -S. Zhang; H. Yamamoto; E. Sumita; S. Yamamoto","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","IEEE Transactions on Audio, Speech, and Language Processing","21 Feb 2006","2006","14","2","365","376","In this paper, we describe the ATR multilingual speech-to-speech translation (S2ST) system, which is mainly focused on translation between English and Asian languages (Japanese and Chinese). There are three main modules of our S2ST system: large-vocabulary continuous speech recognition, machine text-to-text (T2T) translation, and text-to-speech synthesis. All of them are multilingual and are designed using state-of-the-art technologies developed at ATR. A corpus-based statistical machine learning framework forms the basis of our system design. We use a parallel multilingual database consisting of over 600‚Äâ000 sentences that cover a broad range of travel-related conversations. Recent evaluation of the overall system showed that speech-to-speech translation quality is high, being at the level of a person having a Test of English for International Communication (TOEIC) score of 750 out of the perfect score of 990.","1558-7924","","10.1109/TSA.2005.860774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597243","Example-based machine translation (EBMT);minimum description length (MDL);multiclass language model;speech-to-speech translation (S2S);statistical machine translation (SMT);successive state splitting (SSS);text-to-speech (TTS) conversion","Natural languages;Speech synthesis;Speech recognition;Machine learning;Databases;System testing;Surface-mount technology;Humans;History;Vocabulary","","Example-based machine translation (EBMT);minimum description length (MDL);multiclass language model;speech-to-speech translation (S2S);statistical machine translation (SMT);successive state splitting (SSS);text-to-speech (TTS) conversion","","60","5","50","IEEE","21 Feb 2006","","","IEEE","IEEE Journals"
"A Neural Approach to Source Dependence Based Context Model for Statistical Machine Translation","K. Chen; T. Zhao; M. Yang; L. Liu; A. Tamura; R. Wang; M. Utiyama; E. Sumita","Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Graduate School of Science and Engineering, Ehime University, Matsuyama, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","11 Dec 2017","2018","26","2","266","280","In statistical machine translation, translation prediction considers not only the aligned source word itself but also its source contextual information. Learning context representation is a promising method for improving translation results, particularly through neural networks. Most of the existing methods process context words sequentially and neglect source long-distance dependencies. In this paper, we propose a novel neural approach to source dependence-based context representation for translation prediction. The proposed model is capable of not only encoding source long-distance dependencies but also capturing functional similarities to better predict translations (i.e., word form translations and ambiguous word translations). To verify our method, the proposed mode is incorporated into phrase-based and hierarchical phrase-based translation models, respectively. Experiments on large-scale Chinese-to-English and English-to-German translation tasks show that the proposed approach achieves significant improvement over the baseline systems and outperforms several existing context-enhanced methods.","2329-9304","","10.1109/TASLP.2017.2772846","National Key Research and Development Program of China(grant numbers:2015AA015405); National Natural Science Foundation of China (NSFC)(grant numbers:91520204); Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology‚Äù of MIC, Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8105847","Source dependence;context representation;neural network;translation prediction;statistical machine translation","Artificial neural networks;Context modeling;Decoding;Speech;Encoding;Semantics;Speech processing","language translation","Chinese-to-English translation;statistical machine translation;source dependence based context model;English-to-German translation tasks;hierarchical phrase-based translation models;word form translations;encoding source long-distance dependencies;translation prediction;novel neural approach;neural networks;translation results;context representation;source contextual information;aligned source word","","45","","58","IEEE","13 Nov 2017","","","IEEE","IEEE Journals"
"Analysis of Layer-Wise Training in Direct Speech to Speech Translation Using BI-LSTM","L. Arya; A. Agarwal; J. Mishra; S. R. Mahadeva Prasanna","Department of Electrical Engineering, Indian Institute of Technology Dharwad, Dharwad, India; Department of Electrical Engineering, Indian Institute of Technology Dharwad, Dharwad, India; Department of Electrical Engineering, Indian Institute of Technology Dharwad, Dharwad, India; Department of Electrical Engineering, Indian Institute of Technology Dharwad, Dharwad, India","2022 25th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","28 Dec 2022","2022","","","1","6","Speech-to-speech translation (S2ST) is the process of translation of speech from one language to another. Traditional S2ST systems follow a cascaded approach, where three modules automatics speech recognition (ASR), machine translation (MT), and text-to-speech translation (TTS) are concatenated to obtain the final translated speech utterance. The cascaded nature of the system results in the propagation of errors from one module to another. This, in turn, leads to the degradation in the overall performance of the S2ST task. With the evolution of the deep learning approaches to speech processing, many attempts have been made to perform end-to-end and direct speech-to-speech translation (DS2ST). But most of these approaches rely on language transcripts in one way or the other. In this work, we aim to perform the DS2ST task without using language transcripts. In this direction we have performed three experiments: First, we have investigated the direct learning of mapping function from source to target language with the increase in the number of utterances. Second, we have analyzed how the learning function improves with an increase in the number of Bi-LSTM layers. Third, we have observed how the system behaves with the unknown speakers (not used during training) during inference. From the experiments, it has been observed that with the increase in the number of utterances and layers, the quality of translation improves. And also, with a speaker and text-dependent training of approximately 4.4 hrs of speech, the model can generate the target language utterance even for unknown speakers. Though the generated utterance quality is not that good, but intelligent to some extent to be perceived.","2472-7695","979-8-3503-9856-4","10.1109/O-COCOSDA202257103.2022.9997945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997945","Speech to speech translation (S2ST);Voice Conversion (VC);Bidirectional long short term memory (Bi-LSTM)","Training;Degradation;Deep learning;Databases;Machine translation;Task analysis;Speech processing","deep learning (artificial intelligence);language translation;natural language processing;recurrent neural nets;speaker recognition;speech processing","automatic speech recognition;Bi-LSTM layers;deep learning;DS2ST task;final translated speech utterance;language transcripts;machine translation;machine translation;S2ST task;speech processing;speech-to-speech translation;text-to-speech translation;unknown speakers","","","","29","IEEE","28 Dec 2022","","","IEEE","IEEE Conferences"
"Speech Translation Enhanced ASR for European Parliament Speeches - On the Influence of ASR Performance on Speech Translation","S. Stuker; M. Paulik; M. Kolss; C. Fugen; A. Waibel","Institut f√ºr Theoretische Informatik, Universi√§t Karlsruhe, Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, USA; Institut f√ºr Theoretische Informatik, Universi√§t Karlsruhe, Karlsruhe, Germany; Institut f√ºr Theoretische Informatik, Universi√§t Karlsruhe, Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1293","IV-1296","In this paper we describe our work in coupling automatic speech recognition (ASR) and machine translation (MT) in a speech translation enhanced automatic speech recognition (STE-ASR) framework for transcribing and translating European parliament speeches. We demonstrate the influence of the quality of the ASR component on the MT performance, by comparing a series of WERs with the corresponding automatic translation scores. By porting an STE-ASR framework to the task at hand, we show how the word errors for transcribing English and Spanish speeches can be lowered by 3.0% and 4.8% relative, respectively.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218345","Speech Recognition;Machine Translation;European Parliamentary Plenary Sessions;TC-STAR;STE-ASR","Speech enhancement;Automatic speech recognition;Natural languages;Speech recognition;Speech synthesis;Interactive systems;Laboratories;Broadcasting;TV;Europe","language translation;speech recognition","speech translation enhanced ASR;European parliament speeches;automatic speech recognition;machine translation;automatic translation scores;word errors;Spanish speeches","","3","","15","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Modeling Future Cost for Neural Machine Translation","C. Duan; K. Chen; R. Wang; M. Utiyama; E. Sumita; C. Zhu; T. Zhao","Machine Intelligence and Translation Laboratory in School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Machine Intelligence and Translation Laboratory in School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory in School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","2 Feb 2021","2021","29","","770","781","Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, in this article, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a future cost representation is learned based on the current generated target word and its contextual information to compute an additional loss to guide the training of the NMT model. Furthermore, the learned future cost representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline.","2329-9304","","10.1109/TASLP.2020.3042006","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFB1002102,2018YFC0830700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330773","Future cost;future cost representation;neural machine translation;transformer","Computational modeling;Speech processing;Machine translation;Context modeling;Adaptation models;Neural networks;Maximum likelihood decoding","language translation;learning (artificial intelligence);natural language processing;neural nets","neural machine translation;sequence-to-sequence neural networks;target translation word;generated word;trained translation model;subsequent target translation;NMT systems;current generated target word;NMT model;learned future cost representation;translation datasets","","9","","45","IEEE","21 Jan 2021","","","IEEE","IEEE Journals"
"End-To-End Algorithm Optimization and Implementation Based on Speech Translation","Z. Yang","Information Engineering University, Zhengzhou, China","2021 5th Annual International Conference on Data Science and Business Analytics (ICDSBA)","3 Feb 2022","2021","","","504","507","Speech translation is the process of automatically translating the commands from the speech signals of the source language into the text of the specified language by using machine completion instructions. The main operation method now is to associate speech recognition with machine translation. Because speech recognition, machine translation and speech translation are essentially the transformation from one sequence to another, and the end-to-end architecture can also be applied to speech recognition and machine translation, the researchers studied end-to-end speech translation. Because the integration of speech recognition and machine translation into one model modeling makes the input sequence and output sequence more difficult and cumbersome, it is necessary to carry out more difficult training on the model and analyze more training data. On this basis, this paper studies end-to-end speech translation and related training methods respectively.","","978-1-6654-4590-0","10.1109/ICDSBA53075.2021.00103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693882","Speech translation;machine translation;source language;input sequence;model","Training;Analytical models;Training data;Speech recognition;Data models;Machine translation;Noise measurement","language translation;speech processing;speech recognition","speech signals;machine completion instructions;speech recognition;machine translation;end-to-end architecture;end-to-end speech translation;input sequence;output sequence","","","","11","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"NICT/ATR Chinese-Japanese-English speech-to-speech translation system","T. Shimizu; Y. Ashikari; E. Sumita; J. Zhang; S. Nakamura","Knowledge Creating Communication Research Center, National Institute of Information and Communications Technology; ATR Spoken Language Translation Research Laboratories, 2-2-2 Keihanna Science City, Kyoto 619-0288, Japan; Knowledge Creating Communication Research Center, National Institute of Information and Communications Technology; ATR Spoken Language Translation Research Laboratories, 2-2-2 Keihanna Science City, Kyoto 619-0288, Japan; Knowledge Creating Communication Research Center, National Institute of Information and Communications Technology; ATR Spoken Language Translation Research Laboratories, 2-2-2 Keihanna Science City, Kyoto 619-0288, Japan; Knowledge Creating Communication Research Center, National Institute of Information and Communications Technology; ATR Spoken Language Translation Research Laboratories, 2-2-2 Keihanna Science City, Kyoto 619-0288, Japan; Knowledge Creating Communication Research Center, National Institute of Information and Communications Technology; ATR Spoken Language Translation Research Laboratories, 2-2-2 Keihanna Science City, Kyoto 619-0288, Japan","Tsinghua Science and Technology","17 Jan 2012","2008","13","4","540","544","This paper describes the latest version of the Chinese-Japanese-English handheld speech-tospeech translation system developed by NICT/ATR, which is now ready to be deployed for travelers. With the entire speech-to-speech translation function being implemented into one terminal, it realizes real-time, location-free speech-to-speech translation. A new noise-suppression technique notably improves the speech recognition performance. Corpus-based approaches of speech recognition, machine translation, and speech synthesis enable coverage of a wide variety of topics and portability to other languages. Test results show that the character accuracy of speech recognition is 820/0‚Äì940/0 for Chinese speech, with a bilingual evaluation understudy score of machine translation is 0.55‚Äì0.74 for Chinese-Japanese and Chinese-English.","1007-0214","","10.1016/S1007-0214(08)70086-5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6074162","speech-to-speech translation;speech recognition;speech synthesis;machine translation;large-scale corpus","Speech recognition;Speech;Hidden Markov models;Speech synthesis;Accuracy;Adaptation models;Real time systems","","","","10","","","","17 Jan 2012","","","TUP","TUP Journals"
"Re-Translation Strategies for Long Form, Simultaneous, Spoken Language Translation","N. Arivazhagan; C. Cherry; I. Te; W. Macherey; P. Baljekar; G. Foster",Google Research; Google Research; Google Research; Google Research; Google Research; Google Research,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7919","7923","We investigate the problem of simultaneous machine translation of long-form speech content. We target a continuous speech-to-text scenario, generating translated captions for a live audio feed, such as a lecture or play-by-play commentary. As this scenario allows for revisions to our incremental translations, we adopt a re-translation approach to simultaneous translation, where the source is repeatedly translated from scratch as it grows. This approach naturally exhibits very low latency and high final quality, but at the cost of incremental instability as the output is continuously refined. We experiment with a pipeline of industry-grade speech recognition and translation tools, augmented with simple inference heuristics to improve stability. We use TED Talks as a source of multilingual test data, developing our techniques on English-to-German spoken language translation. Our minimalist approach to simultaneous translation allows us to scale our final evaluation to several other target languages, dramatically improving incremental stability for all of them.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054585","Speech Recognition Neural Machine Translation","Signal processing algorithms;Speech recognition;Tools;Signal processing;Stability analysis;Machine translation;Speech processing","language translation;linguistics;speech recognition","Re-Translation Strategies;simultaneous machine translation;long-form speech content;speech-to-text scenario;translated captions;live audio feed;play-by-play commentary;incremental translations;simultaneous translation;industry-grade speech recognition;translation tools;English-to-German spoken language translation;target languages;incremental stability","","6","","16","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Towards More Diverse Input Representation for Neural Machine Translation","K. Chen; R. Wang; M. Utiyama; E. Sumita; T. Zhao; M. Yang; H. Zhao","Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; School of Computer Science of Technology, Machine Intelligence and Translation Laboratory, Harbin Institute of Technology, Harbin, China; School of Computer Science of Technology, Machine Intelligence and Translation Laboratory, Harbin Institute of Technology, Harbin, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Jun 2020","2020","28","","1586","1597","Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks.","2329-9304","","10.1109/TASLP.2020.2996077","JSPS(grant numbers:19K20354); Unsupervised Neural Machine Translation in Universal Scenarios; National Institute of Information and Communications Technology; Toward Intelligent Machine Translation; JSPS KAKENHI(grant numbers:19H05660); National Basic Research Program of China (973 Program)(grant numbers:2018YFC0830700); National Natural Science Foundation of China(grant numbers:61806075); National Basic Research Program of China (973 Program)(grant numbers:2017YFB0304100); Key Projects of National Natural Science Foundation of China(grant numbers:U1836222,61733011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097389","Neural machine translation;source features;diverse input representation","Syntactics;Speech processing;Magnetic heads;Feeds;Task analysis;Indexes;Encoding","language translation;natural language processing","diverse input representation;neural machine translation;source input information;word embedding;self-attention networks;recurrence features;syntactic features;English-to-German machine translation tasks;positional embedding;transformer-based translation system;NIST Chinese-to-English machine translation tasks;input representation layer","","18","","60","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"Hindi-English speech-to-speech translation system for travel expressions","Mrinalini K; Vijayalakshmi P","Department of ECE, SSN College of Engineering, Chennai, India; Department of ECE, SSN College of Engineering, Chennai, India","2015 International Conference on Computation of Power, Energy, Information and Communication (ICCPEIC)","14 Sep 2015","2015","","","0250","0255","Speech-to-speech translation system enables in translation of speech signals in a source language A to target language B. A good speech-to-speech translation (S2ST) system can be characterized by its ability to keep intact the fluency and meaning of the original speech input. An S2ST system to enable translation between Hindi and English is the main idea of the proposed work. A preliminary dataset concentrating on basic travel expressions in both the languages considered is used for this work. In order to develop a successful S2ST system three subsystems are required namely, automatic speech recognition (ASR) system, machine translation (MT) system and text-to-speech synthesis (TTS) system. Hidden Markov models based ASR system is developed for both the languages and their performances are analyzed based on the word error rate (WER). The MT subsystem makes use of the statistical machine translation (SMT) approach for the purpose of translating the text between the two languages involved. The SMT makes use of IBM alignment models and language models to enable proper translation. The performance of MT is analyzed based on translated edit rate (TER) and analysis of the translation table. HMM-based speech synthesis system (HTS) is used to synthesize the translated text. Performance of the synthesizer is analyzed based on mean opinion score (MOS) from a group of listeners.","","978-1-4673-6525-3","10.1109/ICCPEIC.2015.7259472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7259472","Speech-to-Speech Translation System;HMM based speech recognition;Statistical Machine Translation;HMM based speech synthesis system","Hidden Markov models;Analytical models;Accuracy;Speech","error statistics;hidden Markov models;language translation;natural language processing;speech synthesis","Hindi-English speech-to-speech translation system;travel expression;speech signal translation;S2ST system;automatic speech recognition system;ASR system;machine translation system;MT system;text-to-speech synthesis system;TTS system;hidden Markov model;word error rate;WER;MT subsystem;statistical machine translation;SMT approach;IBM alignment model;language model;translated edit rate;TER;translation table;HMM-based speech synthesis system;HTS;mean opinion score;MOS","","7","","14","IEEE","14 Sep 2015","","","IEEE","IEEE Conferences"
"An Interactive System leveraging Automatic Speech Recognition and Machine Translation for learning Hindi as a Second Language","M. K. Rohil; S. Saini; R. K. Rohil","Department of Computer Science and Information Systems, Birla Institute of Technology and Science, Pilani, Pilani, India; Department of Computer Science and Information Systems, Birla Institute of Technology and Science, Pilani, Pilani, India; Department of Mathematics, Birla Institute of Technology and Science, Pilani, Pilani, India","2022 3rd International Conference for Emerging Technology (INCET)","15 Jul 2022","2022","","","1","4","When English speakers are in the early stages of learning Hindi, formulating sentences in Hindi is often attempted by a verbatim translation of English words to corresponding Hindi words. Due to this reason, they are unable to learn Hindi sentences correctly. We have tried to overcome this problem by use of technology for second language learners. The use of Automatic Speech Recognition, and Machine Translation for second language learning, here learning Hindi by English speaker, has been illustrated by taking English speech as input and translating the given English sentences and words into Hindi and then displaying its equivalent construct in Devanagari script. The interactive system under study displays and speaks the same. It has been observed that a second language can be learnt faster by frequently listening to the vocabulary and sentences of the language. Thus the system furnishes the functionality of speaking the sentence in Hindi once it is represented in Devanagari script. The English sentences and words from the grammar tool books are given as input to the system for experimentation. We have observed that the critical problem encountered while doing so is the translation of English to Hindi. Another problem encountered at times is insertion error for letters (only surfaced). The system cannot translate sentences represented using continuous tense and perfect continuous tense correctly. The overall accuracy of the system, otherwise, is approximately 67% which can help the second language learners in the beginning.","","978-1-6654-9499-1","10.1109/INCET54531.2022.9824940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9824940","Natural Language Processing;Automatic Speech Recognition;Language Learning;Machine Translation","Vocabulary;Interactive systems;Natural languages;Grammar;Machine translation;Speech processing;Automatic speech recognition","computer aided instruction;grammars;language translation;linguistics;natural language processing;speech intelligibility;speech recognition","machine translation;English speaker;formulating sentences;verbatim translation;English words;corresponding Hindi words;Hindi sentences;language learners;language learning;English speech;English sentences;interactive system leveraging automatic speech recognition;Devanagari script;grammar tool books;Hindi","","","","11","IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Dialogue processing in a conversational speech translation system","A. Lavie; L. Levin; Y. Qu; A. Waibel; D. Gates; M. Gavalada; L. Mayfield; M. Taboada","Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; NA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","1","","554","557 vol.1","Attempts at discourse processing of spontaneously spoken dialogue face several difficulties: multiple hypotheses that result from the parser's attempts to make sense of the output from the speech recognizer, ambiguity that results from segmentation of multi-sentence utterances, and cumulative error-errors in the discourse context which cause further errors when subsequent sentences are processed. In this paper we describe how the JANUS multi-lingual speech-to-speech translation system addresses problems that arise in discourse processing of spontaneous speech. We describe our robust parsers, our procedures for segmenting long utterances, and two approaches to discourse processing that attempt to deal with ambiguity and cumulative error.","","0-7803-3555-4","10.1109/ICSLP.1996.607177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607177","","Speech processing;Robustness;Natural languages;Face recognition;Speech recognition;Speech analysis;Tellurium","grammars;natural languages;language translation;speech recognition;speech recognition equipment","dialogue processing;conversational speech translation system;spontaneously spoken dialogue;multiple hypotheses;speech recognizer;multi-sentence utterances;cumulative error;discourse context;JANUS multi-lingual speech-to-speech translation system;discourse processing;robust parsers;long utterances;ambiguity","","1","1","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Multilingual speech to speech translation system in bluetooth environment","M. D. F. Ansari; R. S. Shaji; T. J. SivaKarthick; S. Vivek; A. Aravind","Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil","2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","22 Dec 2014","2014","","","1055","1058","Voice Translator is speech to speech translation application for android mobile phone, which translates English speech to Hindi speech and vice versa. Voice Translator includes three modules, Voice Recognition, Machine Translation and Speech Synthesis. Voice Recognition module captures the voice or speech from the mobile user through speaker, identifies then converts the speech into text and then the text send to Machine Translation for further process. Machine Translation module does the process of translation i.e. this module consists of library for both language and when text is received by this module, it converts the text of one language to another as per user choice and thus it sends the translated text to last module. Speech Synthesis module acts as the text to speech translator i.e. when it gets the translated text. This module processes on translated text which converts it into speech and then makes it as user output. Thus, Voice Translation application works by integrating all these three modules and gives the user best output.","","978-1-4799-4190-2","10.1109/ICCICCT.2014.6993116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993116","Voice Recognition;Machine Translation;Multilingual speech","Speech;Speech recognition;Bluetooth;Speech synthesis;Accuracy;Text recognition","Bluetooth;language translation;natural language processing;smart phones;speech recognition;speech synthesis","multilingual speech to speech translation system;Bluetooth environment;voice translator;Android mobile phone;English speech-Hindi speech translation;voice recognition;machine translation;speech synthesis;text-speech translator","","","","9","IEEE","22 Dec 2014","","","IEEE","IEEE Conferences"
"Efficient integration of translation and speech models in dictation based machine aided human translation","L. Rodr√≠guez; A. Reddy; R. Rose","Departamento de Sistemas Inform√°ticos, University of Castilla La Mancha, Spain; Department of Electrical and Computer Engineering, McGill University, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, QUE, Canada","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4949","4952","This paper is concerned with combining models for decoding an optimum translation for a dictation based machine aided human translation (MAHT) task. Statistical language model (SLM) probabilities in automatic speech recognition (ASR) are updated using statistical machine translation (SMT) model probabilities. The effect of this procedure is evaluated for utterances from human translators dictating translations of source language documents. It is shown that computational complexity is significantly reduced while at the same time word error rate is reduced by 30%.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289030","speech recognition;machine translation;speech input interfaces","Lattices;Decoding;Speech;Humans;Computational modeling;Probability;Vocabulary","language translation;speech processing;speech recognition","speech model;dictation based machine aided human translation;statistical language model probability;automatic speech recognition;statistical machine translation model probability;human translator;source language document;computational complexity;word error rate","","1","","13","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"The Evaluation Study of the Deep Learning Model Transformer in Speech Translation","J. -W. Hung; J. -R. Lin; L. -Y. Zhuang","National Chi Nan University, Nantou, Taiwan; National Chi Nan University, Nantou, Taiwan; National Chi Nan University, Nantou, Taiwan","2021 7th International Conference on Applied System Innovation (ICASI)","26 Oct 2021","2021","","","30","33","Neural machine translation (NMT) employs the prevailing deep learning techniques to build a single deep neural network (DNN) that directly maps the input speech utterances of one language to the corresponding texts of the other language. Compared with the conventional statistical machine translation, which separately optimizes each component model (such as acoustic models and language models) in series, NMT can learn the used DNN to directly maximize the overall translation performance. In particular, a novel encoder-decoder DNN structure termed transformer, which Google develops, has been applied in NMT and revealed outstanding translation performance. In this study, we investigate and evaluate the Transformer-based speech translation algorithm by varying the model settings in the training process of the used Transformer. The experiments follow a tutorial script provided in the Tensorflow forum, which are conducted on the TED talk dataset to translate Portuguese to English, which consists of 50,000 utterances for training, 1,100 utterances for validation, and 2,000 utterances for testing. The baseline system, which sets the encoding dimension as 128, the number of encoder/decoder layers as 4, the dropout rate as 0.1 and the negative exponent as ‚àí1.5, gives rise to 68.01% in translation accuracy. While the encoding dimension is increased to be 512, the translation accuracy can be promoted to be 76.02%. Also, changing the number of layers to be 2, the dropout rate to be 0.01 and the negative exponent to be 1 can achieve 70.98%, 80.97% and 75.40% in translation accuracy, respectively. The experimental results indicate that we can further improve the translation performance of the transformer by properly arranging the underlying hyper-parameters.","2768-4156","978-1-6654-4143-8","10.1109/ICASI52993.2021.9568450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568450","speech translation;transformer;translation accuracy;neural machine translation","Deep learning;Training;Technological innovation;Tutorials;Transformers;Encoding;Machine translation","data analysis;language translation;learning (artificial intelligence);natural language processing;neural nets;speech processing;speech recognition","component model;acoustic models;language models;NMT;novel encoder-decoder DNN structure;outstanding translation performance;Transformer-based speech translation algorithm;model settings;encoding dimension;translation accuracy;evaluation study;deep learning model Transformer;neural machine translation;prevailing deep learning techniques;single deep neural network;input speech utterances;conventional statistical machine translation","","3","","22","IEEE","26 Oct 2021","","","IEEE","IEEE Conferences"
"Integrating Prior Translation Knowledge Into Neural Machine Translation","K. Chen; R. Wang; M. Utiyama; E. Sumita","Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","12 Jan 2022","2022","30","","330","339","Neural machine translation (NMT), which is an encoder-decoder joint neural language model with an attention mechanism, has achieved impressive results on various machine translation tasks in the past several years. However, the language model attribute of NMT tends to produce fluent yet sometimes unfaithful translations, which hinders the improvement of translation capacity. In response to this problem, we propose a simple and efficient method to integrate prior translation knowledge into NMT in a universal manner that is compatible with neural networks. Meanwhile, it enables NMT to consider the crossing language translation knowledge from the source-side of the training pipeline of NMT, thereby making full use of the prior translation knowledge to enhance the performance of NMT. The experimental results on two large-scale benchmark translation tasks demonstrated that our approach achieved a significant improvement over a strong baseline.","2329-9304","","10.1109/TASLP.2021.3138714","Japan National Funding(grant numbers:21K21328); National Natural Science Foundation of China(grant numbers:6217020129); Shanghai Pujiang Program(grant numbers:21PJ1406800); CCF-Tencent Open Fund(grant numbers:RAGR20210119); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); ‚ÄúResearch and Development of Advanced Multilingual Translation Technology‚Äù in the ‚ÄúR&D Project for Information and Communications Technology‚Äù(grant numbers:JPMI00316); Ministry of Internal Affairs and Communications (MIC), Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664306","Bilingual lexicon knowledge;prior knowledge representation;self-attention networks;machine translation","Machine translation;Knowledge representation;Training;Transformers;Speech processing;Decoding;Task analysis","language translation;natural language processing;neural nets","neural machine translation;NMT;encoder-decoder joint neural language model;machine translation tasks;language model attribute;translation capacity;neural networks;language translation knowledge;integrate prior translation knowledge","","3","","40","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Translation of conversational speech with JANUS-II","A. Lavie; A. Waibel; L. Levin; D. Gates; M. Gavalda; T. Zeppenfeld; P. Zhan; O. Glickman","Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; NA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","4","","2375","2378 vol.4","We investigate the possibility of translating continuous spoken conversations in a cross talk environment. This is a task known to be difficult for human translators due to several factors. It is characterized by rapid and even overlapping turn taking, a high degree of coarticulation, and fragmentary language. We describe experiments using both push to talk as well as cross talk recording conditions. Our results indicate that conversational speech recognition and translation is possible, even in a free crosstalk environment. To date, our system has achieved performances of over 80%, acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 70-80% word accuracy. The system's performance on spontaneous conversations recorded in a cross talk environment is shown to be as good and even slightly superior to the simpler and easier push to talk scenario.","","0-7803-3555-4","10.1109/ICSLP.1996.607286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607286","","Speech recognition;Speech analysis;Robustness;Natural languages;Face recognition;Speech processing;Lattices;Humans;Crosstalk;System performance","speech recognition;language translation;speech processing","conversational speech translation;JANUS-II;continuous spoken conversations;cross talk environment;overlapping turn taking;coarticulation;fragmentary language;push to talk;cross talk recording conditions;conversational speech recognition;free crosstalk environment;transcribed input;speech input","","2","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Development and application of multilingual speech translation","S. Nakamura","Spoken Language Communication Research Group Project, National Institute of Information and Communications Technology, Japan","2009 Oriental COCOSDA International Conference on Speech Database and Assessments","2 Oct 2009","2009","","","9","12","This paper describes the latest version of handheld speech-to-speech translation system developed by National Institute of Information and Communications Technology, NICT. As the entire speech-to-speech translation functions are implemented into one terminal, it realizes real-time and location free speech-to-speech translation service for many language pairs. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enabled wide range coverage of topic varieties and portability to other languages. Currently, we mainly focus on translation between Japanese, English and Chinese.","","978-1-4244-4400-7","10.1109/ICSDA.2009.5278383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278383","speech-to-speech translation;speech recognition;speech synthesis;machine translation;large-scale corpus","Natural languages;Speech recognition;Surface-mount technology;Speech synthesis;Large-scale systems;Working environment noise;Communications technology;Training data;Loudspeakers;Adaptation model","language translation;speech recognition","multilingual speech translation;handheld speech-to-speech translation system;noise-suppression technique;speech recognition","","1","","13","IEEE","2 Oct 2009","","","IEEE","IEEE Conferences"
"On Efficient Coupling of ASR and SMT for Speech Translation","B. Zhou; L. Besacier; Y. Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-101","IV-104","This paper presents an efficient tightly integrated approach for improved speech translation performance. The proposed approach combines the automatic speech recognition (ASR) and statistical machine translation (SMT) components in a bi-directional fashion. First, our SMT decoder takes the speech recognition lattice to perform an integrated search for the optimal translation by combining various ASR scores and translation models. Our approach is implemented within the recently proposed Folsom SMT framework that employs a multilayer search algorithm to conduct efficient operations on multiple graphs, which not only achieves memory efficiency and fast speed that is critical for real time speech translation applications, but also provides significant accuracy improvements. Secondly, we also report our experiments where the ASR is customized by reinforcing the language model to favor downstream translation component. We evaluated our approach on a large vocabulary speech translation task, and we obtain more than 2 point BLEU improvement over standard cascaded 1-best speech translation.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218047","Integrated speech translation;coupling;speech recognition;machine translation;efficient search","Automatic speech recognition;Surface-mount technology;Lattices;Vocabulary;Speech analysis;Natural languages;Decoding;Speech recognition;Bidirectional control;Nonhomogeneous media","decoding;language translation;search problems;speech coding;speech recognition","integrated speech translation;SMT;ASR;automatic speech recognition;statistical machine translation;SMT decoder;multilayer search algorithm;BLEU","","12","","10","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Neural Machine Translation With Sentence-Level Topic Context","K. Chen; R. Wang; M. Utiyama; E. Sumita; T. Zhao","Machine Intelligence and Translation Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Machine Intelligence and Translation Laboratory, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","9 Sep 2019","2019","27","12","1970","1984","Traditional neural machine translation (NMT) methods use the word-level context to predict target language translation while neglecting the sentence-level context, which has been shown to be beneficial for translation prediction in statistical machine translation. This paper represents the sentence-level context as latent topic representations by using a convolution neural network, and designs a topic attention to integrate source sentence-level topic context information into both attention-based and Transformer-based NMT. In particular, our method can improve the performance of NMT by modeling source topics and translations jointly. Experiments on the large-scale LDC Chinese-to-English translation tasks and WMT'14 English-to-German translation tasks show that the proposed approach can achieve significant improvements compared with baseline systems.","2329-9304","","10.1109/TASLP.2019.2937190","‚ÄúPromotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology‚Äù of the Ministry of Internal Affairs and Communications (MIC), Japan; National Key Technologies R&D Program of China(grant numbers:2017YFB1002102); JSPS KAKENHI(grant numbers:19H05660); JSPS grant-in-aid for early-career scientists(grant numbers:19K20354); NICT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811589","Sentence-level Context;Latent Topic Representation;Convolutional Neural Network;Neural Machine Translation","Task analysis;Speech processing;Decoding;Computer architecture;Convolution;Feature extraction;Context modeling","convolutional neural nets;language translation;natural language processing","sentence-level context;translation prediction;statistical machine translation;latent topic representations;convolution neural network;topic attention;source sentence-level topic context information;LDC Chinese-to-English translation tasks;English-to-German translation tasks;word-level context;target language translation;neural machine translation methods;transformer-based NMT","","24","1","61","IEEE","23 Aug 2019","","","IEEE","IEEE Journals"
"Speech Recognition for Voice-Based Machine Translation","T. Duarte; R. Prikladnicki; F. Calefato; F. Lanubile",Vector Consulting Services; Pontif√≠cia Universidade Cat√≥lica do Rio Grande do Sul; University of Bari; University of Bari,"IEEE Software","28 Feb 2014","2014","31","1","26","31","Real-time voice-based machine translation is stimulated by many international teams who want to understand each other syntactically as well as semantically. Authors Tiago Duarte and Rafael Prikladnicki provide an overview on current technologies for real-time voice-base machine translation.","1937-4194","","10.1109/MS.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750466","software technology;speech recognition;machine translation;language;language translation;voice-based translation","Speech recognition;Speech processing;Hidden Markov models;Java;Language translation;Three-dimensional displays","language translation;real-time systems;speech recognition","real-time voice-based machine translation;speech recognition","","12","","5","IEEE","28 Feb 2014","","","IEEE","IEEE Magazines"
"DataShift: A Cross-Modal Data Augmentation Method for Speech Recognition and Machine Translation","H. Cheng; Y. Guo","School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China","2022 4th International Conference on Natural Language Processing (ICNLP)","19 Sep 2022","2022","","","341","344","Data augmentation has been successful in the tasks of different modalities such as speech and text. In this paper, we present a cross-modal data augmentation method, DataShift, to improve the performance of automatic speech recognition (ASR) and machine translation (MT) by randomly shifting values of the feature sequence along the time or frequency dimensions respectively. Experimental results show that our data augmentation method can improve the performance by 4% of word error rate (WER) and 0.36 BLEU score on average on the ASR and MT datasets separately.","","978-1-6654-9544-8","10.1109/ICNLP55136.2022.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885906","data augmentation;automatic speech recognition;machine translation","Time-frequency analysis;Error analysis;Transformers;Machine translation;Task analysis;Speech processing;Automatic speech recognition","language translation;natural language processing;speech recognition","DataShift;cross-modal data augmentation method;machine translation;automatic speech recognition","","","","21","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Sequential system combination for machine translation of speech","D. Karakos; S. Khudanpur","Center for Language and Speech Processing and Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing and Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","257","260","System combination is a technique which has been shown to yield significant gains in speech recognition and machine translation. Most combination schemes perform an alignment between different system outputs in order to produce lattices (or confusion networks), from which a composite hypothesis is chosen, possibly with the help of a large language model. The benefit of this approach is two-fold: (i) whenever many systems agree with each other on a set of words, the combination output contains these words with high confidence; and (ii) whenever the systems disagree, the language model resolves the ambiguity based on the (probably correct) agreed upon context. The case of machine translation system combination is more challenging because of the different word orders of the translations: the alignment has to incorporate computationally expensive movements of word blocks. In this paper, we show how one can combine translation outputs efficiently, extending the incremental alignment procedure of (A-V.I. Rosti et al., 2008). A comparison between different system combination design choices is performed on an Arabic speech translation task.","","978-1-4244-3471-8","10.1109/SLT.2008.4777889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777889","Machine Translation;System Combination;Confusion Networks;Alignments with reordering","Speech processing;Speech recognition;Automatic speech recognition;Natural languages;Lattices;Context modeling;Voting;Computer numerical control;Contracts;Sequences","language translation;natural language processing;speech recognition","sequential system combination;speech machine translation;speech recognition;large language model;machine translation system combination;incremental alignment procedure;Arabic speech translation","","2","","17","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Multilingual speech-to-speech translation system for mobile consumer devices","S. Yun; Y. -J. Lee; S. -H. Kim","Department of Computer Software, ETRI, Daejeon, Korea; ETRI, Automatic Speech Translation Section, Daejeon, Korea; ETRI, Automatic Speech Translation Section, Daejeon, Korea","IEEE Transactions on Consumer Electronics","3 Nov 2014","2014","60","3","508","516","Along with the advancement of speech recognition technology and machine translation technology in addition to the fast distribution of mobile devices, speech-to-speech translation technology no longer remains as a subject of research as it has become popularized throughout many users. In order to develop a speech-to-speech translation system that can be widely used by many users, however, the system needs to reflect various characteristics of utterances by the users who are actually to use the speech-to-speech translation system other than improving the basic functions under the experimental environment. This study has established a massive language and speech database closest to the environment where speech-to- speech translation device actually is being used after mobilizing plenty of people based on the survey on users' demands. Through this study, it was made possible to secure excellent basic performance under the environment similar to speech-to-speech translation environment, rather than just under the experimental environment. Moreover, with the speech-to-speech translation UI, a user-friendly UI has been designed; and at the same time, errors were reduced during the process of translation as many measures to enhance user satisfaction were employed. After implementing the actual services, the massive database collected through the service was additionally applied to the system following a filtering process in order to procure the best-possible robustness toward both the details and the environment of the users' utterances. By applying these measures, this study is to unveil the procedures where multi-language speech-to-speech translation system has been successfully developed for mobile devices.","1558-4127","","10.1109/TCE.2014.6937337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6937337","","Speech recognition;Databases;Speech;Engines;Performance evaluation;Mobile handsets;Hidden Markov models","audio databases;filtering theory;language translation;linguistics;mobile radio;natural language processing;speech processing;speech-based user interfaces","multilingual speech-to-speech translation system;mobile consumer devices;speech recognition technology;machine translation technology;speech-to-speech translation technology;user utterances;speech database;speech-to- speech translation device;users demands;speech-to-speech translation UI;user-friendly UI;user satisfaction;massive database;filtering process;multilanguage speech-to-speech translation system","","12","","18","IEEE","3 Nov 2014","","","IEEE","IEEE Journals"
"Why word error rate is not a good metric for speech recognizer training for the speech translation task?","X. He; L. Deng; A. Acero","Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5632","5635","Speech translation (ST) is an enabling technology for cross-lingual oral communication. A ST system consists of two major components: an automatic speech recognizer (ASR) and a machine translator (MT). Nowadays, most ASR systems are trained and tuned by minimizing word error rate (WER). However, WER counts word errors at the surface level. It does not consider the contextual and syntactic roles of a word, which are often critical for MT. In the end-to-end ST scenarios, whether WER is a good metric for the ASR component of the full ST system is an open issue and lacks systematic studies. In this paper, we report our recent investigation on this issue, focusing on the interactions of ASR and MT in a ST system. We show that BLEU-oriented global optimization of ASR system parameters improves the translation quality by an absolute 1.5% BLEU score, while sacrificing WER over the conventional, WER-optimized ASR system. We also conducted an in-depth study on the impact of ASR errors on the final ST output. Our findings suggest that the speech recognizer component of the full ST system should be optimized by translation metrics instead of the traditional WER.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947637","Speech translation;speech recognition;machine translation;translation metric;word error rate;BLEU score optimization;log-linear model","Speech recognition;Speech;Hidden Markov models;Measurement;Computational modeling;Training;Optimization","language translation;optimisation;speech recognition;training","word error rate;speech recognizer training;speech translation;crosslingual oral communication;ST system;machine translator;end-to-end ST scenario;ASR component;BLEU oriented global optimization;WER optimized ASR system","","23","","20","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Advances in syntax-based Malay-English speech translation","B. Xiang; B. Zhou; M. Cmejrek","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4801","4804","In this paper, we present advanced techniques that improved the performance of IBM Malay-English speech translation system significantly. During this work, we generated linguistics-driven hierarchical rules to enhance the formal syntax-based translation model; designed an active learning approach with bi-directional translations that outperformed unsupervised training; utilized translation direction information in parallel training corpus to build direction-specific interpolated language models for machine translation. There is 20% relative improvement achieved in the translation performance through all these techniques. A state-of-the-art Malay speech recognition system was also established as one of the crucial modules in the rapidly developed Malay-English speech translation.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960705","Machine Translation;Speech Recognition;Active Learning","Speech recognition;Natural languages;Machine learning;Bidirectional control;Automatic speech recognition;Data mining;Tagging;Training data;Semisupervised learning;Humans","language translation;speech recognition;unsupervised learning","syntax-based Malay-English speech translation;IBM Malay-English speech translation system;linguistics-driven hierarchical rules;formal syntax-based translation model;active learning;bi-directional translations;unsupervised training;Malay speech recognition system","","1","","14","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Development of text and speech corpus for an Indonesian speech-to-speech translation system","M. T. Uliniansyah; H. Riza; A. Santosa; Gunarso; M. Gunawan; E. Nurfadhilah","Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA; Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA; Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA; Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA; Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA; Agency for the Assessment and Application of Technology (BPPT), Center of Information and Communication Technology, Jakarta, INDONESIA","2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)","19 Aug 2019","2017","","","1","5","This paper describes our natural language resources especially text and speech corpora for developing an Indonesian speech-to-speech translation (S2ST) system. The corpora are used to create models for Automatic Speech Recognition (ASR), Statistical Machine Translation (SMT), and Text-to-Speech (TTS) systems. The corpora collected since 1987 from various sources and projects such as Multilingual Machine Translation System (MMTS), PAN Localization, ASEAN MT, U-STAR, etc. Text corpora are created by either collecting from online resources or translating manually from textual sources. Speech corpora are made from several recording projects. Availability of these corpora enables us to develop Indonesian speech-to- speech translation system.","2472-7695","978-1-5386-3333-5","10.1109/ICSDA.2017.8384448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8384448","Text Corpora;Speech Corpora;S2ST;ASR;SMT;TTS;Indonesian Language","Standardization;Databases;Hidden Markov models;Economics;Automatic speech recognition;Natural language processing","language translation;natural language processing;speech recognition;speech synthesis;text analysis","ASEAN MT;PAN localization;U-STAR;TTS;text corpora;multilingual machine translation system;text-to-speech systems;statistical machine translation;automatic speech recognition;Indonesian speech-to-speech translation system;speech corpus;speech corpora","","1","","9","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Speech-to-text translation by a non-word lexical unit based system","M. Penagarikano; G. Bordel","Dpto. Electr. y Electron., Univ. de Pais Vasco, Lejona, Spain; Departmento Electricidad y Electr√≤nica (UPVEHU), Universidad del Pais Vasco, Lejona, Vizcaya, Spain","ISSPA '99. Proceedings of the Fifth International Symposium on Signal Processing and its Applications (IEEE Cat. No.99EX359)","6 Aug 2002","1999","1","","111","114 vol.1","Speech understanding applications where a word based output of the uttered sentence is not needed, can benefit from the use of alternative lexical units. Experimental results from these systems show that the use of non-word lexical units bring us a new degree of freedom in order to improve the system performance (better recognition rate and lower size can be obtained in comparison to word based models). However, if the aim of the system is a speech-to-text translation, a post-processing stage must be included in order to convert the non-word sequences into word sentences. In this paper a technique to perform this conversion as well as an experimental test carried out over a task oriented Spanish corpus are reported. As a conclusion, we see that the whole speech-to-text system neatly outperforms the word-constrained baseline system.","","1-86435-451-8","10.1109/ISSPA.1999.818125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818125","","Natural languages;Signal processing algorithms;Australia;Application specific integrated circuits;System performance;Performance evaluation;Testing;Speech recognition;Automatic speech recognition","speech synthesis;language translation;natural languages","speech-to-text translation;nonword lexical unit based system;speech understanding applications;word based output;uttered sentence;lexical units;experimental results;system performance;recognition rate;word based models;post-processing stage;nonword sequences;word sentences;experimental test;task oriented Spanish corpus;speech-to-text system;word-constrained baseline system","","","3","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Approach toward speech-to-speech translation system by using a collection of sentences and utterances","E. Sumita; H. Nakaiwa; G. Kikui; S. Yamamoto","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","652","657","Corpus-based technology is very promising for speech-to-speech translation. However, the problem is that it is prohibitively expensive to build the vital resource, a large-scale corpus of bilingual dialogues covering many domains. We propose to substitute a combination of two different types of bilingual corpora: (1) a large-scale collection of basic sentences that covers many domains; and (2) a small-scale collection of spoken dialogues that reflects the characteristics of the spoken utterances for the large-scale corpus of dialogues. With these two corpora, we have been building a translation module for a speech-to-speech translation system. By using the basic sentence corpus, we have achieved high-quality translations with several machine-learning approaches. Based on an analysis of the spoken dialogue corpus, we found that splitting utterances into parts and concatenating the translated parts is an effective way to translate the longer utterances that are inherent in a spoken dialogue.","","0-7803-7980-2","10.1109/ASRU.2003.1318517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318517","","Natural languages;Large-scale systems;Speech;Machine learning;Laboratories;Cities and towns;Humans;System testing;Vocabulary;Oral communication","language translation;speech recognition;speech synthesis;learning (artificial intelligence)","speech-to-speech translation system;sentence collection method;utterance collection method;corpus-based technology;bilingual dialogue corpus;spoken dialogue utterance splitting;machine learning methods","","","","23","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Unsupervised Neural Machine Translation With Cross-Lingual Language Representation Agreement","H. Sun; R. Wang; K. Chen; M. Utiyama; E. Sumita; T. Zhao","School of Computer Science of Technology, Machine Intelligence and Translation Laboratory, Harbin Institute of Technology, Harbin, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; School of Computer Science of Technology, Machine Intelligence and Translation Laboratory, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","27 Apr 2020","2020","28","","1170","1182","Unsupervised cross-lingual language representation initialization methods such as unsupervised bilingual word embedding (UBWE) pre-training and cross-lingual masked language model (CMLM) pre-training, together with mechanisms such as denoising and back-translation, have advanced unsupervised neural machine translation (UNMT), which has achieved impressive results on several language pairs, particularly French-English and German-English. Typically, UBWE focuses on initializing the word embedding layer in the encoder and decoder of UNMT, whereas the CMLM focuses on initializing the entire encoder and decoder of UNMT. However, UBWE/CMLM training and UNMT training are independent, which makes it difficult to assess how the quality of UBWE/CMLM affects the performance of UNMT during UNMT training. In this paper, we first empirically explore relationships between UNMT and UBWE/CMLM. The empirical results demonstrate that the performance of UBWE and CMLM has a significant influence on the performance of UNMT. Motivated by this, we propose a novel UNMT structure with cross-lingual language representation agreement to capture the interaction between UBWE/CMLM and UNMT during UNMT training. Experimental results on several language pairs demonstrate that the proposed UNMT models improve significantly over the corresponding state-of-the-art UNMT baselines.","2329-9304","","10.1109/TASLP.2020.2982282","National Institute of Information and Communications Technology; JSPS(grant numbers:19K20354); Unsupervised Neural Machine Translation in Universal Scenarios; National Institute of Information and Communications Technology; Toward Intelligent Machine Translation; National Basic Research Program of China (973 Program)(grant numbers:2017YFB1002102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043536","Unsupervised neural machine translation;un-supervised bilingual word embedding;cross-lingual language model","Training;Decoding;Noise reduction;Sun;Market research;Task analysis","language translation;natural language processing;text analysis;unsupervised learning;word processing","unsupervised neural machine translation;cross-lingual language representation agreement;unsupervised cross-lingual language representation initialization methods;unsupervised bilingual word;cross-lingual masked language model pre-training;denoising translation;back-translation;word embedding layer;UNMT training;UNMT structure;UNMT models;French-English language pairs;German-English language pairs","","13","","51","IEEE","20 Mar 2020","","","IEEE","IEEE Journals"
"Unsupervised training for farsi-english speech-to-speech translation","Bing Xiang; Yonggang Deng; Yuqing Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","4977","4980","Speech-to-speech translation has evolved into an attractive area in recent years with significant progress made by various research groups. However, the translation engines usually suffer from the lack of bilingual training data, especially for low-resource languages. In this paper we present an unsupervised training technique to alleviate this problem by taking advantage of available source language data. Different approaches are proposed and compared through extensive experiments conducted on a speech-to-speech translation task between Farsi and English. The translation performance is significantly improved in both directions with the enhanced translation model. A state-of-the-art Farsi automatic speech recognition system is also established in this work.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518775","Speech Recognition;Machine Translation;Unsupervised Training","Natural languages;Speech recognition;Automatic speech recognition;Hidden Markov models;Probability;Training data;Iterative decoding;Testing;Frequency estimation;Engines","language translation;natural language processing;speech processing;speech recognition;unsupervised learning","unsupervised training;Farsi-English speech-to-speech translation;source language data;automatic speech recognition;bilingual training data;statistical machine translation","","3","","14","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"Speech Recognition System Combination for Machine Translation","M. J. F. Gales; X. Liu; R. Sinha; P. C. Woodland; K. Yu; S. Matsoukas; T. Ng; K. Nguyen; L. Nguyen; J. -L. Gauvain; L. Lamel; A. Messaoudi","University of Cambridge, Cambridge, UK; University of Cambridge, Cambridge, UK; University of Cambridge, Cambridge, UK; University of Cambridge, Cambridge, UK; University of Cambridge, Cambridge, UK; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France; LIMSI-CNRS, Orsay, France","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1277","IV-1280","The majority of state-of-the-art speech recognition systems make use of system combination. The combination approaches adopted have traditionally been tuned to minimising Word Error Rates (WERs). In recent years there has been growing interest in taking the output from speech recognition systems in one language and translating it into another. This paper investigates the use of cross-site combination approaches in terms of both WER and impact on translation performance. In addition the stages involved in modifying the output from a Speech-to-Text (STT) system to be suitable for translation are described. Two source languages, Mandarin and Arabic, are recognised and then translated using a phrase-based statistical machine translation system into English. Performance of individual systems and cross-site combination using cross-adaptation and ROVER are given. Results show that the best STT combination scheme in terms of WER is not necessarily the most appropriate when translating speech.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218341","Machine Translation;Speech Recognition","Speech recognition;Computer numerical control;Error analysis;Surface-mount technology;Voting;Natural languages;Appropriate technology;Costs;Diversity reception","error statistics;language translation;speech recognition","state-of-the-art speech recognition system;machine translation;system combination;word error rates;cross-site combination;speech-to-text system;Mandarin language;Arabic language;phrase-based statistical machine translation system;English language;ROVER","","5","","13","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Sentence Boundary Augmentation for Neural Machine Translation Robustness","D. Li; T. I; N. Arivazhagan; C. Cherry; D. Padfield",Google Research; Google Research; Google Research; Google Research; Google Research,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7553","7557","Neural Machine Translation (NMT) models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are pro-vided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition (ASR), the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness. Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413492","Neural Machine Translation;Automatic Speech Recognition;Robustness;Sentence Boundaries;Segmentation","Training;Error analysis;Syntactics;Signal processing;Robustness;Data models;Machine translation","error analysis;language translation;speech recognition","sentence boundary augmentation;neural machine translation robustness;well-formed training;evaluation data;long-form speech translation systems;input transcripts;automatic speech recognition;NMT models;NMT robustness;in-depth error analysis;sentence boundary segmentation;simple data augmentation;segmentation robustness;neural machine translation models","","","","17","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Moroccan Dialect ‚ÄúDarija‚Äù Automatic Speech Recognition: A Survey","M. Labied; A. Belangour","Laboratory of Information Technology and Modeling LTIM, Hassan II University Ben M‚Äôsik Faculty of Sciences, Casablanca, Morocco; Laboratory of Information Technology and Modeling LTIM, Hassan II University Ben M‚Äôsik Faculty of Sciences, Casablanca, Morocco","2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML)","26 Aug 2021","2021","","","208","213","Nowadays, human-machine interaction is growing swiftly, and Automatic Speech Recognition is gaining immense interest to make the daily routines much easier. This could be illustrated by the various applications of Speech Recognition in our daily lives, such as voice dictation, interactive voice response systems, device control, telephone applications, and others. Besides Automatic Speech Recognition, Natural language processing has gained significant improvements in terms of technologies and used approaches. Till today great results have been achieved in those Fields, especially for international languages such as English, Spanish, French, and Arabic. Whereas few results have been reached for dialects of languages such as the case of Moroccan dialect ‚ÄúDarija‚Äù. The growing use of Moroccan Darija on social media, videos, chatting and others, opens new research directions for Moroccan Darija speech recognition. The leading goal of this paper is to give a literature review on Moroccan Darija Automatic Speech Recognition. Through presenting the dialect specific constraints, the different works conducted in the field of Moroccan Darija speech recognition, and the progress made in recent years.","","978-1-6654-4383-8","10.1109/PRML52754.2021.9520690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520690","Automatic speech recognition;Dialect;Moroccan Darija;Feature extraction;Natural Language Processing;Machine Translation","Social networking (online);Buildings;Morphology;Machine learning;Telephone sets;Speech synthesis;Task analysis","feature extraction;interactive systems;language translation;natural language processing;social networking (online);speech recognition;text analysis","human machine interaction;interactive voice response systems;Natural language processing;Moroccan Darija speech recognition;Automatic Speech Recognition;voice dictation;Moroccan dialect Darija","","","","41","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Recent improvements in BBN's English/Iraqi speech-to-speech translation system","F. Choi; S. Tsakalidis; S. Saleem; C. -l. Kao; R. Meermeier; K. Krstovski; C. Moran; K. Subramanian; D. Stallard; R. Prasad; P. Natarajan","BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA; BBN Technologies, Cambridge, MA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","245","248","We report on recent improvements in our English/Iraqi Arabic speech-to-speech translation system. User interface improvements include a novel parallel approach to user confirmation which makes confirmation cost-free in terms of dialog duration. Automatic speech recognition improvements include the incorporation of state-of-the-art techniques in feature transformation and discriminative training. Machine translation improvements include a novel combination of multiple alignments derived from various pre-processing techniques, such as Arabic segmentation and English word compounding, higher order N-grams for target language model, and use of context in form of semantic classes and part-of-speech tags.","","978-1-4244-3471-8","10.1109/SLT.2008.4777886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777886","Speech-to-Speech translation;Arabic speech recognition;dialog systems","Automatic speech recognition;Natural languages;Speech synthesis;Control systems;User interfaces;Surface-mount technology;Centralized control;Context modeling;Speech recognition;Loudspeakers","computational linguistics;language translation;natural language processing;speech processing;speech recognition;speech synthesis;speech-based user interfaces","English-Iraqi Arabic speech-to-speech translation;user interface;automatic speech recognition;machine translation;Arabic segmentation;English word compounding;higher order N-gram;semantic class;part-of-speech tag;dialog duration","","4","","15","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Rapid integration of Parts of Speech information to improve reordering model for English-Farsi Speech to Speech Translation","S. Maskey; B. Zhou","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5222","5225","Integrating Parts of Speech (POS) information to Machine Translation (MT) model usually amounts to significant changes in the MT decoder. We present a method to rapidly integrate POS information without adding complexity to the decoder. We show how we can re-estimate the lexicalized reordering probability estimates with POS tags during the training time without having to use POS tagger at the decoding phase. We present our empirical results for two different MT decoding algorithms that use lexicalized reordering models.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495001","Machine Translation;Reordering Model;Speech Translation","Engines;Speech coding;Natural languages;Iterative decoding;Probability;Powders;Phase estimation;Statistical analysis;Robustness;Memory management","decoding;language translation;natural language processing;probability;speech coding","parts of speech information;reordering model;English-Farsi speech to speech translation;machine translation decoder;lexicalized reordering probability","","","","11","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Automatic pronunciation prediction for text-to-speech synthesis of dialectal arabic in a speech-to-speech translation system","S. Ananthakrishnan; S. Tsakalidis; R. Prasad; P. Natarajan; A. Namandi Vembu","Raytheon BBN Technologies, Speech, Language and Multimedia Unit, Cambridge, MA, USA; Raytheon BBN Technologies, Speech, Language and Multimedia Unit, Cambridge, MA, USA; Raytheon BBN Technologies, Speech, Language and Multimedia Unit, Cambridge, MA, USA; Raytheon BBN Technologies, Speech, Language and Multimedia Unit, Cambridge, MA, USA; Ming-Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4957","4960","Text-to-speech synthesis (TTS) is the final stage in the speech-tospeech (S2S) translation pipeline, producing an audible rendition of translated text in the target language. TTS systems typically rely on a lexicon to look up pronunciations for each word in the input text. This is problematic when the target language is dialectal Arabic, because the statistical machine translation (SMT) system usually produces undiacritized text output. Many words in the latter possess multiple pronunciations; the correct choice must be inferred from context. In this paper, we present a weakly supervised pronunciation prediction approach for undiacritized dialectal Arabic in S2S systems that leverages automatic speech recognition (ASR) to obtain parallel training data for pronunciation prediction. Additionally, we show that incorporating source language features derived from SMT-generated automatic word alignment further improves automatic pronunciation prediction accuracy.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289032","speech translation;dialectal arabic;pronunciation;speech synthesis","Predictive models;Hidden Markov models;Training;Speech;Mathematical model;Accuracy;Error analysis","language translation;prediction theory;speech recognition;speech synthesis","automatic pronunciation prediction approach;text-to-speech synthesis;dialectal Arabic;speech-to-speech translation system;TTS;S2S translation system;lexicon-look up pronunciation;statistical machine translation system;SMT system;automatic speech recognition;ASR;source language feature;SMT-generated automatic word alignment","","","","7","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Integrating Speech Recognition and Machine Translation","S. Matsoukas; I. Bulyko; B. Xiang; K. Nguyen; R. Schwartz; J. Makhoul","BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; College of Computer & Information Science, Northeastern University, Boston, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1281","IV-1284","This paper presents a set of experiments that we conducted in order to optimize the performance of an Arabic/English machine translation system on broadcast news and conversational speech data. Proper integration of speech-to-text (STT) and machine translation (MT) requires special attention to issues such as sentence boundary detection, punctuation, STT accuracy, tokenization, conversion of spoken numbers and dates to written form, optimization of MT decoding weights, and scoring. We discuss these issues, and show that a carefully tuned STT/MT integration can lead to significant translation accuracy improvements compared to simply feeding the regular STT output to a text MT system.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218342","Speech Recognition;Machine Translation;Sentence Boundary Detection","Speech recognition;Decoding;Pipelines;Broadcast technology;Broadcasting;Loudspeakers;Lattices;Educational institutions;Information science;Contracts","language translation;speech recognition","speech recognition;Arabic-English machine translation system;broadcast news;conversational speech data;speech-to-text;sentence boundary detection;MT decoding weights","","7","","5","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Machine Translation Verbosity Control for Automatic Dubbing","S. M. Lakew; M. Federico; Y. Wang; C. Hoang; Y. Virkar; R. Barra-Chicote; R. Enyedi",Amazon AI; Amazon AI; Amazon AI; Amazon AI; Amazon AI; Amazon AI; Amazon AI,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7538","7542","Automatic dubbing aims at seamlessly replacing the speech in a video document with synthetic speech in a different language. The task implies many challenges, one of which is generating translations that not only convey the original content, but also match the duration of the corresponding utterances. In this paper, we focus on the problem of controlling the verbosity of machine translation out-put, so that subsequent steps of our automatic dubbing pipeline can generate dubs of better quality. We propose new methods to control the verbosity of MT output and compare them against the state of the art with both intrinsic and extrinsic evaluations. For our experiments we use a public data set to dub English speeches into French, Italian, German and Spanish. Finally, we report extensive subjective tests that measure the impact of MT verbosity control on the final quality of dubbed video clips.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414411","Machine Translation;Automatic Dubbing","Conferences;Pipelines;Signal processing;Control systems;Acoustics;Machine translation;Speech synthesis","language translation;natural language processing;speech processing","machine translation verbosity control;video document;synthetic speech;machine translation output;automatic dubbing pipeline;MT output;English speeches;MT verbosity control;dubbed video clips","","3","","33","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Consolidation based speech translation","Chiori Hori; Bing Zhao; S. Vogel; A. Waibel","NiCT-ATR, Kyoto, Japan; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","380","385","To alleviate the degradation of the performance of speech translation, this paper proposes a new approach to translate ASR results through consolidation which extracts meaningful phrases and remove redundant and irrelevant information caused by speaker‚Äôs disfluency and recognition errors. The speech translation results via consolidation are partial translation and can not be directly compared with gold standards in which all words are translated. We would like to propose a new evaluation framework for partial translation by comparing with the most similar set of words extracted from a word network created by merging gradual summarizations of the gold standard translation. Chinese broadcast news speech in RT04 were recognized, consolidated and then translated. The performance of MT results was evaluated using BLEU. We propose Information Preservation Accuracy (IPAccy) and Meaning Preservation Accuracy (MPAccy) for consolidation and consolidation-based MT.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430142","Speech Consolidation;Machine translation;Chinese broadcast news speech;Chinese-English translation","Automatic speech recognition;Data mining;Speech recognition;Natural languages;Degradation;Gold;Broadcasting;Databases;Text recognition;Speech analysis","language translation;natural languages;speech processing;speech recognition","speech translation;ASR results;phrases extraction;speaker disfluency;recognition errors;partial translation;words extraction;word network;gradual summarizations;Chinese broadcast news speech;information preservation accuracy;meaning preservation accuracy","","","","6","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Applying Speech-to-Text Recognition and Computer-Aided Translation for Supporting Multi-lingual Communications in Cross-Cultural Learning Project","R. Shadiev; B. L. Reynolds; Y. -M. Huang; N. Shadiev; W. Wang; R. Laxmisha; W. Wannapipat","Nanjing Normal University, Nanjing, China; University of Macau, Macau, China; National Cheng Kung University, Tainan, Taiwan; Samarkand State University, Samarkand, Uzbekistan; Nanjing Normal University, Nanjing, China; Shandong University of Science and Technology, Qingdao, China; Khon Kaen University, Khon Kaen, Thailand","2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)","7 Aug 2017","2017","","","182","183","We applied a speech-to-text recognition (STR) and computer-aided translation (CAT) systems to support multi-lingual communications students participating in cross-cultural learning project. The participants were engaged in interactions and information exchanges in order to learn and understand cultures and traditions of their peers. Their communications were carried out in their native languages on social communication platforms. The participants spoke and STR system generated texts from their voice inputs. CAT system then simultaneously translated STR-texts into English. Finally, translated texts were posted on social communication platforms along with spoken content in the participants' native languages. We aimed to examine accuracy rates of processes associated with STR and CAT for different languages during multi-lingual communications in our cross-cultural learning project. In addition, the feasibility of our approach to support multi-lingual communications in cross-cultural learning project was investigated. Our results showed that the lowest accuracy rate was for Mongolian and Filipino and the highest was for Spanish, Russian, and French. Our results also demonstrated that cross-cultural learning took place, the participants understood and were able to explain foreign traditions to others as well as to compare foreign traditions with their own local. Based on our results, we made several suggestions and implications for the teaching and research community.","2161-377X","978-1-5386-3870-5","10.1109/ICALT.2017.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001753","Speech-to-text recognition;Computer-aided translation;Cross-cultural learning;Accuracy rate","Cats;Educational technology;Cultural differences;Research and development;Computers;Databases","history;language translation;natural language processing","speech-to-text recognition;computer-aided translation;multilingual communications;cross-cultural learning project;STR;native languages;social communication platforms;CAT system","","2","","14","IEEE","7 Aug 2017","","","IEEE","IEEE Conferences"
"Assessing Evaluation Metrics for Speech-to-Speech Translation","E. Salesky; J. M√§der; S. Klinger","Johns Hopkins University, USA; ETH Z√ºrich, Switzerland; ETH Z√ºrich, Switzerland","2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","3 Feb 2022","2021","","","733","740","Speech-to-speech translation combines machine translation with speech synthesis, introducing evaluation challenges not present in either task alone. How to automatically evaluate speech-to-speech translation is an open question which has not previously been explored. Translating to speech rather than to text is often motivated by unwritten languages or languages without standardized orthographies. However, we show that the previously used automatic metric for this task is best equipped for standardized high-resource languages only. In this work, we first evaluate current metrics for speech-to-speech translation, and second assess how translation to dialectal variants rather than to standardized languages impacts various evaluation methods.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9688073","ETH Z√ºrich Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688073","evaluation;speech synthesis;speech translation;speech-to-speech;dialects","Measurement;Correlation;Conferences;Speech synthesis;Machine translation;Task analysis;Automatic speech recognition","language translation;linguistics;natural language processing;speech processing;speech recognition;speech synthesis","speech-to-speech translation;speech synthesis","","3","","38","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"End-to-End Speech Translation with Self-Contained Vocabulary Manipulation","M. Tu; F. Zhang; W. Liu",Samsung Research China-Beijing (SRC-B); Samsung Research China-Beijing (SRC-B); Samsung Research China-Beijing (SRC-B),"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7929","7933","In machine translation, vocabulary manipulation is a way to reduce the target vocabulary based on the source sentence and the word dictionary, which is effective to lower latency during inference for text translation in industrial application. But vocabulary manipulation is hard to apply to the end-to-end speech-text translation, because neither source text nor speech-to-target mapping is available. We introduce a method that avoids this dependence. Through learning the projection between sentence-level speech encoder output and final target vocabulary, the proposed method allows self-contained vocabulary manipulation without knowing source speech transcripts or dictionaries. Experimental results show that the proposed method speed up about 20% while keep the comparable translation quality.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053431","end-to-end;speech-to-text;speed","Vocabulary;Dictionaries;Conferences;Signal processing;Acoustics;Machine translation;Speech processing","dictionaries;language translation;natural language processing;speech processing;text analysis;vocabulary","dictionaries;sentence-level speech encoder;speech-to-target mapping;end-to-end speech-text translation;source sentence;machine translation;self-contained vocabulary manipulation;source speech transcripts","","","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"The Impact of ASR on Speech-to-Speech Translation Performance","R. Sarikaya; B. Zhou; D. Povey; M. Afify; Y. Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1289","IV-1292","This paper reports on experiments to quantify the impact of automatic speech recognition (ASR) in general and discriminatively trained ASR in particular on the machine translation (MT) performance. The minimum phone error (MPE) training method is employed for building the discriminative ASR acoustic models and a weighted finite state transducer (WEST) based method is used for MT. The experiments are performed on a two-way English/dialectal-Arabic speech-to-speech (S2S) translation task in the military/medical domain. We demonstrate the relationship between ASR and MT performance measured by BLEU and human judgment for both directions of the translation. Moreover, we question the use of BLEU metric for assessing the MT quality, present our observations and draw some conclusions.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218344","Speech Recognition;ASR;Machine Translation;MT;Performance Metric","Automatic speech recognition;Humans;Hidden Markov models;Biomedical acoustics;Biomedical transducers;Acoustic transducers;Acoustic measurements;Speech recognition;Error analysis;Laboratories","language translation;speech recognition","ASR;automatic speech recognition;machine translation;minimum phone error training method;discriminative ASR acoustic models;weighted finite state transducer;two-way English-dialectal-Arabic speech-to-speech translation;BLEU","","2","","13","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Resampling auxiliary data for language model adaptation in machine translation for speech","S. Maskey; A. Sethy","IBM Thomas J. Watson Research Center, New York, NY, USA; IBM Thomas J. Watson Research Center, New York, NY, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4817","4820","Performance of n-gram language models depends to a large extent on the amount of training text material available for building the models and the degree to which this text matches the domain of interest. The language modeling community is showing a growing interest in using large collections of auxiliary textual material to supplement sparse in-domain resources. One of the problems in using such auxiliary corpora is that they may differ significantly from the specific nature of the domain of interest. In this paper, we propose three different methods for adapting language models for a speech to speech (S2S) translation system when auxiliary corpora are of different genre and domain. The proposed methods are based on centroid similarity, n-gram ratios and resampled language models. We show how these methods can be used to select out of domain textual data such as newswire text to improve a S2S system. We were able to achieve an overall relative improvement of 3.8% in BLEU score over a baseline system that uses only in-domain conversational data.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960709","Language Model Adaptation;Machine Translation;Domain Adaptation","Natural languages;Adaptation model;Entropy;Speech coding;Materials testing;System testing;Performance gain;Text categorization;Support vector machines;Support vector machine classification","language translation;speech processing","auxiliary data resampling;language model adaptation;machine translation;n-gram language models;speech to speech translation system;language modeling community","","5","2","16","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Text to speech synthesis system for English to Malayalam translation","A. Anto; K. K. Nisha","Department of Computer Science and Engineering, Rajiv Gandhi Institute of Technology Kottayam, India; Department of Computer Science and Engineering, Rajiv Gandhi Institute of Technology Kottayam, India","2016 International Conference on Emerging Technological Trends (ICETT)","9 Mar 2017","2016","","","1","6","Speech recognition and Speech synthesis are the two emerging technologies in the communication field. Speech recognition is a system which generates text for the given speech, while a speech synthesizer is a system that should be able to read any text aloud. Research is conducting all over the world, to develop an efficient text to speech synthesis (TTS) system for minority languages. India has the largest democracy system in the world, also known as land of unity in diversity and has more than 22 official languages. It becomes difficult to understand the English and the other European languages to common people. This works aims to help people to translate English text to their own language and implement a TTS for the minority language, Malayalam. It is achieved by combining both Machine Translation and TTS. When an English text is given, it is translated to Malayalam with the help of a parser, using grammatical rules, applying morphology and a bilingual dictionary. From each of the translated Malayalam text, syllables are separated. A good number of syllables are recorded and stored in the syllable corpus. Syllables are concatenated to generate a synthesized Malayalam speech. Machine translation of English to Malayalam text is tested and achieved 73 percentage accuracy. For the TTS system, accuracy is verified by checking the naturalness and intelligibility. 87 percentages of the sentences are uttered correctly.","","978-1-5090-3751-3","10.1109/ICETT.2016.7873642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873642","Machine Translation System;Text to Speech synthesis;Speech recognition;Articulatory synthesis;Formant synthesis;Concatenation","Speech;Dictionaries;Speech recognition;Speech synthesis;Syntactics;Market research;Morphology","language translation;speech recognition;speech synthesis","text to speech synthesis system;English translation;Malayalam translation;speech recognition;speech synthesizer;text aloud;TTS system;minority languages;democracy system;European languages;machine translation;English text;parser;grammatical rules;bilingual dictionary;Malayalam text;syllable corpus;Malayalam speech;intelligibility","","2","","19","IEEE","9 Mar 2017","","","IEEE","IEEE Conferences"
"Simultaneous Speech-to-Speech Translation System with Transformer-Based Incremental ASR, MT, and TTS","R. Fukuda; S. Novitasari; Y. Oka; Y. Kano; Y. Yano; Y. Ko; H. Tokuyama; K. Doi; T. Yanagita; S. Sakti; K. Sudoh; S. Nakamura","Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan","2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","3 Jan 2022","2021","","","186","192","In this paper, we present an English-to-Japanese simultaneous speech-to-speech translation (S2ST) system. It has three Transformer-based incremental processing modules for S2ST: automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS). We also evaluated its system-level latency in addition to the module-level latency and accuracy.","2472-7695","978-1-6654-0870-7","10.1109/O-COCOSDA202152914.2021.9660477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660477","Simultaneous translation;Speech translation;English-to-Japanese translation","Costs;Databases;Transformers;Speech;Delays;Machine translation;Automatic speech recognition","language translation;natural language processing;speech recognition;speech synthesis","speech-to-speech translation system;Transformer-based incremental ASR;MT;TTS;S2ST;Transformer-based incremental processing modules;machine translation;text-to-speech synthesis;system-level","","","","26","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation","R. Wang; M. Utiyama; A. Finch; L. Liu; K. Chen; E. Sumita","Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Tencent AI Lab, Shenzhen, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","22 Jun 2018","2018","26","10","1727","1741","Neural machine translation (NMT) has been prominent in many machine translation tasks. However, in some domain-specific tasks, only the corpora from similar domains can improve translation performance. If out-of-domain corpora are directly added into the in-domain corpus, the translation performance may even degrade. Therefore, domain adaptation techniques are essential to solve the NMT domain problem. Most existing methods for domain adaptation are designed for the conventional phrase-based machine translation. For NMT domain adaptation, there have been only a few studies on topics such as fine tuning, domain tags, and domain features. In this paper, we have four goals for sentence level NMT domain adaptation. First, the NMT's internal sentence embedding is exploited and the sentence embedding similarity is used to select out-of-domain sentences that are close to the in-domain corpus. Second, we propose three sentence weighting methods, i.e., sentence weighting, domain weighting, and batch weighting, to balance the data distribution during NMT training. Third, in addition, we propose dynamic training methods to adjust the sentence selection and weighting during NMT training. Fourth, to solve the multidomain problem in a real-world NMT scenario where the domain distributions of training and testing data often mismatch, we proposed a multidomain sentence weighting method to balance the domain distributions of training data and match the domain distributions of training and testing data. The proposed methods are evaluated in international workshop on spoken language translation (IWSLT) English-to-French/German tasks and a multidomain English-to-French task. Empirical results show that the sentence selection and weighting methods can significantly improve the NMT performance, outperforming the existing baselines.","2329-9304","","10.1109/TASLP.2018.2837223","‚ÄúPromotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology‚Äù program of MIC, Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360031","Neural machine translation;domain adaptation","Training;Adaptation models;Task analysis;Speech processing;Testing;Training data;Decoding","language translation;learning (artificial intelligence);natural language processing;statistical analysis","sentence selection;neural machine translation domain adaptation;machine translation tasks;domain-specific tasks;translation performance;out-of-domain corpora;in-domain corpus;domain adaptation techniques;NMT domain problem;conventional phrase-based machine translation;domain tags;sentence level NMT domain adaptation;sentence embedding similarity;out-of-domain sentences;sentence weighting methods;domain weighting;NMT training;dynamic training methods;real-world NMT scenario;multidomain sentence;NMT performance;NMT internal sentence embedding","","17","","85","IEEE","16 May 2018","","","IEEE","IEEE Journals"
"Speech-to-speech translation humanoid robot in doctor's office","S. Shin; E. T. Matson; Jinok Park; Bowon Yang; Juhee Lee; Jin-Woo Jung","M2M Lab, Purdue University, West Lafayette, IN, USA; M2M Lab, Purdue University, West Lafayette, IN, USA; Computer Science and Engineering Dongguk University Seoul, Republic of Korea; Computer Science and Engineering Dongguk University Seoul, Republic of Korea; Computer Science and Engineering Dongguk University Seoul, Republic of Korea; Computer Science and Engineering Dongguk University Seoul, Republic of Korea","2015 6th International Conference on Automation, Robotics and Applications (ICARA)","9 Apr 2015","2015","","","484","489","This paper illustrates the implementation of a speech-to-speech translation humanoid robot in the domain of medical care. At this stage, the proposed system is a one-way translation that is designed to help English speaking patients describe their symptoms to Korean doctors or nurses. A humanoid robot is useful because it can be extended to reach out to people in need first and may substitute the role of human workers, unlike laptops or tablets. The system consists of three main parts - speech recognition, English-Korean translation, and Korean speech generation. It utilizes CMU Sphinx-4 as a speech recognition tool. English-Korean translation in this system is based on the rule-based translation. The success rate of the translation shows reliable results from an experiment with a closed scenario.","","978-1-4799-6466-6","10.1109/ICARA.2015.7081196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081196","Speech-to-Speech Translation;Humanoid Robot;Medical Service Robot;Machine Translation","Speech recognition;Speech;Grammar;Humanoid robots;Hospitals","health care;humanoid robots;language translation;mobile robots;speech recognition","speech-to-speech translation humanoid robot;medical care;English speaking patients;Korean doctors;nurses;English-Korean translation;Korean speech generation;CMU Sphinx-4;speech recognition tool;rule-based translation","","3","","17","IEEE","9 Apr 2015","","","IEEE","IEEE Conferences"
"Europarl-ST: A Multilingual Corpus for Speech Translation of Parliamentary Debates","J. Iranzo-S√°nchez; J. A. Silvestre-Cerd√†; J. Jorge; N. Rosell√≥; A. Gim√©nez; A. Sanchis; J. Civera; A. Juan","Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain; Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN), Universitat Polit√®cnica de Val√®ncia, Spain","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8229","8233","Current research into spoken language translation (SLT), or speech-to-text translation, is often hampered by the lack of specific data resources for this task, as currently available SLT datasets are restricted to a limited set of language pairs. In this paper we present Europarl-ST, a novel multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. This paper describes the corpus creation process and presents a series of automatic speech recognition, machine translation and spoken language translation experiments that highlight the potential of this new resource. The corpus is released under a Creative Commons license and is freely accessible and downloadable.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054626","speech translation;spoken language translation;automatic speech recognition;machine translation;multilingual corpus","Training;Adaptation models;Filtering;Pipelines;Europe;Task analysis;Speech processing","natural language processing;speech recognition;text analysis","Europarl-ST;speech translation;parliamentary debates;speech-to-text translation;specific data resources;language pairs;audio-text samples;European languages;translation directions;European Parliament;corpus creation process;automatic speech recognition;machine translation;spoken language translation experiments;SLT datasets;multilingual SLT corpus","","9","","28","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks","Y. Tang; J. Pino; C. Wang; X. Ma; D. Genzel","Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","6209","6213","Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence. Its success heavily relies on the availability of large amounts of training data. This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition (ASR) and speech translation (ST). In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively. We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks. Our experiments show that the proposed method achieves a relative 10~15% word error rate reduction on the English LIBRISPEECH task compared with our baseline, and improves the speech translation quality on the MUST-C tasks by 3.6~9.2 BLEU.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9415058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415058","Multi-task learning;speech recognition;speech translation","Error analysis;Conferences;Noise reduction;Training data;Speech enhancement;Signal processing;Machine translation","language translation;learning (artificial intelligence);natural language processing;speech recognition","training data;speech applications;labelled speech data;automatic speech recognition;ASR;ST;general multitask learning framework;leverage text data;auxiliary tasks;denoising autoencoder task;machine translation task;text input;phoneme sequences;text corpora;text tasks;English LIBRISPEECH task;speech translation quality;MUST-C tasks;sequence-to-sequence modeling;powerful solution;elegant solution;map one sequence;different sequence","","14","","30","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Using Machine Learning for Speech Extraction and Translation: HiTEK Languages","N. T. Rudrappa; M. V. Reddy","Department of Computer Science, Rani Channamma University, Belagavi, India; Department of Computer Science, Rani Channamma University, Belagavi, India","2022 9th International Conference on Computing for Sustainable Global Development (INDIACom)","2 May 2022","2022","","","267","271","Speech processing deals with retrieving and vocalizing conversational words/sentences i.e. articulatory phonetics, manner of articulation, place of articulation, articulatory gestures, articulatory phonology, articulatory speech recognition, and articulatory synthesis from Multilingual Source to Target language. The research focuses on multilingual speech recording in a single utterance and translation to a target language. Greedy method is used for fetching speech from the user. It consists of the grammatical structures of the speech in the dictionary using cohesion based method for term similarity. It translates speech to text then maps text to a set of phones resulting in target language speech. Multilingual Supervised Speech Dictionary is built for speech to speech translation, currently consisting of four languages Hindi, Telugu, English and Kannada with 100 phones for each language.","","978-93-80544-44-1","10.23919/INDIACom54597.2022.9763300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763300","articulation;cohesion;lexicon;phoneme;speech processing","Dictionaries;Buildings;Speech recognition;Machine learning;Phonetics;Recording;Speech processing","language translation;learning (artificial intelligence);natural language processing;speech recognition;speech synthesis","multilingual supervised speech dictionary;speech translation;target language speech;cohesion based method;fetching speech;multilingual speech;multilingual source;articulatory synthesis;articulatory speech recognition;articulatory phonology;articulatory gestures;articulatory phonetics;speech processing;HiTEK languages;speech extraction;machine learning","","1","","18","","2 May 2022","","","IEEE","IEEE Conferences"
"The RACAI speech translation system challenges of morphologically rich languages","D. Tufi≈ü; T. Boro≈ü; S. Daniel Dumitrescu","Research Institute for Artificial Intelligence, Bucharest, Romania; Research Institute for Artificial Intelligence, Bucharest, Romania; Research Institute for Artificial Intelligence, Bucharest, Romania","2013 7th Conference on Speech Technology and Human - Computer Dialogue (SpeD)","2 Jan 2014","2013","","","1","10","Recent advances in Multilingual Machine Translation and in Speech Processing, coupled with the unprecedented computing power increase of mobile devices, served by faster communication means, made possible the implementation of operational Speech to Speech (S2S) translation systems on smart phones and tablets. Through S2S, a text spoken in one language is automatically recognized, translated and synthesized in another language. This article presents an overview of the first version of our Android-based Romanian-English bi-directional speech translation system and covers the methods and technologies used for implementing it. To the best of our knowledge, this is the first bidirectional S2S for Romanian-English implemented on mobile devices.","","978-1-4799-1065-6","10.1109/SpeD.2013.6682657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682657","automatic speech recognition;machine translation;speech synthesis;speech to speech translation","Tagging;Speech;Speech recognition;Accuracy;Speech processing;Natural language processing;Training","language translation;mobile computing;natural language processing;smart phones;speech processing","RACAI speech translation system challenge;morphologically rich languages;multilingual machine translation;speech processing;mobile devices;operational speech to speech translation systems;S2S translation system;smart phones;tablets;Android-based Romanian-English bi-directional speech translation system","","1","","42","IEEE","2 Jan 2014","","","IEEE","IEEE Conferences"
"ASR error detection in a conversational spoken language translation system","W. Chen; S. Ananthakrishnan; R. Kumar; R. Prasad; P. Natarajan","Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA, USA; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","7418","7422","Detection of automatic speech recognition (ASR) errors is crucial to preventing their further propagation through statistical machine translation (SMT) in conversational spoken language translation (CSLT) systems. In this paper, we venture beyond traditional features obtained from the ASR decoder and hypothesized word sequence, and explore additional information streams provided by an error-robust CSLT system, including SMT confidence estimates and posteriors from named entity detection (NED). Another significant novelty of this work is the use of an automated word boundary detector based on acoustic-prosodic features to verify the existence of ASR-hypothesized word boundaries, which further improves ASR error detection. Offline evaluation on a test set designed to invoke ASR errors showed that at 10% false alarm rate, the proposed features provide 2.8% absolute (4.2% relative) improvement in detection rate over a state-of-the-art baseline error detector that uses a rich set of features traditionally employed in the existing literature.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639104","automatic speech recognition;ASR error detection;conversational speech translation;SMT confidence estimation;word boundary detection","Feature extraction;Speech;Decoding;Detectors;Training;Acoustics;Speech recognition","language translation;speech recognition;statistical analysis","conversational spoken language translation system;automatic speech recognition error detection;statistical machine translation;ASR decoder;hypothesized word sequence;information streams;error-robust CSLT system;SMT confidence;named entity detection;NED;automated word boundary detector;acoustic-prosodic features;false alarm rate;detection rate improvement;baseline error detector","","17","","23","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Improving Speech Transcription for Mandarin-English Translation","M. Tomalin; M. J. F. Gales; X. A. Liu; K. C. Sim; R. Sinha; L. Wang; P. C. Woodland; K. Yu","Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Institute for Infocomm Research, Singapore; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-97","IV-100","This paper describes the development of the CU-HTK Mandarin Speech-To-Text (STT) system and assesses its performance as part of a transcription-translation pipeline which converts broadcastMandarin audio into English text. Recent improvements to the STT system are described and these give Character Error Rate (CER) gains of 14.3% absolute for a Broadcast Conversation (BC) task and 5.1% absolute for a Broadcast News (BN) task. The output of these STT systems is then post-processed, so that it consists of sentence-like segments, and translated into English text using a Statistical Machine Translation (SMT) system. The performance of the transcription-translation pipeline is evaluated using the Translation Edit Rate (TER) and BLEU metrics. It is shown that improving both the STT system and the post-STT segmentations can lower the TER scores by up to 5.3% absolute and increase the BLEU scores by up to 2.7% absolute.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218046","Speech Recognition;Sentence Boundary Detection;Machine Translation","Speech;Lattices;Pipelines;Surface-mount technology;Training data;Broadcasting;Maximum likelihood linear regression;Computer numerical control;Natural languages;Acoustic testing","language translation;natural language processing;speech processing","speech transcription;Mandarin-English translation;CU-HTK Mandarin speech-to-text;transcription-translation pipeline;character error rate;broadcast conversation;broadcast news;statistical machine translation;translation edit rate","","","","11","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"A novel decision function and the associated decision-feedback learning for speech translation","Y. Zhang; L. Deng; X. He; A. Acero","CSAIL, MIT, Cambridge, MA, USA; Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5608","5611","In this paper we report our recent development of an end-to-end integrative design methodology for speech translation. Specifically, a novel decision function is proposed based on the Bayesian analysis, and the associated discriminative learning technique is presented based on the decision-feedback principle. The decision function in our end-to-end design methodology integrates acoustic scores, language model scores and translation scores to refine the translation hypotheses and to determine the best translation candidate. This Bayesian-guided decision function is then embedded into the training process that jointly learns the parameters in speech recognition and machine translation sub-systems in the overall speech translation system. The resulting decision-feedback learning takes a functional form similar to the minimum classification error training. Experimental results obtained on the IWSLT DIALOG 2010 database showed that the proposed system outperformed the baseline system in terms of BLEU score by 2.3 points.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947631","speech translation;decision feedback;integrative design;discriminative training","Speech recognition;Speech;Training;Erbium;Bayesian methods;Acoustics;Error analysis","Bayes methods;decision theory;feedback;language translation;learning (artificial intelligence);speech recognition","Bayesian-guided decision function;associated decision-feedback learning;speech translation system;end-to-end integrative design methodology;Bayesian analysis;associated discriminative learning technique;decision-feedback principle;end-to-end design methodology;acoustic scores;language model scores;translation scores;translation hypotheses;speech recognition;minimum classification error training;IWSLT DIALOG 2010 database;machine translation subsystems","","15","2","14","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Automatic disfluency removal for improving spoken language translation","W. Wang; G. Tur; J. Zheng; N. F. Ayan","Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5214","5217","Statistical machine translation (SMT) systems for spoken languages suffer from conversational speech phenomena, in particular, the presence of speech disfluencies. We examine the impact of disfluencies from broadcast conversation data on our hierarchical phrase-based SMT system and implement automatic disfluency removal approaches for cleansing the MT input. We evaluate the efficacy of proposed approaches and investigate the impact of disfluency removal on SMT performance across different disfluency types. We show that for translating Mandarin broadcast conversational transcripts into English, our automatic disfluency removal approaches could produce significant improvement in BLEU and TER.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5494999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494999","statistical machine translation;spoken language translation;automatic disfluency detection;broadcast conversation","Natural languages;Surface-mount technology;Decoding;System testing;Radio broadcasting;TV broadcasting;Hidden Markov models;Speech analysis;Error analysis;Automatic speech recognition","language translation;speech processing","automatic disfluency removal;spoken language translation;statistical machine translation systems;conversational speech phenomena;speech disfluencies","","8","1","11","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Modeling Homophone Noise for Robust Neural Machine Translation","W. Qin; X. Li; Y. Sun; D. Xiong; J. Cui; B. Wang","School of Computer Science and Technology, Soochow University, Suzhou, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7533","7537","In this paper, we propose a robust neural machine translation (NMT) framework to deal with homophone errors. The framework consists of a homophone noise detector and a syllable-aware NMT model. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese‚ÜíEnglish translation demonstrate that the proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves substantial improvements over them on clean texts.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413586","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413586","Neural Machine Translation;Speech Translation;Robustness;Homophone","Conferences;Detectors;Signal processing;Acoustics;Machine translation;Noise measurement;Speech processing","language translation;natural language processing;neural nets;text analysis","robust neural machine translation framework;homophone noise detector;syllable-aware NMT model;homophone errors;Chinese to English translation;homophone noise modeling","","1","","18","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A low-cost speech-synthesis system for translation of ASCII text to oral language as a vision impaired aid","J. M. Ramirez; G. Bolanos; D. Baez-Lopez","Electrical Engineering Department, Universidad de las Americas Puebla, Cholula, Puebla, Mexico; Dept. of Electr. Eng., Univ. de las Americas, Cholula Puebla, Mexico; Electrical Engineering Department, Universidad de las Americas Puebla, Cholula, Puebla, Mexico","Proceedings Eighth IEEE Symposium on Computer-Based Medical Systems","6 Aug 2002","1995","","","321","324","Presents a simple and low-cost PC-based system to convert and ASCII file into oral language (Spanish) as a vision impaired aid. Three major parts can be distinguished: a database with a set of the most used syllables, which was constructed after research on the Spanish language, an extension board connected to the expansion slot of the PC, and the software required to perform the manipulation of the signals in both parts, speech acquisition and synthesis. The system is currently under testing with good results. General performance and conclusions are discussed.<>","","0-8186-7117-3","10.1109/CBMS.1995.465409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=465409","","Natural languages;Speech synthesis;Databases;Optical character recognition software;Sampling methods;Hardware;Signal synthesis;System testing;Face detection;Biomedical optical imaging","speech synthesis;language translation;handicapped aids;microcomputer applications;add-on boards;natural languages","low-cost speech-synthesis system;ASCII text to oral language translation;vision impaired aid;Spanish language;syllables database;extension;PC expansion slot;signal manipulation;speech acquisition","","","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Rhonda: the architecture of a multilingual speech-to-speech translation pipeline","J. A. Louw; A. Moodley","Human Language Technologies Research Group, Meraka Institute, CSIR, Pretoria, South Africa; Human Language Technologies Research Group, Meraka Institute, CSIR, Pretoria, South Africa","2018 International Conference on Intelligent and Innovative Computing Applications (ICONIC)","6 Jan 2019","2018","","","1","7","Speech-to-speech translation can be described as converting a speech signal from a source language into a speech signal of the same meaning or intent into a target language. This process is achieved by the coordinated cooperation of individual Human Language Technology components, where the most important components to a speech translation system are automatic speech recognition, machine translation and text-to-speech. In this paper we present and discuss the design and architectural building blocks of the Rhonda speech-to-speech translation system, as well as their interactions with each other to facilitate speech-to-speech translation in a reliable, scalable and possibly distributed manner.","","978-1-5386-6477-3","10.1109/ICONIC.2018.8601204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601204","speech-to-speech translation;automatic speech recognition;machine translation;text-to-speech;multilingual","Pipelines;Protocols;Computer architecture;Automatic speech recognition;Engines;Telecommunications","language translation;pipelines;speech recognition","speech-to-speech translation pipeline;speech signal;machine translation;architectural building;Rhonda speech-to-speech translation system;text-to-speech;automatic speech recognition;individual Human Language Technology components","","2","","36","IEEE","6 Jan 2019","","","IEEE","IEEE Conferences"
"Language Model Adaptation in Machine Translation from Speech","I. Bulyko; S. Matsoukas; R. Schwartz; L. Nguyen; J. Makhoul","BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA; BBN Technologies, GTE, Cambridge, MA, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-117","IV-120","This paper investigates the use of several language model adaptation techniques applied to the task of machine translation from Arabic broadcast speech. Unsupervised and discriminative approaches slightly outperform the traditional perplexity-based optimization technique. Language model adaptation, when used for n-best rescoring, improves machine translation performance by 0.3-0.4 BLEU and reduces translation edit rate (TER) by 0.2-0.5% compared to an unadapted LM.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218051","Speech translation;language modeling;domain adaptation","Natural languages;Adaptation model;Testing;Interpolation;Broadcasting;Speech recognition;Decoding;Power measurement;Power system modeling;Broadcast technology","language translation;natural language processing;speech processing","language model adaptation;machine translation;Arabic broadcast speech;unsupervised approach;discriminative approaches;n-best rescoring","","5","","12","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech Translation","A. D. McCarthy; L. Puzon; J. Pino","Center for Language and Speech Processing, Johns Hopkins University; Facebook; Facebook","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7924","7928","We propose autoencoding speaker conversion for training data augmentation in automatic speech translation. This technique directly transforms an audio sequence, resulting in audio thesized to resemble another speaker's voice. Our method compares favorably to SpecAugment on English-French and English-Romanian automatic speech translation (AST) tasks as well as on a low-resource English automatic speech recognition (ASR) task. Further, in ablations, we show the benefits of both quantity and diversity in augmented data. Finally, we show that we can combine our approach with augmentation by machine-translated transcripts to obtain a competitive end-to-end AST model that outperforms a very strong cascade model on an English-French AST task. Our method is sufficiently general that it can be applied to other speech generation and analysis tasks.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053406","automatic speech translation;end-to-end speech translation;data augmentation;speaker normalization","Training;Conferences;Training data;Transforms;Signal processing;Task analysis;Speech processing","encoding;language translation;neural nets;speaker recognition","auto-encoding speaker conversions;automatic speech translation;autoencoding speaker conversion;training data augmentation;audio sequence;low-resource English automatic speech recognition task;machine-translated transcripts;competitive end-to-end AST model;speech generation;English-French AST task","","5","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Multimodal Machine Translation for Sanskrit-Hindi: An Empirical Analysis","N. Sethi; A. Dev; P. Bansal","Information Technology, Indira Gandhi Delhi Technical University for Women, Delhi, India; Information Technology, Indira Gandhi Delhi Technical University for Women, Delhi, India; Artificial Intelligence & Data Science, Indira Gandhi Delhi Technical University for Women, Delhi, India","2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)","17 Mar 2023","2022","","","1","4","Due to its extensive use in ancient Indian religious scriptures, Sanskrit is among the oldest indigenous languages and is rightfully referred to as the language of the gods. However, it is losing favour in contemporary India. Sanskrit is not widely used in current times due in large part to the lack of resources for translation into and out of it. In recent years, machine translation (MT) has improved above and beyond the norm and is now typically performed utilising supervised learning approaches. Due to the paucity of comparable corpora for Sanskrit, new research in the unsupervised MT domain appears to have promise for Sanskrit. With the aid of manually created parallel corpora for the Sanskrit-Hindi language pair, an analysis is conducted between various modelling techniques of building a machine translation system, namely Statistical and Neural, in order to bridge the gap between Sanskrit and its contemporary successor Hindi. In order to provide a fresh viewpoint on the area as a whole, the primary benefits and drawbacks of statistical and neural machine translation has been examined in this work. Our results suggest that Neural machine translation modelling technique performs better than Statistical machine translation.","","978-1-6654-9902-6","10.1109/AIST55798.2022.10064790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064790","Low-resource;Sanskrit;Machine Translation;Statistical-based;Neural-based;Natural language processing","Analytical models;Supervised learning;Buildings;Machine translation;Speech processing;Artificial intelligence","language translation;natural language processing;statistical analysis;supervised learning;unsupervised learning","ancient Indian religious scriptures;contemporary successor Hindi;multimodal machine translation;neural machine translation modelling technique;Sanskrit-Hindi language pair;statistical machine translation;supervised learning approaches;unsupervised MT domain","","","","28","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Towards the implementation of an Attention-based Neural Machine Translation with artificial pronunciation for Nahuatl as a mobile application","S. K. Bello Garc√≠a; E. S√°nchez Lucero; B. E. Pedroza M√©ndez; J. C. Hern√°ndez Hern√°ndez; E. Bonilla Huerta; J. F. Ram√≠rez Cruz","Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico; Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico; Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico; Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico; Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico; Division de Estudios de Posgrado e Investigaci√≥n Tecnol√≥gico Nacional de M√©xico (TecNM), Instituto Tecnol√≥gico de Apizaco (ITA), Tlaxcala, M√©xico","2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT)","31 Dec 2020","2020","","","235","244","There are great translation systems online. However, even though this technology is available for the majority of languages, it is not the case of Nahuatl [1]. For this reason, this paper outlines a master's degree thesis proposal which is aimed to use a Neural Network for Translation with an attention mechanism and Long Short-Term Memory (LSTM) like the one used by Google [2]. In addition, it seeks to implement an artificial Text To Speech (TTS) system trained with a Neural Network with a given dataset of Mel spectrograms in [3] from a person speaking in Nahuatl and it attempts to achieve a natural voice output as a spectrogram, then process it and obtain the sound desired. Finally, once trained, these models can be prepared for being used within mobile devices, and even taking advantage of the neural engine some of them are equipped with. In this way, this technology can reach more people and help to preserve and even spread the language. The early results showed how the limited resources of this language could cause a strong bias in the outputs and also how there could be some loss of information given the morphemes Nahuatl has, given its polysynthetic nature. This also highlights the way it can be tokenized, playing an important role in how the results turn out obtaining a BLEU score of 0.34 at best. Finally, this application and research can be an interesting framework of how a polysynthetic language can be manipulated to be used for fusional languages like Spanish or English. This research work was carried out at the ‚ÄúTecnol√≥gico Nacional de M√©xico‚Äù (TecNM), campus of the ‚ÄúInstituto Tecnol√≥gico de Apizaco‚Äù (ITA).","","978-1-7281-8450-0","10.1109/CONISOFT50191.2020.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307780","Nahuatl;NMT;mobile;translation;attention;machine learning;CoreML;neural network;Mel spectrogram","Spectrogram;Decoding;Tools;Mobile handsets;Internet;Engines;Speech recognition","language translation;learning (artificial intelligence);mobile computing;natural language processing;recurrent neural nets;speech synthesis","Mel spectrograms;natural voice output;spectrogram;mobile devices;neural engine;polysynthetic nature;polysynthetic language;fusional languages;attention-based Neural machine Translation;artificial pronunciation;mobile application;Neural Network;attention mechanism;Long Short-Term Memory;artificial Text;Speech system;Google;Nahuatl","","1","","30","IEEE","31 Dec 2020","","","IEEE","IEEE Conferences"
"Integration of Statistical Models for Dictation of Document Translations in a Machine-Aided Human Translation Task","A. Reddy; R. C. Rose","Department of Electrical and Computer Engineering, McGill University, Montreal, QUE, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, QUE, Canada","IEEE Transactions on Audio, Speech, and Language Processing","7 Sep 2010","2010","18","8","2015","2027","This paper presents a model for machine-aided human translation (MAHT) that integrates source language text and target language acoustic information to produce the text translation of source language document. It is evaluated on a scenario where a human translator dictates a first draft target language translation of a source language document. Information obtained from the source language document, including translation probabilities derived from statistical machine translation (SMT) and named entity tags derived from named entity recognition (NER), is incorporated with acoustic phonetic information obtained from an automatic speech recognition (ASR) system. One advantage of the system combination used here is that words that are not included in the ASR vocabulary can be correctly decoded by the combined system. The MAHT model and system implementation is presented. It is shown that a relative decrease in word error rate of 29% can be obtained by this combined system relative to the baseline ASR performance on a French to English document translation task in the Hansard domain. In addition, it is shown that transcriptions obtained by using the combined system show a relative increase in NIST score of 34% compared to transcriptions obtained from the baseline ASR system.","1558-7924","","10.1109/TASL.2010.2040793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393062","Machine-aided human translation (MAHT);machine translation;named entity recognition;speech recognition","Humans;Automatic speech recognition;Natural languages;Surface-mount technology;Decoding;Error analysis;Speech recognition;Acoustic noise;Probability;Vocabulary","language translation;speech recognition;statistical analysis;text analysis","statistical model integration;document translations;machine-aided human translation task;source language text integration;target language acoustic information;text translation;source language document;statistical machine translation;translation probability;named entity tags;named entity recognition;acoustic phonetic information;automatic speech recognition system;ASR vocabulary;MAHT model;word error rate;French to English document translation task;Hansard domain","","14","1","36","IEEE","19 Jan 2010","","","IEEE","IEEE Journals"
"Task Aware Multi-Task Learning for Speech to Text Tasks","S. Indurthi; M. A. Zaidi; N. Kumar Lakumarapu; B. Lee; H. Han; S. Ahn; S. Kim; C. Kim; I. Hwang","Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea; Samsung Research, Seoul, South Korea","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7723","7727","In general, the direct Speech-to-text translation (ST) is jointly trained with Automatic Speech Recognition (ASR), and Machine Translation (MT) tasks. However, the issues with the current joint learning strategies inhibit the knowledge transfer across these tasks. We propose a task modulation network which allows the model to learn task specific features, while learning the shared features simultaneously. This proposed approach removes the need for separate finetuning step resulting in a single model which performs all these tasks. This single model achieves a performance of 28.64 BLEU score on ST MuST-C English-German, WER of 11.61% on ASR TEDLium v3, 23.35 BLEU score on MT WMT‚Äô15 English-German task. This sets a new state-of-the-art performance (SOTA) on the ST task while outperforming the existing end-to-end ASR systems.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414703","Speech Translation;Speech Recognition;Task Modulation;Multitask Learning","Adaptation models;Conferences;Modulation;Acoustics;Machine translation;Task analysis;Speech processing","language translation;learning (artificial intelligence);natural language processing;speech recognition","machine translation;task modulation network;features learning;ST task;task aware multitask learning;speech-to-text translation;automatic speech recognition","","2","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Parsing-based objective functions for speech recognition in translation applications","D. Hillard; M. Hwang; M. Harper; M. Ostendorf","Electrical Engineering Department, University of Washington, Seattle, WA, USA; Electrical Engineering Department, University of Washington, Seattle, WA, USA; Computer Science Department, University of Maryland, College Park, MD, USA; Electrical Engineering Department, University of Washington, Seattle, WA, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","5109","5112","This paper looks at a parsing-based alternative to word error rate (WER) for optimizing recognition, SParseval, hypothesizing that it may be a better objective for applications such as translation. We find that SParseval is more correlated than WER with human measures of subsequent translation performance, but that optimizing explicitly for SParseval does not give a significant reduction in translation error as measured by automatic methods based on a single translation reference. However, anecdotal examples indicate that SParseval does improve automatic speech recognition (ASR) results, leaving open the possibility that it may be more useful in the future or for other language processing tasks.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518808","speech translation;speech recognition objective;parsing","Speech recognition;Automatic speech recognition;Error analysis;Natural languages;Performance gain;Gold;Application software;Data mining;Computer science;Educational institutions","error statistics;grammars;language translation;speech processing;speech recognition","parsing-based objective functions;speech recognition;machine translation;word error rate;SParseval;automatic speech recognition","","1","","17","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"ASR Error Correction and Domain Adaptation Using Machine Translation","A. Mani; S. Palaskar; N. V. Meripo; S. Konam; F. Metze",Abridge AI; Carnegie Mellon University; Abridge AI; Abridge AI; Abridge AI,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","6344","6348","Off-the-shelf pre-trained Automatic Speech Recognition (ASR) systems are an increasingly viable service for companies of any size building speech-based products. While these ASR systems are trained on large amounts of data, domain mismatch is still an issue for many such parties that want to use this service as-is leading to not so optimal results for their task. We propose a simple technique to perform domain adaptation for ASR error correction via machine translation. The machine translation model is a strong candidate to learn a mapping from out-of-domain ASR errors to in-domain terms in the corresponding reference files. We use two off-the-shelf ASR systems in this work: Google ASR (commercial) and the ASPIRE model (open-source). We observe 7% absolute improvement in word error rate and 4 point absolute improvement in BLEU score in Google ASR output via our proposed method. We also evaluate ASR error correction via a downstream task of Speaker Diarization that captures speaker style, syntax, structure and semantic improvements we obtain via ASR correction.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053126","Domain adaptation;ASR error correction;machine translation;diarization;medical transcription","Adaptation models;Syntactics;Error correction;Internet;Machine translation;Task analysis;Speech processing","computational linguistics;language translation;natural language processing;speaker recognition","ASR error correction;machine translation model;off-the-shelf ASR systems;word error rate;Google ASR output;speech-based products;off-the-shelf pre-trained automatic speech recognition","","17","","17","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Exploiting Translation Model for Parallel Corpus Mining","C. Leong; X. Liu; D. F. Wong; L. S. Chao","Natural Language Processing and Portuguese-Chinese Machine Translation (NLP2CT) Laboratory, University of Macau, Macau, SAR, China; Natural Language Processing and Portuguese-Chinese Machine Translation (NLP2CT) Laboratory, University of Macau, Macau, SAR, China; Natural Language Processing and Portuguese-Chinese Machine Translation (NLP2CT) Laboratory, University of Macau, Macau, SAR, China; Natural Language Processing and Portuguese-Chinese Machine Translation (NLP2CT) Laboratory, University of Macau, Macau, SAR, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","3 Sep 2021","2021","29","","2829","2839","Parallel corpus mining (PCM) is beneficial for many corpus-based natural language processing tasks, e.g., machine translation and bilingual dictionary induction, especially in low-resource languages and domains. It relies heavily on cross-lingual representations to model the interdependencies between different languages and determine whether sentences are parallel or not. In this paper, we take the first step towards exploiting the multilingual Transformer translation model to produce expressive sentence representations for PCM. Since the traditional Transformer lacks an immediate sentence representation, we pool the output representation of the encoder as the sentence representation, which is further optimized as a part of the training flow of the translation model. Experiments conducted on the BUCC PCM task show that the proposed method improves mining performance over the existing methods with the assistance of the pre-trained multilingual BERT. To further test the usability of the proposed method, we mine parallel sentences from public resources and find that the mined sentences can indeed enhance low-resource machine translation.","2329-9304","","10.1109/TASLP.2021.3105798","National Natural Science Foundation of China(grant numbers:61672555); Science and Technology Development Fund, Macau SAR(grant numbers:0101/2019/A2); Multi-year Research Grant from the University of Macau(grant numbers:MYRG2020-00054-FST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516978","Chinese‚ÄìPortuguese translation;neural machine translation;parallel corpus mining","Task analysis;Phase change materials;Decoding;Speech processing;Training;Bit error rate;Machine translation","data mining;dictionaries;language translation;natural language processing;text analysis","parallel corpus mining;bilingual dictionary induction;low-resource languages;cross-lingual representations;multilingual Transformer translation model;expressive sentence representations;traditional Transformer;immediate sentence representation;output representation;BUCC PCM task show;pre-trained multilingual BERT;parallel sentences;mined sentences;low-resource machine translation","","1","","58","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"On statistical machine translation method for lexicon refinement in speech recognition","H. Xu; X. Xiao; E. -S. Chng; H. Li","Temasek Laboratories, Nanyang Technological University, Singapore; Temasek Laboratories, Nanyang Technological University, Singapore; Computer Engineering School, Nanyang Technological University, Singapore; Institute for Infocomm Research, Singapore","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","3 Sep 2015","2015","","","25","29","In low resource Automatic Speech Recognition (ASR), one usually resorts to the Statistical Machine Translation (SMT) technique to learn transform rules to refine grapheme lexicon. To do this, we face two challenges. One is to generate grapheme sequences from the training data as the targets, which is paired with the original transcripts to train SMT models; the other is to effectively prune the learned rules from the translation model. In this paper we further this study. First we propose a simple but effective pruning method; second, to see in which case we are able to learn better rules, different setups with various acoustic and language model combinations are investigated; finally, to examine if the rules in different setups are complementary, lexicons generated via different rule tables are merged in ASR experiments. We report a WER reduction of up to 6.2% with the proposed technique.","","978-1-4799-1948-2","10.1109/ChinaSIP.2015.7230355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230355","lexicon learning;grapheme lexicon;statistical machine translation;system fusion;automatic speech recognition","Hidden Markov models;Acoustics;Training data;Data models;Speech recognition;Speech;Accuracy","language translation;learning (artificial intelligence);speech recognition","statistical machine translation method;SMT;grapheme lexicon refinement;automatic speech recognition;ASR;pruning method","","","","20","IEEE","3 Sep 2015","","","IEEE","IEEE Conferences"
"ASR Normalization for Machine Translation","H. Huang; C. Feng; J. Wang; X. Zhang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Research Center of Computer and Language Information Engineering, Chinese Academy and Sciences, Beijing, China; Research Center of Computer and Language Information Engineering, Chinese Academy and Sciences, Beijing, China","2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics","30 Sep 2010","2010","2","","91","94","In natural spoken language there are many meaningless modal particles and dittographes, furthermore ASR (automatic speech recognition) often has some recognition errors and the ASR results have no punctuations. Therefore, the translation would be rather poor if the ASR results are directly translated by MT (machine translation). In this paper, an ASR normalization approach was introduced for machine translation which based on maximum entropy sequential labeling model. Before translation, the meaningless modal particles and dittograph were deleted, and the recognition errors were corrected, and ASR results were also punctuated. Experiments show that the MT BLEU of 0.2465 is obtained, that improved by 17.3% over the MT baseline without normalization. The positive experimental results confirm that ASR normalization is effective for improvement of translation quality for spoken language machine translation.","","978-1-4244-7869-9","10.1109/IHMSC.2010.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590723","Spoken language;machine translation;automatic speech recognition;maximum entropy model;normalization","Speech recognition;Entropy;Labeling;Computational modeling;Decoding;Acoustics;Natural language processing","language translation;maximum entropy methods;speech recognition","ASR normalization approach;automatic speech recognition;maximum entropy sequential labeling model;translation quality;spoken language machine translation","","","1","13","IEEE","30 Sep 2010","","","IEEE","IEEE Conferences"
"SAMU-XLSR: Semantically-Aligned Multimodal Utterance-Level Cross-Lingual Speech Representation","S. Khurana; A. Laurent; J. Glass","MIT Computer Science, Artificial Intelligence Laboratory, Cambridge, MA, USA; LIUM‚ÄîLe Mans University, Le Mans, France; MIT Computer Science, Artificial Intelligence Laboratory, Cambridge, MA, USA","IEEE Journal of Selected Topics in Signal Processing","18 Oct 2022","2022","16","6","1493","1504","We propose the ($\tt SAMU\text{-}XLSR$): Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation learning framework. Unlike previous works on speech representation learning, which learns multilingual contextual speech embedding at the resolution of an acoustic frame (10‚Äì20 ms), this work focuses on learning multimodal (speech-text) multilingual speech embedding at the resolution of a sentence (5‚Äì10 s) such that the embedding vector space is semantically aligned across different languages. We combine state-of-the-art multilingual acoustic frame-level speech representation learning model $\tt XLSR$ with the Language Agnostic BERT Sentence Embedding ($\tt LaBSE$) model to create an utterance-level multimodal multilingual speech encoder $\tt SAMU\text{-}XLSR$. Although we train $\tt SAMU\text{-}XLSR$ with only multilingual transcribed speech data, cross-lingual speech-text and speech-speech associations emerge in its learned representation space. To substantiate our claims, we use $\tt SAMU\text{-}XLSR$ speech encoder in combination with a pre-trained $\tt LaBSE$ text sentence encoder for cross-lingual speech-to-text translation retrieval, and $\tt SAMU\text{-}XLSR$ alone for cross-lingual speech-to-speech translation retrieval. We highlight these applications by performing several cross-lingual text and speech translation retrieval tasks across several datasets.","1941-0484","","10.1109/JSTSP.2022.3192714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9834099","Cross-lingual speech representation learning;Language-agnostic speech embedding;zero-shot speech-to-text translation retrieval;zero-shot speech-to-speech translation retrieval","Training;Speech processing;Machine translation;Representation learning","language translation;learning (artificial intelligence);natural language processing;speech processing;speech recognition;text analysis","cross-lingual speech-text;cross-lingual speech-to-speech translation retrieval;cross-lingual speech-to-text translation retrieval;Language Agnostic BERT Sentence Embedding model;learned representation space;learning multimodal multilingual speech;multilingual acoustic frame-level speech representation;multilingual contextual speech;multilingual transcribed speech data;multimodal utterance-level cross-lingual speech representation learning framework;SAMU-XLSR;SAMU-XLSR speech;speech translation retrieval tasks;speech-speech associations;utterance-level multimodal multilingual speech","","6","","43","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"AwezaMed: A Multilingual, Multimodal Speech-To-Speech Translation Application for Maternal Health Care","L. Marais; J. A. Louw; J. Badenhorst; K. Calteaux; I. Wilken; N. van Niekerk; G. Stein","Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa; Aweza, Cape Town, South Africa","2020 IEEE 23rd International Conference on Information Fusion (FUSION)","10 Sep 2020","2020","","","1","8","The language contexts of multilingual developing countries such as South Africa are often characterised by communication challenges resulting from language differences. AwezaMed is a multilingual, multimodal speech-to-speech translation application for the health care domain, which was designed to assist in bridging communication barriers and mitigate the risks of miscommunication. The application focuses on the domain of maternal health care. It uses English as source language and Afrikaans, isiXhosa and isiZulu as target languages to enable health care providers to communicate with patients in their own language. It incorporates automatic speech recognition, machine translation and text-to-speech to deliver speech-to-speech translation functionality in a scalable way via a REST API to an Android mobile application. It is being piloted at various health care facilities across South Africa.","","978-0-578-64709-8","10.23919/FUSION45008.2020.9190240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190240","speech-to-speech translation;machine translation;automatic speech recognition;text-to-speech;mobile application","Reliability;Pipelines;Automatic speech recognition;Bridges;Cancer","Android (operating system);application program interfaces;developing countries;gender issues;health care;language translation;mobile computing;natural language processing;speech recognition","South Africa;language differences;AwezaMed;maternal health care;automatic speech recognition;text-to-speech;health care facilities;multilingual developing countries;multimodal speech-to-speech translation;multilingual speech-to-speech translation;English language;Afrikaans;isiXhosa;isiZulu;machine translation;REST API;Android mobile application","","3","","17","","10 Sep 2020","","","IEEE","IEEE Conferences"
"Research on the semantic knowledge base in Chinese-English machine translation","Z. Liu; Y. Jin; W. Chang","CPIC-BNU Joint Laboratory of Machine Translation, Beijing Normal University, Beijing, China; CPIC-BNU Joint Laboratory of Machine Translation, Beijing Normal University, Beijing, China; CPIC-BNU Joint Laboratory of Machine Translation, Beijing Normal University, Beijing, China","2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems","14 Nov 2013","2012","03","","1435","1439","Knowledge bases in machine translation (MT) systems have proved successful in some constrained domains, but have not scaled up for two reasons. One is that the building of knowledge base (KB) is painstakingly handcrafted from scratch, and the other is that the most KBs for machine translation (MT) lack supports from powerful theory system based on semantic understanding. This paper focuses on the building of semantic knowledge base (SKB) guided by the Concept of Hierarchical Network (HNC) theory which is suitable for machine translation. Besides bilingual general attributes, the semantic attributes at all levels are described in a word such as concept category, semantic representation, and sentence category and concept relation. By doing this, we try to solve the semantic mapping problems between Chinese and English at the level of word, chunk and sentence. The SKB has been used both in the analysis of the source language and target language translation process. The accuracy of translation system based on the SKB has increased considerably.","2376-595X","978-1-4673-1857-0","10.1109/CCIS.2012.6664622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664622","machine translation;HNC theory;semantic knowledge base;semantic attributes;semantic mapping","Semantics;Knowledge based systems;Patents;Natural languages;Engines;Speech","language translation;natural language processing","Chinese-English machine translation system;MT systems;semantic knowledge base;KB;semantic understanding;SKB;concept of hierarchical network theory;HNC theory;bilingual general attributes;concept category;semantic representation;sentence category;concept relation;semantic mapping problems;target language translation process;source language translation process","","","","13","IEEE","14 Nov 2013","","","IEEE","IEEE Conferences"
"Syntax Rectification of Speech Using Neural Machine Translation","J. Singh; S. Pote; A. Karhe; A. Agrawal","Department of Computer Science, CDGI, Indore, India; Data Scientist, Big Data Infomatics, Indore, India; Department of Computer Science, CDGI, Indore, India; Department of Computer Science, CDGI, Indore, India","2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on","28 Feb 2019","2018","","","306","311","Speech Recognition is the identification of speech and conversion of the audio signals into textual data. The process of Speech Recognition has been accelerated with advancements in computing and progress in the development of Deep Learning. The mathematics behind Speech Recognition stems from Hidden Markov Models and has evolved into its more advanced form - End to End Speech Recognition. The aim of this paper is to focus on the syntax correction of speech in English language which will make use of Sequence to Sequence Models. The paper will delineate three major domains of grammatical correction of speech which will be achieved through real time interaction with the user.","","978-1-5386-1442-6","10.1109/I-SMAC.2018.8653676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653676","Automatic Speech Recognition;Neural Machine Translation;Sequence to Sequence;Deep Recurrent Neural Networks","Hidden Markov models;Speech recognition;Recurrent neural networks;Training;Conferences;Mathematical model;Probability distribution","audio signal processing;hidden Markov models;language translation;learning (artificial intelligence);neural nets;speech recognition","speech syntax rectification;neural machine translation;audio signal conversion;deep learning;hidden Markov models;end to end speech recognition;English language;sequence to sequence models;speech grammatical correction","","","","25","IEEE","28 Feb 2019","","","IEEE","IEEE Conferences"
"Utterance Classification Using Linguistic and Non-linguistic Information for Network-Based Speech-to-Speech Translation Systems","K. Sugiura; R. Lee; H. Kashioka; K. Zettsu; Y. Kidawara","National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan","2013 IEEE 14th International Conference on Mobile Data Management","29 Jul 2013","2013","2","","212","216","Network-based mobile services, such as speech-to-speech translation and voice search, enable the construction of large-scale log database including speech. We have developed a smartphone application called VoiceTra for speech-to-speech translation and have collected 10,000,000 utterances so far. This huge corpus is unique in size and spatio-temporal information; it contains information on anonymized user locations. This spatiotemporal corpus can be used for improving the accuracy of its speech recognition and machine translation, and it will open the door for the study of the location dependency of vocabulary and new applications for location-based services. This paper first analyzes the corpus and then presents a novel method for classifying utterances using linguistic and non-linguistic information. L2-regularized Logistic Regression is used for utterance classification. Our experiments performed on the VoiceTra log corpus revealed that our proposed method outperformed baseline methods in terms of F measure.","2375-0324","978-0-7695-4973-6","10.1109/MDM.2013.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569092","speech-to-speech translation;smartphone;GIS","Pragmatics;Business;Speech;Mobile communication;Speech recognition;Knowledge discovery;Vectors","audio databases;language translation;linguistics;pattern classification;smart phones;spatiotemporal phenomena;speech recognition;vocabulary","utterance classification;linguistic information;nonlinguistic information;network-based speech-to-speech translation system;network-based mobile services;large-scale log database;smartphone application;spatiotemporal information;spatiotemporal corpus;speech recognition accuracy improvement;machine translation accuracy improvement;location dependency;vocabulary;location-based services;L2-regularized logistic regression;VoiceTra log corpus","","","","12","IEEE","29 Jul 2013","","","IEEE","IEEE Conferences"
"Converting Written Language to Spoken Language with Neural Machine Translation for Language Modeling","S. Ando; M. Suzuki; N. Itoh; G. Kurata; N. Minematsu","Graduate School of Engineering, The University of Tokyo; IBM Research AI; IBM Research AI; IBM Research AI; Graduate School of Engineering, The University of Tokyo","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8124","8128","When building a language model (LM) for spontaneous speech, the ideal situation is to have a large amount of spoken, in-domain training data. Having such abundant data, however, is not realistic. We address this problem by generating texts in spoken language from those in written language by using a neural machine translation (NMT) model. We collected faithful transcripts of fully spontaneous speech and corresponding written versions and used them as a parallel corpus to train the NMT model. We used top-k random sampling, which generates a large variety of texts of higher quality as compared to other generation methods for NMT. We indicate that the NMT model is capable of converting written texts in a certain domain to spoken texts, and that the converted texts are effective for training LMs. Our experimental results show significant improvement of speech recognition accuracy with the LMs.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053226","spontaneous speech;parallel corpus;Transformer;domain adaptation","Training;Adaptation models;Training data;Speech recognition;Data models;Machine translation;Task analysis","language translation;natural language processing;speech recognition;text analysis","NMT model;written texts;spoken texts;converted texts;written language;spoken language;language modeling;neural machine translation model;top-k random sampling;speech recognition","","1","","21","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Cascaded Models with Cyclic Feedback for Direct Speech Translation","T. K. Lam; S. Schamoni; S. Riezler","Computational Linguistics, Germany; Computational Linguistics, Germany; Computational Linguistics, Germany","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7508","7512","Direct speech translation describes a scenario where only speech inputs and corresponding translations are available. Such data are notoriously limited. We present a technique that allows cascades of automatic speech recognition (ASR) and machine translation (MT) to exploit in-domain direct speech translation data in addition to out-of-domain MT and ASR data. After pre-training MT and ASR, we use a feed-back cycle where the downstream performance of the MT system is used as a signal to improve the ASR system by self-training, and the MT component is fine-tuned on multiple ASR outputs, making it more tolerant towards spelling variations. A comparison to end-to-end speech translation using components of identical architecture and the same data shows gains of up to 3.8 BLEU points on LibriVoxDeEn and up to 5.1 BLEU points on CoVoST for German-to-English speech translation.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413719","speech translation;cascaded models;self-supervised learning;multi-input training;low-resource","Adaptation models;Conferences;Signal processing;Acoustics;Machine translation;Speech processing;Automatic speech recognition","language translation;natural language processing;speech recognition","speech inputs;corresponding translations;machine translation;in-domain direct speech translation data;out-of-domain MT;ASR data;pre-training MT;MT system;ASR system;MT component;multiple ASR outputs;end-to-end speech translation;German-to-English speech translation","","1","","31","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Enhancing Speech-To-Speech Translation with Multiple TTS Targets","J. Shi; Y. Tang; A. Lee; H. Inaguma; C. Wang; J. Pino; S. Watanabe",Carnegie Mellon University; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Carnegie Mellon University,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095973","speech-to-speech translation;text-to-speech augmentation;discrete units","Analytical models;Signal processing;Multitasking;Data models;Acoustics;Speech processing","","","","","","38","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Translation Quality Estimation Using Only Bilingual Corpora","L. Liu; A. Fujita; M. Utiyama; A. Finch; E. Sumita","Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology, Tencent AI Lab, Shenzhen, China; Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology, Soraku-gun, Japan; Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology, Soraku-gun, Japan; Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology, Soraku-gun, Japan; Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology, Soraku-gun, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","12 Jul 2017","2017","25","9","1762","1772","In computer-aided translation scenarios, quality estimation of machine translation hypotheses plays a critical role. Existing methods for word-level translation quality estimation (TQE) rely on the availability of manually annotated TQE training data obtained via direct annotation or postediting. However, due to the cost of human labor, such data are either limited in size or is only available for few tasks in practice. To avoid the reliance on such annotated TQE data, this paper proposes an approach to train word-level TQE models using bilingual corpora, which are typically used in machine translation training and is relatively easier to access. We formalize the training of our proposed method under the framework of maximum marginal likelihood estimation. To avoid degenerated solutions, we propose a novel regularized training objective whose optimization is achieved by an efficient approximation. Extensive experiments on both written and spoken language datasets empirically show that our approach yields comparable performance to the standard training on annotated data.","2329-9304","","10.1109/TASLP.2017.2716195","‚ÄúPromotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology‚Äù of MIC, Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949019","Conditional random fields;feed-forward neural networks;machine translation;maximum marginal likelihood estimation;recurrent neural networks;translation quality estimation","Training;Estimation;Training data;Speech;Speech processing;Recurrent neural networks;Standards","approximation theory;feedforward neural nets;language translation;maximum likelihood estimation;optimisation;recurrent neural nets;text analysis","bilingual corpora;computer-aided translation;machine translation hypotheses;word-level translation quality estimation;word-level TQE model training;manually annotated TQE training data;direct annotation;postediting;machine translation training;maximum marginal likelihood estimation;regularized training objective;optimization;approximation;written language datasets;spoken language datasets;feedforward neural networks;recurrent neural networks","","3","","54","IEEE","15 Jun 2017","","","IEEE","IEEE Journals"
"Context-Adaptive Document-Level Neural Machine Translation","L. Zhang; Z. Zhang; B. Chen; W. Luo; L. Si","Zhejiang University; Alibaba DAMO Academy, China; Alibaba DAMO Academy, China; Alibaba DAMO Academy, China; Alibaba DAMO Academy, China","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6232","6236","Document-level translation models are still far from perfect. Most existing document-level neural machine translation (NMT) models leverage a fixed number of the previous or all global sentences to handle the context-independent problem in standard NMT. However, the translating of each source sentence benefits from various sizes of context. And study shows that inappropriate redundant context will increase model burden but not improve the translation performance. This work introduces a data-adaptive method that enables the model to adopt the necessary and helpful context. Specifically, we introduce a light predictor into two document-level translation models to select the explicit context. Experiments demonstrate the proposed approach can significantly improve the performance over the previous methods with a gain up to 1.99 BLEU points.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746245","Machine translation;document-level;context;adaptive","Adaptation models;Conferences;Signal processing;Predictive models;Acoustics;Machine translation;Speech processing","document handling;language translation;natural language processing","model burden;translation performance;source sentence benefits;inappropriate redundant context;context independent problem;document level neural machine translation models;context adaptive document level neural machine translation;NMT","","","","15","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Generating Multilingual Voices Using Speaker Space Translation Based on Bilingual Speaker Data","S. Maiti; E. Marchi; A. Conkie","The Graduate Center, CUNY, Apple, New York, NY; The Graduate Center, CUNY, Apple, New York, NY; The Graduate Center, CUNY, Apple, New York, NY","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7624","7628","We present progress towards bilingual Text-to-Speech which is able to transform a monolingual voice to speak a second language while preserving speaker voice quality. We demonstrate that a bilingual speaker embedding space contains a separate distribution for each language and that a simple transform in speaker space generated by the speaker embedding can be used to control the degree of accent of a synthetic voice in a language. The same transform can be applied even to monolingual speakers.In our experiments speaker data from an English-Spanish (Mexican) bilingual speaker was used, and the goal was to enable English speakers to speak Spanish and Spanish speakers to speak English. We found that the simple transform was sufficient to convert a voice from one language to the other with a high degree of naturalness. In one case the transformed voice outperformed a native language voice in listening tests. Experiments further indicated that the transform preserved many of the characteristics of the original voice. The degree of accent present can be controlled and naturalness is relatively consistent across a range of accent values.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054305","cross-lingual transfer;d-vector;speaker space manipulation;bilingual speaker;text-to-speech synthesis","Conferences;Process control;Transforms;Aerospace electronics;Signal processing;Complexity theory;Speech processing","linguistics;natural language processing;speech synthesis;text analysis","speaker space translation;bilingual speaker data;bilingual text-to-speech;monolingual voice;speaker voice quality;bilingual speaker embedding space;synthetic voice;monolingual speakers;English-Spanish bilingual speaker;native language voice;multilingual voices","","5","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Classification techniques for automatic speech recognition (ASR) algorithms used with real time speech translation","H. H. O. Nasereddin; A. A. R. Omari","Faculty of Information Technology, Middle East University (MEU), Amman, Jordan; iHorizons, Middle East University (MEU), Amman, Jordan","2017 Computing Conference","11 Jan 2018","2017","","","200","207","Speech processing is considered to be one of the most important application area of digital signal processing. Speech recognition and translation systems have consisted into two main systems, the first system represents an ASR system that contains two levels which are level one the feature extraction level As well as, level two the classification technique level using Data Time Wrapping (DTW), Hidden Markov Model (HMM), and Dynamic Bayesian Network (DBN). The second system is the Machine Translation (MT) system that mainly can be achieved by using three approaches which are (A) the statistical-based approach, (B) rule-approach, and (C) hybrid-based approach. In this study, we made a comparative study between classification techniques from ASR point of view, as well as, the translation approaches from MT point of view. The recognition rate was used in the ASR level and the error rate was used to evaluate the accuracy of the translated sentences. Furthermore, we classified the sample text audio files into four categories which were news, conversational, scientific phrases, and control categories.","","978-1-5090-5443-5","10.1109/SAI.2017.8252104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252104","AF (Acoustic Feature);AM (Acoustic Model);ANN (Artificial Neural Network);ASR (Automatic Speech Recognition);BN (Bayesian Network);CNN (Convolutional Neural Networks);CVC (Consonant-Vowel-Consonant);DAG (Direct Acyclic Graph);DBN (Dynamic Bayesian Networks);DCT (Discrete Cosine Transformation);DNN (Deep Neural Network);DTW (Dynamic Time wrapping);HMM (Hidden Markov Model);JPD (Joint Probability Distribution);LM (Language Model);LPCC (Linear Predictive Cepstral Coefficients);MFCC (Mel Frequency Cepstral Coefficients);MT (Machine Translation);PD (Punctuation Dictionary);PLP (Perceptual Linear Prediction Coefficient);QV (Quantization Vector);RV (Random Variables);SLT (Spoken Language Translation);SVM (Support Vector Machine);WER (Word Error Rate);WRR (Word Recognition Rate)","Hidden Markov models;Speech;Bayes methods;Time series analysis;Speech recognition;Acoustics;Wrapping","Bayes methods;belief networks;feature extraction;hidden Markov models;language translation;natural language processing;pattern classification;speech processing;speech recognition","automatic speech recognition algorithms;speech processing;digital signal processing;ASR system;Data Time Wrapping;Hidden Markov Model;Dynamic Bayesian Network;Machine Translation system;statistical-based approach;translation approaches;recognition rate;ASR level;translated sentences;feature extraction level;classification technique level;rule-approach;real time speech translation","","5","","43","IEEE","11 Jan 2018","","","IEEE","IEEE Conferences"
"Obscurity-Quantified Curriculum Learning for Machine Translation Evaluation","C. Zhang; D. F. Wong; E. S. K. Lei; R. Zhan; L. S. Chao","Natural Language Processing and Portuguese-Chinese Machine Translation Laboratory, University of Macau, Macau, China; Natural Language Processing and Portuguese-Chinese Machine Translation Laboratory, University of Macau, Macau, China; Natural Language Processing and Portuguese-Chinese Machine Translation Laboratory, University of Macau, Macau, China; Natural Language Processing and Portuguese-Chinese Machine Translation Laboratory, University of Macau, Macau, China; Natural Language Processing and Portuguese-Chinese Machine Translation Laboratory, University of Macau, Macau, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Jun 2023","2023","31","","2259","2271","The pre-trained language model has been developed for evaluating the quality of machine translation. It achieves state-of-the-art results. However, building a model for the evaluation of machine translation still faces the following challenges: 1) large scale of the training data affects the speed of the optimization; 2) the varied quality of the training data makes the optimization process unstable. To alleviate the issues of data learning, curriculum learning is proposed to rearrange the training sequence following an ‚Äúeasy-to-hard‚Äù process. However, the definition of difficulty can not be directly applied to the training data used in the machine translation evaluation. Hence, we propose an obscurity-quantified curriculum learning framework for this task. Specifically, the obscurity of each training example can be measured from multiple perspectives, including the difficulty of ranking, the fuzziness of reference, the complexity of text, and the unreliability of judgement. To incorporate the obscurity measurements, we also design a dynamic learning strategy to guide the training process from instances with low obscurity to those with high-obscurity. Experimental results show that our proposed methods yield remarkable improvements on the segment-level WMT2019 and WMT2020 Metrics Shared Tasks compared to other baseline methods.","2329-9304","","10.1109/TASLP.2023.3282105","Science and Technology Development Fund, Macau SAR(grant numbers:FDCT/070/2022/AMJ,FDCT/060/2022/AFJ); Multi-year Research Grant from the University of Macau(grant numbers:MYRG2020-00054-FST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145774","Curriculum learning;machine translation evaluation;obscurity-quantified","Measurement;Training;Task analysis;Machine translation;Comets;Speech processing;Data models","inference mechanisms;language translation;learning (artificial intelligence);natural language processing","data learning;dynamic learning strategy;easy-to-hard process;high-obscurity;low obscurity;machine translation evaluation;obscurity measurements;obscurity-quantified curriculum learning framework;optimization process;pre-trained language model;training data;training example;training process;training sequence;varied quality","","","","56","IEEE","7 Jun 2023","","","IEEE","IEEE Journals"
"Class-based named entity translation in a speech to speech translation system","S. R. Maskey; M. Cmejrek; B. Zhou; Y. Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","253","256","Named Entity (NE) Translation is a challenging problem in Machine Translation (MT). Most of the training bi-text corpora for MT lack enough samples of NEs to cover the wide variety of contexts NEs can appear in. In this paper, we present a technique to translate NEs based on their NE types in addition to a phrase-based translation model. Our NE translation model is based on a syntax-based system similar to [1]; but we produce syntax-based rules with non-terminals as NE types instead of general non-terminals. Such classbased rules allow us to better generalize the context NEs. We show that our proposed method obtains an improvement of 0.66 BLEU score absolute as well as 0.26% in F1-measure over the baseline of phrase-based model in NE test set.","","978-1-4244-3471-8","10.1109/SLT.2008.4777888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777888","Named Entities;Machine Translation;Class-based Models","Engines;Decoding;Speech coding;Training data;Surface-mount technology;Testing;Statistical analysis;Probability;Robustness;Context modeling","language translation;speech processing","class-based named entity translation;speech to speech translation system;machine translation;bitext corpora;phrase-based translation;syntax-based system;syntax-based rule","","","1","6","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Multilingual End-to-End Speech Translation","H. Inaguma; K. Duh; T. Kawahara; S. Watanabe","Graduate School of Informatics, Kyoto University, Kyoto, Japan; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","570","577","In this paper, we propose a simple yet effective framework for multilingual end-to-end speech translation (ST), in which speech utterances in source languages are directly translated to the desired target languages with a universal sequence-to-sequence architecture. While multilingual models have shown to be useful for automatic speech recognition (ASR) and machine translation (MT), this is the first time they are applied to the end-to-end ST problem. We show the effectiveness of multilingual end-to-end ST in two scenarios: one-to-many and many-to-many translations with publicly available data. We experimentally confirm that multilingual end-to-end ST models significantly outperform bilingual ones in both scenarios. The generalization of multilingual training is also evaluated in a transfer learning scenario to a very low-resource language pair. All of our codes and the database are publicly available to encourage further research in this emergent multilingual ST topic11Available at https://github.com/espnet/espnet..","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003832","Speech translation;multilingual end-to-end speech translation;attention-based sequence-to-sequence;transfer learning","Training;Task analysis;Decoding;Pipelines;Speech processing;Speech recognition;Training data","language translation;learning (artificial intelligence);natural language processing;speech recognition","multilingual end-to-end speech translation;machine translation;multilingual end-to-end ST models;automatic speech recognition;ASR;MT","","14","","59","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Sequence-to-Sequence Models for Emphasis Speech Translation","Q. T. Do; S. Sakti; S. Nakamura","Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Jun 2018","2018","26","10","1873","1883","Speech-to-speech translation (S2ST) systems are capable of breaking language barriers in cross-lingual communication by translating speech across languages. Recent studies have introduced many improvements that allow existing S2ST systems to handle not only linguistic meaning but also paralinguistic information such as emphasis by proposing additional emphasis estimation and translation components. However, the approach used for emphasis translation is not optimal for sequence translation tasks and fails to easily handle the long-term dependencies of words and emphasis levels. It also requires the quantization of emphasis levels and treats them as discrete labels instead of continuous values. Moreover, the whole translation pipeline is fairly complex and slow because all components are trained separately without joint optimization. In this paper, we make two contributions: 1) we propose an approach that can handle continuous emphasis levels based on sequence-to-sequence models, and 2) we combine machine and emphasis translation into a single model, which greatly simplifies the translation pipeline and make it easier to perform joint optimization. Our results on an emphasis translation task indicate that our translation models outperform previous models by a large margin in both objective and subjective tests. Experiments on a joint translation model also show that our models can perform joint translation of words and emphasis with one-word delays instead of full-sentence delays while preserving the translation performance of both tasks.","2329-9304","","10.1109/TASLP.2018.2846402","JSPS KAKENHI(grant numbers:JP17H06101,JP17K00237); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8379460","Emphasis estimation;emphasis translation;speech-to-speech translation (S2ST);joint optimization of words and emphasis","Pipelines;Task analysis;Speech processing;Hidden Markov models;Quantization (signal);Optimization;Linguistics","language translation;natural language processing;optimisation;speech processing;speech synthesis","Emphasis Speech Translation Speech-to-speech translation systems;S2ST systems;sequence translation tasks;joint optimization;sequence-to-sequence models;emphasis translation task;translation models;joint translation model;translation performance;emphasis estimation","","5","","32","IEEE","11 Jun 2018","","","IEEE","IEEE Journals"
"Neural Machine Translation With Explicit Phrase Alignment","J. Zhang; H. Luan; M. Sun; F. Zhai; J. Xu; Y. Liu","ByteDance AI Lab, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Fanyu Technology Company, Beijing, China; Sogou Inc., Beijing, China; Institute for Artificial Intelligence, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","3 Mar 2021","2021","29","","1001","1010","While neural machine translation has achieved state-of-the-art translation performance, it is unable to capture the alignment between the input and output during the translation process. The lack of alignment in neural machine translation models leads to three problems: it is hard to (1) interpret the translation process, (2) impose lexical constraints, and (3) impose structural constraints. These problems not only increase the difficulty of designing new architectures for neural machine translation, but also limit its applications in practice. To alleviate these problems, we propose to introduce explicit phrase alignment into the translation process of arbitrary neural machine translation models. The key idea is to build a search space similar to that of phrase-based statistical machine translation for neural machine translation where phrase alignment is readily available. We design a new decoding algorithm that can easily impose lexical and structural constraints. Experiments show that our approach makes the translation process of neural machine translation more interpretable without sacrificing translation quality. In addition, our approach achieves significant improvements in lexically and structurally constrained translation tasks.","2329-9304","","10.1109/TASLP.2021.3057831","National Key R&D Program of China(grant numbers:2017YFB0202204); National Natural Science Foundation of China(grant numbers:61925601,61772302); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351682","Alignment;machine translation;natural language processing;neural-networks","Machine translation;Decoding;Speech processing;Data models;Task analysis;Sun;Speech coding","decoding;language translation;neural net architecture;statistical analysis","decoding algorithm;translation performance;explicit phrase alignment;structurally constrained translation tasks;lexically constrained translation tasks;phrase-based statistical machine translation;arbitrary neural machine translation models","","5","","31","IEEE","9 Feb 2021","","","IEEE","IEEE Journals"
"Hypothesis ranking and two-pass approaches for machine translation system combination","D. Karakos; J. Smith; S. Khudanpur","Department of Electrical and Computer Engineering, Center for Language and Speech Processing, Baltimore, MD, USA; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA; Department of Computer Science, Johns Hopkins University, Baltimore, MD, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5202","5205","Given a number of machine translations of a source segment, the goal of system combination is to produce a new translation that has better quality than all of them. This paper describes a number of improvements that were recently added to the JHU system combination scheme: (i) A hypothesis ranking technique which orders the system outputs, on a per-segment basis, according to predicted translation quality, thus improving a subsequent incremental combination step. (ii) A two-pass combination procedure, which first produces several combination outputs with the given translations, and then performs one more combination step with these new outputs. Results from the NIST MT09 informal system combination evaluation on Arabic-to-English and Urdu-to-English1 show that both approaches offer significant BLEU and TER gains over a baseline JHU combination scheme.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5494996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494996","Machine Translation;System Combination;Confusion Network Decoding;Hypothesis Ranking","Skeleton;NIST;Decoding;Error correction;Joints;Space exploration;Computer networks;Support vector machines;Natural languages;Speech processing","language translation;natural language processing","hypothesis ranking;two-pass approaches;machine translation system combination;source segment;JHU system combination;translation quality;NIST MT09 informal system combination;Arabic-to-English translation;Urdu-to-English translation;BLEU gains;TER gains","","","","16","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"From Unsupervised Machine Translation to Adversarial Text Generation","A. Rashid; A. Do-Omri; M. A. Haidar; Q. Liu; M. Rezagholizadeh","Huawei Noah‚Äôs Ark Lab, Montreal Research Centre, Canada; Huawei Noah‚Äôs Ark Lab, Montreal Research Centre, Canada; Huawei Noah‚Äôs Ark Lab, Montreal Research Centre, Canada; Huawei Noah‚Äôs Ark Lab, Montreal Research Centre, Canada; Huawei Noah‚Äôs Ark Lab, Montreal Research Centre, Canada","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8194","8198","We present a self-attention based bilingual adversarial text generator (B-GAN) which can learn to generate text from the encoder representation of an unsupervised neural machine translation system. B-GAN is able to generate a distributed latent space representation which can be paired with an attention based decoder to generate fluent sentences. When trained on an encoder shared between two languages and paired with the appropriate decoder, it can generate sentences in either language. B-GAN is trained using a combination of reconstruction loss for auto-encoder, a cross domain loss for translation and a GAN based adversarial loss for text generation. We demonstrate that B-GAN, trained on monolingual corpora only using multiple losses, generates more fluent sentences compared to monolingual baselines while effectively using half the number of parameters.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053236","GAN;Adversarial Training;Machine Translation;Text Generation","Training;Signal processing;Generative adversarial networks;Decoding;Machine translation;Gallium nitride;Speech processing","language translation;natural language processing;neural nets;text analysis;unsupervised learning","monolingual baselines;monolingual corpora;GAN based adversarial loss;attention based decoder;distributed latent space representation;unsupervised neural machine translation system;encoder representation;self-attention based bilingual adversarial text generator;adversarial text generation;unsupervised machine translation;cross domain loss;auto-encoder;reconstruction loss;B-GAN;fluent sentences","","1","","27","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Khmer Speech Translation Corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC)","K. Soky; M. Mimura; T. Kawahara; S. Li; C. Ding; C. Chu; S. Sam","Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Cambodia Academy of Digital Technology (CADT), Phnom Penh, Cambodia","2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","3 Jan 2022","2021","","","122","127","Speech translation (ST) is a subject of rapidly increasing interest in the area of speech processing research. This interest is apparent from the increasing tools and corpora for this task. However, the lack of sufficient datasets is still the biggest challenge for under-resourced languages. Specifically, ST requires a large corpus of parallel speech, transcription, and translation text. In this work, we construct a large corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC), including simultaneous translation from Khmer into English and French. We also address the problem of sentence segmentation of Khmer by conducting a bilingual sentence alignment from English to Khmer with a monotonic assumption. This corpus has approximately 155 hours of speech in length and 1.7M words of text. We also report the baseline results of automatic speech recognition (ASR), machine translation, and ST systems, which show reasonable performance.","2472-7695","978-1-6654-0870-7","10.1109/O-COCOSDA202152914.2021.9660421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660421","Khmer language;low-resource language;spoken language translation corpus;court dataset","TV;Databases;Machine translation;Speech processing;Task analysis;Automatic speech recognition","language translation;natural language processing;speech processing;speech recognition;text analysis","English;bilingual sentence alignment;machine translation;ST systems;Khmer speech translation corpus;ECCC;underresourced languages;parallel speech;translation text;extraordinary chambers in the Courts of Cambodia;speech processing research;French;Khmer sentence segmentation;automatic speech recognition;ASR","","3","","24","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Challenges with Rapid Adaptation of Speech Translation Systems to New Language Pairs","T. Schultz; A. W. Black","Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon University, USA; Interactive Systems Laboratories, Language Technologies Institute, Carnegie Mellon University, USA","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","Although we have far from solved the issues in porting speech translation systems to new languages, we have gathered sufficient experience by now to identify a number of major challenges in the process. Although well-defined processes exist for building speech recognition, speech synthesis and statistical machine translation models, they still require both significant native speaker involvement and linguistic expertise. As the core technology improves we believe we will see increasing cultural and social issues in contributions from native speakers. This paper identifies some of these issues and presents our initial attempts to build tools that we hope will eventually allow linguistically naive native informants build complete speech translation systems","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661500","","Natural languages;Speech synthesis;Automatic speech recognition;Speech processing;Cultural differences;Interactive systems;Laboratories;Speech recognition;Bridges;Costs","language translation;natural languages;speech recognition;speech synthesis;statistical analysis","speech translation systems;language pairs;speech recognition;speech synthesis;statistical machine translation models","","7","","17","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Name aware speech-to-speech translation for English/Iraqi","R. Prasad; C. Moran; F. Choi; R. Meermeier; S. Saleem; C. -l. Kao; D. Stallard; P. Natarajan","BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","249","252","In this paper, we describe a novel approach that exploits intra-sentence and dialog-level context for improving translation performance on spoken Iraqi utterances that contain named entities (NEs). Dialog-level context is used to predict whether the Iraqi response is likely to contain names and the intra-sentence context is used to determine words that are named entities. While we do not address the problem of translating out-of-vocabulary (OOV) NEs in spoken utterances, we show that our approach is capable of translating OOV names in text input. To demonstrate efficacy of our approach, we present results on internal test set as well as the 2008 June DARPA TRANSTAC name evaluation set.","","978-1-4244-3471-8","10.1109/SLT.2008.4777887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777887","speech recognition;machine translation;named entity detection;name transliteration","Automatic speech recognition;Natural languages;Engines;Surface-mount technology;Speech synthesis;Vocabulary;Context awareness;Speech recognition;Hidden Markov models;Testing","language translation;speech processing","name aware speech-to-speech translation;intra-sentence context;dialog-level context;translation performance;spoken Iraqi utterances;out-of-vocabulary named entities;spoken utterances;name evaluation set","","2","","12","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Applying log linear model based context dependent machine translation techniques to grapheme-to-phoneme conversion","R. Zhang; B. Zhou","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","4634","4637","Grapheme-to-Phoneme conversion is a challenging task for speech recognition and text-to-speech systems for which the functionality of automatically predicting pronunciations for OOV words is highly desirable. In this paper, Grapheme-to-Phoneme conversion is viewed as a special case of sequence translation problem and we propose to tackle it with phrase based log-linear translation model. We improve standard machine translation method by utilizing context dependent units which lead to a better many-to-many alignment between chunks of graphemes and phonemes. Furthermore, hypotheses combination technique is applied to combine outputs generated by multiple translation models trained with different alignment units. Our proposed approach was evaluated on NetTalk and CMUDict datasets. Significant improvements on conversion accuracy are observed on both sets compared to conventional translation method: phoneme level error rates are reduced relatively by 18.4% and 22.5%, respectively. Our approach also performs better than or as good as previously published data driven methods examined on the same tasks.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495551","Grapheme-to-Phone conversion","Context modeling;Predictive models;Hidden Markov models;Speech recognition;Prediction methods;Speech synthesis;Error analysis;Dictionaries;Machine learning algorithms;Classification tree analysis","language translation;speech recognition;speech synthesis","log linear model;context dependent machine translation techniques;grapheme-to-phoneme conversion;speech recognition;text-to-speech systems;OOV words;sequence translation problem;multiple translation models;NetTalk datasets;CMUDict datasets","","1","","16","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Integrating Speech Recognition and Machine Translation: Where do We Stand?","E. Matusov; S. Kanthak; H. Ney","Lehrstuhl f√ºr Informatik VI-Computer Science Department, RWTH Aachen University, Aachen, Germany; Lehrstuhl f√ºr Informatik VI-Computer Science Department, RWTH Aachen University, Aachen, Germany; Lehrstuhl f√ºr Informatik VI-Computer Science Department, RWTH Aachen University, Aachen, Germany","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","This paper describes state-of-the-art interfaces between speech recognition and machine translation. We modify two different machine translation systems to effectively process dense speech recognition lattices. In addition, we describe how to fully integrate speech translation with machine translation based on weighted finite-state transducers. With a thorough set of experiments, we show that both the acoustic model scores and the source language model positively and significantly affect the translation quality. We have found consistent improvements on three different corpora compared with translations of single best recognition results","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661501","","Speech recognition;Automatic speech recognition;Lattices;Natural languages;Computer science;Acoustic transducers","language translation;speech recognition","speech recognition;machine translation;weighted finite-state transducers;acoustic model;source language model","","14","2","14","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Lexicalized reordering in multiple-graph based statistical machine translation","Bowen Zhou; Rong Zhang; Yuqing Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","4981","4984","Reordering is one of the key problems in statistical machine translation (SMT). This paper first presents how lexicalized reordering is embedded into a phrase-based SMT framework modeled by multiple-graph that was formulated in our previous work. Specifically, we show how lazy reordering graph is computed and combined with our previously proposed multiple layer search (MLS) to achieve an efficient reordering decoding that is also flexible to take various reordering models. Secondly, we introduce a variety of lexicalized reordering models employed in our system that significantly improved system performance.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518776","speech translation;multiple-graph decoding;lexicalized reordering;finite automata;machine translation","Surface-mount technology;Decoding;Multilevel systems;Speech;Clustering algorithms;System performance;Automata;NP-complete problem;Viterbi algorithm;Minimization methods","decoding;language translation;speech processing;statistical analysis","lexicalized reordering;multiple-graph based translation;statistical machine translation;lazy reordering graph;reordering decoding","","1","","11","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"Incorporating Statistical Machine Translation Word Knowledge Into Neural Machine Translation","X. Wang; Z. Tu; M. Zhang","Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Tencent AI Lab, Shenzhen, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 Aug 2018","2018","26","12","2255","2266","Neural machine translation (NMT) has gained more and more attention in recent years, mainly due to its simplicity yet state-of-the-art performance. However, previous research has shown that NMT suffers from several limitations: source coverage guidance, translation of rare words, and the limited vocabulary, while statistical machine translation (SMT) has complementary properties that correspond well to these limitations. It is straightforward to improve the translation performance by combining the advantages of two kinds of models. This paper proposes a general framework for incorporating the SMT word knowledge into NMT to alleviate above word-level limitations. In our framework, the NMT decoder makes more accurate word prediction by referring to the SMT word recommendations in both training and testing phases. Specifically, the SMT model offers informative word recommendations based on the NMT decoding information. Then, we use the SMT word predictions as prior knowledge to adjust the NMT word generation probability, which unitizes a neural network based classifier to digest the discrete word knowledge. In this paper, we use two model variants to implement the framework, one with a gating mechanism and the other with a direct competition mechanism. Experimental results on Chinese-to-English and English-to-German translation tasks show that the proposed framework can take advantage of the SMT word knowledge and consistently achieve significant improvements over NMT and SMT baseline systems.","2329-9304","","10.1109/TASLP.2018.2860287","National Natural Science Foundation of China (NSFC)(grant numbers:61525205,61432013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421063","Neural machine translation;statistical machine translation;hybrid translation;translation combination","Decoding;Knowledge engineering;Modeling;Neural networks;Speech processing;Vocabulary;Training;Statistical learning","language translation;natural language processing;neural nets;probability;statistical analysis;vocabulary","NMT decoder;SMT word recommendations;NMT decoding information;NMT word generation probability;neural network;English-to-German translation tasks;SMT word knowledge;neural machine translation;vocabulary;statistical machine translation;neural network based classifier;Chinese-to-Englishh translation","","32","","64","IEEE","26 Jul 2018","","","IEEE","IEEE Journals"
"Speech into Sign Language Statistical Translation System for Deaf People","R. San Segundo; B. Gallo; J. M. Lucas; R. Barra-Chicote; L. F. D'Haro; F. Fernandez","Grupo de Tecnolog√≠a del Habla del Departamento de Ingenier√≠a Electr√≥nica, Universidad Polit√©cnica de Madrid, Madrid, Spain; Grupo de Tecnolog√≠a del Habla del Departamento de Ingenier√≠a Electr√≥nica, Universidad Polit√©cnica de Madrid, Madrid, Spain; Grupo de Tecnolog√≠a del Habla del Departamento de Ingenier√≠a Electr√≥nica, Universidad Polit√©cnica de Madrid, Madrid, Spain; Grupo de Tecnolog√≠a del Habla del Departamento de Ingenier√≠a Electr√≥nica, Universidad Polit√©cnica de Madrid, Madrid, Spain; Universidad Politecnica de Madrid, Madrid, Comunidad de Madrid, ES; Grupo de Tecnolog√≠a del Habla del Departamento de Ingenier√≠a Electr√≥nica, Universidad Polit√©cnica de Madrid, Madrid, Spain","IEEE Latin America Transactions","17 Nov 2009","2009","7","3","400","404","This paper presents a set of experiments used to develop a statistical system from translating speech to sign language for deaf people. This system is composed of an Automatic Speech Recognition (ASR) system, followed by a statistical translation module and an animated agent that represents the different signs. Two different approaches have been used to perform the translations: a phrase-based system and a finite state transducer. For the evaluation, the followings figures have been considered: WER (Word Error Rate), BLEU and NIST. The paper presents translation results of reference sentences and sentences from the Automatic Speech Recognizer. Also three different configurations have been evaluated for the Speech Recognizer. The best results were obtained with the finite state transducer, with a word error rate of 28.21% for the reference text, and 29.27% using the ASR output.","1548-0992","","10.1109/TLA.2009.5336641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336641","alignment;Finite State Transducer;Language Model;phrase;Sign Language;Statistical Machine Translation;Translation Model;word error rate","Telecommunications;Handicapped aids;Deafness;Automatic speech recognition;Transducers;Error analysis;Animation;NIST;Speech analysis;Speech recognition","language translation;speech recognition","statistical machine translation;automatic speech recognition;deaf people;sign language;language model;translation model;word error rate","","9","1","7","IEEE","17 Nov 2009","","","IEEE","IEEE Journals"
"Quality Evaluation Model of Automatic Machine Translation based on Deep Learning Algorithm","N. Peng; Y. Wang; Y. Wei; L. Liu; L. Wang; Y. Wang","Department of Basic Teaching and Research, East University of HeiLongjiang, Harbin, China; Department of Humanities and Social Sciences, East University of HeiLongjiang, Harbin, China; Department of Basic Teaching and Research, East University of HeiLongjiang, Harbin, China; Department of Basic Teaching and Research, East University of HeiLongjiang, Harbin, China; Department of Basic Teaching and Research, East University of HeiLongjiang, Harbin, China; Department of Basic Teaching and Research, East University of HeiLongjiang, Harbin, China","2023 4th International Conference for Emerging Technology (INCET)","10 Jul 2023","2023","","","1","5","With the continuous development of computer technology, machine translation methods have also experienced a long research process. In recent years, the research of artificial neural network has brought new solutions to machine translation. The application of sequence sequence model has made a qualitative leap in the performance of machine translation. The training of neural machine translation model depends on large-scale bilingual parallel data, which contains sufficient knowledge for machine learning. The training process is the process of data representation and knowledge extraction. How to use data enhancement methods to make model learning easier and knowledge extraction more sufficient is an important research topic. Automatic machine translation quality evaluation model based on deep learning algorithm is a method to evaluate the quality of automatic machine translation. This paper introduces the basic idea of qemt and some examples. We also give an example to illustrate how to use it to evaluate the quality of automatic machine translation. Qemt (quality evaluation model) is a method to evaluate the quality of automatic machine translation by comparing human translations generated by different natural language processing models such as part of speech markers or semantic similarity measures.","","979-8-3503-3575-0","10.1109/INCET57972.2023.10169967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10169967","Deep learning;Automatic machine translation;Quality assessment","Training;Deep learning;Reinforcement learning;Predictive models;Transformers;Data models;Machine translation","deep learning (artificial intelligence);language translation;learning (artificial intelligence);natural language processing;neural nets","automatic machine translation quality evaluation model;deep learning algorithm;different natural language processing models;human translations;machine learning;machine translation methods;model learning;neural machine translation model;qemt;sequence sequence model","","","","10","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Towards End-to-end Speech-to-text Translation with Two-pass Decoding","T. -W. Sung; J. -Y. Liu; H. -y. Lee; L. -s. Lee",National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","7175","7179","Speech-to-text translation (ST) refers to transforming the audio in source language to the text in target language. Mainstream solutions for such tasks are to cascade automatic speech recognition with machine translation, for which the transcriptions of the source language are needed in training. End-to-end approaches for ST tasks have been investigated because of not only technical interests such as to achieve globally optimized solution, but the need for ST tasks for the many source languages worldwide which do not have written form. In this paper, we propose a new end-to-end ST framework with two decoders to handle the relatively deeper relationships between the source language audio and target language text. The first-pass decoder generates some useful latent representations, and the second-pass decoder then integrates the output of both the encoder and the first-pass decoder to generate the text translation in target language. Only paired source language audio and target language text are used in training. Preliminary experiments on several language pairs showed improved performance, and offered some initial analysis.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682801","Speech-to-Text Translation;End-to-End Model;Unwritten Language","Decoding;Training;Indexes;Testing;Task analysis;Acoustics;Feature extraction","decoding;language translation;speech recognition;speech synthesis","automatic speech recognition;machine translation;end-to-end approaches;ST tasks;globally optimized solution;source languages worldwide;end-to-end ST framework;decoders;target language text;first-pass decoder;second-pass decoder;paired source language audio;language pairs;end-to-end speech-to-text translation;two-pass decoding;mainstream solutions","","8","","21","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Refining History for Future-Aware Neural Machine Translation","X. Lyu; J. Li; M. Zhang; C. Ding; H. Tanaka; M. Utiyama","School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Dec 2022","2023","31","","500","512","Neural machine translation uses a decoder to generate target words auto-regressively by predicting the next target word conditioned on a given source sentence and its previously predicted target words, i.e, its translation history, which suffers from two limitations: 1) the prediction of next word depends heavily on the quality of its history information. Moreover, the discrepancy between training and inference exacerbates this limitation; 2) this left-to-right decoding way cannot make full use of the target-side future information, which leads to the issue of unbalanced outputs. On the one hand, we alleviate the first limitation with a history-refining module, which learns to examine the quality of each history word by assigning it a confidence score. The confidence score is further used as a gate to control the amount of its word embedding flowing to the decoder. On the other hand, we attack the second limitation with a future-foreseeing module, which learns the distribution of future translation at each decoding time step. More importantly, we further propose refining history for future-aware NMT since the two modules can be closely incorporated as they focus on different kinds of context. Experimental results on various translation tasks with different scaled datasets, including WMT English$\leftrightarrow${German, French, Romanian}, show that our proposed approach achieves significant improvements over strong Transformer-based NMT baselines.","2329-9304","","10.1109/TASLP.2022.3226332","National Natural Science Foundation of China(grant numbers:62036004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989330","Neural machine translation;refine history;foresee future;mutual promotion;left-to-right decoding","Training;Refining;Logic gates;Transformers;Decoding;History;Machine translation","language translation;learning (artificial intelligence);neural nets;regression analysis;word processing","confidence score;decoding time step;future-aware neural machine translation;future-aware NMT;future-foreseeing module;history information;history-refining module;inference exacerbates;source sentence;target words auto-regressively;target-side future information;translation history;word embedding","","","","48","IEEE","15 Dec 2022","","","IEEE","IEEE Journals"
"Measuring human readability of machine generated text: three case studies in speech recognition and machine translation","D. Jones; E. Gibson; W. Shen; N. Granoien; M. Herzog; D. Reynolds; C. Weinstein","MIT Lincoln Laboratories, USA; Department of Brain and Cognitive Sciences, MIT, USA; MIT Lincoln Laboratories, USA; Defense Language Institute, Foreign Language Center; Defense Language Institute, Foreign Language Center; MIT Lincoln Laboratories, USA; MIT Lincoln Laboratories, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","5","","v/1009","v/1012 Vol. 5","We present highlights from three experiments that test the readability of current state-of-the art system output from: (1) an automated English speech-to-text (SST) system; (2) a text-based Arabic-to-English machine translation (MT) system; and (3) an audio-based Arabic-to-English MT process. We measure readability in terms of reaction time and passage comprehension in each case, applying standard psycholinguistic testing procedures and a modified version of the standard defense language proficiency test for Arabic called the DLPT*. We learned that: (1) subjects are slowed down by about 25% when reading system STT output; (2) text-based MT systems enable an English speaker to pass Arabic Level 2 on the DLPT*; and (3) audio-based MT systems do not enable English speakers to pass Arabic Level 2. We intend for these generic measures of readability to predict performance of more application-specific tasks.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1416477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416477","","Anthropometry;Humans;Speech recognition;Measurement standards;Automatic testing;System testing;Art;Time measurement;Psychology;Natural languages","speech recognition;language translation;linguistics","defense language proficiency test;machine generated text human readability measurement;speech recognition;automated English speech-to-text system;text-based Arabic-to-English machine translation;audio-based Arabic-to-English MT process;reaction time;passage comprehension;psycholinguistic testing procedures","","7","","7","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Analysis and effect of speaking style for dialogue speech recognition","K. Aono; K. Yasuda; T. Takezawa; S. Yamamoto; M. Yanagida","Graduate School of Engineering, Doshisha University, Kyotanabe, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; Graduate School of Engineering Doshisha University, Kyotanabe, Kyoto, Japan","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","339","344","This paper analyzes acoustic likelihood calculated from two acoustic models, a spontaneous speech acoustic model and a read speech acoustic model, from the viewpoint of linguistic information, such as word category and language likelihood. Experimental results show a significant tendency in the relationship between speaking style and linguistic information. According to the analysis results, a word's acoustic likelihood calculated from the spontaneous speech acoustic model is higher, or more suitable, than that from the read speech acoustic model in the case when the word is an interjection or an auxiliary verb. On the other hand, even in human-to-human conversation, a word's acoustic likelihood calculated from the read speech acoustic model can be higher than that from the spontaneous speech acoustic model in the case when the word is a noun. Applying this knowledge along with machine learning, post-processing experiments of the results of ASR using these two acoustic models are carried out. In this set of experiments, post-processing, based on a support vector machine, is applied. The experimental results show that the selection scheme, based on word category, reduces word error rate by 1.62 points over the single system.","","0-7803-7980-2","10.1109/ASRU.2003.1318464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318464","","Speech analysis;Speech recognition;Automatic speech recognition;Natural languages;Machine learning;Decoding;Laboratories;Acoustical engineering;Information analysis;Support vector machines","speech recognition;support vector machines;natural languages;learning (artificial intelligence);error statistics","support vector machine;speaking style effects;dialogue speech recognition;acoustic likelihood;spontaneous speech acoustic model;read speech acoustic model;linguistic information;word category selection scheme;language likelihood;interjection;auxiliary verb;noun;machine learning;word error rate reduction","","1","","10","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Ignorance is Bliss: Exploring Defenses Against Invariance-Based Attacks on Neural Machine Translation Systems","A. Chaturvedi; A. Chakrabarty; M. Utiyama; E. Sumita; U. Garain","Indian Statistical Institute, Kolkata, India; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Advanced Speech Translation Research and Development Promotion Center, National Institute of Information and Communications Technology, Kyoto, Japan; Indian Statistical Institute, Kolkata, India","IEEE Transactions on Artificial Intelligence","22 Jul 2022","2022","3","4","518","525","This article addresses an invariance-based attack on the transformer, a state-of-the-art neural machine translation (NMT) system. Such attacks make multiple changes to the source sentence with the goal of keeping the predicted translation unchanged. Since the gold translation is not available for the adversarial sentences, tackling invariance-based attacks is a challenging task. We propose two contrasting defense strategies for the same, learn to deal and learn to ignore. In learn to deal, NMT system is trained not to predict the same translation for a clean text and its noisy counterpart, whereas in learn to ignore, NMT system is trained to output a dummy sentence in the target language whenever it encounters a noisy text. The experiments on two language pairs, English‚ÄìGerman (en‚Äìde) and English‚ÄìFrench (en‚Äìfr), show that learn to deal strategy reduces the attack success rate from 84.0% to 62.2% for en‚Äìde and from 84.6% to 73.8% for en‚Äìfr, whereas learn to ignore strategy reduces the attack success rate from 84.0% to 27.2% for en‚Äìde and from 84.6% to 37.0% for en‚Äìfr.","2691-4581","","10.1109/TAI.2021.3123931","Science and Engineering Research Board; Department of Science and Technology, Government of India(grant numbers:SPR/2020/000495); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612034","Adversarial robustness;deep learning;neural machine translation (NMT)","Noise measurement;Gold;Transformers;Task analysis;Machine translation;Training;Standards","language translation;learning (artificial intelligence);natural language processing","NMT system;attack success rate;invariance-based attack;neural machine translation systems;state-of-the-art neural machine translation system;predicted translation unchanged;gold translation;contrasting defense strategies","","1","","23","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"Syllable-to-Syllable and Word-to-Word Transducers for Burmese Dialect Translation","T. Myint Oo; T. Tanprasert; Y. Kyaw Thu; T. Supnithi","Assumption University, Bangkok, Thailand; Assumption University, Bangkok, Thailand; NECTEC, Pathumthani, Thailand; NECTEC, Pathumthani, Thailand","2022 17th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)","30 Nov 2022","2022","","","1","6","Weighted Finite State Transducers (WFST) can be very efficient to implement Burmese dialects translation. We illustrate this on two Burmese dialect language pairs, Burmese-Beik and Burmese-Rakhine. In this study, we examine syllable and word segmentation schemes and their effect on alignment and transducing between dialect language pairs. We performed alignments with Anymalign, fastalign, pialign, Hieralign, eflomal and GIZA ++ approaches and implemented WFST based machine translation system with OpenFst library. From the overall results, syllable segmentation achieved higher BLEU and chrF scores for Burmese-Rakhine and Rakhine-Burmese translations. However, word segmentation achieved better translation performance for Burmese-Beik and Beik-Burmese translation directions. Alignment techniques fast align, Hieralign, eflomal and GIZA ++ are working well for low-resource Burmese dialects.","2831-4565","978-1-6654-5727-9","10.1109/iSAI-NLP56921.2022.9960259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9960259","WFST;Machine Translation;Word Alignment;Myanmar Dialects;Low-resource Lan-guages","Transducers;Libraries;Machine translation;Artificial intelligence","finite state machines;language translation;natural language processing;speech recognition","Beik-Burmese translation directions;Burmese dialect language pairs;Burmese dialect translation;Burmese dialects translation;Burmese-Beik;Burmese-Rakhine;eflomal GIZA;low-resource Burmese dialects;Rakhine-Burmese translations;syllable segmentation;transducing;translation performance;Weighted Finite State Transducers;WFST based machine translation system;word segmentation schemes;word-to-word Transducers","","","","34","IEEE","30 Nov 2022","","","IEEE","IEEE Conferences"
"Efficient data selection for machine translation","A. Mandal; D. Vergyri; W. Wang; J. Zheng; A. Stolcke; G. Tur; D. Hakkani-Tur; N. F. Ayan","Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; International Computer Science Institute, Berkeley, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","261","264","Performance of statistical machine translation (SMT) systems relies on the availability of a large parallel corpus which is used to estimate translation probabilities. However, the generation of such corpus is a long and expensive process. In this paper, we introduce two methods for efficient selection of training data to be translated by humans. Our methods are motivated by active learning and aim to choose new data that adds maximal information to the currently available data pool. The first method uses a measure of disagreement between multiple SMT systems, whereas the second uses a perplexity criterion. We performed experiments on Chinese-English data in multiple domains and test sets. Our results show that we can select only one-fifth of the additional training data and achieve similar or better translation performance, compared to that of using all available data.","","978-1-4244-3471-8","10.1109/SLT.2008.4777890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777890","machine translation;data selection","Surface-mount technology;Training data;Humans;Probability;Information retrieval;Availability;Natural languages;Web pages;System testing;Speech","language translation;learning (artificial intelligence);natural language processing;probability;statistical analysis","data selection;statistical machine translation systems;parallel corpus;translation probability;training data;active learning;data pool;perplexity criterion;Chinese-English data;translation performance","","6","1","20","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Use of statistical N-gram models in natural language generation for machine translation","Fu-Hua Liu; Liang Gu; Yuqing Gao; M. Picheny","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).","21 May 2003","2003","1","","I","I","Various language modeling issues in a speech-to-speech translation system are described in this paper. First, the language models for the speech recognizer need to be adapted to the specific domain to improve the recognition performance for in-domain utterances, while keeping the domain coverage as broad as possible. Second, when a maximum entropy based statistical natural language generation model is used to generate target language sentence as the translation output, serious inflection and synonym issues arise, because the compromised solution is used in semantic representation to avoid the data sparseness problem. We use N-gram models as a postprocessing step to enhance the generation performance. When an interpolated language model is applied to a Chinese-to-English translation task, the translation performance, measured by an objective metric of BLEU, improves substantially to 0.514 from 0.318 when we use the correct transcription as input. Similarly, the BLEU score is improved to 0.300 from 0.194 for the same task when the input is speech data.","1520-6149","0-7803-7663-3","10.1109/ICASSP.2003.1198861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198861","","Natural languages;Speech recognition;Speech synthesis;Entropy;Data mining;Internet;Globalization;Switches;Scheduling;Speech processing","speech recognition;language translation;statistical analysis;natural language interfaces;maximum entropy methods;speech processing","statistical N-gram models;natural language generation;machine translation;language modeling;speech-to-speech translation system;language models;speech recognizer;recognition performance;in-domain utterances;maximum entropy;target language sentence;postprocessing step;interpolated language model;Chinese-to-English translation task;BLEU;inflection;synonym;semantic representation","","4","","11","IEEE","21 May 2003","","","IEEE","IEEE Conferences"
"Transformer-based Machine Translation for Low-resourced Languages embedded with Language Identification","T. J. Sefara; S. G. Zwane; N. Gama; H. Sibisi; P. N. Senoamadi; V. Marivate","Next Generation Enterprises and Institutions, Council for Scientific and Industrial Research, Pretoria, South Africa; Department of Computer Science, University of Zululand, South Africa; School of Computer Science and Applied Mathematics, University of the Witwatersrand, South Africa; Department of Computer Science, University of Johannesburg, South Africa; Department of Mathematics, University of Zululand, South Africa; Department of Computer Science, University of Pretoria, South Africa","2021 Conference on Information Communications Technology and Society (ICTAS)","6 Apr 2021","2021","","","127","132","Recent research on the development of machine translation (MT) models has resulted in state-of-the-art performance for many resourced European languages. However, there has been a little focus on applying these MT services to low-resourced languages. This paper presents the development of neural machine translation (NMT) for low-resourced languages of South Africa. Two MT models, JoeyNMT and transformer NMT with self-attention are trained and evaluated using BLEU score. The transformer NMT with self-attention obtained state-of-the-art performance on isiNdebele, SiSwati, Setswana, Tshivenda, isiXhosa, and Sepedi while JoeyNMT performed well on isiZulu. The MT models are embedded with language identification (LID) model that presets the language for translation models. The LID models are trained using logistic regression and multinomial naive Bayes (MNB). MNB classifier obtained an accuracy of 99% outperforming logistic regression which obtained the lowest accuracy of 97%.","","978-1-7281-8081-6","10.1109/ICTAS50802.2021.9394996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394996","machine translation;low-resourced languages;neural network;language identification","Buildings;Europe;Communications technology;Machine translation;Logistics","Bayes methods;language translation;learning (artificial intelligence);natural language processing;natural languages;pattern classification;regression analysis;speech recognition","transformer-based machine translation;low-resourced languages;machine translation models;resourced European languages;MT services;neural machine translation;MT models;transformer NMT;language identification model","","4","","17","IEEE","6 Apr 2021","","","IEEE","IEEE Conferences"
"Integrating Multiple ASR Systems into NLP Backend with Attention Fusion","T. Kano; A. Ogawa; M. Delcroix; S. Watanabe","NTT Corporation, Japan; NTT Corporation, Japan; NTT Corporation, Japan; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6237","6241","Spoken language processing (SLP) systems such as speech summarization and translation can be achieved by cascade models. It combines an automatic speech recognition (ASR) frontend and a natural language processing (NLP) backend including machine translation (MT) or text summarization (TS). With this cascade approach, we can exploit large non-paired datasets to independently train state-of-the-art models for each module. However, ASR errors directly affect the performance of the NLP backend in the cascade approach. In this paper, we reduce the impact of ASR errors on the NLP back-end by combining transcriptions from various ASR systems. Recognizer output voting error reduction (ROVER) is a widely used technique for system combination. Although ROVER improves ASR performance, the combination process is not optimized for backend tasks. We propose a system combination that resembles ROVER using attention fusion to achieve the alignment and the combination of multiple ASR hypotheses. This allows the combination process to be optimized for the backend NLP task without changing the ASR frontend. Our proposed technique is general and can be applied to various SLP tasks. We confirm its effectiveness on both speech summarization and translation experiments.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746699","Speech summarization;Speech translation;Automatic speech recognition;Attention fusion;ROVER","Conferences;Signal processing;Natural language processing;Acoustics;Machine translation;Speech processing;Task analysis","language translation;natural language processing;sensor fusion;speech recognition;text analysis","ASR systems;NLP backend;attention fusion;spoken language processing systems;automatic speech recognition;natural language processing;text summarization;ROVER;SLP;machine translation;recognizer output voting error reduction","","1","","32","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Text-to-Speech translation using Support Vector Machine, an approach to find a potential path for human-computer speech synthesizer","Rashmi S; Hanumanthappa M; Jyothi N M","Department of Computer Science and Applications, Bangalore University, Bangalore, India; Department of Computer Science and Applications, Bangalore University, Bangalore, India; Department of Master of Computer Applications, Bapuji Institute of Engineering and Technology, Davangere, India","2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","15 Sep 2016","2016","","","1311","1315","Text-to-Speech (TTS), an astounding feature to assemble computer with intelligence and to induce sound is seemingly a challenging task as it is related to the propagation of uncertainty with the input. This is because TTS evolutes the input based on the probabilities and not with certainty ratios. TTS is accomplished by generating the sound structure/phoneme and then classifying these phonemes in the phonetic dictionary. The Wards' algorithms, BIRCH, Support Vector Machine (SVM) are used to figure out the appropriate sound representation for the given context. To distinguish correct elocution, the SVM procedures are equipped with the principles of pruning. The output was analyzed using divergent stages of uncertainty. In order to study the effect of the output 10 listeners were considered for determining Signal-to-Noise (SNR) ratio. SNR shows that the errors of both type phase and uncertainty were approximately 6% resulting 94% of accuracy. These results manifested that SVM stratagem can be used to obtain better results for TTS synthesizer.","","978-1-4673-9338-6","10.1109/WiSPNET.2016.7566349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7566349","BIRCH;Phonetic Analysis;Prosody;Support Vector Machine (SVM);Wards' Algorithm","Support vector machines;Dictionaries;Speech;Synthesizers;Computers;Uncertainty;Classification algorithms","signal representation;speech processing;speech synthesis;support vector machines","text-to-speech translation;TTS synthesizer;support vector machine;SVM;human computer speech synthesizer;sound structure;sound phoneme;phonetic dictionary;Wards algorithms;BIRCH;sound representation;pruning principles;signal-to-noise ratio;SNR ratio","","2","","7","IEEE","15 Sep 2016","","","IEEE","IEEE Conferences"
"Quality estimation for asr k-best list rescoring in spoken language translation","R. W. M. Ng; K. Shah; W. Aziz; L. Specia; T. Hain","Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5226","5230","Spoken language translation (SLT) combines automatic speech recognition (ASR) and machine translation (MT). During the decoding stage, the best hypothesis produced by the ASR system may not be the best input candidate to the MT system, but making use of multiple sub-optimal ASR results in SLT has been shown to be too complex computationally. This paper presents a method to rescore the k-best ASR output such as to improve translation quality. A translation quality estimation model is trained on a large number of features which aim to capture complementary information from both ASR and MT on translation difficulty and adequacy, as well as syntactic properties of the SLT inputs and outputs. Based on the predicted quality score, the ASR hypotheses are rescored before they are fed to the MT system. ASR confidence is found to be crucial in guiding the rescoring step. In an English-to-French speech-to-text translation task, the coupling of ASR and MT systems led to an increase of 0.5 BLEU points in translation quality.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7178968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178968","Spoken language translation;Quality estimation;System integration","Feature extraction;Decoding;Estimation;Speech;Acoustics;Lattices;Training","computational linguistics;language translation;natural language processing;speech processing;speech recognition","ASR k-best list rescoring;spoken language translation;automatic speech recognition;machine translation;decoding stage;k-best ASR output;translation quality estimation model;complementary information;translation difficulty;translation adequacy;syntactic properties;English-to-French speech-to-text translation task","","4","1","30","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Analysis of Learning Approaches for Machine Translation Systems","S. Satpathy; S. P. Mishra; A. K. Nayak","Computer Science & Engineering, SOA Deemed to be University, Bhubaneswar, India; Computer Science & Information Technology, SOA Deemed to be University, Bhubaneswar, India; Computer Science & Engineering, Sikhsha 'O' Anusandhan, Bhubaneswar, India","2019 International Conference on Applied Machine Learning (ICAML)","10 Feb 2020","2019","","","160","164","Machine Translation(MT) is a part of Natural Language Processing(NLP). It is the method of translating Source Language(SL) text into Target Language(TL). The gap between computer programmer and linguist can be resolved with this system. The problems like lexical ambiguity, part of speech tagging, syntactic and structural ambiguity, synonym etc will arise while developing such systems. To develop a proper bilingual machine translation system for two natural languages is a challenging and demanding task for researchers. It is required to analyze the information as well as technology behind every natural language translation. This work represents the various approaches with current trends of machine translation system. In recent trends various machine learning approaches have been developed to tackle the above said problems. Most recently neural machine translation attain very good result by using different deep neural network techniques and machine learning algorithms.","","978-1-7281-3908-1","10.1109/ICAML48257.2019.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989367","machine translation;natural language processing;machine learning","","computational linguistics;language translation;learning (artificial intelligence);natural language processing;neural nets","deep neural network techniques;syntactic ambiguity;part of speech tagging;target language translation;source language translation;neural machine translation;machine learning algorithms;machine learning approaches;natural language translation;proper bilingual machine translation system;structural ambiguity;computer programmer","","1","","16","IEEE","10 Feb 2020","","","IEEE","IEEE Conferences"
"A survey of voice translation methodologies ‚Äî Acoustic dialect decoder","H. Krupakar; K. Rajvel; B. Bharathi; S. A. Deborah; V. Krishnamurthy","Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering; Sri Sivasubramaniya Nadar College of Engineering, Chennai, Tamil Nadu, IN; Dept. of Computer Science and Engineering, SSN College Of Engineering","2016 International Conference on Information Communication and Embedded Systems (ICICES)","25 Jul 2016","2016","","","1","9","Language Translation has always been about inputting source as text/audio and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) ‚Äî a voice to voice earpiece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), RNNs with LSTM cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device.","","978-1-5090-2552-7","10.1109/ICICES.2016.7518940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518940","Voice Translator;Speech Recognition;Machine Translation;Speech Synthesis;Deep learning;RNN;LSTM;HTK;HTS;HMMs","Hidden Markov models;Speech;Speech recognition;Decoding;Feature extraction;Neural networks;Computer science","acoustic signal processing;hidden Markov models;language translation;recurrent neural nets;speech recognition;speech synthesis;text analysis","voice translation methodologies;acoustic dialect decoder;language translation;ADD;voice-to-voice earpiece translation device;speech engineering;speech recognition;speech translation;machine understanding problem;natural language;recognition unit;source audio;translation unit;source language text;target language text;synthesis unit;target language speech;hidden Markov models;HMM-based tool-kit;HTK;RNN;LSTM cells;HMM based speech synthesis system;HTS;English-to-Tamil translation device","","1","","39","IEEE","25 Jul 2016","","","IEEE","IEEE Conferences"
"Design of Foreign Language Intelligent Translation Recognition System Based on Improved GLR Algorithm","L. Pan","Shandong Transport Vocational College, Weifang, China","2022 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)","23 May 2022","2022","","","1296","1299","With the continuous progress of information technology, foreign language intelligent translation recognition is also constantly updated. From the comparison of mobile translation tools at home and abroad, foreign mobile translation tools started earlier, with mature technology and rich types. This paper summarizes the concept of GLR algorithm, puts forward the improved scheme of GLR algorithm, discusses statistical machine translation, analyzes intelligent matching, briefly introduces the speech recognition system, and finally tests the online translation software system. The results show that the query module can not work, mainly because after opening the voice setting, some models of operating systems do not support Pico voice engine.","","978-1-6654-0902-5","10.1109/IPEC54454.2022.9777507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777507","GLR algorithm;foreign language intelligent translation;recognition system;intelligent matching","Operating systems;Image processing;Software algorithms;Speech recognition;Information processing;Software systems;Machine translation","language translation;linguistics;speech recognition","foreign language intelligent translation recognition system;improved GLR algorithm;information technology;foreign mobile translation tools;statistical machine translation;intelligent matching;speech recognition system;online translation software system;operating systems","","","","12","IEEE","23 May 2022","","","IEEE","IEEE Conferences"
"DsetGenS: An Automated Technique for Building Dataset From Speech with respect to Gujarati-English","M. Patel; B. K. Joshi","Department of Computer Science and Engineering, Indore Institute of Science and Technology, Indore, India; Department of Informatics, Military College of Telecommunication Engineering, Mhow, India","2022 IEEE 11th International Conference on Communication Systems and Network Technologies (CSNT)","8 Jun 2022","2022","","","314","317","The computer has seen significant evolution in recent years, with applications in variety of disciplines like Machine Learning, Deep Learning etc. Machine Translation (MT) technology has advanced significantly as a subfield, with many methodologies and techniques. The number of individuals using the internet has risen tremendously. Most documents are written in English since that is the most extensively used language on the internet. If a user‚Äôs first dialect is Gujarati, he or she will naturally prefer to access the information in Gujarati whenever possible. Even though there are already various MT systems and tools that support Indian languages; however, the translation‚Äôs quality is mediocre and might be improved. As observed, when models are trained on limited quantities of parallel data, their performance declines. Learned models often have limited performance (inaccurate translations and feature scores) along with low coverage (high out-of-vocabulary rates). Furthermore, the researchers were driven to present novel methodologies and solutions that would automatically construct Datasets for MT due to the increased demand for effective technologies to process and translate information from/to Gujarati Language. Our objective of generating a Gujarati-English dataset has been met in two ways. We have already introduced GEDset, that is automatic Dataset Builder for Machine Translation System with Specific Reference to Gujarati-English [1]. Here, in this paper we are proposing a model to automatically build Gujarati-English dataset from audio that is available in Gujarati Language through Speech Processing.","2329-7182","978-1-6654-8038-3","10.1109/CSNT54456.2022.9787677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787677","Machine Translation;Speech Processing;Dataset;Machine Learning","Training;Deep learning;Machine learning algorithms;Conferences;Pipelines;Data models;Machine translation","deep learning (artificial intelligence);Internet;language translation;natural language processing;speech processing","automated technique;automatic dataset builder;Deep Learning;DsetGenS;GEDset;Gujarati Language;Gujarati-English dataset;Indian languages;Internet;Machine Learning;Machine Translation System;MT technology;parallel data quantities;translation quality","","","","13","IEEE","8 Jun 2022","","","IEEE","IEEE Conferences"
"Finite-state transducers for speech-input translation","F. Casacuberta","Dpt. de Sistemes Infom√†tics i Computaci√≥, Institut Tecnol√≤gic dE28099Infom√†tica, Universitat Polit√®cnica de Val√®ncia, Valencia, Spain","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","375","380","Nowadays, hidden Markov models (HMMs) and n-grams are the basic components of the most successful speech recognition systems. In such systems, HMMs (the acoustic models) are integrated into a n-gram or a stochastic finite-state grammar (the language model). Similar models can be used for speech translation, and HMMs (the acoustic models) can be integrated into a finite-state transducer (the translation model). Moreover, the translation process can be performed by searching for an optimal path of states in the integrated network. The output of this search process is a target word sequence associated to the optimal path. In speech translation, HMMs can be trained from a source speech corpus, and the translation model can be learned automatically from a parallel training corpus. This approach has been assessed in the framework of the EUTRANS project, founded by the European Union. Extensive speech-input experiments have been carried out with translations from Spanish to English and from Italian to English translation, in an application involving the interaction (by telephone) of a customer with a receptionist at the front-desk of a hotel. A summary of the most relevant results are presented in this paper.","","0-7803-7343-X","10.1109/ASRU.2001.1034664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034664","","Hidden Markov models;Acoustic transducers;Stochastic processes;Speech recognition;Stochastic systems;Telephony;Prototypes;Search engines;Natural languages;Decoding","language translation;finite state machines;transducers;hidden Markov models;speech recognition","finite-state transducers;speech-input translation;hidden Markov models;HMMs;n-grams;speech recognition systems;acoustic models;stochastic finite-state grammar;language model;speech translation;integrated network;search process;target word sequence;Spanish to English;Italian to English;EUTRANS project;speech-input experiments;receptionist;hotel front-desk","","7","","20","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Arabic ASR and MT Integration for GALE","Y. Al-Onaizan; L. Mangu","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1285","IV-1288","In this paper we describe our work in machine translation of Arabic speech into English. This work was done within the context of the GALE research program. We describe several integration techniques between our ASR and MT system. Our initial results suggest that tighter coupling between ASR and MT system improves the translation quality of speech input. We explore the effect of each integration technique on the overall system.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218343","Speech Translation;Statistical Machine Translation;Arabic Spoken Language Translation;GALE Translation System","Automatic speech recognition;Surface-mount technology;Natural languages;Vocabulary;Speech recognition;Broadcasting;Gaussian processes;Decoding;Cost function;Europe","language translation;speech recognition","Arabic ASR-MT integration;GALE;machine translation;Arabic speech;translation quality;automatic speech recognition","","4","","6","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Testing generality in JANUS: a multi-lingual speech translation system","L. Osterholtz; C. Augustine; A. McNair; I. Rogina; H. Saito; T. Sloboda; J. Tebelskis; A. Waibel","Computational Linguistics Program, Carnegie Mellon University, USA; Computer Science Department, University of Pennsylvania, USA; School of Computer Science, Carnegie Mellon University, USA; Fakult√§t F√ºr Informatik, Universit√§t Karlsruhe, Germany; Keio University, Japan; Fakult√§t F√ºr Informatik, Universit√§t Karlsruhe, Germany; School of Computer Science, Carnegie Mellon University, USA; School of Computer Science, Carnegie Mellon University, USA","[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1992","1","","209","212 vol.1","For speech translation to be practical and useful, speech translation systems should be portable to multiple languages without substantial modification. The authors present results of expanding the English-based JANUS speech translation system to translate from spoken German sentences to English and Japanese utterances. The authors also report the results of implementing part of the linked predictive neural network (LPNN) speech recognition module on a massively parallel machine. The JANUS approach generalizes well, with overall system performance of 97%. This surpasses English-based JANUS performance.<>","1520-6149","0-7803-0532-9","10.1109/ICASSP.1992.225935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=225935","","System testing;Speech recognition;Speech synthesis;Neural networks;Natural languages;Databases;Predictive models;Computational linguistics;Computer science;Parallel machines","language translation;natural languages;neural nets;speech recognition equipment","multilingual speech translations system;German to English translation;German to Japanese translation;LPNN speech recognition module;generality;JANUS;spoken German sentences;linked predictive neural network;performance","","12","","11","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Deep Learning-Based Intelligent Quality Detection Model for Machine Translation","M. Chen","College of Foreign Languages, Wuchang Shouyi University, Wuhan, China","IEEE Access","28 Aug 2023","2023","11","","89469","89477","With more and more active international connections, the complex scenes-aware machine translation has been a novel concern in the area of natural language processing. Although various machine translation methods have been proposed during the past few years, automatic and intelligent quality detection for translation results failed to receive sufficient attention. Actually, the real-time quality evaluation for machine translation results remains important, because it can facilitate constant debugging and optimization of machine translation products. Existing approaches mostly focused on the offline written contents rather than real-time extensive oral contents. To bridge current gap, a sentence-level machine translation quality estimation method is deployed in this paper. In particular, a specifical recurrent neural network with double directions (Double-RNN) is proposed as the backbone network structure. The feature extraction process utilizes the Double-RNN translation model, which makes full use of a large amount of parallel corpus. The evaluations show that Double-RNN method proposed in this paper is the closest to the standard quality assessment, and thus can also evaluate the quality of Chinese and English translations more fairly.","2169-3536","","10.1109/ACCESS.2023.3305397","Teaching Research Division, Wuchang Shouyi University, under the Project of Study on the Teaching Mode of Intercultural Communication Under Outcomes-Based Education(grant numbers:2021Y06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10217819","Quality detection;deep learning;machine translation;complex scenes","Machine translation;Speech recognition;Feature extraction;Linguistics;Web and internet services;Quality assessment;Manuals;Deep learning;Machine translation;Complex systems;Scene classification","deep learning (artificial intelligence);feature extraction;language translation;natural language processing;recurrent neural nets","active international connections;automatic quality detection;complex scenes-aware machine translation;constant debugging;deep learning-based intelligent quality detection model;Double-RNN method;Double-RNN translation model;feature extraction process;machine translation methods;machine translation products;machine translation results;natural language processing;offline written contents;optimization;real-time extensive oral contents;real-time quality evaluation;sentence-level machine translation quality estimation method;specifical recurrent neural network;standard quality assessment","","","","36","CCBY","15 Aug 2023","","","IEEE","IEEE Journals"
"JANUS-II-translation of spontaneous conversational speech","A. Waibel; M. Finke; D. Gates; M. Gavalda; T. Kemp; A. Lavie; L. Levin; M. Maier; L. Mayfield; A. McNair; I. Rogina; K. Shima; T. Sloboda; M. Woszczyna; T. Zeppenfeld; P. Zhan","Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; University of Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA","1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings","6 Aug 2002","1996","1","","409","412 vol. 1","JANUS-II is a research system to design and test components of speech-to-speech translation systems as well as a research prototype for such a system. We focus on two aspects of the system: (1) the new features of the speech recognition component JANUS-SR, and (2) the end-to-end performance of JANUS-II, including a comparison of two machine translation strategies used for JANUS-MT (PHOENIX and GLR*).","1520-6149","0-7803-3192-3","10.1109/ICASSP.1996.541119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=541119","","Speech recognition;Natural languages;Prototypes;Crosstalk;Databases;Switches;Processor scheduling;Vocabulary;Interactive systems;Laboratories","language translation;speech recognition;speech recognition equipment","spontaneous conversational speech translation;research system;speech-to-speech translation systems;JANUS-II;research prototype;speech recognition;performance;machine translation;JANUS-MT;PHOENIX;JANUS-SR;GLR*","","10","","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A finite-state approach to machine translation","S. Bangalore; G. Riccardi","AT&T Labs-Research, Florham Park, NJ, USA; AT&T Labs-Research, Florham Park, NJ","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","381","388","The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection; (b) lexical reordering. We propose stochastic finite-state models for these two subproblems. Stochastic finite-state models are efficiently able to learn from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. We present a method for learning stochastic finite-state models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese translation and present the performance of these models for translation of speech and text. We also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.","","0-7803-7343-X","10.1109/ASRU.2001.1034665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034665","","Stochastic processes;Decoding;Surface-mount technology;Calculus;Transducers;Speech processing;Natural languages;Context modeling;Speech recognition;Routing","language translation;speech recognition;finite state machines;stochastic automata;decoding;natural language interfaces;learning (artificial intelligence);text analysis","finite-state approach;machine translation;lexical selection;lexical reordering;stochastic models;decoding;language processing;English-Japanese translation;call routing task;speech utterances;speech recognition","","4","3","26","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Some results with a trainable speech translation and understanding system","V. M. Jimenez; A. Castellanos; E. Vidal","Departmento de Sistemas Inform√°ticos Y Computaci√≥n, Universidad Polit√©cnica de Valencia, Spain; Departmento de Sistemas Inform√°ticos Y Computaci√≥n, Universidad Polit√©cnica de Valencia, Spain; Departmento de Sistemas Inform√°ticos Y Computaci√≥n, Universidad Polit√©cnica de Valencia, Spain","1995 International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1995","1","","113","116 vol.1","The problems of limited-domain spoken language translation and understanding are considered. A standard continuous speech recognizer is extended for using automatically learnt finite-state transducers as translation models. Understanding is considered as a particular case of translation where the target language is a formal language. From the different approaches compared, the best results are obtained with a fully integrated approach, in which the input language acoustic and lexical models, and (N-gram) language models of input and output languages, are embedded into the learnt transducers. Optimal search through this global network obtains the best translation for a given input acoustic signal.","1520-6149","0-7803-2431-5","10.1109/ICASSP.1995.479286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479286","","Natural languages;Automatic speech recognition;Formal languages;Acoustic transducers;Speech processing;Command languages;Databases;Standards development;Training data;Loudspeakers","language translation;speech recognition;natural languages;grammars;transducers;finite state machines;formal languages;acoustic signal processing","trainable speech translation system;speech understanding system;limited-domain spoken language translation;spoken language understanding;continuous speech recognizer;automatically learnt finite-state transducers;translation model;target language;formal language;input language acoustic model;input language lexical model;N-gram language models;output languages;optimal search;global network;input acoustic signal","","5","3","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Detecting Source Contextual Barriers for Understanding Neural Machine Translation","G. Li; L. Liu; C. Zhu; R. Wang; T. Zhao; S. Shi","Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Tencent AI Lab, Shenzhen, Guangdong, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Advanced Translation Technology Laboratory, National Institute of Information and Communications Technology (NICT), Kyoto, Japan; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Tencent AI Lab, Shenzhen, Guangdong, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Nov 2021","2021","29","","3158","3169","In machine translation evaluation, the traditional wisdom measures model's generalization ability in an average sense, for example by using corpus BLEU. However, the statistics of corpus BLEU cannot provide comprehensive understanding and fine-grained analysis on model's generalization ability. As a remedy, this paper attempts to understand NMT at fine-grained level, by detecting contextual barriers within an unseen input sentence that cause the degradation in model's translation quality. It proposes a principled definition of source contextual barriers as well as its modified version which is tractable in computation and operates at word-level. Based on the modified one, three simple methods are proposed for barrier detection by search-aware risk estimation through counterfactual generation. Extensive analyses are conducted on those detected contextual barrier words on both Zh $\Leftrightarrow$ En NIST benchmarks. Potential usages motivated from barrier words are also discussed.","2329-9304","","10.1109/TASLP.2021.3085119","National Key R&D Program of China(grant numbers:2020AAA0108000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592701","Neural machine translation;evaluation;generalization analysis","Task analysis;Training;Machine translation","language translation;natural language processing","detecting source contextual barriers;neural machine translation;fine-grained level;unseen input sentence;word-level;three simple methods;search-aware risk estimation;NMT;wisdom measures model","","","","61","IEEE","28 Oct 2021","","","IEEE","IEEE Journals"
"Child Speech Recognition on End-to-End Neural ASR Models","S. Shraddha; J. L. G; S. K. S","Computational Engineering and Networking, Amrita Vishwa Vidhyapeetham, Coimbatore, India; Computational Engineering and Networking, Amrita Vishwa Vidhyapeetham, Coimbatore, India; Computational Engineering and Networking, Amrita Vishwa Vidhyapeetham, Coimbatore, India","2022 2nd International Conference on Intelligent Technologies (CONIT)","18 Aug 2022","2022","","","1","6","Automatic speech recognition (ASR) is an area which is having maximum improvement and is developing to a very high extent as far as adult speech is concerned. Nevertheless, child speech recognition is an area which is least explored. This is because of the scarcity of a neatly labelled corpora. Precisely, to train a speech recognition model from scratch and to make it work child speech is a challenging task as the availability of child speech data is limited. In this context, exploring child speech recognition can also help us improve the current speech recognition systems since on validating it in such a manner that it will be useful for child speech may help the systems work in a good way such that we will be able to validate it for everyone irrespective of the age variation in which a speech utterance is produced. State-of-art models in this domain perform various complex speech recognition tasks such as machine translation and ASR on multilingual datasets as well as it is useful in performing self supervised speech recognition tasks. This process has proved to work well for speech data which consists of recordings with adult data and it is not yet validated on child speech. So the present work tries to validate the performance six state-of-the-art ASR models on children speech and report the corresponding word error rate.","","978-1-6654-8407-7","10.1109/CONIT55038.2022.9847929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847929","Automatic Speech Recognition models;Child speech;state-of-art end-to-end speech recognition","Error analysis;Speech recognition;Data models;Recording;Machine translation;Task analysis;Speech processing","neural nets;speech recognition","child speech recognition;end-to-end neural ASR models;automatic speech recognition;adult speech;speech recognition model;work child speech;child speech data;speech recognition systems;speech utterance;self supervised speech recognition tasks;word error rate;machine translation","","2","","14","IEEE","18 Aug 2022","","","IEEE","IEEE Conferences"
"Towards language portability in statistical speech translation","A. Waibel; T. Schultz; S. Vogel; C. Fugen; M. Honal; M. Kolss; J. Reichert; S. Stuker","Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA; Interactive Systems Laboratories, Carnegie Mellon University, USA","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","3","","iii","765","Speech translation has made significant advances over the last years. We believe that we can overcome today's limits of language and domain portable conversational speech translation systems by relying more radically on learning approaches and by the use of multiple layers of reduction and transformation to extract the desired content in another language. Therefore, we cascade stochastic source-channel models that extract an underlying message from a corrupt observed output. The three models effectively translate: (1) speech to word lattices (automatic speech recognition, ASR); (2) ill-formed fragments of word strings into a compact well-formed sentence (Clean); (3) sentences in one language to sentences in another (machine translation, MT). We present results of our research efforts towards rapid language portability of all these components. The results on translation suggest that MT systems can be successfully constructed for any language pair by cascading multiple MT systems via English. Moreover, end-to-end performance can be improved, if the interlingua language is enriched with additional linguistic information that can be derived automatically and monolingually in a data-driven fashion.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1326657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326657","","Natural languages;Cleaning;Automatic speech recognition;Surface-mount technology;Speech recognition;Speech enhancement;Interactive systems;Laboratories;Stochastic processes;Lattices","statistical analysis;language translation;speech recognition;learning (artificial intelligence);stochastic processes;natural languages","language portability;statistical speech translation;learning approaches;stochastic source-channel models;automatic speech recognition;ASR;statistical machine translation;MT;linguistic information","","2","2","11","IEEE","30 Aug 2004","","","IEEE","IEEE Conferences"
"Phrase-based translation of speech recognizer word lattices using loglinear model combination","E. Matusov; H. Ney; R. Schluter","Lehrstuhl f√ºr Informatik VI, Computer Science Department, RWTH Aachen University, Aachen, Germany; Lehrstuhl f√ºr Informatik VI, Computer Science Department, RWTH Aachen University, Aachen, Germany; Lehrstuhl f√ºr Informatik VI, Computer Science Department, RWTH Aachen University, Aachen, Germany","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","110","115","This paper presents a phrase-based speech translation system that combines phrasal lexicon, language, and acoustic model features in a loglinear model. Automatic speech recognition and machine translation are coupled by using large word lattices as the input for translation. For the first time, all features are directly integrated into the decoding process. The feature weights are iteratively optimized for an objective error measure. We prove that acoustic recognition scores of the recognized words in the lattices together with a source language model score positively and significantly affect the translation quality. We show the advantage of using loglinear model combination for a robust optimization of scaling factors. We report consistent improvements compared with translations of single best recognition output on an Italian-to-English translation task. First encouraging results were also obtained on a large vocabulary task of translating European parliamentary speeches","","0-7803-9478-X","10.1109/ASRU.2005.1566491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566491","","Speech recognition;Lattices;Automatic speech recognition;Natural languages;Vocabulary;Computer science;Iterative decoding;Acoustic measurements;Robustness;Acoustic transducers","language translation;natural languages;speech recognition;vocabulary","phrase-based speech translation;speech recognizer word lattices;loglinear model combination;phrasal lexicon;acoustic model features;automatic speech recognition;machine translation;source language model;vocabulary task","","3","","11","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Consensus Network Decoding for Statistical Machine Translation System Combination","K. C. Sim; W. J. Byrne; M. J. F. Gales; H. Sahbi; P. C. Woodland","Institute for Infocomm Research, Singapore; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-105","IV-108","This paper presents a simple and robust consensus decoding approach for combining multiple machine translation (MT) system outputs. A consensus network is constructed from an N-best list by aligning the hypotheses against an alignment reference, where the alignment is based on minimising the translation edit rate (TER). The minimum Bayes risk (MBR) decoding technique is investigated for the selection of an appropriate alignment reference. Several alternative decoding strategies proposed to retain coherent phrases in the original translations. Experimental results are presented primarily based on three-way combination of Chinese-English translation outputs, and also presents results for six-way system combination. It is shown that worthwhile improvements in translation performance can be obtained using the methods discussed.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218048","Machine translation;system combination;consensus decoding;Minimum Bayes Risk (MBR) decoding","Decoding;Erbium;Error analysis;Natural languages;NIST;Robustness;Speech recognition;Voting;Subcontracting;US Government","Bayes methods;decoding;language translation;natural language processing;speech coding","consensus network decoding;statistical machine translation system;robust consensus decoding;N-best list;translation edit rate;minimum Bayes risk decoding;alignment reference;Chinese-English translation outputs","","26","7","12","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Dynamic boundary detection for speech translation","N. Zhou; X. Wang; A. Aw","Institute for Infocomm Research (I2R), Singapore; Institute for Infocomm Research (I2R), Singapore; Institute for Infocomm Research (I2R), Singapore","2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","8 Feb 2018","2017","","","651","656","Speech translation is usually the pipeline task of automatic speech recognition (ASR), translation unit segmentation and machine translation (MT). Segmenting the ASR output to translation units poses a challenge of balancing the translation quality and efficiency for real-time speech translation. In this paper, we firstly propose a parser-based semantic boundary detection method to detect all semantic boundaries based on our definition. To realize the translation of the semantic units, a word-boundary language model is secondly proposed to improve the translation quality. Experiments on English to Chinese and Chinese to English speech translation have shown that the proposed method yields improved translation quality and lower latency, when compared to the conventional punctuated methods.","","978-1-5386-1542-3","10.1109/APSIPA.2017.8282107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282107","","Semantics;Speech;Labeling;Training;Ice;Task analysis;Real-time systems","language translation;natural language processing;speech recognition","semantic units;word-boundary language model;translation quality;dynamic boundary detection;translation unit segmentation;machine translation;real-time speech translation;semantic boundary detection method;automatic speech recognition;ASR;translation efficiency;English-to-Chinese speech translation;Chinese-to-English speech translation","","1","","20","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Intent transfer in speech-to-speech machine translation","G. K. Anumanchipalli; L. C. Oliveira; A. W. Black","Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; INESC-ID/IST, Spoken Language Systems Laboratory, Lisboa, Portugal; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2012 IEEE Spoken Language Technology Workshop (SLT)","31 Jan 2013","2012","","","153","158","This paper presents an approach for transfer of speaker intent in speech-to-speech machine translation (S2SMT). Specifically, we describe techniques to retain the prominence patterns of the source language utterance through the translation pipeline and impose this information during speech synthesis in the target language. We first present an analysis of word focus across languages to motivate the problem of transfer. We then propose an approach for training an appropriate transfer function for intonation on a parallel speech corpus in the two languages within which the translation is carried out. We present our analysis and experiments on English‚ÜîPortuguese and English‚ÜîGerman language pairs and evaluate the proposed transformation techniques through objective measures.","","978-1-4673-5126-3","10.1109/SLT.2012.6424214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424214","Speech Translation;Prominence;Focus;Speech Synthesis;Cross-lingual Transfer","Speech;Vectors;Databases;Pragmatics;Manuals;Joints;Predictive models","language translation;speech synthesis;transfer functions","speech-to-speech machine translation;S2SMT;speaker intent transfer;source language utterance;translation pipeline;speech synthesis;transfer function;parallel speech corpus;English-Portuguese language pairs;English-German language pairs;transformation techniques;objective measures","","9","1","21","IEEE","31 Jan 2013","","","IEEE","IEEE Conferences"
"Concept-Based Speech-to-Speech Translation Using Maximum Entropy Models for Statistical Natural Concept Generation","L. Gu; Y. Gao; F. . -H. Liu; M. Picheny","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","IEEE Transactions on Audio, Speech, and Language Processing","21 Feb 2006","2006","14","2","377","392","The IBM Multilingual Automatic Speech-To-Speech TranslatOR (MASTOR) system is a research prototype developed for the Defense Advanced Research Projects Agency (DARPA) Babylon/CAST speech-to-speech machine translation program. The system consists of cascaded components of large-vocabulary conversational spontaneous speech recognition, statistical machine translation, and concatenative text-to-speech synthesis. To achieve highly accurate and robust conversational spoken language translation, a unique concept-based speech-to-speech translation approach is proposed that performs the translation by first understanding the meaning of the automatically recognized text. A decision-tree based statistical natural language understanding algorithm extracts the semantic information from the input sentences, while a natural language generation (NLG) algorithm predicts the translated text via maximum-entropy-based statistical models. One critical component in our statistical NLG approach is natural concept generation (NCG). The goal of NCG is not only to generate the correct set of concepts in the target language, but also to produce them in an appropriate order. To improve maximum-entropy-based concept generation, a set of new approaches is proposed. One approach improves concept sequence generation in the target language via forward‚Äìbackward modeling, which selects the hypothesis with the highest combined conditional probability based on both the forward and backward generation models. This paradigm allows the exploration of both the left and right context information in the source and target languages during concept generation. Another approach selects bilingual features that enable maximum-entropy-based model training on the preannotated parallel corpora. This feature is augmented with word-level information in order to achieve higher NCG accuracy while minimizing the total number of distinct concepts and, hence, greatly reducing the concept annotation and natural language understanding effort. These features are further expanded to multiple sets to enhance model robustness. Finally, a confidence threshold is introduced to alleviate data sparseness problems in our training corpora. Experiments show a dramatic concept generation error rate reduction of more than 40% in our speech translation corpus within limited domains. Significant improvements of both word error rate and BiLingual Evaluation Understudy (BLEU) score are also achieved in our experiments on speech-to-speech translation.","1558-7924","","10.1109/TSA.2005.860769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597244","Maximum-entropy models;natural concept generation (NCG);speech-to-speech translation;statistical concept-based translation","Entropy;Natural languages;Speech synthesis;Robustness;Error analysis;Prototypes;Speech recognition;Automatic speech recognition;Text recognition;Data mining","","Maximum-entropy models;natural concept generation (NCG);speech-to-speech translation;statistical concept-based translation","","17","4","32","IEEE","21 Feb 2006","","","IEEE","IEEE Journals"
"Extracting clues from human interpreter speech for spoken language translation","M. Paulik; A. Waibel","Carnegie Mellon University, USA; Carnegie Mellon University, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","5097","5100","In previous work, we reported dramatic improvements in automatic speech recognition (ASR) and spoken language translation (SLT) gained by applying information extracted from spoken human interpretations. These interpretations were artificially created by collecting read sentences from a clean parallel text corpus. Real human interpretations are significantly different. They suffer from frequent synopses, omissions and self-corrections. Expressing these differences in BLEU score by evaluating human interpretations with carefully created human translations, we found that human interpretations perform two to three times worse than state-of-the art SLT. Facing these stark differences, we address the question if and how ASR and SLT can profit from human interpretations. In the following we describe initial experiments that apply knowledge derived from real human interpretations for improving English and Spanish ASR and SLT. Our experiments are conducted on a small European Parliamentary Plenary Sessions development set.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518805","spoken language translation;STE-ASR;tight coupling","Humans;Natural languages;Automatic speech recognition;Data mining;Broadcasting;Interactive systems;Laboratories;Performance evaluation;Art;Contracts","language translation;natural language processing;speech recognition","human interpreter speech;spoken language translation;automatic speech recognition;spoken human interpretations","","7","","9","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"Optimization and Comparative Modeling of Technology English Automatic Translation Algorithms for Mobile Platforms","H. Liu","School of Foreign Language and Trade, Guangzhou City Construction College, Guangzhou, Guangdong, China","2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA)","29 Dec 2022","2022","","","1615","1618","Educator Bruner said, ‚ÄúThe best stimulus to learning is interest in the material.‚Äù According to research: in teaching activities, images are more likely to attract people's attention than words. And it is very convenient to disseminate images in smart mobile devices. The design of mobile learning platform is mainly to facilitate learners to use mobile devices to access the resources on the server through wireless network at any time. The system mainly includes three parts of the learning resource server, wireless network, mobile terminal. In data search, different modes of complete matching and fuzzy matching can be selected, and the fuzzy matching mode is mainly used to improve translation memory. Translation memory is the key feature and core part of automatic translation system. At present, the evaluation of all translation products revolves around a certain fixed standard to evaluate a certain attribute of the product. There is no fixed evaluation standard to evaluate the quality of machine automatic translation. Therefore, it is difficult to evaluate the quality of machine automatic translation with high accuracy. Therefore, an automatic translation machine speech signal recognition method based on spectrum features is proposed. The frequency spectrum energy function is used to separate the noise characteristic coefficients and adjust the speech signal filter. According to the frequency threshold of core region, the filter parameter correction is determined, and the attenuation coefficient is introduced. Related experiments and simulations show that this newly designed Chinese-English phrase translation combined machine automatic translation system has a higher translation accuracy and recall rate of 11.2%, an error rate of 1.3%, and a strong intelligence improvement of 6.8%.","","978-1-6654-9707-7","10.1109/ICIRCA54612.2022.9985647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985647","Technology English;English Automatic Translation;Translation Algorithms;Mobile Platforms","Software design;Wireless networks;Software algorithms;Semantics;Speech recognition;Mobile handsets;Software","computer aided instruction;fuzzy set theory;language translation;mobile computing;natural language processing;speech processing;speech recognition","automatic translation machine speech signal recognition method;Chinese-English phrase translation combined machine automatic translation system;complete matching;disseminate images;Educator Bruner;fixed evaluation standard;fixed standard;fuzzy matching mode;higher translation accuracy;learning resource server;mobile learning platform;mobile platforms;mobile terminal;people;recall rate;smart mobile devices;teaching activities;technology English automatic translation algorithms;translation memory;translation products;wireless network","","","","24","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Hybrid machine translation for English to Marathi: A research evaluation in Machine Translation: (Hybrid translator)","P. Salunkhe; A. D. Kadam; S. Joshi; S. Patil; D. Thakore; S. Jadhav","Dept. Comp. Engg, BVDUCOEP; Dept. Info. Tech., BVDUCOEP; Dept. Computer Engg, BVDUCOEP; Dept. Computer Engg, BVDUCOEP; Dept. Computer Engg, BVDUCOEP; Dept.comp Engg, Ghrce SSPU","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","24 Nov 2016","2016","","","924","931","Information Present in Different language and Structure gives Rise to language as barrier in information retrieval. Informative Document on queen Elizabeth is been writing by foreign language English which makes its difficult for a Marathi reader to understand and seek History of England, on Similar lines Literature Work on Shivaji is mostly documented in Marathi which makes foreign Historians difficult to gain know, in both case user is at times unknown of facts due to language and may lose interest on information. Vital information on current happing on village and taluka level are been published in newspaper with local language which setback information spread among other Masses. Many Time government documents and forms are been presented in English Language where a lay man from Marathi language background finds difficulty to understand information and even avoid such procedures therefore it highly urges for need of automated Software based Translation system which would assist in cross Domain information Retrieval. Machine Translation assist to translate Information presented in one language to other language. Information can be present in form of text, speech and image translating this information helps for sharing of information and ultimately information gain. A lot of work has been done on Translation of English to Hindi, Tamil Bangla and other foreign languages also. Machine Translation is challenging Research Area with numerous issues due to language ambiguity like grammar, Structure and even fluency of use. Numerous Methodologies have been proposed and developed which have uplifts and downfalls also, Although statistical and rule based at core with each having limitations, rule based produce accurate mapped translation and are trainable system but costly, whereas statistical produce fluent translation but lack accuracy and sense. Hybrid is combine approach which integrated approach and helps to optimize translation output. The research manuscript we present hybrid machine Translator for English to Marathi language which translated Web pages, text Documents on Agriculture (crops fruits for farmer), Medical reports in Marathi and tourism related information. Proposed System consists of Parallel Multi-Engines which process statistical and rule based Translation for same input document and produce a optimized result by performing statistical over rule based which give fluent language sense outputs. Mapper algorithm is been used in rule based Translation, with Agriculture corpus, medical and tourism corpus for statistical evaluation. Marathi wordnet has been implemented to enhance dictionary and incorporate better translation result Currently System has been proposed for text Document which can be extended to speech and voice. Comparative analysis for point view in one dimension of only limited set of Queries is done with Google Translator. Holding hybrid approach as better methodology. A Systematic survey of only 10 key articles used in research has been done. This research article is extension of our previous research surveys and partial implementations. And innovative S-measure has been new parameter proposed and evaluated by our research team.","","978-1-4673-9939-5","10.1109/ICEEOT.2016.7754822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754822","Machine Translation;Rule based Translation;Statistical Translation;Mapping Rules;hybrid translation;Google Translator","Computers;Syntactics;Knowledge based systems;Speech;Grammar;Agriculture;Context","information retrieval;language translation","cross-domain information retrieval;text information;speech information;image information;information gain;S-measure;Google Translator;comparative analysis;software based translation system;Marathi language;English language;Time government documents;informative document;information retrieval;English-to-Marathi translation;hybrid machine translation","","13","","15","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Trends and challenges in language modeling for speech recognition and machine translation","H. Schwenk","University of Le Mans, France","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","23","23","Summary form only given. Language models play an important role in large vocabulary continuous speech recognition (LVCSR) systems and statistical approaches to machine translation (SMT), in particular when modeling morphologically rich languages. Despite intensive research over more than 20 years, state-of-the-art LVCSR and SMT systems seem to use only one dominant approach: n-gram back-off language models. This talk first reviews the most important approaches to language modeling. I then discuss some of the recent trends and challenges for the future. An interesting alternative to the back-off n-gram approach are the so-called continuous space methods. The basic idea is to perform the probability estimation in a continuous space. By these means better probability estimations of unseen word sequences can be expected. There is also a relative large body of works on adaptive language models. The adaptation can aim to tailor a language model to a particular task or domain, or it can be performed over time. Another very active research area are discriminative language models. Finally, I will review the challenges and benefits of language models trained an very large amounts of training material.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373531","","Natural languages;Speech recognition;Surface-mount technology;Vocabulary","estimation theory;language translation;speech recognition","large vocabulary continuous speech recognition;machine translation statistical approach;morphologically rich languages;continuous space methods;discriminative language models;adaptive language models;n-gram back-off language models;probability estimation;word sequences","","2","","","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Latent Attribute Based Hierarchical Decoder for Neural Machine Translation","X. Liu; D. F. Wong; L. S. Chao; Y. Liu","Natural Language Processing & Portuguese-Chinese Machine Translation (NLPCT) Laboratory, University of Macau, Macau, China; Natural Language Processing & Portuguese-Chinese Machine Translation (NLPCT) Laboratory, University of Macau, Macau, China; Natural Language Processing & Portuguese-Chinese Machine Translation (NLPCT) Laboratory, University of Macau, Macau, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","30 Sep 2019","2019","27","12","2103","2112","Neural machine translation (NMT) has achieved state-of-the-art performance in many translation tasks. However, because the computational cost increases with the size of the search space for predicting the target words, the translation quality of NMT is constrained by the limited vocabulary. To alleviate this problem, we propose a novel dynamic hierarchical decoder for NMT to utilize all of the target words in the training and decoding process. In the proposed model, a target word is represented by two latent attribute vectors rather than a word vector. The model is trained to dynamically put together those words that share similar linguistic attributes. The prediction of a target word is, therefore, turned into the prediction of attribute vectors, where the $\mathrm{softmax}$ functions are performed at the attribute level. This greatly reduces the model size and the decoding time. Our experimental results demonstrate that the proposed model significantly outperforms the NMT baselines in both Chinese-English and English-German translation tasks.","2329-9304","","10.1109/TASLP.2019.2941587","National Natural Science Foundation of China(grant numbers:61672555); Joint Project of the Science and Technology Development Fund; Macau SAR; National Natural Science Foundation of China(grant numbers:045/2017/AFJ,6176116608); Science and Technology Development Fund(grant numbers:0101/2019/A2); Multi-year Research Grant from the University of Macau(grant numbers:MYRG2017-00087-FST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839081","Latent attribute;limited vocabulary;hierarchical decoder;neural machine translation (NMT)","Vocabulary;Decoding;Computational modeling;Training;Speech processing;Linguistics;Semantics","decoding;language translation;learning (artificial intelligence);natural language processing","neural machine translation;target word;translation quality;dynamic hierarchical decoder;decoding process;latent attribute vectors;word vector;attribute level;decoding time;NMT baselines;English-German translation tasks;linguistic attributes","","4","","54","IEEE","16 Sep 2019","","","IEEE","IEEE Journals"
"Neural vs Statistical Machine Translation: Revisiting the Bangla-English Language Pair","M. A. Hasan; F. Alam; S. A. Chowdhury; N. Khan","Cognitive Insight Limited, Bangladesh; QCRI, Qatar; QCRI, Qatar; Dhaka University, Bangladesh","2019 International Conference on Bangla Speech and Language Processing (ICBSLP)","4 May 2020","2019","","","1","5","Machine translation systems facilitate our communication and access to information, taking down language barriers. It is a well-researched area of Natural Language Processing (NLP), especially for resource-rich languages (e.g., language pairs in Europarl Parallel corpus). Besides these languages, there is also work on other language pairs including the Bangla-English language pair. In the current study, we aim to revisit both Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) approaches using well-known, publicly available corpora for the Bangla-English (Bangla to English) language pair. We reported how the performance of the models differ based on the data and modeling techniques; consequently, we also compared the results obtained with Google's machine translation system. Our findings, across different corpora, indicates that NMT based approaches outperform SMT systems. Our results also outperform existing baselines by a large margin.","","978-1-7281-5241-7","10.1109/ICBSLP47725.2019.201502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9084044","Machine Translation;Bangla-to-English;Statistical Machine Translation;Neural Machine Translation;Bidirectional LSTM","","language translation;natural language processing;statistical analysis","language pairs;Bangla-English language pair;Google;statistical machine translation;language barriers;natural language processing;resource-rich languages;machine translation systems;neural machine translation","","3","","37","IEEE","4 May 2020","","","IEEE","IEEE Conferences"
"Recent advances in SRI'S IraqComm‚Ñ¢ Iraqi Arabic-English speech-to-speech translation system","M. Akbacak; H. Franco; M. Frandsen; S. Hasan; H. Jameel; A. Kathol; S. Khadivi; Xin Lei; A. Mandal; S. Mansour; K. Precoda; C. Richey; D. Vergyri; Wen Wang; M. Yang; J. Zheng","SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; RWTH Aachen University, Aachen, Germany; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; RWTH Aachen University, Aachen, Germany; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; RWTH Aachen University, Aachen, Germany; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; SRI International, Inc., Menlo Park, CA, USA; University of Washington, Seattle, WA, USA; SRI International, Inc., Menlo Park, CA, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4809","4812","We summarize recent progress on SRI's IraqCommtrade Iraqi Arabic-English two-way speech-to-speech translation system. In the past year we made substantial developments in our speech recognition and machine translation technology, leading to significant improvements in both accuracy and speed of the IraqComm system. On the 2008 NIST-evaluation dataset our twoway speech-to-text (S2T) system achieved 6% to 8% absolute improvement in BLEU in both directions, compared to our previous year system.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960707","speech translation;spoken language translation system","Lattices;Automatic speech recognition;Decoding;Speech recognition;Natural languages;Hidden Markov models;Testing;Protection;Hardware;Data processing","language translation;natural languages;speaker recognition","speech-speech translation system;SRI's IraqComm system;NIST-evaluation dataset;Iraqi Arabic-English system;speech recognition;machine translation","","9","","8","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Weakly supervised spoken term discovery using cross-lingual side information","S. Bansal; H. Kamper; S. Goldwater; A. Lopez","Institute for Language Cognition, and Computation; Centre for Speech Technology Research School of Informatics, University of Edinburgh, UK; Institute for Language Cognition, and Computation; Institute for Language Cognition, and Computation","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","5760","5764","Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953260","Unsupervised term discovery;low-resource speech processing;speech translation;weakly supervised learning","Speech;Acoustics;Semantics;Matched filters;Noise measurement;Training data","language translation;learning (artificial intelligence);natural language processing;speech processing","spoken term discovery;cross-lingual side information;unsupervised term discovery;UTD;audio text translations;Spanish audio;English translations;data preprocessing methods","","","","30","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Javanese speech levels machine translation: Improved parallel text alignment based on impossible pair limitation","A. P. Wibawa; A. Nafalski; W. F. Mahmudy","Department of Electrical Engineering, University of South Australia, Malang, Indonesia; School of Engineering, University of South Australia, Adelaide, Australia; Department of Computer Science, Brawijaya University, Malang, Indonesia","2013 IEEE International Conference on Computational Intelligence and Cybernetics (CYBERNETICSCOM)","26 Jul 2014","2013","","","16","20","A machine translation is developed to preserve the existence of Javanese speech levels. The machine translation relies on a phrase-based bi-text alignment to form the language corpora. The edit shifting distance is applied to increase the alignment efficiency. However, improper alignment contributed by recorded impossible pair and insufficient data training is still detected. This paper proposes a new improvement of the developed alignment algorithm based on the impossible pair restriction. The paper compares three situations: the fundamental approach (AL1) the basic algorithm with extended data training (AL2) and improved algorithm with standard data training (AL3). Based on experimental results, AL3 (A=90.5%)is remarkably accurate than AL1 (A=79.6%) and AL2 (A=85.9).","","978-1-4673-6053-1","10.1109/CyberneticsCom.2013.6865773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865773","Javanese speech levels;parallel text;impossible pair limitation","Training;Speech;Accuracy;Algorithm design and analysis;Databases;Educational institutions;Australia","language translation;speech processing","Javanese speech levels;machine translation;parallel text alignment;impossible pair limitation;phrase-based bi-text alignment;edit shifting distance;data training","","","","12","IEEE","26 Jul 2014","","","IEEE","IEEE Conferences"
"A Unified Framework for Multilingual Speech Recognition in Air Traffic Control Systems","Y. Lin; D. Guo; J. Zhang; Z. Chen; B. Yang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Neural Networks and Learning Systems","3 Aug 2021","2021","32","8","3608","3620","This work focuses on robust speech recognition in air traffic control (ATC) by designing a novel processing paradigm to integrate multilingual speech recognition into a single framework using three cascaded modules: an acoustic model (AM), a pronunciation model (PM), and a language model (LM). The AM converts ATC speech into phoneme-based text sequences that the PM then translates into a word-based sequence, which is the ultimate goal of this research. The LM corrects both phoneme- and word-based errors in the decoding results. The AM, including the convolutional neural network (CNN) and recurrent neural network (RNN), considers the spatial and temporal dependences of the speech features and is trained by the connectionist temporal classification loss. To cope with radio transmission noise and diversity among speakers, a multiscale CNN architecture is proposed to fit the diverse data distributions and improve the performance. Phoneme-to-word translation is addressed via a proposed machine translation PM with an encoder‚Äìdecoder architecture. RNN-based LMs are trained to consider the code-switching specificity of the ATC speech by building dependences with common words. We validate the proposed approach using large amounts of real Chinese and English ATC recordings and achieve a 3.95% label error rate on Chinese characters and English words, outperforming other popular approaches. The decoding efficiency is also comparable to that of the end-to-end model, and its generalizability is validated on several open corpora, making it suitable for real-time approaches to further support ATC applications, such as ATC prediction and safety checking.","2162-2388","","10.1109/TNNLS.2020.3015830","National Science Foundation of China (NSFC) and the Civil Aviation Administration of China (CAAC)(grant numbers:U1833115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174746","Acoustic model (AM);air traffic control (ATC);machine translation pronunciation model (PM);multiscale CNN (MCNN);multilingual;robust speech recognition","Hidden Markov models;Task analysis;Atmospheric modeling;Speech recognition;Vocabulary;Decoding;Real-time systems","air traffic control;decoding;language translation;learning (artificial intelligence);neural nets;recurrent neural nets;speech recognition","multilingual speech recognition;air traffic control systems;robust speech recognition;single framework;acoustic model;pronunciation model;language model;converts ATC speech;phoneme-based text sequences;word-based sequence;phoneme- word-based errors;decoding results;convolutional neural network;recurrent neural network;spatial dependences;temporal dependences;speech features;connectionist temporal classification loss;radio transmission noise;multiscale CNN architecture;diverse data distributions;phoneme-to-word translation;machine translation PM;encoder-decoder architecture;RNN-based LMs;common words;English words;end-to-end model;support ATC applications;safety checking","Acoustics;Algorithms;Aviation;Computer Systems;Language;Multilingualism;Neural Networks, Computer;Reproducibility of Results;Speech;Speech Recognition Software;Translations","27","","64","IEEE","24 Aug 2020","","","IEEE","IEEE Journals"
"Speech translation enhanced automatic speech recognition","M. Paulik; S. Stuker; C. Fugen; T. Schultz; T. Schaaf; A. Waibel","Carnegie Mellon University, USA; Universit√§t Karlsruhe, Germany; Universit√§t Karlsruhe, Germany; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","121","126","Nowadays official documents have to be made available in many languages, like for example in the EU with its 20 official languages. Therefore, the need for effective tools to aid the multitude of human translators in their work becomes easily apparent. An ASR system, enabling the human translator to speak his translation in an unrestricted manner, instead of typing it, constitutes such a tool. In this work we improve the recognition performance of such an ASR system on the target language of the human translator by taking advantage of an either written or spoken source language representation. To do so, machine translation techniques are used to translate between the different languages and then the involved ASR systems are biased towards the gained knowledge. We present an iterative approach for ASR improvement and outperform our baseline system by a relative word error rate reduction of 35.8%/29.9% in the case of a written/spoken source language representation. Further, we show how multiple target languages, as for example provided by different simultaneous translators during European Parliament debates, can be incorporated into our system design for an improvement of all involved ASR systems.","","0-7803-9478-X","10.1109/ASRU.2005.1566488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566488","","Speech enhancement;Automatic speech recognition;Humans;Natural languages;Iterative methods;Streaming media;Interactive systems;Laboratories;Target recognition;Error analysis","speech recognition;language translation;natural languages","speech translation;automatic speech recognition;human translator;source language representation;machine translation;multiple target languages","","14","","7","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Design and evaluation of the 2006 BBN english/Iraqi two-way speech translation system","D. Stallard; F. Choi; K. Krstovski; P. Natarajan; R. Prasad; S. Saleem; R. Suleiman","BBN Technologies, Inc., USA; BBN Technologies, Inc., USA; BBN Technologies, Inc., USA; BBN Technologies, Inc., USA; BBN Technologies, Inc., USA; BBN Technologies, Inc., USA; BBN Technologies, Inc., USA","2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","254","257","In this paper, we present a 2-way speech-to-speech translation system for English and Iraqi colloquial Arabic, the dialect of Arabic spoken by ordinary people in Iraq. The application domain of the system is military force protection, including municipal services surveys, detainee screening, and descriptions of people, houses, vehicles, etc. The system uses statistical speech recognition, and a combination of prerecorded questions and statistical machine translation with speech synthesis to translate the speech recognition output. We present evaluation results, along with an analysis of the gap between Iraqi-to-English and English-to-Iraqi translation performance.","","1-4244-0872-5","10.1109/SLT.2006.326803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123410","","Speech analysis;Speech recognition;Speech synthesis;Surface-mount technology;Protection;Vehicles;Performance analysis;Natural languages;Writing;Broadcasting","language translation;military computing;natural language processing;speech recognition;speech synthesis","2006 BBN English/Iraqi two-way speech translation system;Iraqi colloquial Arabic;military force protection;municipal services surveys;detainee screening;statistical speech recognition;statistical machine translation;speech synthesis;speech recognition","","1","","15","IEEE","19 Mar 2007","","","IEEE","IEEE Conferences"
"Application of Pretrained Models for Machine Translation","K. Zhang","School of computer and science, Beijing Language and Culture University","2021 International Conference on Communications, Information System and Computer Engineering (CISCE)","9 Jun 2021","2021","","","849","853","In recent years, the pre-training model represented by BERT has achieved significant improvement in many NLP tasks. However, because the MLM pre-training task used by the pre-training language model is different from the auto-regressive language model, it is difficult to apply it in the field of machine translation. This article introduces the basic concepts of machine translation, pre-training language models and BERT. In addition, it introduces the current research on pre-trained language models that can be used for machine translation and the prospects for future research in detail.","","978-1-6654-0352-8","10.1109/CISCE52179.2021.9446005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446005","pre-training;machine translation;BERT","Computational modeling;Bit error rate;Machine translation;Task analysis;Information systems","language translation;learning (artificial intelligence);natural language processing;speech recognition","pretrained models;machine translation;pre-training model;MLM pre-training task;pre-training language model;auto-regressive language model;pre-trained language models","","","","8","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Statistical Machine Translation for Speech: A Perspective on Structures, Learning, and Decoding","B. Zhou","IBM T. J. Watson Research Center, New York, NY, USA","Proceedings of the IEEE","17 Apr 2013","2013","101","5","1180","1202","In this paper, we survey and analyze state-of-the-art statistical machine translation (SMT) techniques for speech translation (ST). We review key learning problems, and investigate essential model structures in SMT, taking a unified perspective to reveal both connections and contrasts between automatic speech recognition (ASR) and SMT. We show that phrase-based SMT can be viewed as a sequence of finite-state transducer (FST) operations, similar in spirit to ASR. We further inspect the synchronous context-free grammar (SCFG)-based formalism that includes hierarchical phrase-based and many linguistically syntax-based models. Decoding for ASR, FST-based, and SCFG-based translation is also presented from a unified perspective as different realizations of the generic Viterbi algorithm on graphs or hypergraphs. These consolidated perspectives are helpful to catalyze tighter integrations for improved ST, and we discuss joint decoding and modeling toward coupling ASR and SMT.","1558-2256","","10.1109/JPROC.2013.2249491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497459","Discriminative training;finite-state transducer (FST);graph;hypergraph;speech translation (ST);statistical machine translation (SMT);synchronous context-free grammar (SCFG);Viterbi search","Speech processing;Information processing;Training;Statistical learning;Viterbi algorithm;Automata;Transducers;Context awareness","graph theory;language translation;learning (artificial intelligence);speech coding;speech recognition;statistical analysis;transducers;Viterbi decoding","statistical machine translation;structures;learning;decoding;SMT techniques;speech translation;automatic speech recognition;ASR;finite state transducer;FST;Viterbi algorithm;hypergraphs","","4","","100","IEEE","12 Apr 2013","","","IEEE","IEEE Journals"
"Speech-to-speech Low-resource Translation","H. -C. Liu; M. -Y. Day; C. -C. Wang","Graduate Institute of Information Management National Taipei University, New Taipei City, Taiwan; Graduate Institute of Information Management National Taipei University, New Taipei City, Taiwan; College of Management, Dayeh University, Changhua, Taiwan","2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)","30 Aug 2023","2023","","","91","95","Speech-to-speech translation (S2ST), particularly in the context of low-resource languages, plays a vital role in facilitating global communication. However, comprehensive research in this emerging field is lacking, especially concerning translation without the use of text. The objective of this study is to bridge the gap by conducting a systematic review of existing literature on S2ST for low-resource languages. We discovered 455 articles by searching the Scopus, IEEE Xplore, and ACM Digital Library databases, focusing on identifying research trends. The results highlight significant topics covered in the literature, marking a transition from traditional neural network methodologies to advanced transformer-based models. Our findings provide a robust overview of the S2ST landscape, identifying challenges and potential solutions for future research, particularly regarding the application of this technology in low-resource settings. The research contribution of this study is the insights gleaned will benefit academics and professionals seeking a comprehensive understanding of S2ST for low-resource languages.","2835-5776","979-8-3503-3458-6","10.1109/IRI58017.2023.00023","National Science and Technology Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229412","speech-to-speech;low-resource;textless;machine translation;systematic literature review","Systematics;Filtering;Databases;Focusing;Market research;Transformers;Libraries","deep learning (artificial intelligence);language translation;natural language processing;speech processing","ACM Digital Library databases;global communication;IEEE Xplore;low-resource languages;low-resource settings;S2ST landscape;Scopus;speech-to-speech low-resource translation;transformer-based models","","","","23","IEEE","30 Aug 2023","","","IEEE","IEEE Conferences"
"Robust lecture speech translation for speech misrecognition and its rescoring effect from multiple candidates","K. Sahashi; N. Goto; H. Seki; K. Yamamoto; T. Akiba; S. Nakagawa","Department of Computer Science and Engineering, Toyohashi University of Technology, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Japan; Department of Computer Science, Chubu University, Kasugai, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Japan; Department of Computer Science, Chubu University, Kasugai, Japan","2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)","2 Nov 2017","2017","","","1","6","We describe a scheme to translate spoken English lectures into Japanese consisting of a deep neural network based English automatic speech recognition system (ASR) and an English to Japanese phrase-based statistical machine translation system (SMT). The bad influence of speech misrecognition for the translation model is focused. For coping with bad influence caused by speech misrecognition, we utilized the actual misrecognition results as a parallel corpus. We prepared four ASR systems. Pairs of the results including speech misrecognition and the correct translation into target language are added to an original parallel corpus. When the sentences including misrecognition were added to an original corpus, the baseline model was improved. Next, we prepared speech misrecognition results by using a simulated ASR system. This method also improved the baseline system by about 2.0 BLEU as well as actual ASR systems. Finally, we investigated the effectiveness of optimal selection from multiple candidates / outputs by rescoring based on language models. We found that the performance was improved further by 2.0 ~ 3.0 BLEU, if we can select the optimal candidate.","","978-1-5386-3001-3","10.1109/ICAICTA.2017.8090985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090985","speech misrecognition;speech translation;simulated ASR;parallel corpus;rescoring","Speech;Adaptation models;Speech recognition;Mathematical model;Computational modeling;Acoustics;Error probability","language translation;natural language processing;neural nets;speech recognition;statistical analysis","robust lecture speech translation;ASR system;speech misrecognition;deep neural network;statistical machine translation system;English automatic speech recognition system;English lectures","","","","24","IEEE","2 Nov 2017","","","IEEE","IEEE Conferences"
"A Purely Monotonic Approach to Machine Translation for Similar Languages","Y. K. Thu; A. Finch; E. Sumita; Y. Sagisaka","Multilingual Translation Laboratory, National Institute of Information and Communications Technology, Universal Communication Research Institute, Kyoto, Japan; Multilingual Translation Laboratory, National Institute of Information and Communications Technology, Universal Communication Research Institute, Kyoto, Japan; Multilingual Translation Laboratory, National Institute of Information and Communications Technology, Universal Communication Research Institute, Kyoto, Japan; GITI, Department of Applied Mathematics Language & Speech Science Research Laboratory, Waseda University, Tokyo, Japan","2013 International Conference on Asian Language Processing","24 Oct 2013","2013","","","107","110","This paper investigates the effect of taking a strictly monotonic approach to machine translation for a restricted set of suitable language pairs. We studied the effect of decoding monotonically for a set of language pairs which has similar word order characteristics and found that for some language pairs - namely language pairs where both languages are in SOV order - there was almost no difference in machine translation quality. The results of this experiment motivated the extension of the monotonic approach into the alignment stage of the training. We used a Bayesian non-parametric aligner that has been shown to out-perform GIZA++ in combination with the grow-diag-final- and heuristic on transliteration data. Our results show that the monotonic aligner was able to match the performance of the GIZA++ baseline, and gains in translation performance were obtained by integrating both aligners into the systems.","","978-0-7695-5063-3","10.1109/IALP.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646015","monotonic decoding;machine translation;bilingual alignment","Decoding;Computational linguistics;Training;Bayes methods;Interpolation;Complexity theory;Data models","language translation;natural languages","purely monotonic approach;machine translation;similar languages;Bayesian nonparametric aligner;GIZA++;transliteration data","","","","13","IEEE","24 Oct 2013","","","IEEE","IEEE Conferences"
"A hybrid phonotactic language identification system with an SVM back-end for simultaneous lecture translation","M. Heck; S. St√ºker; A. Waibel","Institute for Anthropomatics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Anthropomatics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Anthropomatics, Karlsruhe Institute of Technology, Karlsruhe, Germany","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4857","4860","In this paper we describe our work in constructing a language identification system for use in our simultaneous lecture translation system. We first built PPR and PPRLM baseline systems that produce score-fusing language cue feature vectors for language discrimination and utilize an SVM back-end classifier for the actual language identification. On our bi-lingual lecture tasks the PPRLM system clearly outperforms the PPR system in various segment length conditions, however at the cost of slower run-time. By using lexical information in the form of keyword spotting, and additional language models we show ways to improve the performance of both baseline systems. In order to combine the faster run-time of the PPR system with the better performance of the PPRLM system we finally built a hybrid of both approaches that clearly outperforms the PPR system while not adding any additional computing time. This hybrid system is therefore our choice for the use in the lecture translation system due to its faster run-time and good performance.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289007","language identification;support vector machines;speech translation;lecture translation","Hidden Markov models;Support vector machines;Decoding;Training;Acoustics;Data models;Computational modeling","language translation;natural language processing;speech processing;support vector machines","hybrid phonotactic language identification system;simultaneous lecture translation;baseline system;score-fusing language cue feature vectors;language discrimination;SVM backend classifier;bilingual lecture task;lexical information;keyword spotting;additional language model;hybrid system;lecture translation system","","4","","16","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"LEAPT: Learning Adaptive Prefix-to-Prefix Translation For Simultaneous Machine Translation","L. Lin; S. Li; X. Shi","Ministry of Culture and Tourism, Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), China; Ministry of Culture and Tourism, Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), China; Ministry of Culture and Tourism, Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan (Xiamen University), China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Simultaneous machine translation, which aims at a realtime translation, is useful in many live scenarios but very challenging due to the trade-off between accuracy and latency. To achieve the balance for both, the model needs to wait for appropriate streaming text (READ policy) and then generates its translation (WRITE policy). However, WRITE policies of previous work either are specific to the method itself due to the end-to-end training or suffer from the input mismatch between training and decoding for the non-end-to-end training. Therefore, it is essential to learn a generic and better WRITE policy for simultaneous machine translation. Inspired by strategies utilized by human interpreters and ""wait"" policies, we propose a novel adaptive prefix-to-prefix training policy called LEAPT, which allows our machine translation model to learn how to translate source sentence prefixes and make use of the future context. Experiments show that our proposed methods greatly outperform competitive baselines and achieve promising results.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10096545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096545","Machine Translation;Simultaneous Machine Translation","Training;Adaptation models;Signal processing;Acoustics;Decoding;Machine translation;Speech processing","","","","","","21","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Sequence-to-sequence models for punctuated transcription combining lexical and acoustic features","O. Klejch; P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","5700","5704","In this paper we present an extension of our previously described neural machine translation based system for punctuated transcription. This extension allows the system to map from per frame acoustic features to word level representations by replacing the traditional encoder in the encoder-decoder architecture with a hierarchical encoder. Furthermore, we show that a system combining lexical and acoustic features significantly outperforms systems using only a single source of features on all measured punctuation marks. The combination of lexical and acoustic features achieves a significant improvement in F-Measure of 1.5 absolute over the purely lexical neural machine translation based system.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953248","punctuation;speech recognition;neural machine translation;rich transcription","Acoustics;Training;Stochastic processes;Hidden Markov models;Speech;Decoding;Data models","acoustic signal processing;decoding;encoding;language translation;neural nets;speech recognition","sequence-to-sequence models;punctuated transcription;lexical features;acoustic features;neural machine translation;word level representations;encoder-decoder architecture;hierarchical encoder;punctuation marks;F-measure;speech recognition","","15","","37","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Improving Automatic Call Classification using Machine Translation","T. A. Faruquie; N. Rajput; V. Raj","IBM India Research Laboratory, New Delhi, India; IBM India Research Laboratory, New Delhi, India; Department of Electrical Engineering, IIT, Chennai, India","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-129","IV-132","Utterance classification is an important task in spoken-dialog systems. The response of the system is dependent on category assigned to the speaker's utterance by the classifier. However, often the input speech is spontaneous and noisy which results in high word error rates. This results in unsatisfactory system performance. In this paper we describe a method to improve the natural language call classification task using statistical machine translation (SMT). We utilize the translation model in SMT to capture the relation between truth and the ASR transcribed text. The model is trained using the human transcribed text and the ASR transcribed text. During deployment SMT is used to sanitize the ASR transcribed text. Our experiments with IBM model 2 shows significant improvement in call classification accuracy.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218054","call classification;call routing;ASR;statistical machine translation","Automatic speech recognition;Humans;Surface-mount technology;Natural languages;Routing;Boosting;Error analysis;System performance;Robustness;Minimization methods","language translation;natural language processing;speech processing","automatic call classification;utterance classification;spoken-dialog systems;statistical machine translation;natural language call classification;human transcribed text;ASR transcribed text","","","","9","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Computer-assisted translation using speech recognition","E. Vidal; F. Casacuberta; L. Rodriguez; J. Civera; C. D. M. Hinarejos","Departamento de Sistemas Inform√°ticos y Computaci√≥n, Instituto Tecnol√≥gico de Inform√°tica, Universitat Polit√©cnica de Val√©ncia, Valencia, Spain; Departamento de Sistemas Inform√°ticos y Computaci√≥n, Instituto Tecnol√≥gico de Inform√°tica, Universitat Polit√©cnica de Val√©ncia, Valencia, Spain; Departamento de Sistemas Inform√°ticos y Computaci√≥n, Instituto Tecnol√≥gico de Inform√°tica, Universitat Polit√©cnica de Val√©ncia, Valencia, Spain; Departamento de Sistemas Inform√°ticos y Computaci√≥n, Instituto Tecnol√≥gico de Inform√°tica, Universitat Polit√©cnica de Val√©ncia, Valencia, Spain; Departamento de Sistemas Inform√°ticos y Computaci√≥n, Instituto Tecnol√≥gico de Inform√°tica, Universitat Polit√©cnica de Val√©ncia, Valencia, Spain","IEEE Transactions on Audio, Speech, and Language Processing","18 Apr 2006","2006","14","3","941","951","Current machine translation systems are far from being perfect. However, such systems can be used in computer-assisted translation to increase the productivity of the (human) translation process. The idea is to use a text-to-text translation system to produce portions of target language text that can be accepted or amended by a human translator using text or speech. These user-validated portions are then used by the text-to-text translation system to produce further, hopefully improved suggestions. There are different alternatives of using speech in a computer-assisted translation system: From pure dictated translation to simple determination of acceptable partial translations by reading parts of the suggestions made by the system. In all the cases, information from the text to be translated can be used to constrain the speech decoding search space. While pure dictation seems to be among the most attractive settings, unfortunately perfect speech decoding does not seem possible with the current speech processing technology and human error-correcting would still be required. Therefore, approaches that allow for higher speech recognition accuracy by using increasingly constrained models in the speech recognition process are explored here. All these approaches are presented under the statistical framework. Empirical results support the potential usefulness of using speech within the computer-assisted translation paradigm.","1558-7924","","10.1109/TSA.2005.857788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621206","Computer-assisted translation (CAT);speech recognition;statistical machine translation","Speech recognition;Humans;Speech processing;Productivity;Natural languages;Decoding;Space technology;Computer errors;Error correction","language translation;speech recognition","computer-assisted translation;speech recognition;text-to-text translation;statistical framework","","16","1","37","IEEE","18 Apr 2006","","","IEEE","IEEE Journals"
"Speech Recognition, Machine Translation, and Speech Translation‚ÄîA Unified Discriminative Learning Paradigm [Lecture Notes]","X. He; L. Deng","Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA","IEEE Signal Processing Magazine","25 Aug 2011","2011","28","5","126","133","In the past two decades, significant progress has been made in automatic speech recognition (ASR) [2], [9] and statistical machine translation (MT) [12]. Despite some conspicuous differences, many problems in ASR and MT are closely related and techniques in the two fields can be successfully cross-pollinated. In this lecture note, we elaborate on the fundamental connections between ASR and MT, and show that the unified ASR discriminative training paradigm recently developed and presented in [7] can be extended to train MT models in the same spirit.","1558-0792","","10.1109/MSP.2011.941852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999580","","Speech recognition;Hidden Markov models;Machine intelligence;Speech processing;Decoding","language translation;learning (artificial intelligence);speech recognition","speech translation;unified discriminative learning paradigm;automatic speech recognition;statistical machine translation;MT models","","19","","28","IEEE","25 Aug 2011","","","IEEE","IEEE Magazines"
"A Maximum-Entropy Segmentation Model for Statistical Machine Translation","D. Xiong; M. Zhang; H. Li","Department of Human Language Technology, Institute for Infocomm Research, Singapore; Department of Human Language Technology, Institute for Infocomm Research, Singapore; Department of Human Language Technology, Institute for Infocomm Research, Singapore","IEEE Transactions on Audio, Speech, and Language Processing","19 Sep 2011","2011","19","8","2494","2505","Segmentation is of great importance to statistical machine translation. It splits a source sentence into sequences of translatable segments. We propose a maximum-entropy segmentation model to capture desirable phrasal and hierarchical segmentations for statistical machine translation. We present an approach to automatically learning the beginning and ending boundaries of cohesive segments from word-aligned bilingual data without using any additional resources. The learned boundaries are then used to define cohesive segments in both phrasal and hierarchical segmentations. We integrate the segmentation model into phrasal statistical machine translation (SMT) and conduct experiments on the newswire and broadcast news domain to investigate the effectiveness of the proposed segmentation model on a large-scale training data. Our experimental results show that the maximum-entropy segmentation model significantly improves translation quality in terms of BLEU. We further validate that 1) the proposed segmentation model significantly outperforms syntactic constraints which are used in previous work to constrain segmentations; and 2) it is necessary to capture hierarchical segmentations besides phrasal segmentations.","1558-7924","","10.1109/TASL.2011.2144971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753927","Bracketing transduction grammar (BTG)-based phrasal machine translation;hierarchical segmentation;maximum entropy;phrasal segmentation;statistical machine translation (SMT)","Training data;Decoding;Entropy;Training;Feature extraction;Syntactics","computational linguistics;language translation;maximum entropy methods;statistical analysis","maximum-entropy segmentation model;source sentence;translatable segments;desirable phrasal segmentations;hierarchical segmentations;cohesive segments;word-aligned bilingual data;learned boundary;phrasal statistical machine translation;SMT;large-scale training data;translation quality;syntactic constraints","","9","","25","IEEE","21 Apr 2011","","","IEEE","IEEE Journals"
"Speech Translation with Phrase Based Stochastic Finite-State Transducers","A. Perez; M. I. Torres; F. Casacuberta","Department Electricity and Electronics, University of Basque Country (UPV-EHU), Spain; Department Electricity and Electronics, University of Basque Country (UPV-EHU), Spain; Department Information Systems and Computation, Technical University of Valencia, Spain","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-113","IV-116","Stochastic finite-state transducers constitute a type of word-based models that allow an easy integration with acoustic model for speech translation. The aim of this work is to develop a novel approach to phrase-based statistical finite-state transducers. In this work, we explore the use of linguistically motivated phrases to build phrase-based models. The proposed phrase-based transducer has been tested and compared to a word-based equivalent machine, yielding promising results in the reported preliminary text and speech translation experiments.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218050","Speech recognition;natural languages;transducers;statistics","Stochastic processes;Acoustic transducers;Natural languages;Probability distribution;Vocabulary;Acoustic testing;Speech recognition;Statistics;Humans;Information systems","language translation;natural language processing;speech processing;speech recognition;stochastic processes;transducers","speech translation;phrase based stochastic finite-state transducers;word-based models;acoustic model","","2","2","8","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Lattice-Based ASR-MT Interface for Speech Translation","E. Matusov; H. Ney","RWTH Aachen University, Aachen, Germany; RWTH Aachen University, Aachen, Germany","IEEE Transactions on Audio, Speech, and Language Processing","10 Feb 2011","2011","19","4","721","732","The usual approach to improve the interface between automatic speech recognition (ASR) and machine translation (MT) is to use ASR word lattices for translation. In comparison with the previous research along this line, this paper presents an efficient algorithm for lattice-based search in MT. This algorithm utilizes confusion network information to enable phrase-level reordering, and is also able to process general lattices. The proposed search is not constrained to be monotonic; thus, it is able to perform the same type of reordering given lattice input as any statistical phrase-based search algorithm with a single sentence input. Using the concept described in this paper, we are able to significantly improve speech translation results on several small and large vocabulary tasks. The improvements of the MT quality as measured by BLEU are as high as 5% relative. We also show that the proposed lattice-based translation can outperform state-of-the-art translation of confusion networks and has advantages in terms of translation speed. Furthermore, we propose and evaluate a novel approach that shares the benefits of lattice-based translation with those translation systems which are not designed to process word lattices.","1558-7924","","10.1109/TASL.2010.2060483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518378","Machine translation (MT);natural languages;SLP-SSMT;speech processing","Lattices;Automatic speech recognition;Natural languages;Speech recognition;Speech processing;Permission;Vocabulary;Art;Process design;Face recognition","language translation;search problems;speech recognition;vocabulary","lattice-based ASR-MT interface;speech translation;automatic speech recognition;machine translation;ASR word lattices;lattice-based search;confusion network information;phrase-level reordering;statistical phrase-based search algorithm;large vocabulary task;MT quality","","7","","32","IEEE","23 Jul 2010","","","IEEE","IEEE Journals"
"Language modeling for voice search: A machine translation approach","Xiao Li; Yun-Cheng Ju; G. Zweig; A. Acero","Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA; Microsoft Research Limited, Redmond, WA, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","4913","4916","This paper presents a novel approach to language modeling for voice search based on the idea and method of statistical machine translation. We propose an n-gram based translation model that can be used for listing-to-query translation. We then leverage the query forms translated from listings to improve language modeling. The translation model is trained in an unsupervised manner using a set of transcribed voice search queries. Experiments show that the translation approach yielded drastic perplexity reductions compared with a baseline language model where no translation is applied.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518759","language modeling;machine translation;voice search;directory assistance","Information retrieval;Automatic speech recognition;Humans;Telephony;Speech recognition;Robustness;Predictive models","language translation;statistical analysis","language modeling;voice search;statistical machine translation;n-gram based translation model;listing-to-query translation","","1","4","12","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"Incremental translation using hierarchichal phrase-based translation system","M. Siahbani; R. M. Seraj; B. Sankaran; A. Sarkar","Simon Fraser University, School of Computing Science, Burnaby BC, CANADA; Simon Fraser University, School of Computing Science, Burnaby BC, CANADA; Simon Fraser University, School of Computing Science, Burnaby BC, CANADA; Simon Fraser University, School of Computing Science, Burnaby BC, CANADA","2014 IEEE Spoken Language Technology Workshop (SLT)","2 Apr 2015","2014","","","71","76","Hierarchical phrase-based machine translation [1] (Hiero) is a prominent approach for Statistical Machine Translation usually comparable to or better than conventional phrase-based systems. But Hiero typically uses the CKY decoding algorithm which requires the entire input sentence before decoding begins, as it produces the translation in a bottom-up fashion. Left-to-right (LR) decoding [2] is a promising decoding algorithm for Hiero that produces the output translation in left to right order. In this paper we focus on simultaneous translation using the Hiero translation framework. In simultaneous translation, translations are generated incrementally as source language speech input is processed. We propose a novel approach for incremental translation by integrating segmentation and decoding in LR-Hiero. We compare two incremental decoding algorithms for LR-Hiero and present translation quality scores (BLEU) and the latency of generating translations for both decoders on audio lectures from the TED collection.","","978-1-4799-7129-9","10.1109/SLT.2014.7078552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078552","Statistical Machine Translation (SMT);Incremental Decoding;Hierarchical Phrase-based Translation (Hiero);Left-to-Right Decoding","Abstracts;History;Feature extraction;Training;Decoding;Joints","language translation;natural language processing;speech coding;statistical analysis","incremental translation;hierarchical phrase-based machine translation system;statistical machine translation;CKY decoding algorithm;left-to-right decoding;LR decoding;Hiero translation framework;source language speech input;segmentation;LR-Hiero;incremental decoding algorithm;translation quality scores;BLEU;audio lectures;TED collection","","","","21","IEEE","2 Apr 2015","","","IEEE","IEEE Conferences"
"Duration Modeling of Neural TTS for Automatic Dubbing","J. Effendi; Y. Virkar; R. Barra-Chicote; M. Federico",Amazon AI; Amazon AI; Amazon AI; Amazon AI,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","8037","8041","Automatic dubbing (AD) addresses the problem of translating speech in a video with speech in another language while preserving the viewer experience. A most important requirement of AD is isochrony, i.e. dubbed speech has to closely match the timing of speech and pauses of the original audio. In our automatic dubbing system, isochrony is modeled by controlling the verbosity of machine translation; inserting pauses in the translations, a.k.a. prosodic alignment; and controlling the duration of text-to-speech (TTS) utterances. The latter two steps heavily rely on speech duration information, either to predict or control TTS duration. So far, duration prediction was based on a proxy method while duration control on linear warping of the TTS speech spectrogram. In this study, we propose novel duration models for neural TTS that can be leveraged both to predict and control TTS duration. Experimental results show that compared to previous work, the new models improve or match the performance of prosodic alignment and significantly enhance neural TTS speech quality for both slow and fast speaking rates.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747158","speech translation;text-to-speech;automatic dubbing;duration modelling","Training;Computational modeling;Force;Pipelines;Predictive models;Speech enhancement;Speech","language translation;natural language processing;neural nets;speech synthesis","duration modeling;viewer experience;automatic dubbing system;isochrony;machine translation;prosodic alignment;text-to-speech utterances;speech duration information;TTS duration;duration prediction;TTS speech spectrogram;neural TTS speech quality;speech translation;verbosity;linear warping;speaking rates","","","","36","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Evaluating a Speech Communication System for Deaf People","V. Lopez-Ludena; R. San-Segundo; R. Martin; D. Sanchez; A. Garcia","Grupo de Tecnolog√≠a del Habla, Universidad Polit√©cnica de Madrid, Spain; Grupo de Tecnolog√≠a del Habla, Universidad Polit√©cnica de Madrid, Spain; Grupo de Tecnolog√≠a del Habla, Universidad Polit√©cnica de Madrid, Spain; Fundaci√≥n CNSE, Spain; Fundaci√≥n CNSE, Spain","IEEE Latin America Transactions","22 Aug 2011","2011","9","4","565","570","This paper describes the development of an Advanced Speech Communication System for Deaf People and its field evaluation in a real application domain: the renewal of Driver's License. The system is composed of two modules. The first one is a Spanish into Spanish Sign Language (LSE: Lengua de Signos Espa√±ola) translation module made up of a speech recognizer, a natural language translator (for converting a word sequence into a sequence of signs), and a 3D avatar animation module (for playing back the signs). The second module is a Spoken Spanish generator from sign-writing composed of a visual interface (for specifying a sequence of signs), a language translator (for generating the sequence of words in Spanish), and finally, a text to speech converter. For language translation, the system integrates three technologies: an example-based strategy, a rule-based translation method and a statistical translator. This paper also includes a detailed description of the evaluation carried out in the Local Traffic Office in the city of Toledo (Spain) involving real government employees and deaf people. This evaluation includes objective measurements from the system and subjective information from questionnaires. Finally, the paper reports an analysis of the main problems and a discussion about possible solutions.","1548-0992","","10.1109/TLA.2011.5993744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5993744","Deaf people;Driver's License renewal;e-Inclusion;Sign Animation;Spanish Sign Language (LSE);Spoken Language Translation","Handicapped aids;Speech;Laboratories;Visualization;Avatars;Conferences","gesture recognition;handicapped aids;language translation;natural language processing;speech processing;speech recognition","deaf people;advanced speech communication system;drivers license;Spanish sign language;speech recognizer;natural language translator","","12","","","IEEE","22 Aug 2011","","","IEEE","IEEE Journals"
"Reinforcing language model for speech translation with auxiliary data","J. Cui; Y. Deng; B. Zhou","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","502","506","Language model domain adaption usually uses a large quantity of auxiliary data in different genres and domains. It has mostly been relying on scoring functions for selection and it is typically independent of intended applications such as machine translation. In this paper, we present a novel domain adaptation approach that is directly motivated by the need of translation engine. We first identify interesting phrases by examining phrase translation tables, and then use those phrases as anchors to select useful and relevant sentences from general domain data, with the goal of improving domain coverage or providing additional contextual information. The experimental results on Farsi to English translation in military force protection domain and Chinese to English translation in travel domain show statistical significant gain using the reinforced language models over the baseline.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373308","","Natural languages;Adaptation model;Context modeling;Buildings;Engines;Protection;Automatic speech recognition;Acoustic noise;Loudspeakers;Training data","language translation;natural language processing;speech processing","speech translation;language model;auxiliary data;scoring functions;machine translation;phrase translation tables;English translation;Farsi;military force protection domain;Chinese","","1","1","18","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Computing consensus translation from multiple machine translation systems","B. Bangalore; G. Bordel; G. Riccardi","AT and T Research Laboratories, Florham Park, NJ, USA; Universidad del Pais Vasco, Lejona, Spain; AT and T Research Laboratories, Florham Park, NJ, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","351","354","We address the problem of computing a consensus translation given the outputs from a set of machine translation (MT) systems. The translations from the MT systems are aligned with a multiple string alignment algorithm and the consensus translation is then computed. We describe the multiple string alignment algorithm and the consensus MT hypothesis computation. We report on the subjective and objective performance of the multilingual acquisition approach on a limited domain spoken language application. We evaluate five domain-independent off-the-shelf MT systems and show that the consensus-based translation performance is equal to or better than any of the given MT systems, in terms of both objective and subjective measures.","","0-7803-7343-X","10.1109/ASRU.2001.1034659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034659","","Natural languages;Tagging;Text categorization;Performance evaluation;Optical wavelength conversion;Impedance matching;Robustness;Speech recognition;Stochastic processes;Automatic speech recognition","language translation;natural language interfaces;speech recognition","machine translation systems;multiple string alignment algorithm;consensus hypothesis computation;multilingual acquisition approach;spoken language;speech recognition","","23","7","9","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"The Asian network-based speech-to-speech translation system","S. Sakti; N. Kimura; M. Paul; C. Hori; E. Sumita; S. Nakamura; J. Park; C. Wutiwiwatchai; B. Xu; H. Riza; K. Arora; C. M. Luong; H. Li","National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; Electronics and Telecommunications Research Institute, South Korea; National Electronics and Computer Technology Center, Thailand; Institute of Automation,CASIA, Chinese Academy and Sciences, China; Agency for Assessment and Application of Technology (BPPT), Indonesia; Centre for Development of Advanced Computing, India; Institute of Information Technology IOIT, Vietnam; Institute for Infocomm Research, Singapore","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","507","512","This paper outlines the first Asian network-based speech-to-speech translation system developed by the Asian Speech Translation Advanced Research (A-STAR) consortium. The system was designed to translate common spoken utterances of travel conversations from a certain source language into multiple target languages in order to facilitate multiparty travel conversations between people speaking different Asian languages. Each A-STAR member contributes one or more of the following spoken language technologies: automatic speech recognition, machine translation, and text-to-speech through Web servers. Currently, the system has successfully covered 9 languages-namely, 8 Asian languages (Hindi, Indonesian, Japanese, Korean, Malay, Thai, Vietnamese, Chinese) and additionally, the English language. The system's domain covers about 20,000 travel expressions, including proper nouns that are names of famous places or attractions in Asian countries. In this paper, we discuss the difficulties involved in connecting various different spoken language translation systems through Web servers. We also present speech-translation results on the first A-STAR demo experiments carried out in July 2009.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373353","","Natural languages;Automatic speech recognition;Telecommunication computing;Speech synthesis;Web server;Communications technology;Computer networks;Paper technology;Automation;Information technology","language translation;speech recognition","Asian network;speech-to-speech translation;Asian speech translation advanced research;Asian languages;automatic speech recognition;machine translation;text-to-speech","","7","","20","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Machine translation of conversation on the digitized battlefield","J. Moody; E. Steinbrecher; R. Frederking; A. Black; R. Brown","Lockheed Martin Systems Integration, Owego, NY, USA; Lockheed Martin Systems Integration, Owego, NY, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA","2001 MILCOM Proceedings Communications for Network-Centric Operations: Creating the Information Force (Cat. No.01CH37277)","6 Aug 2002","2001","1","","635","639 vol.1","The technology of the information age and the digitized battlefield apply not only to machine-to-machine communications but support an unprecedented advancement in communication across human language barriers. Modern military operations, including ""peace-keeping, "" coordination with joint and multinational forces, and operations other than war, often involve the interaction of troops with indigenous peoples where language issues can have a serious impact on mission success. The Audio Voice Translation Guide System (Tongues) is a mobile, networked, speech-to-speech machine translation system that both benefits from and contributes to the digitized battlefield. The Tongues prototype runs on a commercial off-the-shelf (COTS) mobile computing platform, capable of fitting in a cargo pocket, and running a Microsoft Windows operating system. It supports real time interactive translation of bilingual conversation, within and without the selected target domain, with the highest quality appearing within the target domain. Tongues has a modular architecture, allowing for easy upgrades to the speech and language engines and multiple conversational domains and languages. The prototype system supports English/Croatian conversation for deployed US Army chaplains, a domain that includes religious support, humanitarian aid, and operations other than war. The system supports text-entry as well as speech input and provides feedback for verification of translation accuracy. The Tongues system can automatically capture and store textual and audio conversation transcripts in formats that comply with the Joint Technical Architecture-Army (JTA-A). It supports a wide variety of JTA-A compliant means for transferring that information to other information systems. Tongues can run in a stand-alone mode, or it can make a wireless network connection to remote machine translation servers, allowing for increased performance and flexibility.","","0-7803-7225-5","10.1109/MILCOM.2001.985914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985914","","Prototypes;Speech;Natural languages;Peace technology;Mobile communication;Humans;Mobile computing;Military computing;Operating systems;Computer architecture","language translation;natural languages;speech processing;military communication;real-time systems;interactive systems;speech recognition","machine conversation translation;digitized battlefield;military operations;peace-keeping;multinational forces;joint multinational forces;Audio Voice Translation Guide System;Tongues;mobile machine translation system;COTS mobile computing platform;commercial off-the-shelf platform;Microsoft Windows operating system;real time interactive translation;bilingual conversation;modular architecture;speech engine;English/Croatian conversation;language engine;translation accuracy verification;US Army;humanitarian aid;religion;Joint Technical Architecture-Army;JTA-A;voice recognition engine","","1","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Spoken Language Translation using Conformer model","L. A. Kumar; D. K. Renuka; V. H. Priya; S. Sudarshan","Department of EEE, PSG College of Technology, Coimbatore, India; Department of IT, PSG College of Technology, Coimbatore, India; Department of EEE, PSG College of Technology, Coimbatore, India; Department of EEE, PSG College of Technology, Coimbatore, India","2023 International Conference on Intelligent Systems for Communication, IoT and Security (ICISCoIS)","19 Apr 2023","2023","","","466","471","One of the earliest significant projects for automatic speech translation is Spoken Language Translator (SLT). The SLT system can switch between English and other languages with an accuracy of roughly 75% and a vocabulary of about 1500 words. The SRI Core Language Engine serves as the foundation for the language processing components, which are specified and essentially built on top of it by employing a mixture of general grammars and techniques that allow for easy customization to specific domains. The process of translating spoken phrases into the target language is known as spoken language translation. This encourages communication between language speakers. The ASR and NMT systems are combined in the speech translation system. A speech recognizer collects the phrase after the speaker speaks. The input is then transformed into a sequence of words using a language's vocabulary and grammar using a sizable corpus of text. The text is translated into another language by the machine translation function. This description of SLT is a crucial source for academics curious about the current state of SLT.","","979-8-3503-3583-5","10.1109/ICISCoIS56541.2023.10100421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100421","Spoken language translation;Automatic speech recognition;Machine translation","Deep learning;Vocabulary;Neural networks;Switches;Transformers;Grammar;Security","grammars;language translation;natural language processing;speech recognition","automatic speech translation;earliest significant projects;language processing components;language speakers;Language Translator;machine translation function;SLT system;speech translation system;spoken language translation;spoken phrases;SRI Core Language Engine;target language","","","","25","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"A Cascaded Approach to the Optimization of Translation Rules","S. -j. Liu; M. -y. Yang; T. -j. Zhao","MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of NLP and Speech, Harbin Institute of Technology, Harbin, China","2006 International Conference on Machine Learning and Cybernetics","4 Mar 2009","2006","","","4089","4092","As far as the rule-based machine translation (RBMT) is concerned, the rule acquisition remains as a bottle-neck problem. This paper proposes a cascaded approach to optimize the rule base, which is automatically acquired from the bilingual corpus. Observing the more risk of errors in the upper layer of the parsing tree, we propose in this paper a method which advocates the optimization of rules by a bottom-up strategy so as to take the advantage of correctness of parsing results near the leaf nodes. The experimental results further prove that such cascaded optimization out-performs the usual practice","2160-1348","1-4244-0061-9","10.1109/ICMLC.2006.258866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4028787","Machine Translation;Rule Optimization;Syntactic Tree","Frequency;Laboratories;Speech;Computer science;Error correction;Optimization methods;Natural languages;Filters;Testing;NIST","computational linguistics;grammars;knowledge acquisition;knowledge based systems;language translation;linguistics;natural languages;optimisation;tree data structures","cascaded optimization approach;translation rule optimization;rule-based machine translation;rule acquisition;bilingual corpus;parsing tree;bottom-up strategy;leaf nodes","","2","","10","IEEE","4 Mar 2009","","","IEEE","IEEE Conferences"
"Machine Translation to Sign Language Using Post-Translation Replacement Without Placeholders","T. Miyazaki; N. Nakatani; T. Uchida; H. Kaneko; M. Sano","NHK Science & Technology Research Laboratories, Tokyo, Japan; NHK Science & Technology Research Laboratories, Tokyo, Japan; NHK Science & Technology Research Laboratories, Tokyo, Japan; NHK Science & Technology Research Laboratories, Tokyo, Japan; NHK Science & Technology Research Laboratories, Tokyo, Japan","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","Sign language is typically the first language for those who are born deaf or who lose their hearing in early childhood. To provide important information for these individuals, it is better to use sign language than to transcribe spoken languages. We have been developing a system that translates Japanese into Japanese Sign Language (JSL) and then generates computer graphics (CG) animation of JSL.In this paper, we propose a machine translation method for translating Japanese into JSL. The proposed method is based on an encoder-decoder model that utilizes a pre-trained model as the encoder, and the proper names in the translation result are revised using a dictionary by means of a post-translation replacement method without placeholders. Our experimental results demonstrate that using the pre-trained model as the encoder and performing the post-translation replacement of proper names both contributed to improving the translation quality.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193419","Sign language;machine translation;proper names;post-translation replacement","Dictionaries;Fluctuations;Computational modeling;Conferences;Gesture recognition;Assistive technologies;Signal processing","handicapped aids;language translation;natural language processing","Japanese Sign Language;JSL;machine translation method;post-translation replacement method;spoken languages;translation quality;translation result","","","","20","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Constrained phrase-based translation using weighted finite-state transducers","Bowen Zhou; S. F. Chen; Yuqing Gao","IBM T. J. Watson Research Center, New York, USA; IBM T. J. Watson Research Center, New York, USA; IBM T. J. Watson Research Center, New York","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","1","","I/1017","I/1020 Vol. 1","Phrase-based translation models have shown clear advantages over word-based models, and weighted finite-state transducers (WFSTs) provide a unified framework for integrating the various components of a speech-to-speech translation system, such as speech recognition and machine translation. The paper combines these two ideas by proposing a constrained phrase-based statistical machine translation system that we implement using WFSTs. We evaluate the proposed model on a bidirectional Chinese-English translation task and show improvements over our previous system.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415289","","Transducers;Surface-mount technology;Speech recognition;Natural languages;Context modeling;Data mining;Frequency estimation;Speech processing;Decoding;Training data","language translation;speech recognition;statistical analysis;natural languages","constrained phrase-based translation models;weighted finite-state transducers;word-based translation models;speech-to-speech translation system;speech recognition;statistical machine translation system;bidirectional Chinese-English translation task","","4","5","10","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Machine translation based on translation corresponding tree structure","F. Wong; M. Dong; D. Hu","Department of Automation, Tsinghua University, Beijing 100084, China; Department of Automation, Tsinghua University, Beijing 100084, China; Department of Automation, Tsinghua University, Beijing 100084, China","Tsinghua Science and Technology","17 Jan 2012","2006","11","1","25","31","A representation schema called translation corresponding tree (TCT) has been applied to a Portuguese to Chinese example-based machine translation system. The translation examples are annotated by the representation of the TCT structure. Each TCT describes not only the syntactic structure of the source sentence (i.e., Portuguese in our system) but also the translation correspondences (i.e., Chinese translation). In addition, the TCT nodes describe the corresponding linguistic relationships between the source and target languages. The translation examples can be effectively represented with this annotation schema and organized in the bilingual knowledge database or example base. In the real machine translation process, the target language is synthesized with higher quality by referring to the TCT translation information.","1007-0214","","10.1016/S1007-0214(06)70150-X","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075857","translation corresponding tree (TCT);machine translation (MT);Portuguese to Chinese MT;example representation schema","Syntactics;Knowledge based systems;Pragmatics;Speech;Buildings;Educational institutions;Indexes","","","","2","","","","17 Jan 2012","","","TUP","TUP Journals"
"A Study on the Portuguese Translation Strategies of Culturally Loaded Words in ‚ÄúFrog‚Äù from the Perspective of Manipulation Theory - Based on the comparison of machine translation and manual translation","Y. Yu; X. Hongying","Department of Humanities, Gannan University of Science and Technology, Ganzhou City, Jiangxi Province, China; Department of Humanities, Gannan University of Science and Technology, Ganzhou City, Jiangxi Province, China","2021 International Conference on Internet, Education and Information Technology (IEIT)","7 Sep 2021","2021","","","412","416","As artificial intelligence and big data develop, machine translation has made big strides in terms of translation quality, language coverage and knowledge acquisition. And it has the great potential to replace manual translation. Therefore, the speech that machine translation dominates the world is on the rise. While, for Chinese literature, whether it can be ‚Äúgoing out‚Äù successfully ou not. Firstly, it is necessary to clarify how to make foreign readers understand the culturally loaded words in Chinese literature. Therefore, the author analyzes the Portuguese translation strategy of culturally loaded words in Frog.","","978-1-6654-2563-6","10.1109/IEIT53597.2021.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526070","manipulation;machine translation;culturally loaded words;translation strategies","Knowledge acquisition;Education;Manuals;Big Data;Internet;Machine translation;Cultural differences","artificial intelligence;Big Data;knowledge acquisition;language translation;natural language processing","machine translation;manual translation;big data;translation quality;language coverage;knowledge acquisition;culturally loaded words;Portuguese translation;artificial intelligence;Chinese literature","","","","12","IEEE","7 Sep 2021","","","IEEE","IEEE Conferences"
"Simultaneous machine translation of german lectures into english: Investigating research challenges for the future","M. Wolfel; M. Kolss; F. Kraft; J. Niehues; M. Paulik; A. Waibel","Fakult√§t f√ºr Informatik, Universit√§t Karlsruhe, Germany; Fakult√§t f√ºr Informatik, Universit√§t Karlsruhe, Germany; Fakult√§t f√ºr Informatik, Universit√§t Karlsruhe, Germany; Fakult√§t f√ºr Informatik, Universit√§t Karlsruhe, Germany; Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","233","236","An increasingly globalized world fosters the exchange of students, researchers or employees. As a result, situations in which people of different native tongues are listening to the same lecture become more and more frequent. In many such situations, human interpreters are prohibitively expensive or simply not available. For this reason, and because first prototypes have already demonstrated the feasibility of such systems, automatic translation of lectures receives increasing attention. A large vocabulary and strong variations in speaking style make lecture translation a challenging, however not hopeless, task. The scope of this paper is to investigate a variety of challenges and to highlight possible solutions in building a system for simultaneous translation of lectures from German to English. While some of the investigated challenges are more general, e.g. environment robustness, other challenges are more specific for this particular task, e.g. pronunciation of foreign words or sentence segmentation. We also report our progress in building an end-to-end system and analyze its performance in terms of objective and subjective measures.","","978-1-4244-3471-8","10.1109/SLT.2008.4777883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777883","automatic speech recognition;machine translation;speech-to-speech translation","Natural languages;Automatic speech recognition;Microphones;Humans;Robustness;Space technology;Acoustic distortion;Prototypes;Vocabulary;Performance analysis","language translation","simultaneous machine translation;German lectures;English;globalized world;human interpreters;automatic translation;large vocabulary;environment robustness;sentence segmentation","","2","","11","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Design of Chinese Speech Intelligibility Evaluation System Based on Machine Learning Algorithm","S. Pan","Xinjiang Career Technical College, Wulumuqi, Xinjiang, China","2023 Asia-Europe Conference on Electronics, Data Processing and Informatics (ACEDPI)","26 Jun 2023","2023","","","118","121","With the acceleration of globalization and the popularization of computer technique, the links between different parts of the world are getting closer and closer, and the translation between different languages has become a key research topic of domestic and foreign scholars. Speech recognition is to identify the original speech signal into the corresponding text or other forms of message that can be processed by the computer. Speech recognition technique is an important research direction in the domain of artificial intelligence, which has high research value and commercial value. The method adopted in this paper is machine learning algorithm, and the deep learning technique among machine learning algorithms is selected. In this paper, the influence of high-order harmonic distortion, high frequency and mixed elements on the quality of speech signals caused by human vocal organs and speech signal acquisition equipment is studied. Windowing and framing are performed, so that the whole speech waveform will be divided into many small speech segments with overlapping 25ms, and then appropriate acoustic feature extraction algorithm is used to extract the corresponding acoustic features from the 25ms speech segments. After research, this method ameliorates the efficiency of speech recognition, which is higher than that of traditional methods.","","979-8-3503-0115-1","10.1109/ACEDPI58926.2023.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158109","Machine learning algorithm;Chinese;Speech intelligibility","Deep learning;Machine learning algorithms;Training data;Speech recognition;Speech enhancement;Markov processes;Big Data","deep learning (artificial intelligence);feature extraction;natural language processing;speech intelligibility;speech processing;speech recognition","acoustic feature extraction algorithm;artificial intelligence;chinese speech intelligibility evaluation system;deep learning technique;high-order harmonic distortion;human vocal organs;machine learning algorithm;speech recognition technique;speech signal acquisition;speech waveform","","","","15","IEEE","26 Jun 2023","","","IEEE","IEEE Conferences"
"Topic-Independent Speaking-Style Transformation of Language Model for Spontaneous Speech Recognition","Y. Akita; T. Kawahara","Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan; Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-33","IV-36","For language modeling of spontaneous speech, we propose a novel approach, based on the statistical machine translation framework, which transforms a document-style model to the spoken style. For better coverage and more reliable estimation, incorporation of POS (part-of-speech) information is explored in addition to lexical information. In this paper, we investigate several methods that combine POS-based model or integrate POS information in the ME (maximum entropy) scheme. They achieve significant reduction in perplexity and WER in a meeting transcription task. Moreover, the model is applied to different domains or committee meetings of different topics. As a result, even larger perplexity reduction is achieved compared with the case tested in the same domain. The result demonstrates the generality and portability of the model.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218030","language model;statistical transformation;spontaneous speech;automatic speech recognition","Natural languages;Speech recognition;Probability;Entropy;Testing;Automatic speech recognition;Differential equations;Costs;Databases;Speech synthesis","language translation;maximum entropy methods;speech recognition;statistical analysis","topic-independent speaking-style transformation;language model;spontaneous speech recognition;statistical machine translation framework;document-style model;part-of-speech information;lexical information;maximum entropy scheme","","7","","8","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"JANUS 93: towards spontaneous speech translation","M. Woszczyna; N. Aoki-Waibel; F. D. Buo; N. Coccaro; K. Horiguchi; T. Kemp; A. Lavie; A. McNair; T. Polzin; I. Rogina; C. P. Rose; T. Schultz; B. Suhm; M. Tomita; A. Waibel","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; NA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; NA","Proceedings of ICASSP '94. IEEE International Conference on Acoustics, Speech and Signal Processing","6 Aug 2002","1994","i","","I/345","I/348 vol.1","We present first results from our efforts toward translation of spontaneously spoken speech. Improvements include increasing coverage, robustness, generality and speed of JANUS, the speech-to-speech translation system of Carnegie Mellon and Karlsruhe University. The recognition and machine translation engine have been upgraded to deal with requirements introduced by spontaneous human to human dialogs. To allow for development and evaluation of our system on adequate data, a large database with spontaneous scheduling dialogs is being gathered for English, German and Spanish.<>","1520-6149","0-7803-1775-0","10.1109/ICASSP.1994.389285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=389285","","Humans;Databases;Speech synthesis;Vocabulary;Robustness;Engines;Switches;Springs;Face;Calendars","language translation;speech recognition;natural languages","coverage;JANUS 93;spontaneous speech translation;robustness;generality;speed;Karlsruhe University;Carnegie Mellon University;recognition;machine translation engine;spontaneous human to human dialogs;database;spontaneous scheduling dialogs;English;German;Spanish","","19","","14","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Part of Speech Tagging for Tamil Language Using Deep Learning","H. Visuwalingam; R. Sakuntharaj; R. G. Ragel","Centre for Information & Communication Technology, Eastern University, Sri Lanka; Centre for Information & Communication Technology, Eastern University, Sri Lanka; Department of Computer Engineering, University of Peradeniya, Sri Lanka","2021 IEEE 16th International Conference on Industrial and Information Systems (ICIIS)","3 Jan 2022","2021","","","157","161","Part of Speech (POS) tagging is the process of marking up a word in a sentence to a corresponding part of speech. POS tagging is considered one of the pre-processing steps in Natural Language Processing (NLP) applications such as speech recognition, machine translation and sentiment analysis. A few works have been conducted to determine the POS tags for the Tamil words. However, the performance of the POS tagger with unknown words (words that do not appear in the lexicon) is not explored in the literature. The appearance of unknown words is a frequently occurring problem in POS tagging because, in real-world use, the NLP application will encounter words that are not in its lexicon. This paper proposes a deep learning-based POS tagger for the Tamil language using Bi-directional Long Short Term Memory (BLSTM). Our experiments use two corpora, one is AU-KBC annotated corpus, and the other is MeitY corpus. We also analysed the performance of the POS tagger with unknown words. Test results show that the POS tags for Tamil words determined by this approach have 99.8%, 99.5% and 96.5% accuracies for only known words, around 9.8% unknown words and 47.6% unknown words in test sentences respectively.","2164-7011","978-1-6654-2637-4","10.1109/ICIIS53135.2021.9660738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660738","Part of Speech;Deep Learning;Tamil;BLSTM","Deep learning;Sentiment analysis;Conferences;Speech recognition;Bidirectional control;Tagging;Machine translation","deep learning (artificial intelligence);language translation;natural language processing;recurrent neural nets;speech recognition;text analysis","known words;part of speech tagging;POS tagging;natural language processing applications;speech recognition;Tamil words;deep learning-based POS tagger;Tamil language;NLP;machine translation;sentiment analysis;bi-directional long short term memory;BLSTM;AU-KBC annotated corpus;MeitY corpus","","2","","18","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Low Resource Speech-to-Speech Translation of English videos to Kannada with Lip-Synchronization","R. V. Malage; H. Ashish; S. Hukkeri; E. Kavya; R. Jayashree","PES University, Bangalore, India; PES University, Bangalore, India; PES University, Bangalore, India; PES University, Bangalore, India; PES University, Bangalore, India","2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS)","8 Jun 2023","2023","","","1680","1687","The introduction of online education has revolutionized the way people acquire knowledge. With resources from top universities like Stanford and MIT, students have access to an immense wealth of educational videos and other material. However, the majority of these resources are in English, creating a language barrier that prevents non-native speakers from utilizing them effectively. In addition, Indic languages have very limited educational resources available online and manual dubbing of videos is time-consuming and labour-intensive. Automated subtitling is not a feasible alternative either, as reading Kannada subtitles alongside English videos can be strenuous. To address these issues, a system is needed that can dub English videos into a specific target language. The focus of this work is on developing such a system for Kannada. The proposed solution is an exhaustive audio-visual system that generates Kannada videos from English videos with accurate lip synchronization, even in dynamic environments, with any kind of face or voice. The system consists of four stages, namely speech-to-text, neural machine translation, text-to-speech, and lip-synchronization. Each component has been evaluated using different metrics, such as Word Error Rate (WER), BiLingual Evaluation Understudy (BLEU) scores, and Mean Opinion Score (MOS), based on surveys conducted to test the output. Extensive quantitative evaluations portray that the naturalness of the lip-synced output video is superior to manually dubbed videos, proved by an overall Mean Opinion Score (MOS) of 4.56. The proposed system has the potential to revolutionize the film industry by replacing the entire manual dubbing process and generating Pan-Global movies. It will also bridge the language gap for people with little or no understanding of English and allow them to access valuable educational resources online in their native language.","2768-5330","979-8-3503-9725-3","10.1109/ICICCS56967.2023.10142578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142578","Dubbing;Lip synchronization;Kannada;Voice cloning;Neural Machine Translation;Automatic Speech Recognition;Text to Speech;Indic languages","Surveys;Lips;Entertainment industry;Speech recognition;Streaming media;Synchronization;Speech synthesis","audio-visual systems;language translation;natural language processing;speech recognition","access valuable educational resources;accurate lip synchronization;educational resources available online;educational videos;English videos;entire manual dubbing process;Kannada subtitles;Kannada videos;lip-synced output video;lip-synchronization;low resource speech-to-speech translation;online education;speech-to-text;text-to-speech","","","","18","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"Language resources for Myanmar NLP and speech processing","W. Pa Pa","Natural Language Processing Lab, University of Computer Studies, Yangon, Myanmar","2019 22nd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","23 Apr 2020","2019","","","1","7","Collection of slides from author's presentation is given. The following topics are dealt with: automatic speech recognition; building large pronunciation dictionary; enhancing naturalness of Myanmar speech synthesis with linguistic information and LSTM-RNN; machine translation; and name entity recognition.","2472-7695","978-1-7281-2449-0","10.1109/O-COCOSDA46868.2019.9060834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060834","","Machine translation;Conferences;Speech synthesis;Silicon compounds;Linguistics;Dictionaries;Buildings","natural language processing;recurrent neural nets;speech recognition;speech synthesis","long-short term memory;recurrent neural network;name entity recognition;machine translation;LSTM-RNN;linguistic information;Myanmar speech synthesis;pronunciation dictionary;automatic speech recognition;Myanmar language;speech processing;Myanmar NLP;language resources","","","","","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"ASCII based transcription systems for languages with the Arabic script: the case of Persian","S. Ganjavi; P. G. Georgiou; S. Narayanan","Department of Linguistics, University of Southern California, USA; Department of Electrical Engineering Speech Analysis and Interpretation Laboratory, University of Southern California, USA; Department of Linguistics, University of Southern California, USA","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","595","600","We discuss transcription systems needed for automated spoken language processing applications in languages such as Persian that use the Arabic script for writing. The work is described in the context of a speech-to-speech translation system development for English and Persian. This system can easily be modified for Arabic, Dari, Urdu and any other language that uses the Arabic script. The proposed system has two components. One is a phonemic based transcription of sounds for acoustic modeling in automatic speech recognizers and for text-to-speech synthesizers, using ASCII based symbols, rather than International Phonetic Alphabet symbols. The other is a hybrid system, that provides a minimally-ambiguous lexical representation that explicitly includes vocalic information; such a representation is needed for language modeling and machine translation.","","0-7803-7980-2","10.1109/ASRU.2003.1318507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318507","","Computer aided software engineering;Natural languages;Writing;Automatic speech recognition;Speech synthesis;Speech analysis;Laboratories;Text recognition;Synthesizers;Speech recognition","natural languages;natural language interfaces;speech recognition;speech synthesis;language translation;signal representation;speech-based user interfaces;linguistics;text analysis","ASCII based transcription systems;Arabic script;Persian language;automated spoken language processing;speech-to-speech translation system;English language;sound transcription;automatic speech recognition;text-to-speech synthesizers;International Phonetic Alphabet symbols;language modeling;machine translation","","","","7","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Gamayun - Language Technology for Humanitarian Response","A. √ñktem; M. A. Jaam; E. DeLuca; G. Tang",Translators without Borders; Translators without Borders; Translators without Borders; Translators without Borders,"2020 IEEE Global Humanitarian Technology Conference (GHTC)","8 Feb 2021","2020","","","1","4","Over half of the world's population do not have access to knowledge and information because it's not available in their language. Translators without Borders (TWB) wants to change this with Gamayun, an initiative to promote language equality. Gamayun uses advanced language technologies to improve communication with people who speak marginalized languages in humanitarian and development contexts. In this paper, we present the early implementation results of the project in building machine translation and automatic speech recognition systems for various marginalized languages.","2377-6919","978-1-7281-7388-7","10.1109/GHTC46280.2020.9342939","Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9342939","language technology;machine translation;automatic speech recognition;low-resource languages","Sociology;Tools;Machine translation;Task analysis;Statistics;Open source software;Automatic speech recognition","language translation;natural language processing;speech recognition","humanitarian response;language equality;language technologies;marginalized languages;humanitarian development contexts;machine translation;Gamayun;Translators without Borders;automatic speech recognition systems","","2","","18","IEEE","8 Feb 2021","","","IEEE","IEEE Conferences"
"ISOMETRIC MT: Neural Machine Translation for Automatic Dubbing","S. M. Lakew; Y. Virkar; P. Mathur; M. Federico",Amazon AI; Amazon AI; Amazon AI; Amazon AI,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6242","6246","Automatic dubbing (AD) is among the machine translation (MT) use cases where translations should match a given length to allow for synchronicity between source and target speech. For neural MT, generating translations of length close to the source length (e.g. within ¬±10% in character count), while preserving quality is a challenging task. Controlling MT output length comes at a cost to translation quality, which is usually mitigated with a two step approach of generating N-best hypotheses and then re-ranking based on length and quality. This work introduces a self-learning approach that allows a transformer model to directly learn to generate outputs that closely match the source length, in short Isometric MT. In particular, our approach does not require to generate multiple hypotheses nor any auxiliary ranking function. We report results on four language pairs (English ‚Üí French, Italian, German, Spanish) with a publicly available benchmark. Automatic and manual evaluations show that our method for Isometric MT outperforms more complex approaches proposed in the literature.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747023","Machine Translation;Isometric Translation;Automatic Dubbing","Costs;Impedance matching;Conferences;Manuals;Signal processing;Benchmark testing;Transformers","language translation;learning (artificial intelligence);natural language processing","MT output length;translation quality;self-learning approach;auxiliary ranking function;neural machine translation;automatic dubbing;target speech;neural MT;AD;isometric MT","","1","","31","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates","H. Inaguma; S. Dalmia; B. Yan; S. Watanabe","Kyoto University, Japan; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","3 Feb 2022","2021","","","922","929","The multi-decoder (MD) end-to-end speech translation model has demonstrated high translation quality by searching for better intermediate automatic speech recognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass decoding model decomposing the overall task into ASR and machine translation sub-tasks. However, the decoding speed is not fast enough for real-world applications because it conducts beam search for both sub-tasks during inference. We propose Fast-MD, a fast MD model that generates HI by non-autoregressive (NAR) decoding based on connectionist temporal classification (CTC) outputs followed by an ASR decoder. We investigated two types of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR decoder and (2) masked HI by using Mask-CTC, which combines CTC and the conditional masked language model. To reduce a mismatch in the ASR decoder between teacher-forcing during training and conditioning on CTC outputs during testing, we also propose sampling CTC outputs during training. Experimental evaluations on three corpora show that Fast-MD achieved about 2√ó and 4√ó faster decoding speed than that of the na√Øve MD model on GPU and CPU with comparable translation quality. Adopting the Conformer encoder and intermediate CTC loss further boosts its quality without sacrificing decoding speed.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9687894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687894","End-to-end speech translation;multi-decoder;non-autoregressive decoding;CTC;Mask-CTC","Training;Bridges;Conferences;Graphics processing units;Transformers;Decoding;Machine translation","autoregressive processes;decoding;deep learning (artificial intelligence);inference mechanisms;language translation;sampling methods;speech recognition","CTC output sampling;masked HI;parallel HI;connectionist temporal classification outputs;ASR decoder;decoding speed;intermediate CTC loss;conditional masked language model;Mask-CTC;autoregressive Transformer ASR decoder;NAR HI;machine translation sub-tasks;intermediate automatic speech recognition decoder states;high translation quality;multidecoder end-to-end speech translation model;nonautoregressive hidden intermediates;Fast-MD","","2","","70","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"WordNet Based Sign Language Machine Translation: from English Voice to ISL Gloss","K. Saija; S. Sangeetha; V. Shah","Radisys India Pvt. Ltd., Bengaluru, India; National Institute of Technology, Trichy, India; Indian Institute of Technology, Guwahati, India","2019 IEEE 16th India Council International Conference (INDICON)","12 Mar 2020","2019","","","1","4","Communication is an essential part of human life. For the person with hearing and speaking disability, it is inconvenient to communicate with other people. In this paper, an end-to-end system to convert English voice to Indian Sign Language (ISL) gloss (written form of sign language) is proposed which will help deaf to communicate with others and vice versa. This system accepts English voice as an input and, converts it into the text using the speech recognition. From the recognized English text, ISL gloss is generated using the lexical database called WordNet. The focus of our work is to build a robust sign language machine translation system to convert the English text to ISL gloss using the linguistic database WordNet.","2325-9418","978-1-7281-2327-1","10.1109/INDICON47234.2019.9029074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9029074","Indian Sign Language;Sign language machine translation;WordNet;HMM;Natural Language Processing","Assistive technology;Gesture recognition;Dictionaries;Speech recognition;Hidden Markov models;Databases;Statistical analysis","handicapped aids;human computer interaction;language translation;natural language processing;sign language recognition;speech recognition;text analysis","ISL gloss;speaking disability;end-to-end system;Indian Sign Language gloss;linguistic database;English text recognition;English voice input;speech recognition;WordNet based sign language machine translation","","7","","16","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"English to Japanese spoken language translation system for classroom lectures","V. Ferdiansyah; S. Nakagawa","Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan","2014 International Conference of Advanced Informatics: Concept, Theory and Application (ICAICTA)","12 Jan 2015","2014","","","34","38","This paper presents our attempt to create English automatic speech recognition (ASR) and English to Japanese statistical machine translation system (SMT). We used MIT OpenCourseWare lectures as our test lecture corpus. Wall Street Journal (WSJ) corpus adapted with MIT OpenCourseWare lectures was used as our acoustic model. MIT OpenCourseWare lecture transcriptions were utilized to create our language model. As for the parallel corpus, we used TED Talks and Japanese-English News Article Alignment Data (JENAAD). Our proposed ASR system can achieve 32.1% 0word error rate (WER) and our SMT system can achieve 10.95 BLEU.","","978-1-4799-5100-0","10.1109/ICAICTA.2014.7005911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005911","automatic speech recognition;machine translation;classroom lectures;MIT OCW","Adaptation models;Speech;Acoustics;Hidden Markov models;Data models;Speech recognition;Informatics","courseware;language translation;speech recognition","classroom lectures;English automatic speech recognition;English-to-Japanese statistical machine translation system;MIT OpenCourseWare lectures;Wall Street Journal corpus;WSJ corpus;acoustic model;lecture transcriptions;TED Talks;JapaneseEnglish News Article Alignment Data;JENAAD;ASR system;word error rate;WER;SMT system","","2","","30","IEEE","12 Jan 2015","","","IEEE","IEEE Conferences"
"Language model adaptation for ASR of spoken translations using phrase-based translation models and named entity models","J. Pelemans; T. Vanallemeersch; K. Demuynck; L. Verwimp; H. Van hamme; P. Wambacq","ESAT, KU Leuven, Belgium; Centre for Computational Linguistics, KU Leuven, Belgium; ELIS, Ghent University, Belgium; ESAT, KU Leuven, Belgium; ESAT, KU Leuven, Belgium; ESAT, KU Leuven, Belgium","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","5985","5989","Language model adaptation based on Machine Translation (MT) is a recently proposed approach to improve the Automatic Speech Recognition (ASR) of spoken translations that does not suffer from a common problem in approaches based on rescoring i.e. errors made during recognition cannot be recovered by the MT system. In previous work we presented an efficient implementation for MT-based language model adaptation using a word-based translation model. By omitting renormalization and employing weighted updates, the implementation exhibited virtually no adaptation overhead, enabling its use in a real-time setting. In this paper we investigate whether we can improve recognition accuracy without sacrificing the achieved efficiency. More precisely, we investigate the effect of both state-of-the-art phrase-based translation models and named entity probability estimation. We report relative WER reductions of 6.2% over a word-based LM adaptation technique and 25.3% over an unadapted 3-gram baseline on an English-to-Dutch dataset.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472826","speech recognition;spoken translations;language model adaptation;phrase-based machine translation;named entities","Adaptation models;Computational modeling;Speech recognition;Context;Estimation;Speech;Computational complexity","estimation theory;probability;speech recognition","language model adaptation;ASR;spoken translation;machine translation system;automatic speech recognition;MT system;word-based translation model;phrase-based translation model;named entity probability estimation;WER;word-based LM adaptation technique;unadapted 3-gram baseline;English-to-Dutch dataset","","2","","20","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Comparative study on corpora for speech translation","G. Kikui; S. Yamamoto; T. Takezawa; E. Sumita","NTT Cyber Space Laboratory, Kanagawa, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan","IEEE Transactions on Audio, Speech, and Language Processing","21 Aug 2006","2006","14","5","1674","1682","This paper investigates issues in preparing corpora for developing speech-to-speech translation (S2ST). It is impractical to create a broad-coverage parallel corpus only from dialog speech. An alternative approach is to have bilingual experts write conversational-style texts in the target domain, with translations. There is, however, a risk of losing fidelity to the actual utterances. This paper focuses on balancing a tradeoff between these two kinds of corpora through the analysis of two newly developed corpora in the travel domain: a bilingual parallel corpus with 420 K utterances and a collection of in-domain dialogs using actual S2ST systems. We found that the first corpus is effective for covering utterances in the second corpus if complimented with a small number of utterances taken from monolingual dialogs. We also found that characteristics of in-domain utterances become closer to those of the first corpus when more restrictive conditions and instructions to speakers are given. These results suggest the possibility of a bootstrap-style of development of corpora and S2ST systems, where an initial S2ST system is developed with parallel texts, and is then gradually improved with in-domain utterances collected by the system as restrictions are relaxed","1558-7924","","10.1109/TASL.2006.878262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677987","Corpus;machine translation;speech translation;spoken dialog","Natural languages;Humans;Speech recognition;Global communication;Strontium;Speech synthesis;Parameter estimation;State estimation;Communications technology;Laboratories","language translation;speech processing","speech-to-speech translation;corpora;dialog speech;bilingual experts;conversational-style texts;travel domain;bilingual parallel corpus;in-domain utterances","","21","","28","IEEE","21 Aug 2006","","","IEEE","IEEE Journals"
"A continuous speech recognition system using finite state network and Viterbi beam search for the automatic interpretation","Nam-Yong Han; Hoi-Rin Kim; Kyu-Woong Hwang; Young-Mok Ahn; Joon-Hyung Ryoo","Automatic Interpretation Section, Electronics and Telecommunications Research Institute, USA; Automatic Interpretation Section, Electronics and Telecommunications Research Institute, USA; Automatic Interpretation Section, Electronics and Telecommunications Research Institute, USA; Automatic Interpretation Section, Electronics and Telecommunications Research Institute, USA; Automatic Interpretation Section, Electronics and Telecommunications Research Institute, USA","1995 International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1995","1","","117","120 vol.1","This paper describes a Korean continuous speech recognition system using phone based semi-continuous hidden Markov model (SCHMM) method for automatic interpretation. The task domain is hotel reservation. The system (composed of speech recognition, machine translation and speech synthesis) has the following three features. First, an embedded bootstrapping training method is used that enables us to train each phone model without the need for a phoneme segmentation database. Second, a hybrid estimation method which is composed of the forward-backward algorithm and the Viterbi algorithm is proposed for the HMM parameter estimation. Third, a between-word modeling technique is used at the function word boundaries. The recognition results in speaker independent experiments are as follows. In the case of Version 1, the continuous speech recognition result is 89.1% and in Version 2, the result is 97.6%.","1520-6149","0-7803-2431-5","10.1109/ICASSP.1995.479287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479287","","Speech recognition;Viterbi algorithm;Gold;Hidden Markov models;Vocabulary;Linear predictive coding;Dictionaries;Cepstral analysis;Electronic mail;Spatial databases","speech recognition;parameter estimation;maximum likelihood estimation;hidden Markov models;finite state machines;hotel industry;reservation computer systems;language translation;natural languages;search problems;speech synthesis","phone based semi-continuous hidden Markov model;finite state network;Viterbi beam search;automatic interpretation;Korean continuous speech recognition system;SCHMM;hotel reservation;embedded bootstrapping training method;phone model;hybrid estimation method;forward-backward algorithm;Viterbi algorithm;HMM parameter estimation;between-word modeling technique;function word boundaries;recognition results;speaker independent experiments;language model;machine translation;speech synthesis","","","1","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Rethinking the Reasonability of the Test Set for Simultaneous Machine Translation","M. Liu; W. Zhang; X. Li; J. Luan; B. Wang; Y. Guo; S. Chen","Beijing Institute of Technology, Beijing, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Simultaneous machine translation (SimulMT) models start translation before the end of the source sentence, making the translation monotonically aligned with the source sentence. However, the general full-sentence translation test set is acquired by offline translation of the entire source sentence, which is not designed for SimulMT evaluation, making us rethink whether this will underestimate the performance of SimulMT models. In this paper, we manually annotate a monotonic test set based on the MuST-C English-Chinese test set, denoted as SiMuST-C. Our human evaluation confirms the acceptability of our annotated test set. Evaluations on three different SimulMT models verify that the underestimation problem can be alleviated on our test set. Further experiments show that finetuning on an automatically extracted monotonic training set improves SimulMT models by up to 3 BLEU points.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095739","Machine Translation;Simultaneous Machine Translation Evaluation","Training;Annotations;Signal processing;Acoustics;Machine translation;Speech processing","","","","","","26","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Lattice-based Viterbi decoding techniques for speech translation","G. Saon; M. Picheny","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","386","389","We describe a cardinal-synchronous Viterbi decoder for statistical phrase-based machine translation which can operate on general ASR lattices (as opposed to confusion networks). The decoder implements constrained source reordering on the input lattice and makes use of an outbound distortion model to score the possible reorderings. The phrase table, representing the decoding search space, is encoded as a weighted finite state acceptor which is determined and minimized. At a high level, the search proceeds by performing simultaneous transitions in two pairs of automata: (input lattice, phrase table FSM) and (phrase table FSM, target language model). An alternative decoding strategy that we explore is to break the search into two independent subproblems: first, we perform monotone lattice decoding and find the best foreign path through the ASR lattice and then, we decode this path with reordering using standard sentence-based SMT. We report experimental results on several testsets of a large scale Arabic-to-English speech translation task in the context of the global autonomous language exploitation (or GALE) DARPA project. The results indicate that, for monotone search, lattice-based decoding outperforms 1-best decoding whereas for search with reordering, only the second decoding strategy was found to be superior to 1-best decoding. In both cases, the improvements hold only for shallow lattices.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430143","","Viterbi algorithm;Decoding;Lattices;Automatic speech recognition;Surface-mount technology;Large-scale systems;Error analysis;Electronic mail;Automata;Testing","automata theory;language translation;natural language interfaces;speech processing;statistical analysis;Viterbi decoding","lattice-based Viterbi decoding technique;speech translation;statistical phrase-based machine translation;outbound distortion model;weighted finite state acceptor;automata theory;standard sentence-based SMT","","6","","13","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Combination of stochastic understanding and machine translation systems for language portability of dialogue systems","B. Jabaian; L. Besacier; F. Lef√®vre","LIA, University of Avignon, Avignon, France; LIG, Joseph Fourier University, Grenoble, France; LIA, University of Avignon, Avignon, France","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5612","5615","In this paper, several approaches for language portability of dialogue systems are investigated with a focus on the spoken language understanding (SLU) component. We show that the use of statistical machine translation (SMT) can greatly reduce the time and cost of porting an existing system from a source to a target language. Using automatically translated training data we study phrase-based machine translation as an alternative to conditional random fields for conceptual decoding to compensate for the loss of a precise concept-word alignment. Also two ways to increase SLU robustness to translation errors (smeared training data and translation post editing) are shown to improve performance when test data are translated then decoded in the source language. Overall the combination of all these approaches allows to reduce even further the concept error rate. Experiments were carried out on the French MEDIA dialogue corpus with a subset manually translated into Italian.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947632","Spoken Dialogue Systems;Spoken Language Understanding;Language Portability;Statistical Machine Translation","Training;Training data;Media;Robustness;Semantics;Data models;Noise measurement","language translation;natural language processing;word processing","stochastic understanding;machine translation systems;language portability;dialogue systems;spoken language understanding;statistical machine translation;phrase based machine translation;conceptual decoding;precise concept word alignment;SLU robustness;translation errors;smeared training data;translation post editing;source language;French MEDIA dialogue corpus","","12","","15","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Softmax-margin training for statistical machine translation","W. Zhang; L. Liu; H. Cao; T. Zhao","MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China","2012 8th International Conference on Natural Computation","9 Jul 2012","2012","","","838","842","The training procedure is very important in statistical machine translation (SMT). It has a great influence on the final performance of a translation system. The widely used method in SMT is the minimum error rate training (MERT). It is effective to estimate the feature function weights. However, MERT does not use regularization and has been observed to over-fit. In this paper, we describe a method named softmax-margin, which is a modification of the max-margin training. This approach is simple, efficient, and easy to implement. We conduct our work using data sets from the WMT shared tasks. The results of experiment on small scale French-English translation task reach a competitive performance compared to MERT.","2157-9563","978-1-4577-2133-5","10.1109/ICNC.2012.6234638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6234638","Statistical Machine Translation;MERT;Softmax-margin","Training;NIST;Cost function;Probabilistic logic;Error analysis;Gold","language translation;statistical analysis","softmax-margin training;statistical machine translation;training procedure;final performance;translation system;minimum error rate training;feature function weights;French-English translation","","","","26","IEEE","9 Jul 2012","","","IEEE","IEEE Conferences"
"Analysis of HE(***) in patent Chinese-English machine translation","Y. Liu; Y. Jin","Institute of Chinese Information Processing, CPIC-BNU Joint Laboratory of Machine Translation Beijing Normal University, Beijing, 100875, China; Institute of Chinese Information Processing, CPIC-BNU Joint Laboratory of Machine Translation Beijing Normal University, Beijing, 100875, China","2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems","14 Nov 2013","2012","03","","1479","1484","In machine translation, Œóe(***) mainly acts as noun, preposition and conjunction for patent text. Different parts of speech have different ways to translate in machine translation system, especially when it acts as conjunction which refers to coordinate structure. In traditional linguistic, there are papers researched Œóe(***) but only concerned on how to distinguish the preposition and conjunction of it. In papers that studied coordinate structure, researches of Œóe(***) are not systemic. In this paper, we worked out rules and strategy using semantic and grammatical information based on Hierarchical Network of Concepts theory to distinguish the noun, preposition and conjunction of Œóe(***) and identify the beginning and end boundaries of coordinate structures connected by Œóe(***). Compared with Google translator and another rule based translation system we got a relatively satisfied enhancement on the identification precision.","2376-595X","978-1-4673-1857-0","10.1109/CCIS.2012.6664631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664631","HE(***);part of speech;identification;coordinate structure;boundary;machine translation;patent","","","","","","","","IEEE","14 Nov 2013","","","IEEE","IEEE Conferences"
"Language Model Bootstrapping Using Neural Machine Translation for Conversational Speech Recognition","S. Punjabi; H. Arsikere; S. Garimella","Alexa Machine Learning, Amazon, Bangalore, India; Alexa Machine Learning, Amazon, Bangalore, India; Alexa Machine Learning, Amazon, Bangalore, India","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","487","493","Building conversational speech recognition systems for new languages is constrained by the availability of utterances capturing user-device interactions. Data collection is expensive and limited by speed of manual transcription. In order to address this, we advocate the use of neural machine translation as a data augmentation technique for bootstrapping language models. Machine translation (MT) offers a systematic way of incorporating collections from mature, resource-rich conversational systems that may be available for a different language. However, ingesting raw translations from a general purpose MT system may not be effective owing to the presence of named entities, intra sentential code-switching and the domain mismatch between the conversational data being translated and the parallel text used for MT training. To circumvent this, we explore following domain adaptation techniques: (a) sentence embedding based data selection for MT training, (b) model finetuning, and (c) rescoring and filtering translated hypotheses. Using Hindi language as the experimental testbed, we supplement transcribed collections with translated US English utterances. We observe a relative word error rate reduction of 7.8-15.6%, depending on the bootstrapping phase. Fine grained analysis reveals that translation particularly aids the interaction scenarios underrepresented in the transcribed data.","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003982","speech recognition;neural machine translation;domain adaptation;code-switching","Training;Data models;Adaptation models;Decoding;Buildings;Speech recognition;Architecture","natural language processing;speech recognition;statistical analysis","language model bootstrapping;neural machine translation;conversational speech recognition systems;user-device interactions;data collection;data augmentation technique;language models;resource-rich conversational systems;general purpose MT system;conversational data;domain adaptation techniques;model finetuning;rescoring;filtering translated hypotheses;Hindi language;supplement transcribed collections;translated US English utterances;transcribed data;intrasentential code-switching","","1","","20","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Decoupled Non-Parametric Knowledge Distillation for end-to-End Speech Translation","H. Zhang; N. Si; Y. Chen; W. Zhang; X. Yang; D. Qu; Z. Li","University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China; University of Information Engineering, Zhengzhou, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Existing techniques often attempt to make knowledge transfer from a powerful machine translation (MT) to speech translation (ST) model with some elaborate techniques, which often requires transcription as extra input during training. However, transcriptions are not always available, and how to improve the ST model performance without transcription, i.e., data efficiency, has rarely been studied in the literature. In this paper, we propose Decoupled Non-parametric Knowledge Distillation (DNKD) from data perspective to improve the data efficiency. Our method follows the knowledge distillation paradigm. However, instead of obtaining the teacher distribution from a sophisticated MT model, we construct it from a non-parametric datastore via k-Nearest-Neighbor (kNN) retrieval, which removes the dependence on transcription and MT model. Then we decouple the classic knowledge distillation loss into target and non-target distillation to enhance the effect of the knowledge among non-target logits, which is the prominent ""dark knowledge"". Experiments on MuST-C corpus show that, the proposed method can achieve consistent improvement over the strong baseline without requiring any transcription.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10096899","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096899","Speech translation;Knowledge distillation;k-Nearest-Neighbor;Non-parametric","Training;Signal processing;Data models;Acoustics;Machine translation;Speech processing;Task analysis","","","","","","29","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Finite-state speech-to-speech translation","E. Vidal","Departmento Sistcriias Infonn√°ticos y Computsci√≥n, Universidad Polit√©cnica de Valencia, Valencia, Spain","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","1","","111","114 vol.1","A fully integrated approach to speech input language translation in limited domain applications is presented. The mapping from the input to the output language is modeled in terms of a finite state translation model which is learned from examples of input output sentences of the task considered. This model is tightly integrated with standard acoustic phonetic models of the input language and the resulting global model directly supplies, through Viterbi search, an optimal output language sentence for each input language utterance. Several extensions to this framework, recently developed to cope with the increasing difficulty of translation tasks, are reviewed. Finally, results for a task in the framework of hotel front desk communication, with a vocabulary of about 700 words, are reported.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.599563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599563","","Speech recognition;Natural languages;Vocabulary;Forward contracts;Standards development;Telecommunications;Speech synthesis;Prototypes;Training data;Computational linguistics","language translation;speech recognition;natural languages;finite state machines;learning by example","finite state speech to speech translation;fully integrated approach;speech input language translation;limited domain applications;finite state translation model;learning by example;input output sentences;standard acoustic phonetic models;global model;Viterbi search;optimal output language sentence;input language utterance;translation tasks;hotel front desk communication;vocabulary","","33","","21","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Head automata for speech translation","H. Alshawi","AT and T Research, Murray Hill, NJ, USA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","4","","2360","2363 vol.4","Presents statistical language and translation models based on collections of small finite-state machines that we call ""head automata"". The models are intended to capture the lexical sensitivity of N-gram models and direct statistical translation models, while at the same time taking account of the hierarchical phrasal structure of language. Two types of head automata are defined: relational head automata, which are suitable for translation by the transfer of dependency trees, and head transducers, which are suitable for direct recursive lexical translation.","","0-7803-3555-4","10.1109/ICSLP.1996.607282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607282","","Automata;Speech;Magnetic heads;Transducers;Robustness;Context modeling;Automatic programming;Functional programming;Stochastic processes;Natural languages","finite state machines;language translation;speech processing;nomograms;grammars;trees (mathematics);statistics;computational linguistics","dependency tree transfer;speech translation;statistical language models;finite-state machines;lexical sensitivity;N-gram models;direct statistical translation models;hierarchical phrasal structure;relational head automata;head transducers;direct recursive lexical translation","","3","7","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Comparative Study of Different Tokenization Strategies for Streaming End-to-End ASR","S. Singh; A. Gupta; A. Maghan; D. Gowda; S. Singh; C. Kim",Samsung Research; Samsung Research; Samsung Research; Samsung Research; Samsung Research; Samsung Research,"2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","3 Feb 2022","2021","","","388","394","Most End-to-End Automatic Speech Recognition (ASR) models use character-based vocabularies: characters, sub-words (BPE), or words. While these work well for training a monolingual model, there are certain limitations when ap-plying these to a multilingual model. Rare characters from character-rich languages like Korean can easily result in large vocabulary size, limiting the model's compactness. Repre-senting text at the level of bytes has also been proposed. However, a byte sequence representation of text is often much longer, which increases the decoding time and makes it computationally expensive for on-device use. Byte-based sub-words (BBPE) are proposed in neural machine translation for word representation but are still unexplored in the ASR domain. In this work, we conduct an empirical study comparing the above three tokenization strategies across three metrics: Word Error Rate (WER), model size, and the decoding time, which are critical for an on-device ASR. We did extensive experiments for both monolingual and bilingual, with languages belonging to same (English and Spanish) and different (English and Korean) language families. Our exper-iments show that BBPE and BPE models yield a similar WER for English and Spanish. While for a character-rich language like Korean, we get 26% and 14% relative WER improvement with BBPE monolingual and bilingual models, respectively. In contrast, the byte models trade-off small model size and a fixed vocabulary at the cost of high xRT. Among all three, we found the BBPE strategy to be the most flexible and optimal for most cases.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9687921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687921","end-to-end speech recognition;multilin-gual;RNN-Transducer","Training;Measurement;Vocabulary;Limiting;Error analysis;Computational modeling;Tokenization","language translation;natural language processing;speech recognition;vocabulary","decoding time;on-device ASR;Korean;BPE models;character-rich language;bilingual models;byte models;BBPE strategy;different tokenization strategies;streaming End-to-End ASR;End-to-End Automatic Speech Recognition models use character-based vocabularies;monolingual model;multilingual model;rare characters;vocabulary size;repre-senting text;byte-based sub-words;word representation;ASR domain","","1","","28","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Discriminative language model adaptation for Mandarin broadcast speech transcription and translation","X. A. Liu; W. J. Byrne; M. J. F. Gales; A. de Gispert; M. Tomalin; P. C. Woodland; K. Yu","Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK; Engineering Department, University of Cambridge, Cambridge, UK","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","153","158","This paper investigates unsupervised test-time adaptation of language models (LM) using discriminative methods for a Mandarin broadcast speech transcription and translation task. A standard approach to adapt interpolated language models to is to optimize the component weights byminimizing the perplexity on supervision data. This is a widely made approximation for language modeling in automatic speech recognition (ASR) systems. For speech translation tasks, it is unclear whether a strong correlation still exists between perplexity and various forms of error cost functions in recognition and translation stages. The proposed minimum Bayes risk (MBR) based approach provides a flexible framework for unsupervised LM adaptation. It generalizes to a variety of forms of recognition and translation error metrics. LM adaptation is performed at the audio document level using either the character error rate (CER), or translation edit rate (TER) as the cost function. An efficient parameter estimation scheme using the extended Baum-Welch (EBW) algorithm is proposed. Experimental results on a state-of-the-art speech recognition and translation system are presented. The MBR adapted language models gave the best recognition and translation performance and reduced the TER score by up to 0.54% absolute.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430101","speech recognition and translation;language model adaptation;discriminative training","Natural languages;Adaptation model;Broadcasting;Speech recognition;Automatic speech recognition;Error analysis;Cost function;Interpolation;Surface-mount technology;Optimization methods","approximation theory;Bayes methods;error statistics;estimation theory;interpolation;language translation;natural languages;speech recognition","discriminative language model adaptation;Mandarin broadcast speech transcription;unsupervised test-time adaptation;interpolation;approximation theory;automatic speech recognition;error cost function;minimum Bayes risk;character error rate;translation edit rate;parameter estimation;extended Baum-Welch algorithm;statistical machine translation","","","","13","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Speech Recognition and Machine Translation Using Neural Networks","R. F. Gibadullin; M. Y. Perukhin; A. V. Ilin","Department of computer systems, Kazan National Research Technical University named after A. N. Tupolev - KAI, Kazan, Russia; Department of automated systems for collecting and processing information, Kazan National Research Technological University, Kazan, Russia; Department of computer systems, Kazan National Research Technical University named after A. N. Tupolev - KAI, Kazan, Russia","2021 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)","11 Jun 2021","2021","","","398","403","The application of deep recurrent neural networks LSTM (long short-term memory) for English-Russian translation from speech to text is studied. Deep neural network learning and validation was performed based on English audiobooks. The neural network learning dataset was compiled by splitting an English audiobook and Russian text into sentences, and extracting features from audio files. Algorithms for learning and validating a neural network are available as part of the TensorFlow machine learning library. The calculations were performed on a Linux server with two high-performance NVIDIA video cards. Two types of models were learned: models for translating text to text and models for translating speech to text. As a result of the work, it was revealed that models based on deep neural networks are effective for machine translation of a language.","","978-1-7281-4587-7","10.1109/ICIEAM51226.2021.9446474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446474","deep learning;machine translation;recurrent neural networks","Recurrent neural networks;Machine learning algorithms;Linux;Neural networks;Speech recognition;Machine learning;Feature extraction","feature extraction;language translation;learning (artificial intelligence);Linux;neural nets;recurrent neural nets;speech recognition;text analysis","deep recurrent neural networks LSTM;long short-term memory;English-Russian translation;English audiobooks;neural network learning dataset;TensorFlow machine learning library;high-performance NVIDIA video cards;deep neural networks;machine translation;speech recognition","","15","","14","IEEE","11 Jun 2021","","","IEEE","IEEE Conferences"
"Large-Scale Distributed Language Modeling","A. Emami; K. Papineni; J. Sorensen","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Yahoo Research, Inc., NewYork, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-37","IV-40","A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218031","Statistical language modeling;Speech recognition;Statistical machine translation;Client-server systems;Distributed memory systems","Large-scale systems;Decoding;Natural languages;Training data;Automatic speech recognition;Vocabulary;Probability;Speech recognition;Error analysis;Surface-mount technology","client-server systems;decoding;interpolation;language translation;natural language processing;speech coding;speech recognition;statistical analysis","re-ranking N-best lists;speech recognition system;word error rate;n-gram order;interpolation weights;language model probabilities;client-server paradigm;machine translation decoder;large-scale distributed language modeling","","7","7","12","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"Research of Language Recognition Methods Based on Machine Learning","D. L. Khabarov; V. V. Bazanov; A. V. Kuchebo; M. Zavgorodnii; A. Y. Rybakova","(Moscow Engineering Physics Institute), National Research Nuclear University MEPhI, Moscow, Russian Federation; (Moscow Engineering Physics Institute), National Research Nuclear University MEPhI, Moscow, Russian Federation; (Moscow Engineering Physics Institute), National Research Nuclear University MEPhI, Moscow, Russian Federation; (Moscow Engineering Physics Institute), National Research Nuclear University MEPhI, Moscow, Russian Federation; Kazan National Research Technological University, Kazan, Russian Federation","2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)","9 Apr 2021","2021","","","438","442","This article is focused on speech recognition methods research and analysis of their efficiency. It presents a review of the most known ways to convert a person's speech into text, their effectiveness, and complexity in exploitation. The article scribes the application of machine learning in speech recognition tasks. A program was developed with a neural network for the research realization - a new technique of speech translation automation. The experiment provides information about comparing methods of speech recognition and analyzing the results of using them. Conclusions about the further realization of the research were drawn based on these results.","2376-6565","978-1-6654-0476-1","10.1109/ElConRus51938.2021.9396491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396491","machine learning;speech recognition;neural networks;machine hearing;language determination","Automation;Conferences;Neural networks;Speech recognition;Machine learning;Complexity theory;Task analysis","image recognition;language translation;learning (artificial intelligence);natural language processing;speech recognition","language recognition methods;machine learning;speech recognition methods research;known ways;person;speech recognition tasks;research realization;speech translation automation","","","","10","IEEE","9 Apr 2021","","","IEEE","IEEE Conferences"
"A Study of Myanmar (Burmese) to English Machine Translation Performance with Various Myanmar Translated Styles","H. Htun; Y. K. Thu; N. N. Oo; T. Supnithi","Department of Computer Engineering and Information Technology, Yangon Technological University, Yangon, Myanmar; National Electronics and Computer Technology Center (NECTEC), Pathum Thani, Thailand; Department of Computer Engineering and Information Technology, Yangon Technological University, Yangon, Myanmar; Language and Semantic Technology Research Team(LST), National Electronics and Computer Technology Center (NECTEC), Pathum Thani, Thailand","2020 IEEE Conference on Computer Applications(ICCA)","5 Mar 2020","2020","","","1","7","This paper contributes the first investigation of machine translation (MT) performance differences between Myanmar and English languages with the use of several possible Myanmar translations for the specific primary educational domain. We also developed both one to one and many Myanmar translations corpora (over 8K and 34K sentences) based on old and new English textbooks (including Grade 1 to 3) which are published by the Ministry of Education. Our developing parallel corpora were used for phrase-based statistical machine translation (PBSMT) which is the de facto standard of statistical machine translation. We measured machine translation performance differences among one-to-many English to Myanmar translation corpora. The differences range between 19.68 and 52.38 BLEU scores for English to Myanmar and between 50.17 and 75.12 BLEU scores for Myanmar to English translation. We expect this study can be applied in Myanmar-to-English automatic speech recognition (ASR) development for primary English textbooks. The main purpose is to translate primary English textbooks data correctly even if the children use in several Myanmar conversation styles.","","978-1-7281-5925-6","10.1109/ICCA49400.2020.9022851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022851","Phrase-based Statistical Machine Translation (PBSMT);One to Many Parallel Corpus;Myanmar-English Machine Translation;Primary English Textbooks of Myanmar;Word Error Rate (WER)","Training;Mathematical model;Buildings;Testing;Dictionaries;Information technology","computer aided instruction;language translation;natural language processing;speech recognition","English machine translation performance;machine translation performance differences;English languages;Myanmar translations corpora;phrase-based statistical machine translation;Myanmar translation corpora;English translation;Myanmar-to-English automatic speech recognition development;primary English textbooks data;Myanmar conversation styles;primary educational domain;parallel corpora","","","","23","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Research on the Translation System of Spoken English Situational Dialogue Based on Computer Artificial Intelligence","Y. Zhu","Anhui Sanlian University, Hefei, China","2023 IEEE 3rd International Conference on Electronic Technology, Communication and Information (ICETCI)","17 Jul 2023","2023","","","1166","1170","In this paper, a sequence-to-sequence model is proposed to deal with continuous emphasis levels, and machine translation and emphasis translation are combined into a spoken English situational translation system. In the aspect of hardware design, the peripheral circuit is built with the chip as the core to form the speech recognition module. In the process of software design, relaxation endpoint method is used to improve the operation speed to realize floating point operation in the kernel. A database suitable for spoken translation system is constructed by using complex feature vectors, and remote data synchronization is realized by using SQL Server Mobile 2015 database merge replication technology. The system uses a translation engine to translate English and Chinese. There are nearly 200,000 bilingual sentences debugging in the system, which improves the language translation quality of the system.","","979-8-3503-9841-0","10.1109/ICETCI57876.2023.10176371","Anhui Sanlian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176371","computer;artificial intelligence;oral English;situational dialogue;translation system","Training;Structured Query Language;Software design;Databases;Software algorithms;Speech recognition;Oral communication","artificial intelligence;interactive systems;language translation;natural language processing;speech recognition;SQL","computer artificial intelligence;continuous emphasis levels;emphasis translation;floating point operation;hardware design;language translation quality;machine translation;operation speed;peripheral circuit;relaxation endpoint method;sequence-to-sequence model;software design;speech recognition module;spoken English situational dialogue;spoken English situational translation system;spoken translation system;SQL Server Mobile 2015 database;translation engine","","","","8","IEEE","17 Jul 2023","","","IEEE","IEEE Conferences"
"Prosodic annotation enriched statistical machine translation","P. Guo; H. Huang; P. Jian; Y. Guo","Department of Computer Science and Technology, Beijing Institute of technology, China; Department of Computer Science and Technology, Beijing Institute of technology, China; Department of Computer Science and Technology, Beijing Institute of technology, China; Department of Computer Science and Technology, Beijing Institute of technology, China","2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)","4 May 2017","2016","","","1","5","More and more linguistic information has been employed to improve the performance of machine translation, such as part of speech, syntactic structures, discourse contexts, and so on. However, conventional approaches typically ignore the key information beyond the text such as prosody. In this paper, we exploit and employ three prosodic features: pronunciation (phonetic alphabet and tone), prosodic boundaries and emphasis. Based on the annotated data, a conditional random fields (CRF) sequential tagger is used to label the prosodic tags for Chinese sentences, and three methods are presented to integrate these features: (1) factored translation models where the prosodic features are incorporated as factors; (2) a word lattice decoding model where the prosodic boundaries are considered to be an alternative to the tokenization boundaries; (3) re-ranking models where the prosodic features are integrated in the language model to re-score the n-best translation candidates. We evaluate the proposed methods with bilingual evaluation understudy (BLEU) score both in English-to-Chinese (E2C) and Chinese-to-English (C2E) translation directions. Experiments show that with prosodic features, the re-ranking model achieves significant improvement, while the word lattice decoding and the factored translation models also improve the performance.","","978-1-5090-4294-4","10.1109/ISCSLP.2016.7918437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918437","prosody;machine translation;factored model;word lattice;re-ranking","Lattices;Computational modeling;Training;Speech;Decoding;Data models;Feature extraction","language translation;random processes;statistical analysis","prosodic annotation enriched statistical machine translation;linguistic information;prosodic features;pronunciation;prosodic boundaries;prosodic emphasis;conditional random field sequential tagger;prosodic tags;Chinese sentences;factored translation models;word lattice decoding model;n-best translation candidates;bilingual evaluation;English-to-Chinese translation directions;Chinese-to-English translation directions","","","","18","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"‚ÄúCan you give me another word for hyperbaric?‚Äù: Improving speech translation using targeted clarification questions","N. F. Ayan; A. Mandal; M. Frandsen; J. Zheng; P. Blasco; A. Kathol; F. B√©chet; B. Favre; A. Marin; T. Kwiatkowski; M. Ostendorf; L. Zettlemoyer; P. Salletmayr; J. Hirschberg; S. Stoyanchev","SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; SRI International, Menlo Park, USA; Aix Marseille Universit√©, Marseille, France; Aix Marseille Universit√©, Marseille, France; University of Washington, Seattle, USA; University of Washington, Seattle, USA; University of Washington, Seattle, USA; University of Washington, Seattle, USA; Graz Institute of Technology, Austria; Columbia University, New York, USA; Columbia University, New York, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8391","8395","We present a novel approach for improving communication success between users of speech-to-speech translation systems by automatically detecting errors in the output of automatic speech recognition (ASR) and statistical machine translation (SMT) systems. Our approach initiates system-driven targeted clarification about errorful regions in user input and repairs them given user responses. Our system has been evaluated by unbiased subjects in live mode, and results show improved success of communication between users of the system.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639302","Speech translation;error detection;error correction;spoken dialog systems","Speech;Speech recognition;Syntactics;Lattices;Pragmatics;Merging;Hidden Markov models","language translation;speech recognition","hyperbaric;targeted clarification questions;system-driven targeted clarification;SMT systems;statistical machine translation;ASR;automatic speech recognition;automatically detecting errors;speech-to-speech translation systems","","9","3","28","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Research on automatic acquisition of translation template based on error-driven learning method","Chun-Xiang Zhang; Sheng Li; Tie-Jun Zhao; Hai-Long Cao","MOE-MS Key Laboratory of Natural Language Processing and Speech, Doctor in School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China","2005 International Conference on Machine Learning and Cybernetics","7 Nov 2005","2005","6","","3723","3727 Vol. 6","Automatic acquisition of translation templates is important for MT system to improve its translation quality and its ability of adaptation to new domain. In this paper, translation equivalences are obtained from translation corresponding trees of bilingual sentence pairs. Error-driven learning method is employed to acquire templates from extracted equivalences. At the same time, optimization method based on automatic translation evaluation is used to clean these templates. Then they are applied to a transfer-based MT system, and ""863"" dialog corpus in 2003 is used for open test. Experimental results show that the performance of new acquired templates exceeds that of original ones. Combination of new acquired templates and original ones makes 5-gram Nist assessment score of open test corpus improve by 8.11%.","2160-1348","0-7803-9091-1","10.1109/ICMLC.2005.1527588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1527588","Translation template;error-driven learning;automatic translation evaluation;Nist assessment score","Learning systems;Natural languages;NIST;Computer errors;Computer science;Optimization methods;System testing;Laboratories;Natural language processing;Speech processing","language translation;computational linguistics;trees (mathematics);equivalence classes","translation template;machine translation system;translation equivalence;bilingual sentence pair;error-driven learning method;optimization;automatic translation evaluation;Nist assessment score","","","","9","IEEE","7 Nov 2005","","","IEEE","IEEE Conferences"
"Adaptation of lecture speech recognition system with machine translation output","R. W. M. Ng; T. Hain; T. Cohn","Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Sheffield, UK","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8401","8405","In spoken language translation, integration of the ASR and MT components is critical for good performance. In this paper, we consider the recognition setting where a text translation of each utterance is also available. We present experiments with different ASR system adaptation techniques to exploit MT system outputs. In particular, N-best MT outputs are represented as an utterance-specific language model, which are then used to rescore ASR lattices. We show that this method improves significantly over ASR alone, resulting in an absolute WER reduction of more than 6% for both indomain and out-of-domain acoustic models.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639304","TED talks;speech translation;language model adaptation","Speech;Adaptation models;Training;Interpolation;Speech recognition;Acoustics;Data models","language translation;speech recognition","lecture speech recognition system adaptation;machine translation output;spoken language translation;ASR components;MT components;recognition setting;text translation;ASR system adaptation techniques;MT system;N-best MT outputs;utterance-specific language model;ASR lattices;WER reduction;out-of-domain acoustic models;in-domain acoustic models","","1","3","31","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Natural Language Processing based Machine Translation for Hindi-English using GRU and Attention","J. Singh; S. Sharma; B. J","Dept. of Computer Science, SRM Institute of Science and Technology, Tamil Nadu, India; Dept. of Computer Science, SRM Institute of Science and Technology, Tamil Nadu, India; Dept. of Computing Technologies, SRM Institute of science and technology, Tamil Nadu, India","2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","16 Jun 2022","2022","","","965","969","Machine translation, abbreviated as MT, is basically an extension of language computation that studies the concept of translating text or speech from one language to another. On the most rudimentary level, machine translation basically conducts mechanical reinstatement of words from one particular language into another, however, this alone rarely yields a good translation because it needs to recognize the closest match between the complete sentence and the target language. Not every term in one language has an equivalent word in another, and many words have many meanings. Unlike our everyday normal phrase-based translation systems, which are usually made up of many small and tiny sub-components that can be tweaked individually, NMT i.e., neural machine translation seeks to design and train a simple, massive, end-to-end neural network system that analyses a sentence and provides an accurate translation for the respective language. Neural Machine translation is a radical newer technique in the field of language translation and localization that trains neural models using deep neural networks and artificial intelligence. The main advantage of this approach is that you can train a single system directly with source and target text. This eliminates the need for special system pipelines required for statistical machine learning. This means you can perform training and translation on an end-to-end model at once.","","978-1-6654-9710-7","10.1109/ICAAIC53929.2022.9793214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793214","Translation;Machine;Words;Text;Localization;Subfield;Target;Meanings;Phrases;Software","Training;Location awareness;Deep learning;Target recognition;Computational modeling;Neural networks;Pipelines","language translation;learning (artificial intelligence);natural language processing;neural nets","natural language processing based machine translation;hindi-english using GRU;language computation;particular language;good translation;target language;equivalent word;everyday normal phrase-based translation systems;neural machine translation;end-to-end neural network system;accurate translation;respective language;language translation;localization;trains neural models;deep neural networks;statistical machine learning","","","","15","IEEE","16 Jun 2022","","","IEEE","IEEE Conferences"
"FOLSOM: a fast and memory-efficient phrase-based approach to statistical machine translation","B. Zhou; S. F. Chen; Y. Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","226","229","In this work, we propose a novel framework for performing phrase-based statistical machine translation using weighted finite-state transducers (WFST's) that is significantly faster than existing frameworks while also being memory-efficient. In particular, we represent the entire translation model with a single WFST that is statically optimized, in contrast to previous work that represents the translation model as multiple WFST's that must be composed on the fly. We describe a new search algorithm that conveniently and efficiently combines multiple knowledge sources during decoding. The proposed approach is particularly suitable for converged real-time speech translation on scalable computing devices. We were able to develop a SMT system that can translate more than 3000 words/second while still retaining excellent accuracy.","","1-4244-0872-5","10.1109/SLT.2006.326796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123403","","Surface-mount technology;Transducers;Decoding;Personal digital assistants;Lattices;Speech recognition;Delay;Handheld computers;Encoding;Viterbi algorithm","decoding;finite state machines;language translation;speech processing","FOLSOM;phrase-based approach;statistical machine translation;weighted finite-state transducers;search algorithm;decoding;real-time speech translation","","10","","13","IEEE","19 Mar 2007","","","IEEE","IEEE Conferences"
"Automatic translation system for characters of foreign literary works based on sparse representation","X. Zhai","Foundational Courses Department, Chongqing City Vocational College, China","2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS)","26 May 2021","2021","","","1815","1818","Automatic translation system for the characters of foreign literary works based on sparse representation is studied in this study. The powerful advantages and wide application of computer-assisted translation prove that machine translation and human translation are not opposed, and the relationship between the two is actually complementary. This paper uses the sparse coding theory to construct the intelligent model that can analyze the character patterns with semetic understanding. The image processing and computer system models are combined to construct the system. The proposed method is validated on the public database, and compared with the latest algorithms.","","978-1-6654-1272-8","10.1109/ICICCS51141.2021.9432172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432172","Sparse representation;data mining;automatic translation;character recognition","Analytical models;Databases;Computational modeling;Image processing;Control systems;Machine translation;Character recognition","image representation;knowledge based systems;language translation;natural language processing;speech processing","character patterns;image processing;computer system models;automatic translation system;foreign literary works;sparse representation;computer-assisted translation;machine translation;human translation;sparse coding theory","","","","21","IEEE","26 May 2021","","","IEEE","IEEE Conferences"
"Enabling Zero-Shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders","C. Escolano; M. R. Costa-juss√†; J. A. R. Fonollosa; C. Segura","TALP Research Center, Universitat Polit√®cnica de Catalunya, Barcelona; TALP Research Center, Universitat Polit√®cnica de Catalunya, Barcelona; TALP Research Center, Universitat Polit√®cnica de Catalunya, Barcelona; Telef√≥nica I+D, Barcelona","2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","3 Feb 2022","2021","","","694","701","Current end-to-end approaches to Spoken Language Translation (SLT) rely on limited training resources, especially for multilingual settings. On the other hand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on higher-quality and more massive data sets. Our proposed method extends a MultiNMT architecture based on language-specific encoders-decoders to the task of Multilingual SLT (Multi-SLT). Our method entirely eliminates the dependency from MultiSLT data and it is able to translate while training only on ASR and MultiNMT data. Our experiments on four different languages show that coupling the speech encoder to the MultiNMT architecture produces similar quality translations compared to a bilingual baseline (¬±0.2 BLEU) while effectively allowing for zero-shot MultiSLT. Additionally, we propose using an Adapter module for coupling the speech inputs. This Adapter module produces consistent improvements up to +6 BLEU points on the proposed architecture and +1 BLEU point on the end-to-end baseline.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9688026","European Research Council (ERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688026","zero-shot;multilinguality;direct speech-to-text translation;neural machine translation;deep-learning","Training;Couplings;Bridges;Adaptation models;Conferences;Decoding;Machine translation","decoding;language translation;natural language processing;neural nets;speech recognition","end-to-end approaches;MultiNMT architecture;language-specific encoders-decoders;speech encoder;zero-shot MultiSLT;zero-shot multilingual spoken language translation;multilingual neural machine translation approach;multilingual SLT;adapter module","","1","","23","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Recurrent neural network language model with part-of-speech for Mandarin speech recognition","C. Gong; X. Li; X. Wu","Speech and Hearing Research Center, Peking University, Beijing; Speech and Hearing Research Center, Peking University, Beijing; Speech and Hearing Research Center, Peking University, Beijing","The 9th International Symposium on Chinese Spoken Language Processing","27 Oct 2014","2014","","","459","463","Recurrent neural network language models (RNNLMs) have been successfully applied in a variety of language processing applications ranging from speech recognition to machine translation. They can fight the curse of dimensionality by learning a distributed representation (word vector). The components of these vectors measure the co-occurrence of the word with context features over a corpus. However, RNNLMs ignore the fact that the meaning of word can vary substantially in different contexts (e.g., for polysemous words). In this paper, we investigate part-of-speech information to address this issue to some extent on the basis of information about the meaning of a word they could provide. Experimental results on Mandarin speech recognition task show that a significant character error reduction of 1.18% absolute (7.72% relative) was obtained when using recurrent neural network language model with part-of-speech.","","978-1-4799-4219-0","10.1109/ISCSLP.2014.6936636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936636","part-of-speech;recurrent neural network language model;speech recognition","Computational modeling;Recurrent neural networks;Speech recognition;Speech;Refining;Acoustics;Dictionaries","natural language processing;recurrent neural nets;speech recognition","language processing applications;machine translation;RNNLM;part-of-speech information;Mandarin speech recognition task;character error reduction;recurrent neural network language model","","1","","16","IEEE","27 Oct 2014","","","IEEE","IEEE Conferences"
"Measuring the impact of cognates in parallel text alignment","V. M. D. Bilbao; J. G. P. Lopes; T. Ildefonso","Departamento de Inform√°tica, scuela Superior de Ingenieria Inform√°tica, Universidad de Vigo, Orense, Spain; CITI, Departamento de Inform√°tica, Faculdade de Ci√™ncias e Tecnologia, Universidade Nova de Lisboa, Caparica, Portugal; CITI, Departamento de Inform√°tica, Faculdade de Ci√™ncias e Tecnologia, Universidade Nova de Lisboa, Caparica, Portugal","2005 portuguese conference on artificial intelligence","10 Apr 2007","2005","","","338","343","This paper evaluates the impact of considering cognates for aligning parallel texts using our aligner. By considering that two words may be cognates if their similarity is higher than a certain threshold and if they are automatically selected as aligners, we tested different degrees of cognativeness, by varying threshold values. A new methodology to measure the quality of resulting alignments is presented. Two language pairs are addressed: Portuguese-Spanish, and Portuguese-English","","0-7803-9365-1","10.1109/EPIA.2005.341306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145984","Parallel text alignment;Machine translation;evaluation;cognates","Automatic testing;Documentation;Databases;Information retrieval;Data mining;White spaces;Humans;Error analysis;Speech recognition","language translation;natural language processing;text analysis","parallel text alignment;threshold values;Portuguese-Spanish languages;Portuguese-English languages;machine translation","","1","1","14","IEEE","10 Apr 2007","","","IEEE","IEEE Conferences"
"Improved punctuation recovery through combination of multiple speech streams","J. Miranda; J. P. Neto; A. W. Black","INESC-ID, Instituto Superior T√©cnico, Lisboa, Portugal; INESC-ID, Instituto Superior T√©cnico, Lisboa, Portugal; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","9 Jan 2014","2013","","","132","137","In this paper, we present a technique to use the information in multiple parallel speech streams, which are approximate translations of each other, in order to improve performance in a punctuation recovery task. We first build a phraselevel alignment of these multiple streams, using phrase tables to link the phrase pairs together. The information so collected is then used to make it more likely that sentence units are equivalent across streams. We applied this technique to a number of simultaneously interpreted speeches of the European Parliament Committees, for the recovery of the full stop, in four different languages (English, Italian, Portuguese and Spanish). We observed an average improvement in SER of 37% when compared to an existing baseline, in Portuguese and English.","","978-1-4799-2756-2","10.1109/ASRU.2013.6707718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707718","speech recognition;machine translation;punctuation;multistream;combination","Speech;Lattices;Speech recognition;Feature extraction;Europe;Measurement;Entropy","language translation;natural language processing;speech processing;speech recognition","automatic speech recognition;ASR;machine translation;SER improvement;Spanish language;Portuguese language;Italian language;English language;full stop recovery;European Parliament Committees;simultaneously interpreted speeches;sentence units;phrase pairs;phrase tables;phrase-level alignment;approximate translations;multiple parallel speech streams;multiple speech stream combination;punctuation recovery","","2","","18","IEEE","9 Jan 2014","","","IEEE","IEEE Conferences"
"Studying the SPEA2 algorithm for optimising a pattern-recognition based machine translation system","S. Sofianopoulos; G. Tambouratzis","Machine Translation Department, Institute of Language and Speech Processing, Athens, Greece; Machine Translation Department, Institute of Language and Speech Processing, Athens, Greece","2011 IEEE Symposium on Computational Intelligence in Multicriteria Decision-Making (MDCM)","11 Jul 2011","2011","","","97","104","In this article, aspects regarding the optimisation of machine translation systems via evolutionary computation algorithms are examined. The article focuses on pattern-recognition based machine translation systems that use large monolingual corpora in the target language from which statistical information is extracted. The research reported here uses a specific machine translation as a representative for experimentation. Based on previous studies, SPEA2 is selected as the optimisation method. Issues examined in this article include the effect of population size on the optimisation process and the number of epochs required for the algorithm to settle to near-optimal results. In addition, the effects of different parameters on the translation process are examined, with the aim of reducing the set of system parameters that are actively involved in the optimisation process and thus reducing the optimisation processing time.","","978-1-61284-069-7","10.1109/SMDCM.2011.5949279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949279","Machine Translation;Evolutionary Computation;Multiobjective Optimisation;Genetic Algorithms;SPEA2","Optimization;Measurement;Evolutionary computation;Approximation algorithms;Prototypes;Europe;Genetic algorithms","evolutionary computation;language translation;statistical analysis","SPEA2 algorithm;pattern recognition based machine translation system;optimisation;evolutionary computation algorithms;monolingual corpora;statistical information","","2","","25","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"A Cloud Cluster Test Framework for English Machine Translation based on Prior Information","Y. Wang","School of Foreign Languages, Wuhan Business University, Wuhan, Hubei, China","2023 International Conference on Inventive Computation Technologies (ICICT)","1 Jun 2023","2023","","","702","706","Machine translation technology can convert source language speech into target language text, and is one of the important branches in the field of natural language processing. In this study, the cloud cluster test framework for the English machine translation based on prior information is proposed. The shortcomings of traditional rule-based machine translation are also obvious, because the rule base and dictionary base based on manual customization are highly dependent on the human experience and knowledge, which is time-consuming and labor-intensive. Its core advantage is the prior information that will help to handle the complex scenarios. In the designed framework, the novel semantic algorithm is proposed to assist the extraction of the text features. Then, the proposed information translation algorithm is proposed with the Transformer model. Lastly, to meet the real-time requirement, the cloud cluster is combined. In the test, the BP neural network and Ada boost based algorithms are compared and the proposed model has the higher accuracy","2767-7788","979-8-3503-9849-6","10.1109/ICICT57646.2023.10134230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10134230","Cloud cluster;Test framework;Machine translation;Prior information","Cloud computing;Computational modeling;Neural networks;Clustering algorithms;Transforms;Transformer cores;Feature extraction","backpropagation;language translation;natural language processing;neural nets","Ada boost based algorithms;cloud cluster test framework;designed framework;dictionary base;english machine translation;information translation algorithm;machine translation technology;natural language processing;rule base;source language speech;target language text;traditional rule-based machine translation","","","","21","IEEE","1 Jun 2023","","","IEEE","IEEE Conferences"
"Augmenting Training Data for Low-Resource Neural Machine Translation via Bilingual Word Embeddings and BERT Language Modelling","A. Ramesh; H. U. Uhana; V. B. Parthasarathy; R. Haque; A. Way","Iconic Translation Machines, Dublin 9, Ireland; Department of Computer Science and Engineering, GCETT, Serampore, India; KantanMT, Dublin 9, Ireland; ADAPT Centre, School of Computing, National College of Ireland, Dublin 1, Ireland; ADAPT Centre, School of Computing, Dublin City University, Dublin 9, Ireland","2021 International Joint Conference on Neural Networks (IJCNN)","22 Sep 2021","2021","","","1","8","Neural machine translation (NMT) is often described as ‚Äòdata hungry‚Äô as it typically requires large amounts of parallel data in order to build a good-quality machine translation (MT) system. However, most of the world's language-pairs are low-resource or extremely low-resource. This situation becomes even worse if a specialised domain is taken into consideration for translation. In this paper, we present a novel data augmentation method which makes use of bilingual word embeddings (BWEs) learned from monolingual corpora and bidirectional encoder representations from transformer (BERT) language models (LMs). We augment a parallel training corpus by introducing new words (i.e. out-of-vocabulary (OOV) items) and increasing the presence of rare words on both sides of the original parallel training corpus. Our experiments on the simulated low-resource German‚ÄìEnglish and French‚ÄìEnglish translation tasks show that the proposed data augmentation strategy can significantly improve state-of-the-art NMT systems and outperform the state-of-the-art data augmentation approach for low-resource NMT.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9534211","Science Foundation Ireland (SFI) Research Centres Programme(grant numbers:13/RC/2106,13/RC/2077); European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534211","Machine translation;Neural machine translation;Transformer;Language modelling","Training;Bit error rate;Training data;Collaboration;Tools;Transformers;Data models","language translation;learning (artificial intelligence);natural language processing;neural nets;speech recognition","data hungry;parallel data;good-quality machine translation system;language-pairs;data augmentation method;bilingual word embeddings;bidirectional encoder representations;transformer language models;rare words;original parallel training corpus;low-resource German-English;French-English translation tasks;data augmentation strategy;state-of-the-art NMT systems;state-of-the-art data augmentation approach;low-resource NMT;training data;low-resource neural machine translation;BERT language modelling","","","","43","IEEE","22 Sep 2021","","","IEEE","IEEE Conferences"
"Improved Word Alignment System for Myanmar-English Machine Translation","N. N. Han; A. Thida","AI Research Lab, University of Computer Studies, Mandalay, Mandalay, Myanmar; AI Research Lab, University of Computer Studies, Mandalay, Mandalay, Myanmar","2020 23rd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","28 Dec 2020","2020","","","178","182","Word alignment is an essential task for every Statistical Machine Translation (SMT) system. An alignment is the arrangement of two or more alignments between the parallel sentences. The problem of word alignment in SMT is to find the strong alignment in the corresponding sentence pairs. Moreover, popular word alignment system (GIZA++) needs improvement in Myanmar-English machine translation because Myanmar is inflected, and it is also a language scarce resource. For this reason, this paper presents the idea of word alignment system by adding the extra resources: word and Name Entity Recognition (NER) translation pairs to the existing training data to improve the word alignment system. Experimental results show that the proposed word alignment system reduces the Alignment Error Rate (AER) than baseline.","2472-7695","978-1-7281-9896-5","10.1109/O-COCOSDA50338.2020.9295043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9295043","word alignment;Name Entity Recognition;Statistical Machine Translation","Hidden Markov models;Data models;Vocabulary;Data mining;Training data;Training;Task analysis","language translation;natural language processing;statistical analysis","Myanmar-English Machine Translation;Statistical Machine Translation system;strong alignment;popular word alignment system;Myanmar-English machine translation;Alignment Error Rate","","1","","23","IEEE","28 Dec 2020","","","IEEE","IEEE Conferences"
"Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation","C. Qiang; P. Yang; H. Che; J. Xiao; X. Wang; Z. Wang","Kwai, Beijing, P.R. China; Kwai, Beijing, P.R. China; Kwai, Beijing, P.R. China; Kwai, Beijing, P.R. China; Kwai, Beijing, P.R. China; Kwai, Beijing, P.R. China","2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","21 Dec 2022","2022","","","1915","1919","Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest challenges is the task of polyphone disambiguation. Most of the previous polyphone disambiguation models are trained on manually annotated datasets, and publicly available datasets for polyphone disambiguation are scarce. In this paper we propose a simple back-translation-style data augmentation method for mandarin Chinese polyphone disambiguation, utilizing a large amount of unlabeled text data. Inspired by the back-translation technique proposed in the field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme (P2G) model to predict pronunciation into text. Meanwhile, a window-based matching strategy and a multi-model scoring strategy are proposed to judge the correctness of the pseudo-label. We design a data balance strategy to improve the accuracy of some typical polyphonic characters in the training set with imbalanced distribution or data scarcity. The experimental result shows the effectiveness of the proposed back-translation-style data augmentation method.","2640-0103","978-616-590-477-3","10.23919/APSIPAASC55919.2022.9980115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9980115","","Training;Information processing;Predictive models;Machine translation;Task analysis","image matching;language translation;learning (artificial intelligence);natural language processing;speech processing;speech synthesis;text analysis","back-translation technique;Chinese Text-To-Speech systems;data balance strategy;G2P;Grapheme-to-Phoneme model;machine translation;mandarin Chinese polyphone disambiguation;Mandarin Chinese polyphone disambiguation;manually annotated datasets;multimodel scoring strategy;Phoneme-to-Grapheme model;polyphonic character;previous polyphone disambiguation models;simple back-translation-style data augmentation method;typical polyphonic characters;unlabeled text data","","1","","25","","21 Dec 2022","","","IEEE","IEEE Conferences"
"The IBM 2008 GALE Arabic speech transcription system","G. Saon; H. Soltau; U. Chaudhari; S. Chu; B. Kingsbury; H. -K. Kuo; L. Mangu; D. Povey","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Microsoft Research Limited, Redmond, WA, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","4378","4381","This paper describes the Arabic broadcast transcription system fielded by IBM in the GALE Phase 3.5 machine translation evaluation. Key advances compared to our Phase 2.5 system include improved discriminative training, the use of Subspace Gaussian Mixture Models (SGMM), neural network acoustic features, variable frame rate decoding, training data partitioning experiments, unpruned n-gram language models and neural network language models. These advances were instrumental in achieving a word error rate of 8.9% on the evaluation test set.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495640","Speech recognition","Broadcasting;Phase estimation;Decoding;Maximum likelihood linear regression;Neural networks;Speech recognition;Acoustic testing;Natural languages;Speech analysis;Loudspeakers","acoustic signal processing;decoding;error statistics;Gaussian processes;language translation;natural language processing;neural nets;speech coding;speech recognition","IBM 2008 GALE Arabic speech transcription system;Arabic broadcast transcription system;GALE Phase 3.5 machine translation evaluation;subspace Gaussian mixture model;neural network acoustic features;variable frame rate decoding;training data partitioning;n-gram language model;neural network language model;word error rate;speech recognition","","15","","11","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"One-to-Many Multilingual End-to-End Speech Translation","M. A. Di Gangi; M. Negri; M. Turchi","DiSI, University of Trento, Italy; MT unit, Fondazione Bruno Kessler, Italy; MT unit, Fondazione Bruno Kessler, Italy","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","585","592","Nowadays, training end-to-end neural models for spoken language translation (SLT) still has to confront with extreme data scarcity conditions. The existing SLT parallel corpora are indeed orders of magnitude smaller than those available for the closely related tasks of automatic speech recognition (ASR) and machine translation (MT), which usually comprise tens of millions of instances. To cope with data paucity, in this paper we explore the effectiveness of transfer learning in end-to-end SLT by presenting a multilingual approach to the task. Multilingual solutions are widely studied in MT and usually rely on ‚Äútarget forcing‚Äù, in which multilingual parallel data are combined to train a single model by prepending to the input sequences a language token that specifies the target language. However, when tested in speech translation, our experiments show that MT-like target forcing, used as is, is not effective in discriminating among the target languages. Thus, we propose a variant that uses target-language embed-dings to shift the input representations in different portions of the space according to the language, so to better support the production of output in the desired target language. Our experiments on end-to-end SLT from English into six languages show important improvements when translating into similar languages, especially when these are supported by scarce data. Further improvements are obtained when using English ASR data as an additional language (up to +2.5 BLEU points).","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9004003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9004003","deep learning;speech translation;multi-linguality;direct speech-to-text translation","Task analysis;Two dimensional displays;Data models;Computational modeling;Training;Decoding;Training data","language translation;learning (artificial intelligence);natural language processing;speech recognition","end-to-end speech translation;end-to-end neural models;spoken language translation;extreme data scarcity conditions;machine translation;MT;data paucity;end-to-end SLT;multilingual approach;multilingual solutions;multilingual parallel data;language token;English ASR data;target language","","9","","47","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Statistical machine translation based text normalization with crowdsourcing","T. Schlippe; C. Zhu; D. Lemcke; T. Schultz","Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Karlsruhe Institute of Technology, Germany","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8406","8410","In [1], we have proposed systems for text normalization based on statistical machine translation (SMT) methods which are constructed with the support of Internet users and evaluated those with French texts. Internet users normalize text displayed in a web interface in an annotation process, thereby providing a parallel corpus of normalized and non-normalized text. With this corpus, SMT models are generated to translate non-normalized into normalized text. In this paper, we analyze their efficiency for other languages. Additionally, we embedded the English annotation process for training data in Amazon Mechanical Turk and compare the quality of texts thoroughly annotated in our lab to those annotated by the Turkers. Finally, we investigate how to reduce the user effort by iteratively applying an SMT system to the next sentences to be edited, built from the sentences which have been annotated so far.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639305","text normalization;statistical machine translation;rapid language adaptation;crowdsourcing","Computational modeling;Training;Speech;Internet;Training data;Conferences;Noise measurement","language translation;natural languages;statistical analysis;text analysis","statistical machine translation;text normalization;crowdsourcing;Internet users support;French texts;parallel corpus;normalized text;nonnormalized text;English annotation process;training data;Amazon Mechanical Turk","","3","","18","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Mizo to English Machine Translation: An Evaluation Benchmark","V. Hnamte; H. Thangkhanhau; J. Hussain; C. Lalnunmawii; L. Tlaisun; Vanlalruata","Department of Mathematics and Computer Science, Mizoram University, Aizawl, India; Department of Computer Science, Govt. Zirtiri Residential Science College, Aizawl, India; Department of Mathematics and Computer Science, Mizoram University, Aizawl, India; Department of Mathematics and Computer Science, Mizoram University, Aizawl, India; Department of Mathematics and Computer Science, Mizoram University, Aizawl, India; Department of Mathematics and Computer Science, Mizoram University, Aizawl, India","2022 International Conference on Futuristic Technologies (INCOFT)","13 Apr 2023","2022","","","1","6","Speech is the most natural method for people to convey emotions and communicate. Traditional input techniques are used for machine communication. Communication across distinct regional languages is always unavoidable and takes significant technological effort to make meaningful and effective understanding. Methods of Machine Translation (MT) have been developed in an effort to circumvent this obstacle. Neural Machine Translation (NMT) is one of the projects that has lately seen a significant improvement in terms of human judgement compared to traditional methods such as phrase-based machine translation (PMT) and statistical machine translation (SMT). Therefore, developments in NMT have an influence on the perceived difficulty of humans. Numerous internet translation and mobile application solutions, such as Google Translate, etc., were created to solve this issue. However, such simple translation methods are not accurate for the parallel MizoEnglish languages. In this research, we have attempted to improve language translation by using the superior capacity of NMT to boost the Mizo-to-English translation in the BiLingual Evaluation Understudy (BLEU) measure and achieved 42.65 BLEU Score on the 4 grams. Publicly available Mizo-English corpus was developed.","","978-1-6654-5046-1","10.1109/INCOFT55651.2022.10094376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094376","Mizo;English;Language;Translation;Neural Machine;BiLSTM","Training;Training data;Benchmark testing;Data models;Internet;Mobile applications;Machine translation","Internet;language translation;mobile computing;natural language processing;neural nets;statistical analysis","bilingual evaluation understudy measure;BLEU measure;Google Translate;Internet translation;language translation;machine communication;Mizo-to-English machine translation;mobile application;neural machine translation;NMT;phrase-based machine translation;regional languages;statistical machine translation","","1","","18","IEEE","13 Apr 2023","","","IEEE","IEEE Conferences"
"A maximum entropy-based sentence simplifier for machine translation","A. Finch; M. Shimohata; E. Sumita","ATR Spoken Language Translation Research Laboratories, Kansai, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kansai, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kansai, Kyoto, Japan","2005 International Conference on Natural Language Processing and Knowledge Engineering","27 Feb 2006","2005","","","646","650","We present a method for removing unnecessary words from sentences to expedite automatic machine translation. Our hypothesis is that the resulting simplified sentences are easier to automatically translate, giving improved translation performance. We evaluate the sentence simplifier in two ways. Firstly the system is tested directly against humans in the word deletion task. The output of our system is evaluated against a set of reference sentences and its performance compared to a test set of human-shortened sentences. We show the system is able to perform at close to human performance on this task. Secondly we evaluate the system when used as a preprocessor to two different machine translation systems. We show that we are able to significantly improve the performance of a machine translation (MT) system based on the publicly available GIZA++ software by pre-processing the input, and make a small improvement to the performance of the more capable ATR translation system.","","0-7803-9361-9","10.1109/NLPKE.2005.1598816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598816","","Speech;Humans;System testing;Entropy;History;Natural languages;Laboratories;Cities and towns;Software performance;Natural language processing","language translation;maximum entropy methods;natural languages","automatic machine translation system;word deletion task;reference sentences;GIZA++ software;ATR translation system;maximum entropy-based sentence simplifier","","","","14","IEEE","27 Feb 2006","","","IEEE","IEEE Conferences"
"Malayalam machine translation using hybrid approach","R. P. Haroon; T. A. Shaharban","Dept. of CSE, Ilahia College of Engineering & Technology, Kerala, India; Dept. of CSE, Ilahia College of Engineering & Technology, Kerala, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","24 Nov 2016","2016","","","1013","1017","Machine translation is one of the challenging field in NLP. The goal of machine translation is to automatically translate a sentence from one language to another. In this paper we can introduce an hybrid approach for translating Malayalam sentences to corresponding English sentence. The hybrid approach combines both example based machine translation technique and transfer approaches to deliver greater quality and functionality. Machine translation requires a deep and rich understanding of the source language and input text.","","978-1-4673-9939-5","10.1109/ICEEOT.2016.7754839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754839","Example based machine translation(EBMT);Machine translation(MT);Natural language processing(NLP)","Syntactics;Tagging;Silicon;Dictionaries;Speech;Decoding;Knowledge acquisition","language translation;natural language processing","Malayalam machine translation;hybrid approach;NLP;English sentence;source language;input text","","5","","22","IEEE","24 Nov 2016","","","IEEE","IEEE Conferences"
"Automatic speech translation based on the semantic structure","J. Muller; H. Stahl; M. Lang","Institute for Human-Machine-Communication, Munich University of Technology, Munich, Germany; Institute for Human-Machine-Communication, Munich University of Technology, Munich, Germany; Institute for Human-Machine-Communication, Munich University of Technology, Munich, Germany","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","2","","658","661 vol.2","Describes a system for the semantic-based translation of spoken or written limited-domain utterances. The semantic structure, as the output of a semantic decoder, serves as the interlingua level. A word-chain generator combined with a linguistic post-processor produces the corresponding word chain in the target language. Both the semantic decoder and the word chain generator work with pure stochastic and trainable knowledge bases. The grammatical features of certain words can be easily extracted with the aid of both the word chain and the semantic structure.","","0-7803-3555-4","10.1109/ICSLP.1996.607447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607447","","Decoding;Natural languages;Speech synthesis;Tin;Stochastic processes;Logic;Graphics","language translation;speech recognition;natural languages;decoding;linguistics","automatic speech translation;semantic structure;spoken utterances;written utterances;limited-domain utterances;semantic decoder;interlingua level;word-chain generator;linguistic post-processor;stochastic knowledge bases;trainable knowledge bases;grammatical feature extraction;speech understanding;language production;semantic model;syntactic model;inflectional model","","2","1","15","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Rule Based Machine Translation System from English to Tamil","M. Kasthuri; S. B. R. Kumar","Dept. of Com. Science, Bishop Heber College (Autonomous), Tiruchirappalli, Tamil Nadu, India; Dept. of Com. Science, St. Joseph's College (Autonomous), Tiruchirappalli, Tamil Nadu, India","2014 World Congress on Computing and Communication Technologies","3 Apr 2014","2014","","","158","163","The main goal of this research is to develop English-Tamil machine translation system using rule-based approaches. For rule based approach, considering the structural difference between English and Tamil languages, syntax transfer based methodology is adopted for translation. This translation engine is a parser, which analyzes the source text, and the corresponding target structure is generated through the transfer lexicon. Morphological generator for Tamil is required to generate the proper Tamil sentence.","","978-1-4799-2877-4","10.1109/WCCCT.2014.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755127","Machine Translation;Syntax Based Approach;Lexicon;PoS;Parse;Paninian grammar","Syntactics;Grammar;Educational institutions;Databases;Generators;Feature extraction;Speech","language translation;natural language processing","rule based machine translation system;English-Tamil machine translation system;structural difference;syntax transfer based methodology;translation engine;parser;source text;transfer lexicon;morphological generator","","3","","12","IEEE","3 Apr 2014","","","IEEE","IEEE Conferences"
"Improvements to Prosodic Alignment for Automatic Dubbing","Y. Virkar; M. Federico; R. Enyedi; R. Barra-Chicote",Amazon; Amazon; Amazon; Amazon,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7543","7574","Automatic dubbing is an extension of speech-to-speech translation such that the resulting target speech is carefully aligned in terms of duration, lip movements, timbre, emotion, prosody, etc. of the speaker in order to achieve audiovisual coherence. Dubbing quality strongly depends on isochrony, i.e., arranging the translation of the original speech to optimally match its sequence of phrases and pauses. To this end, we present improvements to the prosodic alignment component of our recently introduced dubbing architecture. We present empirical results for four dubbing directions ‚Äì English to French, Italian, German and Spanish ‚Äì on a publicly available collection of TED Talks. Compared to previous work, our enhanced prosodic alignment model significantly improves prosodic alignment accuracy and provides segmentation perceptibly better or on par with manually annotated reference segmentation.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414966","speech translation;text to speech;automatic dubbing","Lips;Conferences;Semantics;Coherence;Signal processing;Acoustics;Timbre","image segmentation;language translation;speech processing;speech synthesis","automatic dubbing;speech-to-speech translation;resulting target speech;lip movements;audiovisual coherence;dubbing quality;original speech;pauses;prosodic alignment component;recently introduced dubbing architecture;dubbing directions;enhanced prosodic alignment model;prosodic alignment accuracy","","4","","29","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond","Y. Xiao; L. Wu; J. Guo; J. Li; M. Zhang; T. Qin; T. -Y. Liu","Soochow University, Suzhou, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Soochow University, Suzhou, China; Soochow University, Suzhou, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","5 Sep 2023","2023","45","10","11407","11427","Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications.","1939-3539","","10.1109/TPAMI.2023.3277122","NSFC(grant numbers:62206194); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129160","Non-autoregressive;neural machine translation;transformer;sequence generation;natural language processing","Training;Decoding;Surveys;Mathematical models;Machine translation;Data models;Computational modeling","inference mechanisms;language translation;learning (artificial intelligence);natural language processing;speech recognition;text analysis","advanced NAR models;autoregressive generation;inference speed;machine learning;NAR generation;natural language processing communities;neural machine translation;nonautoregressive generation;nonautoregressive translation models;sacrificed translation accuracy","","4","","242","IEEE","18 May 2023","","","IEEE","IEEE Journals"
"End-to-end named entity recognition for Vietnamese speech","T. -H. Nguyen; T. -B. Nguyen; Q. -T. Do; T. -L. Nguyen","Thai Nguyen University of Education, Thainguyen, Vietnam; Vietnam Artificial Intelligence System, Hanoi, Vietnam; Vietnam Artificial Intelligence System, Hanoi, Vietnam; Thai Nguyen University of Technology, Thainguyen, Vietnam","2022 25th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","28 Dec 2022","2022","","","1","5","One of the first steps in comprehending natural or spoken language is named-entity recognition. It plays a critical role in natural language processing applications such as text clustering, subject detection, topic detection, and text summarization. The most common techniques in information extraction using named-entity recognition can be applied to building automatic question-answering systems, semantic web technology, automatic translation machines, and so on. For input text with correct formatting, studies on the named entity recognition (NER) task have achieved excellent and nearly human-equal results. However, for the output documents of the speech recognition system (ASR), the normative signs of the text, such as punctuation and uppercase, no longer exist, causing difficulties for the researchers. In this paper, we present the process of building a Vietnamese speech dataset for the NER task and propose a new end-to-end approach for the NER of Vietnamese speech. In addition, we combine the multi-task learning model with the punctuation and uppercase (CaPu) recovery model and demonstrate the combination‚Äôs effectiveness when documenting the approximately 5% improvement in the F1 score.","2472-7695","979-8-3503-9856-4","10.1109/O-COCOSDA202257103.2022.9997862","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997862","Named entity recognition;Vietnamese speech;end-to-end","Training;Semantic Web;Text recognition;Buildings;Pipelines;Speech recognition;Natural language processing","language translation;learning (artificial intelligence);natural language processing;semantic Web;speech recognition;text analysis","automatic translation machines;building automatic question-answering systems;comprehending natural spoken language;end-to-end approach;end-to-end named entity recognition;named entity recognition task;named-entity recognition;natural language processing applications;speech recognition system;subject detection;text clustering;text summarization;topic detection;Vietnamese speech dataset;Vietnamese speech NER","","","","12","IEEE","28 Dec 2022","","","IEEE","IEEE Conferences"
"Word sense disambiguation system for Myanmar word in support of Myanmar-English machine translation","N. T. T. Aung; N. L. Thein","University of Computer Studies, Yangon, Myanmar; University of Computer Studies, Yangon, Myanmar","SICE Annual Conference 2011","27 Oct 2011","2011","","","2835","2840","Word Sense Disambiguation (WSD) has always been a key problem in Natural Language Processing. WSD is defined as the task of finding the correct sense of a word in a specific context. It is an intermediate task essential to many natural language processing problems, including machine translation, information retrieval and speech processing. There is not any cited work for resolving ambiguity of words in Myanmar language. In this paper, we propose an approach to solve the ambiguity of Myanmar words for Myanmar-English machine translation. Our approach is based on Nearest Neighbor Cosine classifier to disambiguate ambiguous words with part-of-speech `noun' and `verb', which uses topical feature that represent co-occurring words in bag-of-word feature. The system uses Myanmar-English parallel corpus as training data. The proposed system can improve the translation accuracy for Myanmar-English machine translation system.","","978-4-907764-39-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6060465","Word Sense Disambiguation;machine translation;Myanmar-English parallel corpus","Context;Training;Natural language processing;Semantics;Support vector machine classification;Dairy products;Dictionaries","computational linguistics;language translation;natural language processing;pattern classification;word processing","word sense disambiguation;Myanmar word;Myanmar-English machine translation;WSD;natural language processing;Myanmar language;nearest neighbor cosine classifier;disambiguate ambiguous words;part-of-speech;co-occurring words;bag-of-word feature;Myanmar-English parallel corpus","","","","14","","27 Oct 2011","","","IEEE","IEEE Conferences"
"Comparing Neural Networks for Speech Emotion Recognition in Customer Service Interactions","B. Waelbers; S. Bromuri; A. P. Henkel","Computer Science Department, Open Universiteit, Heerlen, the Netherlands; Computer Science Department, Open Universiteit, Heerlen, the Netherlands; Organization Department, Open Universiteit, Heerlen, the Netherlands","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","Automatic speech emotion recognition (SER) may assist call center service employees in deciphering and regulating customer emotions. In order to contribute to a successful augmentation of service employees with AI, the main goal of this study is to identify effective machine learning approaches to classify discrete basic emotions in customer service conversations. A comparison is presented of the recognition performance of different neural network architectures on speech features extracted from service interactions in a naturalistic customer service setting. Baseline classifiers, including a zerorule classifier, a random classifier, a frequency classifier, and nonsequential multi-class classifiers are compared to different neural network architectures. A multi-layer perceptron (MLP), a one-dimensional convolutional neural network (CNN), and a neural machine translation (NMT) outperform the baseline classifiers, suggesting a pattern in the data relating to emotion labels. While the neural machine translation model with attention attains the highest f1-score, no significant difference in performance among the neural networks is detected. Results therefore support the use of the the multi-label multi-layer perceptron as the simplest model.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892165","Deep neural networks;Neural machine translation;Speech emotion recognition;Call center service interactions","Emotion recognition;Customer services;Neural networks;Speech recognition;Machine learning;Feature extraction;Machine translation","customer services;emotion recognition;feature extraction;language translation;learning (artificial intelligence);multilayer perceptrons;natural language processing;neural nets;pattern classification;speech recognition","effective machine;discrete basic emotions;customer service conversations;recognition performance;different neural network architectures;speech features;service interactions;naturalistic customer service setting;baseline classifiers;zerorule classifier;random classifier;frequency classifier;nonsequential multiclass classifiers;one-dimensional convolutional neural network;emotion labels;neural machine translation model;neural networks;multilabel multilayer perceptron;automatic speech emotion recognition;call center service employees;deciphering regulating customer emotions;successful augmentation","","2","","44","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Extending motor imagery by speech imagery for brain-computer interface","L. Wang; X. Zhang; Y. Zhang","School of Electronic Science and Engineering, Southeast University, Nanjing, China; School of Electronic Science and Engineering, Southeast University, Nanjing, China; School of Electronic Science and Engineering, Southeast University, Nanjing, China","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","26 Sep 2013","2013","","","7056","7059","An electroencephalogram (EEG)-based brain computer interface (BCI) is a novel tool that translates brain intentions into control signals. As the operational dimensions of motor imagery are limited, we describe in this paper an extension of its capability by including speech imagery. Our new system was tested with the help of subjects, whose native language is Chinese. The tests were divided into two steps. The first step was speech imagery; consequently motor imagery and speech imagery were merged in the second step. Feature vectors of EEG signals were extracted from both common spatial patterns (CSP) and cross-correlation functions; then these vectors were classified by a support vector machine (SVM). The distinguishing accuracies of two intentions were found to be between 79.33% and 88.26%. This result shows that the capability of BCI for motor imagery can be extended by combining motor imagery and speech imagery.","1558-4615","978-1-4577-0216-7","10.1109/EMBC.2013.6611183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611183","","Electroencephalography;Speech;Feature extraction;Synchronization;Training;Support vector machines;Accuracy","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;signal classification;speech processing;support vector machines","motor imagery;speech imagery;electroencephalogram-based brain computer interface;brain intention translation;control signal;operational dimensions;native language;Chinese;feature vector extraction;EEG signals;common spatial patterns;cross-correlation functions;vector classification;support vector machine","Adult;Brain-Computer Interfaces;Electroencephalography;Female;Humans;Imagination;Male;Motor Activity;Motor Cortex;Speech;Support Vector Machine;Young Adult","7","2","13","IEEE","26 Sep 2013","","","IEEE","IEEE Conferences"
"GTrans: Grouping and Fusing Transformer Layers for Neural Machine Translation","J. Yang; Y. Yin; L. Yang; S. Ma; H. Huang; D. Zhang; F. Wei; Z. Li","State Key Lab of Software Development Environment, Beihang University, Beijing, China; NLC Team, Microsoft Research Asia, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; NLC Team, Microsoft Research Asia, Beijing, China; NLC Team, Microsoft Research Asia, Beijing, China; NLC Team, Microsoft Research Asia, Beijing, China; NLC Team, Microsoft Research Asia, Beijing, China; State Key Lab of Software Development Environment, Beihang University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","18 Apr 2023","2023","31","","1489","1498","Transformer structure, stacked by a sequence of encoder and decoder network layers, achieves significant development in neural machine translation. However, vanilla Transformer mainly exploits the top-layer representation, assuming the lower layers provide trivial or redundant information and thus ignoring the bottom-layer feature that is potentially valuable. In this work, we propose the Group-Transformer model (GTrans) that flexibly divides multi-layer representations of both encoder and decoder into different groups and then fuses these group features to generate target words. To corroborate the effectiveness of the proposed method, extensive experiments and analytic experiments are conducted on three bilingual translation benchmarks and three multilingual translation tasks, including the IWLST-14, IWLST-17, LDC, WMT-14, WMT-21 and OPUS-100 benchmark. Experimental and analytical results demonstrate that our model outperforms its Transformer counterparts by a consistent gain. Furthermore, it can be successfully scaled up to 60 encoder layers and 36 decoder layers.","2329-9304","","10.1109/TASLP.2022.3221040","National Natural Science Foundation of China(grant numbers:U1636211,61672081,61370126); 2022 Tencent Wechat Rhino-Bird Focused Research Program; Fund of the State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2021ZX-18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944969","Neural machine translation;deep transformer;multi-layer representation fusion;multilingual translation","Decoding;Transformers;Task analysis;Machine translation;Training;Speech processing;Benchmark testing","language translation;natural language processing","analytic experiments;bilingual translation benchmarks;bottom-layer feature;decoder layers;decoder network layers;encoder layers;GTrans;IWLST-14;IWLST-17;multilayer representations;multilingual translation tasks;neural machine translation;top-layer representation;transformer structure;vanilla transformer;WMT-14;WMT-21","","1","","60","IEEE","10 Nov 2022","","","IEEE","IEEE Journals"
"A language acquisition method based on statistical machine translation for application to robots","K. Takabuchi; N. Iwahashi; T. Kunishima","Okayama Prefectural University, Okayama, Japan; Okayama Prefectural University, Okayama, Japan; Okayama Prefectural University, Okayama, Japan","2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","9 Feb 2017","2016","","","300","301","A method based on statistical machine translation is presented that will enable robots to learn language. The proposed method is novel in two ways: (1) the language acquisition problem is formulated as a statistical machine translation problem, and (2) language-specific morphological analysis is not necessary. Promising experimental results were obtained by applying a traditional statistical machine translation method for sequence-to-sequence conversion.","2161-9484","978-1-5090-5069-7","10.1109/DEVLRN.2016.7846837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846837","language acquisition;statistical machine translation;robot","Robots;Speech recognition;Speech;Hidden Markov models;Training data;Pragmatics;Object recognition","control engineering computing;language translation;learning (artificial intelligence);robot programming","language acquisition method;statistical machine translation;robot learning;sequence-to-sequence conversion","","1","","6","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"M3ST: Mix at Three Levels for Speech Translation","X. Cheng; Q. Dong; F. Yue; T. Ko; M. Wang; Y. Zou","ADSPLAB, School of ECE, Peking University, China; ByteDance; ByteDance; ByteDance; ByteDance; ADSPLAB, School of ECE, Peking University, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","How to solve the data scarcity problem for end-to-end speech-to-text translation (ST)? It‚Äôs well known that data augmentation is an efficient method to improve performance for many tasks by enlarging the dataset. In this paper, we propose Mix at three levels for Speech Translation (M3ST) method to increase the diversity of the augmented training corpus. Specifically, we conduct two phases of fine-tuning based on a pre-trained model using external machine translation (MT) data. In the first stage of fine-tuning, we mix the training corpus at three levels, including word level, sentence level and frame level, and fine-tune the entire model with mixed data. At the second stage of fine-tuning, we take both original speech sequences and original text sequences in parallel into the model to fine-tune the network, and use Jensen-Shannon divergence to regularize their outputs. Experiments on MuST-C speech translation benchmark and analysis show M3ST outperforms current strong baselines and achieves state-of-the-art results on eight directions with an average BLEU of 29.9.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095090","Speech Translation;Mix At Three Levels;Data Augmentation","Training;Signal processing;Benchmark testing;Data models;Acoustics;Machine translation;Task analysis","","","","1","","25","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech","A. Conneau; M. Ma; S. Khanuja; Y. Zhang; V. Axelrod; S. Dalmia; J. Riesa; C. Rivera; A. Bapna",Meta AI Research; Google Research; Carnegie Mellon University; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research,"2022 IEEE Spoken Language Technology Workshop (SLT)","27 Jan 2023","2023","","","798","805","We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.1.","","979-8-3503-9690-4","10.1109/SLT54892.2023.10023141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023141","Massively Multilingual Speech Recognition;Low-Resource Language Dataset;Speech Language Identification;Speech Information Retrieval;Few-/Zero- Shot Learning","Conferences;Buildings;Speech recognition;Benchmark testing;Machine translation;Task analysis;Automatic speech recognition","learning (artificial intelligence);speech recognition;word processing","approximately 12 hours;Automatic Speech Recognition;FEW-shot Learning Evaluation;Few-shot Learning Evaluation;FLEURS;low-resource speech understanding;machine translation FLoRes-101 benchmark;n-way parallel speech;Speech benchmark;Speech LangID;Speech Language Identification;speech supervision;speech tasks;speech technology;speech-only w2v-BERT [1];Speech-Text Retrieval;time 12.0 hour;Universal Representations","","1","","38","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"The feasibility analysis of re-ranking for N-best lists on English-Turkish machine translation","E. Yƒ±ldƒ±rƒ±m; A. C. Tantuƒü","Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey","2013 IEEE INISTA","15 Aug 2013","2013","","","1","5","In this paper, we present the results of re-ranking for N-best list on machine translations. The main purpose of this research is to determine the upper bound of MT success that can be gained by reordering possible candidate translations. We use Google Translate Research API1 as our Statistical Machine Translation (SMT) system to get the N-best lists consisting of possible Turkish translations for a given English sentence. We evaluate the effect of reordering using three simple methods: unigram count (UC), unigram ratio (UR), and first four characters match (FFCM). We collected 720 sentences in order to give to the SMT system, and then we used 3 different sets of Turkish translations of them to evaluate our work on the N-best lists. Success of re-ranking is determined by using BLEU metric, besides an inclusive investigation which is necessary especially for agglutinative languages (e.g. Turkish, Czech, Hungarian, and Finnish) is performed by using BLEU+ MT scoring tool. We observe an improvement in BLEU score from 31.71 for the baseline system to 35.46 which is about 11.81% relative for the re-ranked model using UR.","","978-1-4799-0661-1","10.1109/INISTA.2013.6577652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577652","re-ranking;n-best list;machine translation;BLEU+","Measurement;Algorithm design and analysis;Computers;Speech recognition;Educational institutions;Google;Computational modeling","information retrieval;language translation;natural language processing;statistical analysis","English-Turkish machine translation;reranking feasibility analysis;N-best list;candidate translation reordering;Google Translate Research;API;statistical machine translation;unigram count;unigram ratio;first four characters match;FFCM;SMT system;BLEU metric;agglutinative language;BLEU+ MT scoring tool;baseline system","","","","7","IEEE","15 Aug 2013","","","IEEE","IEEE Conferences"
"Studying the Role of Data Quality on Statistical and Neural Machine Translation","K. K. Arora; G. S. Tomar; S. S. Agrawal","Speech and Natural Language Processing, Centre for Development of Advanced Computing, Noida, India; Rajkiya Engineering College, Sonbhadra, UP, India; KIIT Group of Institutions, Gurugram","2021 10th IEEE International Conference on Communication Systems and Network Technologies (CSNT)","12 Aug 2021","2021","","","199","204","Statistical and Neural Machine translation techniques are based on the parallel data used for training models. The general belief is that more training data would result in better models. We studied the available corpora and ambient noises present in them. It revealed that the available data is highly noisy. The paper describes various types of noises present in there and how these are identified. Different types of noise filters are developed and normalization processes have been applied on the corpora. Statistical and neural machine translation models are trained to study the impact of cleaning of noisy data. We performed experiments with noisy data and with cleaned data after discarding noisy data from the training corpus. Standard test set WMT-14 has been used for performing evaluation. The quality of machine translation has been measured through BLEU scores. It was observed that even after discarding a significant volume of noisy data, the models without noisy data performed better than the corpus containing noises. It proves that quality of data has significant impact and mere having huge piles of uncleaned data in not a good choice. The test case presented here is for English-Hindi language pair. It also shows a path that for low resource language pairs, paying attention to the quality of data would bring returns in form of better translation performance. As the noises discussed in paper are general in nature, the findings should be true for any other Indian language pair also, due to inherent similarity among Indian languages.","2329-7182","978-1-6654-2306-9","10.1109/CSNT51715.2021.9509604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509604","component;Statistical Machine Translation;Neural Machine Translation;Corpus quality","Training;Solid modeling;Communication systems;Data integrity;Conferences;Training data;Data models","language translation;natural language processing","low resource language pairs;English-Hindi language pair;BLEU scores;standard test set WMT-14;uncleaned data;cleaned data;noisy data;training data;parallel data;neural machine translation techniques;data quality","","","","20","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"Intelligent Recognition English Translation Model Based on ID3 Algorithm","L. Pan","Shandong Transport Vocational College, Weifang, China","2022 IEEE 2nd International Conference on Mobile Networks and Wireless Communications (ICMNWC)","7 Feb 2023","2022","","","1","5","Today's science and technology are developing faster and faster, and the number of scientific and technical documents is also increasing. Although manual translation can express the original text well, with the increasing number of documents, the speed of manual translation seems to be stretched. Machine translation is an important means to solve this problem. Therefore, this paper studies translation models to improve translation speed and hope to achieve accurate translation. In this paper, an intelligent recognition English translation system is designed for English translation. The system needs to obtain English corpus through data clarity, data processing, etc., and through the ID3 algorithm of decision tree and decision tree tool to process English vocabulary, a pure corpus can be obtained, and It can restore and mark the parts of speech in English sentences to achieve more accurate translation results. After optimizing the data of the English translation model constructed in this paper, the model can store more data and save system disk space.","","978-1-6654-9111-2","10.1109/ICMNWC56175.2022.10031764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031764","ID3 algorithm;intelligent identification;English translation;data optimization","Wireless communication;Vocabulary;Manuals;Data processing;Data models;Decision trees;Machine translation","decision trees;iterative methods;language translation;natural language processing;storage management;text analysis","data storage;decision tree tool;English corpus;English sentences;English vocabulary;ID3 algorithm;intelligent recognition English translation model;machine translation;parts of speech;scientific document;technical document","","","","11","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Cache based recurrent neural network language model inference for first pass speech recognition","Z. Huang; G. Zweig; B. Dumoulin","Speech at Microsoft, Sunnyvale, CA; Microsoft Research, Redmond, WA; Speech at Microsoft, Sunnyvale, CA","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","6354","6358","Recurrent neural network language models (RNNLMs) have recently produced improvements on language processing tasks ranging from machine translation to word tagging and speech recognition. To date, however, the computational expense of RNNLMs has hampered their application to first pass decoding. In this paper, we show that by restricting the RNNLM calls to those words that receive a reasonable score according to a n-gram model, and by deploying a set of caches, we can reduce the cost of using an RNNLM in the first pass to that of using an additional n-gram model. We compare this scheme to lattice rescoring, and find that they produce comparable results for a Bing Voice search task. The best performance results from rescoring a lattice that is itself created with a RNNLM in the first pass.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854827","recurrent neural network language model;cache;computational efficiency;voice search","Computational modeling;Decoding;History;Recurrent neural networks;Data models;Hidden Markov models;Speech recognition","language translation;recurrent neural nets;search problems;speech recognition","cache based recurrent neural network language model inference;first pass speech recognition;machine translation;word tagging;computational expense;language processing tasks;RNNLM calls;n-gram model;lattice rescoring;Bing Voice search task","","17","1","16","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Extending BLEU Evaluation Method with Linguistic Weight","M. Yang; J. Zhu; J. Li; L. Wang; H. Qi; S. Li; L. Daxin","MOE-MS Joint Key Lab of NLP &Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP &Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP &Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Foreign Languages, Harbin Institute of Technology, Harbin, China; Department of Computer, Heilongjiang Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP &Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Engineering of Technology, Harbin, China","2008 The 9th International Conference for Young Computer Scientists","12 Dec 2008","2008","","","1683","1688","BLEU is one of the most popular metrics for automatic evaluation of machine translation quality. Focusing on its ignorance of different effects of various translation units upon translation quality, this paper extends proper weights to different words and n-grams in the framework of BLEU. The linear regression method is adopted to capture the human perception on translation quality via word types and n-gram length. Compared with other linguistic-rich metrics based on machine learning, the proposed approach is simple and largely preserves BLEUpsilas advantage of language independence. Experimental results indicate that this method brings a much better evaluation performance for both human translation and machine translation than original BLEU.","","978-0-7695-3398-8","10.1109/ICYCS.2008.362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709226","Machine translation evaluation;BLEU;Linear regression;linguistic weight.","Humans;Linear regression;Machine learning;Computer science;Support vector machines;Support vector machine classification;Speech analysis;Natural languages;Automatic testing;Performance evaluation","language translation;linguistics;regression analysis","BLEU evaluation method;linguistic weight;machine translation quality;linear regression method;machine learning","","4","","10","IEEE","12 Dec 2008","","","IEEE","IEEE Conferences"
"Language Identification From Speech Features Using SVM and LDA","J. S. Anjana; S. S. Poorna","Department of Electronics and Communication Engineering, Amrita Vishwa Vidyapeetham, Amritapuri, India; Department of Electronics and Communication Engineering, Amrita Vishwa Vidyapeetham, Amritapuri, India","2018 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","18 Nov 2018","2018","","","1","4","Speech based language identification system has a wide range of applications in the field of telephone services, multilingual translation services, government intelligence and monitoring etc. Identifying the exact speech feature for classification is an important problem in the language identification research area. In this work, we are comparing the performance measures of a language identification system using two different supervised learning algorithms. Mel frequency cepstral coefficients and formant feature vectors are extracted for classification purpose. The system which is developed using the database of seven different Indian languages is capable of identifying languages with LDA giving a maximum classification accuracy of 93.88% when compared to SVM with a classification accuracy of 84%.","","978-1-5386-3624-4","10.1109/WiSPNET.2018.8538638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538638","Speech;Language Identification;Feature Extraction;MFCC;Formants;SVM;LDA","Mel frequency cepstral coefficient;Support vector machines;Feature extraction;Conferences;Speech processing;Resonant frequency","cepstral analysis;feature extraction;language translation;learning (artificial intelligence);natural language processing;pattern classification;speech processing;speech recognition;support vector machines;vectors","language identification research area;mel frequency cepstral coefficients;formant feature vectors;SVM;speech features;speech based language identification system;telephone services;multilingual translation services;government intelligence;supervised learning algorithms;LDA","","12","","15","IEEE","18 Nov 2018","","","IEEE","IEEE Conferences"
"The Karlsruhe-Verbmobil speech recognition engine","M. Finke; P. Geutner; H. Hild; T. Kemp; K. Ries; M. Westphal","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","1","","83","86 vol.1","Verbmobil, a German research project, aims at machine translation of spontaneous speech input. The ultimate goal is the development of a portable machine translator that will allow people to negotiate in their native language. Within this project the University of Karlsruhe has developed a speech recognition engine that has been evaluated on a yearly basis during the project and shows very promising speech recognition word accuracy results on large vocabulary spontaneous speech. We introduce the Janus Speech Recognition Toolkit underlying the speech recognizer. The main new contributions to the acoustic modeling part of our 1996 evaluation system-speaker normalization, channel normalization and polyphonic clustering-are discussed and evaluated. Besides the acoustic models we delineate the different language models used in our evaluation system: word trigram models interpolated with class based models and a separate spelling language model were applied. As a result of using the toolkit and integrating all these parts into the recognition engine the word error rate on the German spontaneous scheduling task (GSST) could be decreased from 30% word error rate in 1995 to 13.8% in 1996.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.599552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599552","","Speech recognition;Engines;Natural languages;Object oriented modeling;Interactive systems;Laboratories;Error analysis;Handwriting recognition;Hidden Markov models;Speech analysis","speech recognition;language translation;natural language interfaces;vocabulary;performance evaluation;computational linguistics;errors;scheduling;interpolation","Karlsruhe-Verbmobil speech recognition engine;German research project;machine translation;spontaneous speech input;University of Karlsruhe;word accuracy;large vocabulary spontaneous speech;Janus Speech Recognition Toolkit;acoustic modeling;speaker normalization;channel normalization;polyphonic clustering;language models;word trigram models;class based models;spelling language model;German spontaneous scheduling task;word error rate","","34","1","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Spoken Spanish generation from sign language","R. San-Segundo; J. M. Pardo; J. Ferreiros; V. Sama; R. Barra-Chicote; J. M. Lucas; D. S√°nchez; A. Garc√≠a",NA; NA; NA; NA; NA; NA; NA; NA,"Interacting with Computers","18 Jan 2018","2010","22","2","123","139","This paper describes the development of a Spoken Spanish generator from sign-writing. The sign language considered was the Spanish sign language (LSE: Lengua de Signos Espa√±ola). This system consists of an advanced visual interface (where a deaf person can specify a sequence of signs in sign-writing), a language translator (for generating the sequence of words in Spanish), and finally, a text to speech converter. The visual interface allows a sign sequence to be defined using several sign-writing alternatives. The paper details the process for designing the visual interface proposing solutions for HCI-specific challenges when working with the Deaf (i.e. important difficulties in writing Spanish or limited sign coverage for describing abstract or conceptual ideas). Three strategies were developed and combined for language translation to implement the final version of the language translator module. The summative evaluation, carried out with Deaf from Madrid and Toledo, includes objective measurements from the system and subjective information from questionnaires. The paper also describes the first Spanish-LSE parallel corpus for language processing research focused on specific domains. This corpus includes more than 4000 Spanish sentences translated into LSE. These sentences focused on two restricted domains: the renewal of the identity document and driver‚Äôs license. This corpus also contains all sign descriptions in several sign-writing specifications generated with a new version of the eSign Editor. This new version includes a grapheme to phoneme system for Spanish and a SEA-HamNoSys converter.","1873-7951","","10.1016/j.intcom.2009.11.011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147058","Spanish sign language (LSE);Speech generation from LSE;LSE corpus;Sign editor;LSE translation;Driver‚Äôs license renewal","","","","","1","","","","18 Jan 2018","","","OUP","OUP Journals"
"ASR and Translation for Under-Resourced Languages","L. Besacier; V. . -B. Le; C. Boitet; V. Berment","UJF, CLIPS-IMAG Laboratory, Grenoble, France; UJF, CLIPS-IMAG Laboratory, Grenoble, France; UJF, CLIPS-IMAG Laboratory, Grenoble, France; UJF, CLIPS-IMAG Laboratory, Grenoble, France","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","There are more than 6000 languages in the world but only a small number possess the resources required for implementation of human language technologies (HLT). Thus, HLT are mostly concerned by languages for which large resources are available or which have suddenly become of interest because of the economic or political scene. On the contrary, languages from developing countries or minorities have been less worked on in the past years. One way of improving this ""language divide"" is do more research on portability of HLT for multilingual applications. In this paper, we concentrate on speech-to-speech translation. We present here our methodology for fast development of ASR systems for under-resourced languages or, as they are called now, pi-languages (poorly equipped). We present the resources collected for Vietnamese, and the experimental results of our first Vietnamese ASR system. The current validation of our methodology for Khmer is described next. We also discuss some issues related to machine translation and present first contributions of our laboratory in this context of ""pi-languages""","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661502","","Automatic speech recognition;Natural languages;Optical character recognition software;Text processing;Speech synthesis;Speech processing;Laboratories;Dictionaries;Visualization;Printing","language translation;natural languages;speech recognition","under-resourced languages;language translation;human language technologies;speech-to-speech translation;automatic speech recognition","","10","","11","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Need for Hindi-Dogri machine translation system","P. Dubey","Department of computer science & IT, University of Jammu acceptable, Jammu, INDIA","2014 International Conference on Computing for Sustainable Global Development (INDIACom)","12 Jun 2014","2014","","","136","140","Machine translation (MT) is the translation of text or speech in one language called the source language into another language called the target language. Machine translation systems (MTS) are available for various Indian languages. The very first system for the translation of Hindi text to Dogri has been developed by the authors. This paper presents the methods of machine translation and the various machine translation systems developed in India based on these approaches. This paper then discusses need for the development of Hindi-Dogri MTS systems and the approach used for its development. The potential use of the system has also been discussed.","","978-93-80544-12-0","10.1109/IndiaCom.2014.6828116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828116","lexicon;Machine translation;Morphological Analyzer","Educational institutions;Dictionaries;Information technology;Computer science;Accuracy;Artificial neural networks;Artificial intelligence","language translation;natural language processing;text analysis","Hindi-Dogri machine translation system;text translation;speech translation;source language;target language;Indian language;Hindi text;Dogri text;Hindi-Dogri MTS system","","2","","35","","12 Jun 2014","","","IEEE","IEEE Conferences"
"User-centered evaluation for machine translation of spoken language","D. D. Palmer","Virage Advanced Technology Group, Woburn, MA, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","5","","v/1013","v/1016 Vol. 5","In this paper, we discuss a user-centered method for qualitative comparison of machine translation systems based on rankings of system output. System ranking requires no reference transcript, which can be expensive to generate and difficult to define for spoken language input. Ranking can be performed by monolingual users with no training in machine translation evaluation. We present results of experiments ranking four Arabic-to-English and three Mandarin-to-English machine translation systems processing spoken language transcripts with word error rates of 20-30%.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1416478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416478","","Natural languages;Humans;Automatic speech recognition;Performance evaluation;Error analysis;Information filtering;Information filters;Data mining;Real time systems;Feeds","language translation;natural languages;error statistics","machine translation system output quality;machine translation user-centered evaluation;spoken language transcripts;word error rate;translation system ranking;Arabic-to-English translation;Mandarin-to-English translation","","3","","6","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Speech to Text Translation enabling Multilingualism","S. Bano; P. Jithendra; G. L. Niharika; Y. Sikhi","Department Of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department Of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department Of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department Of CSE, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India","2020 IEEE International Conference for Innovation in Technology (INOCON)","1 Jan 2021","2020","","","1","4","Speech acts as a barrier to communication between two individuals and helps them in expressing their feelings, thoughts, emotions, and ideologies among each other. The process of establishing a communicational interaction between the machine and mankind is known as Natural Language processing. Speech recognition aids in translating the spoken language into text. We have come up with a Speech Recognition model that converts the speech data given by the user as an input into the text format in his desired language. This model is developed by adding Multilingual features to the existent Google Speech Recognition model based on some of the natural language processing principles. The goal of this research is to build a speech recognition model that even facilitates an illiterate person to easily communicate with the computer system in his regional language.","","978-1-7281-9744-9","10.1109/INOCON50539.2020.9298280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298280","SpeechRecognition;Tkinter;Recognizer;Multilingual text;Natural Language","Speech recognition;Text recognition;Hidden Markov models;Speech processing;Computational modeling;Internet;Microphones","computational linguistics;language translation;natural language processing;speech recognition;text analysis","spoken language;speech data;text format;multilingual features;natural language processing;regional language;speech to text translation;communicational interaction;multilingualism;Google;computer system;speech recognition","","14","","21","IEEE","1 Jan 2021","","","IEEE","IEEE Conferences"
"Bangla Natural Language Processing: A Comprehensive Analysis of Classical, Machine Learning, and Deep Learning-Based Methods","O. Sen; M. Fuad; M. N. Islam; J. Rabbi; M. Masud; M. K. Hasan; M. A. Awal; A. Ahmed Fime; M. T. Hasan Fuad; D. Sikder; M. A. Raihan Iftee","Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, Saudi Arabia; Department of Electrical and Electronic Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Electronics and Communication Engineering Discipline, Khulna University, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh","IEEE Access","18 Apr 2022","2022","10","","38999","39044","The Bangla language is the seventh most spoken language, with 265 million native and non-native speakers worldwide. However, English is the predominant language for online resources and technical knowledge, journals, and documentation. Consequently, many Bangla-speaking people, who have limited command of English, face hurdles to utilize English resources. To bridge the gap between limited support and increasing demand, researchers conducted many experiments and developed valuable tools and techniques to create and process Bangla language materials. Many efforts are also ongoing to make it easy to use the Bangla language in the online and technical domains. There are some review papers to understand the past, previous, and future Bangla Natural Language Processing (BNLP) trends. The studies are mainly concentrated on the specific domains of BNLP, such as sentiment analysis, speech recognition, optical character recognition, and text summarization. There is an apparent scarcity of resources that contain a comprehensive review of the recent BNLP tools and methods. Therefore, in this paper, we present a thorough analysis of 75 BNLP research papers and categorize them into 11 categories, namely Information Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing and Recognition. We study articles published between 1999 to 2021, and 50% of the papers were published after 2015. Furthermore, we discuss Classical, Machine Learning and Deep Learning approaches with different datasets while addressing the limitations and current and future trends of the BNLP.","2169-3536","","10.1109/ACCESS.2022.3165563","Taif University Researchers Supporting, Taif University, Taif, Saudi Arabia(grant numbers:TURSP-2020/10); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751052","Bangla natural language processing;sentiment analysis;speech recognition;support vector machine;artificial neural network;long short-term memory;gated recurrent unit;convolutional neural network","Hidden Markov models;Speech recognition;Speech processing;Sentiment analysis;Machine learning;Task analysis;Machine translation","deep learning (artificial intelligence);language translation;optical character recognition;sentiment analysis;speech recognition","machine learning;nonnative speakers worldwide;online resources;Bangla-speaking people;English resources;Bangla language materials;online domains;Bangla natural language processing;sentiment analysis;speech recognition;optical character recognition;text summarization;BNLP tools;deep learning;information extraction;machine translation;named entity recognition;parsing;parts of speech tagging;question answering system;fake detection;word sense disambiguation;speech processing","","12","","229","CCBY","7 Apr 2022","","","IEEE","IEEE Journals"
"Automatic Translation from Arabic to Arabic Sign Language: A Review","K. Ayadi; Y. O. M. ElHadj; A. Ferchichi","Computer dept. ISG, CCK Prince Sattam bin Abdulaziz University AlKharj-Riyadh, KSA; Doha Arabic Historical Dictionary Doha Institute for Graduate Studies Doha, Qatar; Computer dept. ISG Universit√© de Tunis Tunis, Tunisia","2018 JCCO Joint International Conference on ICT in Education and Training, International Conference on Computing in Arabic, and International Conference on Geocomputing (JCCO: TICET-ICCA-GECO)","30 May 2019","2018","","","1","5","This paper presents the different machine translation approaches. We distinguish two classes: the classical approaches (Direct, Transfer-Based and Interlingua) and the corpus-based approaches (Memory, Example and Statistical). We describe languages challenges such as morphology, syntax and structure. We illustrate a previous systems related to foreign (American, British and European) sign language machine translation. We list a review of the most important works related to Arabic Sign Language (ArSL) machine translation.","","978-1-5386-6880-1","10.1109/ICCA-TICET.2018.8726197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726197","machine translation;Arabic sign language;approaches;corpus","Assistive technology;Gesture recognition;Avatars;Speech recognition;Streaming media;Grammar;Semantics","language translation;natural language processing;sign language recognition","Arabic Sign Language machine translation;automatic translation;language syntax;language structure;corpus-based approach;direct approach;interlingua approach;American sign language machine translation;statistical approach;British sign language machine translation;European sign language machine translation;ArSL machine translation;example approach;memory approach;transfer-based approach","","2","","24","IEEE","30 May 2019","","","IEEE","IEEE Conferences"
"Rule Based Machine Translation from English to Malayalam","R. Rajan; R. Sivan; R. Ravindran; K. P. Soman","National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Japan; Aalto University, Espoo, Finland; Aalto University, Espoo, Finland; NA","2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies","12 Jan 2010","2009","","","439","441","Here we propose a method for translating English sentences to Malayalam. This machine translation is done by rule based method. The core process is mediated by bilingual dictionaries and rules for converting source language structures into target language structures. The rules used in this approach are prepared based on the Parts Of Speech (POS) tag and dependency information obtained from the parser. There are mainly two types of rules used here, one is transfer link rule and the other is morphological rules. In this method, the transfer link rules are used for generating target structure. Morphological rules are used for assigning morphological features. The bilingual dictionary used here is English, Malayalam bilingual dictionary. By using this approach, a given English sentence can be translated to its Malayalam equivalent.","","978-1-4244-5321-4","10.1109/ACT.2009.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5376582","Machine Translation;Morphological Processing","Dictionaries;Natural languages;Speech;Humans;Telecommunication computing;Telecommunication control;Natural language processing;Surface-mount technology;Neural networks","dictionaries;grammars;language translation","rule-based machine translation;bilingual dictionaries;source language structures;target language structures;parts of speech tag;parser;transfer link rules;morphological rules","","18","","5","IEEE","12 Jan 2010","","","IEEE","IEEE Conferences"
"Video-Based Sign Language Translation System Using Machine Learning","B. Sonare; A. Padgal; Y. Gaikwad; A. Patil","Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India; Department of Information Technology, Pimpri Chinchwad College of Engineering, Pune, India","2021 2nd International Conference for Emerging Technology (INCET)","22 Jun 2021","2021","","","1","4","The development of an interactive real-time video-based sign language translation system powered by efficient machine learning algorithms which is commonly developed for deaf-dumb people who are not able to hear or speak and is difficult for them to communicate among themselves or with normal people. Gesture and human activity recognition both are crucial for detecting the sign language as well as the behavior of an individual. These components are rapidly growing domains, enabling higher automation in households as well as in industries. Since extracting the features from continuous hand movements is complex and traditional sign language recognition gloves are costly, the combination of two deep learning algorithms, CNN and RNN can be used for automated sign language recognition. When both of these algorithms are used accuracy of the system also increases (the estimated accuracy is noted to be 92.4% on dynamic hand gestures studied on most of the available datasets). The system will then be able to translate the recognized sign language to desired text and then to speech for further communication using open-source Text-To-Speech API with python. This type of system has the potential in the future to enable a person to give the presentation or join a video conference in business or educational platform in which image or video-based representation of sign language can be projected as the person is speaking on a real-time basis. This architecture is well constructed and hence can solve many difficulties in communication purposes.","","978-1-7281-7029-9","10.1109/INCET51464.2021.9456176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9456176","dynamic hand gesture;computer vision;RNN;CNN;LSTM;Sign Language;Text to Speech","Machine learning algorithms;Text recognition;Assistive technology;Heuristic algorithms;Gesture recognition;Speech recognition;Streaming media","convolutional neural nets;handicapped aids;human computer interaction;language translation;learning (artificial intelligence);recurrent neural nets;sign language recognition;video signal processing","machine learning;real-time video-based sign language translation system;deaf-dumb people;human activity recognition;deep learning algorithms;automated sign language recognition;video conference;open-source text-to-speech API;Python;CNN;RNN","","7","","8","IEEE","22 Jun 2021","","","IEEE","IEEE Conferences"
"T-GSA: Transformer with Gaussian-Weighted Self-Attention for Speech Enhancement","J. Kim; M. El-Khamy; J. Lee","SOC R&D, Samsung Semiconductor, Inc.; SOC R&D, Samsung Semiconductor, Inc., USA; SOC R&D, Samsung Semiconductor, Inc., USA","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","6649","6653","Transformer neural networks (TNN) demonstrated state-ofart performance on many natural language processing (NLP) tasks, replacing recurrent neural networks (RNNs), such as LSTMs or GRUs. However, TNNs did not perform well in speech enhancement, whose contextual nature is different than NLP tasks, like machine translation. Self-attention is a core building block of the Transformer, which not only enables parallelization of sequence computation, but also provides the constant path length between symbols that is essential to learning long-range dependencies. In this paper, we propose a Transformer with Gaussian-weighted self-attention (T-GSA), whose attention weights are attenuated according to the distance between target and context symbols. The experimental results show that the proposed T-GSA has significantly improved speech-enhancement performance, compared to the Transformer and RNNs.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053591","Self-attention;Transformer;LSTM","Performance evaluation;Recurrent neural networks;Speech enhancement;Transformer cores;Signal processing;Machine translation;Task analysis","learning (artificial intelligence);natural language processing;recurrent neural nets;speech enhancement","NLP tasks;machine translation;constant path length;attention weights;T-GSA;speech-enhancement performance;RNNs;speech enhancement;natural language processing tasks;recurrent neural networks;Transformer neural networks;Transformer with Gaussian-weighted self-attention","","63","","22","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Syntactic annotation under dependency scheme on Chinese spontaneous speech","Xuefei Liu; Aijun Li; Yuan Jia; Yiqing Zu","Institute of Linguistics, Chinese Academy of Social Sciences, Beijing, China; Institute of Linguistics, Chinese Academy of Social Sciences, Beijing, China; Institute of Linguistics, Chinese Academy of Social Sciences, Beijing, China; iFlyTek, Anhui, Hefei, China","2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)","2 Mar 2015","2014","","","1","6","Syntactic or semantic annotation is an indispensable work for understanding the intention of the spoken discourses or dialogues. As we know that dependency relation annotation is a kind of syntactic or low shallow semantic annotation, however the present annotation schemes are almost all for text rather than spoken discourses or dialogues. By analysis the online spontaneous chatting data, this paper tries to propose a dependency scheme for spoken discourses. Based on the HIT scheme(Harbin Institute of Technology), we finally proposed 26 kinds of dependencies, where four dependencies are added as ‚ÄúTranslocation‚Äù, ‚ÄúRepetition‚Äù, ‚ÄúDuplication‚Äù and ‚ÄúOmission‚Äù, and three are modified as ‚ÄúIndependent Structure‚Äù, ‚ÄúIndependent Clause‚Äù and ‚ÄúDependent Clause‚Äù. The refined dependency scheme enriches the annotation to Chinese spontaneous speech, which would benefit for speech recognition, semantic comprehension and machine translation.","","978-1-4799-7094-0","10.1109/ICSDA.2014.7051415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051415","dependency grammar;HIT dependency parser;dependency annotation;spontaneous speech","Earth Observing System;Speech;Computers;Integrated circuits","language translation;linguistics;natural language processing;speech recognition","syntactic annotation;dependency scheme;Chinese spontaneous speech;semantic annotation;spoken discourses;dialogues;online spontaneous chatting data;HIT scheme;Harbin Institute of Technology;speech recognition;machine translation","","1","","23","IEEE","2 Mar 2015","","","IEEE","IEEE Conferences"
"Democratizing Language Learning using Machine Learning","A. Gangopadhyay; I. Bardhan; A. Das; N. S. Soman; S. Das","Washington University in St. Louis, St. Louis, MO, USA; Indrajit Bardhan LLC, New York City, NY, USA; Domani Systems, Shelton, CT, USA; Domani Systems, Shelton, CT, USA; Domani Systems, Shelton, CT, USA","2022 56th Annual Conference on Information Sciences and Systems (CISS)","14 Apr 2022","2022","","","137","141","Most apps available in the market for learning a new language are severely limited in terms of the number of languages users can learn on the platform (‚Äútarget language‚Äù), as well as the language in which users can receive instruction (‚Äúsource language‚Äù). Users are also limited by the fixed sets of lessons or the curriculum provided by these apps. In this work, we present an app framework which allows users free choice of source and target languages, as well as the flexibility in learning any input word, phrase or sentence of their choice using machine translation, whose performance and coverage of new languages is continuously improving. The app provides real-time feedback on the correctness of user pronunciation for any input using a text-based similarity metric, and helps learners practice their pronunciation until they perfect it. The app also provides a conversation platform where intermediate and advanced learners can engage in simple, real-time conversations with a chatbot on topics they are most likely to engage in while learning a new language. The conversation platform uses machine translation and speech-to-text tools to convert user query in any source language into an English query, gets the chatbot response and converts it back to the source language. This simple and novel approach allows users to freely converse in any language they want to practice, while having the chatbot trained only on English language-based corpus. The chatbot also integrates a novel intent classification module that classifies user query into one of several available topics, thereby enabling the chatbot to continue conversation with the user in the same topic. Finally, the chatbot is also capable of integrating search capability for specific queries (e.g., weather) with the help of available public domain resources so that it can provide users with real-time updates for such queries, thus making language learning fun and interesting.","","978-1-6654-1796-9","10.1109/CISS53076.2022.9751168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751168","language learning;machine learning;machine translation;chatbot systems;intent classification;integrated search;speech-to-text;text-to-speech","Measurement;Machine learning;Chatbots;Real-time systems;Machine translation;Meteorology","computer aided instruction;interactive systems;language translation;learning (artificial intelligence);linguistics;mobile computing;natural language interfaces;natural language processing;pattern classification;query processing","user query;source language;chatbot;English language-based;language learning fun;machine learning;languages users;target language;app framework;users free choice;target languages;user pronunciation;conversation platform","","1","","15","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"Research on the standardization processing of Chinese sentences in Mandarin-to-English speech translation system","Zong Chengqing; Huang Heyan; Chen Zhaoxiong","IMT Research Center, Institute of Computing Technology, Chinese Academy and Sciences, Beijing, China; IMT Research Center, Institute of Computing Technology, Chinese Academy and Sciences, Beijing, China; IMT Research Center, Institute of Computing Technology, Chinese Academy and Sciences, Beijing, China","1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335)","6 Aug 2002","1997","2","","1823","1827 vol.2","The informal sentence is one of the most important factors to affect the translation precision ratio of a machine translation (MT) system. Especially, in a speech translation system which translates Chinese spoken language to a foreign language, the informal Chinese sentence processing becomes a key link of processing before translation. The characteristics of Chinese spoken language are summarized, and the strategies for Standardization Processing of Chinese Sentences (SPCS) are presented. The strategies combine the method of system automatic processing with the method to check the results with the help of human computer interaction. The paper discusses in detail the related problems in Chinese sentence analysis.","","0-7803-4253-4","10.1109/ICIPS.1997.669372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669372","","Standardization;Natural languages;Speech processing;Human computer interaction","language translation;speech processing;natural languages;user interfaces;interactive systems","standardization processing of Chinese sentences;Mandarin-to-English speech translation system;informal sentence;translation precision ratio;speech translation system;Chinese spoken language;informal Chinese sentence processing;system automatic processing;human computer interaction;Chinese sentence","","","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Improving adversarial neural machine translation with prior knowledge","Y. Yang; X. Li; T. Jiang; J. Kong; B. Ma; X. Zhou; L. Wang","Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry; Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry; Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry; Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry; Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry; Speech and Language Information Processing, Xinjiang Laboratory of Minority, Urumqi, China; Chinese Academy of Sciences, Xinjiang Technical Institute of Physics and Chemistry","2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","8 Mar 2018","2017","","","1373","1377","Generative adversarial networks (GANs) has achieved great success in the field of image processing, Adversarial Neural Machine Translation(NMT) is the application of GANs to machine translation. Unlike previous work training NMT model through maximizing the likelihood of the human translation, Adversarial NMT minimizes the distinction between human translation and the translation generated by a NMT model. Even though Adversarial NMT has achieved impressive results, while using little in the way of prior knowledge. In this paper, we integrated bilingual dictionaries to Adversarial NMT by leveraging a character model. Extensive experiment shows that our proposed methods can achieve remarkable improvement on the translation quality of Adversarial NMT, and obtain better result than several strong baselines.","","978-1-5090-5990-4","10.1109/GlobalSIP.2017.8309186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8309186","Neural Machine Translation;Prior Knowledge;DeepLearning;Uyghur","Training;Dictionaries;Neural networks;Computational modeling;Decoding;Convolution;Gallium nitride","dictionaries;language translation;neural nets","generative adversarial networks;GANs;human translation;Adversarial NMT;Adversarial Neural Machine Translation;NMT model","","","","16","IEEE","8 Mar 2018","","","IEEE","IEEE Conferences"
"An unsupervised boosting technique for refiningword alignment","S. Ananthakrishnan; R. Prasad; P. Natarajan","Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA","2010 IEEE Spoken Language Technology Workshop","24 Jan 2011","2010","","","177","182","Translation rules extracted from automatic word alignment form the basis of statistical machine translation (SMT) systems. An unsupervised expectation-maximization (EM) algorithm is typically used to obtain a word alignment from parallel corpora. Being statistically-driven, the alignments produced by this technique are often erroneous. In this paper, we propose an unsupervised boosting strategy for refining automatic word alignment with the goal of improving SMT performance. The proposed approach results in fewer unaligned words, a significant reduction in the number of extracted translation phrase pairs, a corresponding improvement in SMT decoding speed, and a consistent improvement in translation accuracy, as measured by BLEU, across multiple language pairs and test sets. The reduction in storage and processing requirements coupled with improved accuracy make the proposed technique ideally suited for interactive translation services, facilitating applications such as mobile speech-to-speech translation.","","978-1-4244-7902-3","10.1109/SLT.2010.5700847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5700847","word alignment;boosting;statistical machine translation;mobile speech-to-speech translation","Boosting;Decoding;Viterbi algorithm;Training;Accuracy;Mathematical model;Equations","language translation;speech processing;statistical analysis;unsupervised learning","unsupervised boosting technique;translation rules extraction;automatic word alignment;statistical machine translation;SMT;unsupervised expectation-maximization;EM;parallel corpora;interactive translation services;speech-to-speech translation;word alignment refinement","","1","1","10","IEEE","24 Jan 2011","","","IEEE","IEEE Conferences"
"Measuring domain similarity for statistical machine translation","Lin Liu; Hailong Cao; Tiejun Zhao","MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China","2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","19 May 2014","2013","","","611","615","It is well known that the statistical machine translation (SMT) performance suffers when a model is applied to out-of-domain data. It is also known that the more similar the test domain and the training domain are, the more efficient the training data are for SMT performance. Hence, measuring the similarity of domains is an important task to select appropriate training data. The most widely used method uses the cosine similarity function and word frequency. The lack of exploring other approaches motivates us to propose and compare several similarity measures. Aiming for better SMT performance, we compared 10 similarity measures, which are a combination of 2 feature representations and 5 similarity functions. The results show that using the relative word frequency as the feature representation and using the skew divergence as the similarity function performs the best amongst the 10 measures and outperforms random data selection.","","978-1-4673-5253-6","10.1109/FSKD.2013.6816269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816269","domain adaptation;domain similarity;statistical machine translation(SMT)","Training;Adaptation models;Training data;Frequency measurement;Data models;Business;Transportation","language translation","skew divergence;relative word frequency;similarity functions;feature representations;cosine similarity function;training domain;test domain;statistical machine translation;domain similarity measurement","","","","15","IEEE","19 May 2014","","","IEEE","IEEE Conferences"
"Joint optimization of LCMV beamforming and acoustic echo cancellation for automatic speech recognition","W. Herbordtt; S. Nakamura; W. Kellermann","ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Soraku-gun, Kyoto, Japan; University Erlangen-Nuremberg, Telecommunications Laboratory, Erlangen, Germany","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","3","","iii/77","iii/80 Vol. 3","For full-duplex hands-free acoustic human/machine interfaces, a combination of acoustic echo cancellation and speech enhancement is often required in order to suppress acoustic echoes, local interference and noise. In order to exploit positive synergies between acoustic echo cancellation and speech enhancement optimally, we previously presented a combined least-squares (LS) optimization criterion for the integration of acoustic echo cancellation and adaptive linearly-constrained minimum variance (LCMV) beamforming (Herbordt, W. et al., Proc. EURASIP European Sig. Process. Conf., 2004). By means of speech recognition experiments, we now illustrate the efficiency of the proposed solution in situations with high levels of background noise and with time-varying echo paths and frequent double-talk.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415650","","Array signal processing;Echo cancellers;Automatic speech recognition;Speech enhancement;Humans;Echo interference;Interference suppression;Acoustic noise;Noise cancellation;Speech recognition","optimisation;echo suppression;acoustic signal processing;speech recognition;audio user interfaces;speech-based user interfaces;natural language interfaces;speech enhancement;interference suppression;acoustic noise;random noise;array signal processing","adaptive linearly-constrained minimum variance beamforming;acoustic echo cancellation;automatic speech recognition;hands-free acoustic human/machine interfaces;speech enhancement;interference suppression;noise suppression;least-squares optimization criterion;time-varying echo paths;double-talk;adaptive beamforming microphone arrays","","18","15","14","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Influence of PoS on the accuracy translation from Indonesian to interlingua","H. Sujaini; Kuspriyanto; A. A. Arman","School of Electrical Engineering and Informatics, Bandung Insitute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Insitute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Insitute of Technology, Bandung, Indonesia","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","19 Sep 2011","2011","","","1","6","Part of speech (PoS) is the classification of words according to form, function, and meaning. POS tagging is an important problem and is the first step in natural language processing. POS tagging directly affect the accuracy on the next steps in the processing of natural language like syntax parsing, ambiguity of the word, and machine translation","2155-6830","978-1-4577-0752-0","10.1109/ICEEI.2011.6021737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021737","Machine Translation;Part of Speech;Interlingua","Grammar;Software;Semantics;Tagging;Production;Speech;Accuracy","computational linguistics;grammars;knowledge based systems;language translation;natural language processing;pattern classification","POS tagging;part of speech;word classification;natural language processing;syntax parsing;word ambiguity;direct system;transfer systems;rule-based systems;predicate calculus;minimal recursion semantics;conduct analysis Interlingua representation;Indonesian grammar;PoS Indonesian interlingua machine translation;rule-production rule;predicate logic;syntax-directed translation scheme method;SDTS","","","","13","IEEE","19 Sep 2011","","","IEEE","IEEE Conferences"
"Machine translation based data augmentation for Cantonese keyword spotting","G. Huang; A. Gorin; J. -L. Gauvain; L. Lamel","LIMSI CNRS, Spoken Language Processing Group, Orsay Cedex, France; LIMSI CNRS, Spoken Language Processing Group, Orsay Cedex, France; LIMSI CNRS, Spoken Language Processing Group, Orsay Cedex, France; LIMSI CNRS, Spoken Language Processing Group, Orsay Cedex, France","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","6020","6024","This paper presents a method to improve a language model for a limited-resourced language using statistical machine translation from a related language to generate data for the target language. In this work, the machine translation model is trained on a corpus of parallel Mandarin-Cantonese subtitles and used to translate a large set of Mandarin conversational telephone transcripts to Cantonese, which has limited resources. The translated transcripts are used to train a more robust language model for speech recognition and for keyword search in Cantonese conversational telephone speech. This method enables the keyword search system to detect 1.5 times more out-of-vocabulary words, and achieve 1.7% absolute improvement on actual term-weighted value.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472833","keyword spotting;data augmentation;language modelling;neural networks;low-resourced languages","Hidden Markov models;Speech recognition;Training;Speech;Data models;Recurrent neural networks;Dictionaries","language translation","statistical machine translation;data augmentation;Cantonese keyword spotting;language model;parallel Mandarin-Cantonese subtitles;Mandarin conversational telephone transcripts;Cantonese conversational telephone speech","","6","","24","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Using non-word lexical units in automatic speech understanding","M. Penagarikano; G. Bordel; A. Varona; K. Lopez de Ipina","Dpto. Electricidad y Electr√≥nica,EHU, Universidad del Pa√¨s Vasco, Vizcaya, Spain; Dpto. Electricidad y Electr√≥nica,EHU, Universidad del Pa√¨s Vasco, Vizcaya, Spain; Dpto. Electricidad y Electr√≥nica,EHU, Universidad del Pa√¨s Vasco, Vizcaya, Spain; Dpto. Electricidad y Electr√≥nica,EHU, Universidad del Pa√¨s Vasco, Vizcaya, Spain","1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)","6 Aug 2002","1999","2","","621","624 vol.2","If the objective of a continuous automatic speech understanding system is not a speech-to-text translation, words are not strictly needed, and then the use of alternative lexical units (LUs) will bring us a new degree of freedom to improve the system performance. Consequently, we experimentally explore some methods to automatically extract a set of LUs from a Spanish training corpus and verify that the system can be improved in two ways: reducing the computational costs and increasing the recognition rates. Moreover, preliminary results point out that, even if the system target is a speech-to-text translation, using non-word units and post-processing the output to produce the corresponding word chain outperforms the word based system.","1520-6149","0-7803-5041-3","10.1109/ICASSP.1999.759743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759743","","Speech;Computational efficiency;Natural languages;System performance;Costs;Joining processes;Adaptation model;Lead;Automatic testing;Databases","speech recognition;computational complexity;natural languages","nonword lexical units;continuous automatic speech understanding system;alternative lexical units;system performance;Spanish training corpus;computational costs;recognition rates;speech-to-text translation;post-processing;word chain","","3","","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Farsi - German statistical machine translation through bridge language","S. Bakhshaei; S. Khadivi; N. Riahi","Department of Computer & Technology, Alzahra University, Tehran, Iran; Department of Computer Engineering, Amirkabir University of TechnologyÏä†, Tehran, Iran; Department of Computer & Technology, Alzahra University, Tehran, Iran","2010 5th International Symposium on Telecommunications","17 Mar 2011","2010","","","557","561","Since statistical machine translation outperforms other approaches in the field of machine translation, we used this approach to develop a Farsi-German and a German-Farsi translation system. In this work, first we used an existing English-German bilingual corpus and then we manually translate a large part of the English corpus to Farsi, in order to have the required training data. Thereafter, we employ the idea of using English as the bridge language to build the Farsi-German translation systems, in addition to build the systems on the corresponding Farsi-German bilingual corpus. Because different amount of recourses exist for Farsi-English and German-English language pairs, we investigate different approaches to combine these two machine translation systems. We will show that the BLEU score of Farsi-German system has increased about 15% relatively compared to the baseline system.","","978-1-4244-8183-5","10.1109/ISTEL.2010.5734087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734087","Statistical Machine Translation;SMT;Communication tools;Pivot Language;Bridge Language;Farsi-German;English","Bridges;Training;Speech;Dictionaries;Data mining;Equations;Computers","language translation;natural language processing","Farsi-German statistical machine translation;bridge language;German-Farsi translation system;English-German bilingual corpus;Farsi-English language pair;German-English language pair","","5","1","13","IEEE","17 Mar 2011","","","IEEE","IEEE Conferences"
"Leveraging Weakly Supervised Data to Improve End-to-end Speech-to-text Translation","Y. Jia; M. Johnson; W. Macherey; R. J. Weiss; Y. Cao; C. -C. Chiu; N. Ari; S. Laurenzo; Y. Wu",Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","7180","7184","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683343","Speech translation;sequence-to-sequence model;weakly supervised learning;synthetic training data","Training;Data models;Decoding;Task analysis;Training data;Predictive models;Computer architecture","language translation;learning (artificial intelligence);speech recognition;text analysis","multitask learning;high quality end-to-end ST model;weakly supervised datasets;synthetic data;unlabeled monolingual text;synthetic speech;weakly supervised data;end-to-end speech-to-text translation;translated transcript pairs;pre-trained components;weakly supervised training data;speech-to-transcript;text-to-foreign-text pairs;text-to-speech synthesis models;speech-to-translation pairs;ST training;error compounding","","32","","33","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Toward translating Indonesian spoken utterances to/from other languages","S. Sakti; M. Paul; R. Maia; S. Sakai; N. Kimura; Y. Ashikari; E. Sumita; S. Nakamura","NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan; NICT Spoken Language Communication Research Group, Japan","2009 Oriental COCOSDA International Conference on Speech Database and Assessments","2 Oct 2009","2009","","","137","142","This paper outlines the National Institute of Information and Communications Technology / Advanced Telecommunications Research Institute International (NICT/ATR) research activities in developing a spoken language translation system, specially for translating Indonesian spoken utterances into/from Japanese or English. Since the NICT/ATR Japanese-English speech translation system is an established one and has been widely known for many years, our focus here is only on the additional components that are related to the Indonesian spoken language technology. This includes the development of an Indonesian large vocabulary continuous speech recognizer, Indonesian-Japanese and Indonesian-English machine translators, and an Indonesian speech synthesizer. Each of these component technologies was developed by using corpus-based speech and language processing approaches. Currently, all these components have been successfully incorporated into the mobile terminal of the NICT/ATR multilingual speech translation system.","","978-1-4244-4400-7","10.1109/ICSDA.2009.5278362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278362","","Natural languages;Speech synthesis;Speech recognition;Vocabulary;Speech processing;Research and development;Cities and towns;Communications technology;Synthesizers;Globalization","language translation;linguistics;speech recognition;vocabulary","Indonesian spoken language technology;National Institute of Information and Communications Technology;Advanced Telecommunications Research Institute International;spoken language translation system;Japanese-English speech translation system;vocabulary continuous speech recognizer;Indonesian-Japanese machine translator;Indonesian-English machine translators;Indonesian speech synthesizer;corpus-based speech;multilingual speech translation system","","3","","13","IEEE","2 Oct 2009","","","IEEE","IEEE Conferences"
"Applications of Statistical Machine Translation Approaches to Spoken Language Understanding","K. Macherey; O. Bender; H. Ney","Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Computer Science Department, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Audio, Speech, and Language Processing","27 Mar 2009","2009","17","4","803","818","In this paper, we investigate two statistical methods for spoken language understanding based on statistical machine translation. The first approach employs the source-channel paradigm, whereas the other uses the maximum entropy framework. Starting with an annotated corpus, we describe the problem of natural language understanding as a translation from a source sentence to a formal language target sentence. We analyze the quality of different alignment models and feature functions and show that the direct maximum entropy approach outperforms the source channel-based method. Furthermore, we investigate how both methods perform if the input sentences contain speech recognition errors. Finally, we investigate a new approach to combine speech recognition and spoken language understanding. For this purpose, we employ minimum error rate training which directly optimizes the final evaluation criterion. By combining all knowledge sources in a log-linear way, we show that we can decrease both the word error rate and the slot error rate. Experiments were carried out on two German inhouse corpora for spoken dialogue systems.","1558-7924","","10.1109/TASL.2009.2014262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806285","Combined approach;machine translation;maximum entropy;minimum error rate training;speech recognition;spoken language understanding","Natural languages;Entropy;Error analysis;Automatic speech recognition;Speech recognition;Surface-mount technology;Computer science;Statistical analysis;Formal languages;Humans","entropy;formal languages;language translation;natural language processing;speech recognition;statistical analysis","statistical machine translation;spoken language understanding;source-channel paradigm;maximum entropy framework;natural language understanding;formal language target sentence;alignment models;direct maximum entropy approach;speech recognition errors;minimum error rate training;final evaluation criterion;German inhouse corpora;spoken dialogue systems","","12","3","28","IEEE","27 Mar 2009","","","IEEE","IEEE Journals"
"Mandarin Learning Using Speech and Language Technologies: A Translation Game in the Travel Domain","Y. Xu; S. Seneff","MIT Computer Science and Artificial Intelligence Laboratory, USA; MIT Computer Science and Artificial Intelligence Laboratory, USA","2008 6th International Symposium on Chinese Spoken Language Processing","30 Dec 2008","2008","","","1","4","This paper describes a new Web-based translation game we have designed to help a student learn spoken Chinese. The student talks to the system in Chinese and the system compares the recognized sentence against a set of English prompts to judge whether it is a suitable translation of any one of them. The game can also provide translation assistance upon request. The game was developed using the IWSLT corpus of utterances in the tourist domain, and is oriented towards helping the student communicate effectively during foreign travel. In a preliminary evaluation, the system performed correctly on over 90% of test utterances. The system received positive feedback from the subjects.","","978-1-4244-2942-4","10.1109/CHINSL.2008.ECP.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4730273","","Speech;Natural languages;Computer science;Artificial intelligence;Laboratories;Performance evaluation;System testing;Feedback;Machine learning;Education","computer games;language translation;speech processing;travel industry","Mandarin learning;speech technologies;language technologies;Web-based translation game;travel domain;translation assistance;IWSLT corpus;tourist domain","","","","8","IEEE","30 Dec 2008","","","IEEE","IEEE Conferences"
"State-transition cost functions and an application to language translation","H. Alshawi; A. L. Buchsbaum","AT&T Labs., Murray Hill, NJ, USA; AT and T Laboratories, Murray Hill, NJ, USA","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","1","","103","106 vol.1","We define a general method for ranking the solutions of a search process by associating costs with equivalence classes of state transitions of the process. We show how the method accommodates models based on probabilistic, discriminative, and distance cost functions, including assignment of costs to unseen events. By applying the method to our machine translation prototype, we are able to experiment with different cost functions and training procedures, including an unsupervised procedure for training the numerical parameters of our English-Chinese translation model. Results from these experiments show that the choice of cost function leads to significant differences in translation quality.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.599558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599558","","Cost function;Prototypes;Costing;Context modeling;Distortion measurement;Speech processing;Probability;Lattices;Search problems","computational linguistics;natural language interfaces;language translation;speech recognition;search problems;equivalence classes;probability;unsupervised learning","state-transition cost functions;language translation;search process;equivalence classes;probabilistic cost functions;discriminative cost functions;distance cost functions;unseen events;machine translation prototype;experiment;training procedures;unsupervised procedure;English-Chinese translation model;speech translation","","","3","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reranking machine translation hypotheses with structured and web-based language models","Wen Wang; A. Stolcke; Jing Zheng","Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Inc., Menlo Park, CA, USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","159","164","In this paper, we investigate the use of linguistically motivated and computationally efficient structured language models for reranking N-best hypotheses in a statistical machine translation system. These language models, developed from Constraint Dependency Grammar parses, tightly integrate knowledge of words, morphological and lexical features, and syntactic dependency constraints. Two structured language models are applied for N-best rescoring, one is an almost-parsing language model, and the other utilizes more syntactic features by explicitly modeling syntactic dependencies between words. We also investigate effective and efficient language modeling methods to use N-grams extracted from up to 1 teraword of web documents. We apply all these language models for N-best re-ranking on the NIST and DARPA GALE program1 2006 and 2007 machine translation evaluation tasks and find that the combination of these language models increases the BLEU score up to 1.6% absolutely on blind test sets.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430102","Statistical machine translation;N-best reranking;structured language model;web-based language modeling;smoothing","Smoothing methods;Surface-mount technology;Decoding;Natural languages;Entropy;Speech;Laboratories;NIST;Testing;Error analysis","computational linguistics;document handling;grammars;language translation;natural languages;statistical analysis","reranking machine translation;Web-based language model;reranking N-best hypotheses;statistical machine translation system;constraint dependency grammar parses;syntactic dependency constraint;structured language model;N-best rescoring;parsing language model;Web documents","","2","","18","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Dropped pronoun generation for dialogue machine translation","L. Wang; X. Zhang; Z. Tu; H. Li; Q. Liu","ADAPT Centre, Dublin City University; ADAPT Centre, Dublin City University; Noah's Ark Lab, Huawei Technologies; Noah's Ark Lab, Huawei Technologies; ADAPT Centre, Dublin City University","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","6110","6114","Dropped pronoun (DP) is a common problem in dialogue machine translation, in which pronouns are frequently dropped in the source sentence and thus are missing in its translation. In response to this problem, we propose a novel approach to improve the translation of DPs for dialogue machine translation. Firstly, we build a training data for DP generation, in which the DPs are automatically added according to the alignment information from a parallel corpus. Then we model the DP generation problem as a sequence labelling task, and develop a generation model based on recurrent neural networks and language models. Finally, we apply the DP generator to machine translation task by completing the source sentences with the missing pronouns. Experimental results show that our approach achieves a significant improvement of 1.7 BLEU points by recalling possible DPs in the source sentences.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472851","Machine Translation;Dialogue;Dropped Pronoun","Training data;Training;Recurrent neural networks;Generators;Data models;Labeling;Motion pictures","language translation;recurrent neural nets","dropped pronoun generation;dialogue machine translation;source sentence;DP generation;sequence labelling task;recurrent neural network;language model","","6","","23","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"A Comparative Study on Transformer vs RNN in Speech Applications","S. Karita; N. Chen; T. Hayashi; T. Hori; H. Inaguma; Z. Jiang; M. Someki; N. E. Y. Soplin; R. Yamamoto; X. Wang; S. Watanabe; T. Yoshimura; W. Zhang","NTT Communication Science Laboratories; Johns Hopkins University; Human Dataware Lab. Co., Ltd.; Mitsubishi Electric Research Laboratories; Kyoto University; Johns Hopkins University; Nagoya University; Waseda University; LINE Corporation; Johns Hopkins University; Johns Hopkins University; Human Dataware Lab. Co., Ltd.; Shanghai Jiao Tong University","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","449","456","Sequence-to-sequence models have been widely used in end-to-end speech processing, for example, automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS). This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003750","Transformer;Recurrent Neural Networks;Speech Recognition;Text-to-Speech;Speech Translation","Decoding;Training;Task analysis;Xenon;Recurrent neural networks;Speech recognition;Transforms","language translation;natural language processing;recurrent neural nets;speech processing;speech recognition","Transformer;RNN;speech applications;sequence-to-sequence models;end-to-end speech processing;neural machine translation;natural language processing;recurrent neural networks","","268","","51","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Improving language models for ASR using translated in-domain data","S. Kombrink; T. Mikolov; M. Karafi√°t; L. Burget","Brno University of Technology, Czech Republic; Brno University of Technology, Czech Republic; Brno University of Technology, Czech Republic; Brno University of Technology, Czech Republic","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4405","4408","Acquisition of in-domain training data to build speech recognition systems for under-resourced languages can be a costly, time-demanding and tedious process. In this work, we propose the use of machine translation to translate English transcripts of telephone speech into Czech language in order to improve a Czech CTS speech recognition system. The translated transcripts are used as additional language model training data in a scenario where the baseline language model is trained on off- and close-domain data only. We report perplexities, OOV and word error rates and examine different data sets and translators on their suitability for the described task.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6288896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288896","Low Resource ASR;Language Modeling;Machine Translation","Data models;Speech;Dictionaries;Google;Speech recognition;Acoustics;Decoding","language translation;natural language processing;speech recognition","language models;ASR;in-domain training data;speech recognition systems;under-resourced languages;machine translation;English transcripts;telephone speech;Czech language","","1","","6","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Research on Query Translation Disambiguation for CLIR Based on HowNet","H. Zhu; D. Zheng; T. Zhao","MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China","2008 The 9th International Conference for Young Computer Scientists","12 Dec 2008","2008","","","1677","1682","Query translation is an important task for cross-language information retrieval (CLIR), which aims at translating the query described in source language into target language. The approach to query translation based on bilingual dictionary is becoming the mainstream thinking because of its simplicity and the increasing availability of machine readable bilingual dictionary. However, this kind of approach faces two necessary problems that is ambiguity in translation and the incompleteness of the dictionary. This paper focuses on the first problem, and it presents three statistical models based on HowNet to resolve query translation ambiguity of CLIR: query translation selection based on semantic relation; bilingual decaying co-occurrence model and semantic decaying co-occurrence model. Through test and summarizing this paper gets the best algorithm to integrate the traits of the three models, which gradually filters and optimizes the translation.","","978-0-7695-3398-8","10.1109/ICYCS.2008.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4709225","Query translation;CLIR;statistical method;translation selection;OOV","Dictionaries;Information retrieval;Natural languages;Testing;Laboratories;Natural language processing;Speech processing;Filters;Statistical analysis;Availability","dictionaries;information retrieval;language translation;statistical analysis","query translation disambiguation;cross-language information retrieval;HowNet;machine readable bilingual dictionary;statistical model;bilingual decaying co-occurrence model;semantic decaying co-occurrence model","","1","","11","IEEE","12 Dec 2008","","","IEEE","IEEE Conferences"
"Code-Switch Speech Rescoring with Monolingual Data","G. Liu; L. Cao",Tencent AI Lab; Tencent AI Lab,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","6229","6233","In the automatic speech recognition (ASR) system, how to solve the problem of code-switch speech recognition has been a concern. Code-switch speech recognition is challenging due to data scarcity as well as diverse syntactic structures across languages. In this paper, we focus on the code-switch speech recognition in mainland China, which is obviously different from the Hong Kong and Southeast Asia area in linguistic characteristics. We propose a novel approach that only uses monolingual data for code-switch second-pass speech recognition which is also named language model rescoring. The approach converts the code-switch sentence to a monolingual sentence by a word mapping and language model determination step, therefore the issue of data scarcity is unnecessary to be considered. The word pairs during the word mapping step are generated by a fine-designed generation process that incorporates machine translation, word alignment, etc. We show that the proposed approach achieves an over 7.23% relative WER reduction from the naive monolingual language model (MLM) rescoring in our test set.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414158","code-switch;speech recognition;language model rescoring;word mapping;language model determination","Speech coding;Conferences;Speech recognition;Syntactics;Signal processing;Linguistics;Data models","language translation;natural language processing;speech recognition","code-switch speech recognition;data scarcity;monolingual data;code-switch second-pass speech recognition;code-switch sentence;naive monolingual language model rescoring;code-switch speech rescoring;automatic speech recognition system","","1","","19","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"WERD: Using social text spelling variants for evaluating dialectal speech recognition","A. Ali; P. Nakov; P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, UK; HBKU, Qatar Computing Research Institute, Doha, Qatar; Centre for Speech Technology Research, University of Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, UK","2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","25 Jan 2018","2017","","","141","148","We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Such a situation is typical for machine translation (MT), and thus we borrow ideas from an MT evaluation metric, namely TERp, an extension of translation error rate which is closely-related to WER. In particular, in the process of comparing a hypothesis to a reference, we make use of spelling variants for words and phrases, which we mine from Twitter in an unsupervised fashion. Our experiments with evaluating ASR output for Egyptian Arabic, and further manual analysis, show that the resulting WERd (i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER for evaluating dialectal ASR.","","978-1-5090-4788-8","10.1109/ASRU.2017.8268928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268928","Automatic speech recognition;dialectal ASR;ASR evaluation;word error rate;multi-reference WER","Speech;Measurement;Standards;Training;Error analysis;Twitter","language translation;natural language processing;social networking (online);speech recognition","translation error rate;WER;dialectal speech recognition;automatic speech recognition systems;dialectal speech input;machine translation","","3","","35","IEEE","25 Jan 2018","","","IEEE","IEEE Conferences"
"Big Data for Speech and Language Processing","X. Huang","Microsoft, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","2","2","Amongst all creatures the human species stands unique in Darwin's natural selection process. It is no exaggeration that speech and language helped to differentiate human intelligence from animal intelligence in the evolution process. The impact of big data and cloud to speech and language evolution is foundational to realize the society's AI vision. This talk will review how Microsoft achieved human parity on both conversational speech recognition and news machine translation research tasks and highlight significant challenges remaining to make speech and language production services mainstream in our AI journey.","","978-1-5386-5035-6","10.1109/BigData.2018.8622003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622003","","Artificial intelligence;Big Data;Speech recognition;Speech processing;Task analysis;Conferences;Animals","artificial intelligence;language translation;natural language processing;speech processing;speech recognition","language processing;language evolution;human parity;conversational speech recognition;Big Data;speech processing;news machine translation;AI vision","","","","0","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Layer-Level Progressive Transformer With Modality Difference Awareness for Multi-Modal Neural Machine Translation","J. Guo; J. Ye; Y. Xiang; Z. Yu","Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","11 Aug 2023","2023","31","","3015","3026","Multi-modal neural machine translation (MNMT) aims to translate sentences from the source language into the target language with the aid of corresponding images. Unfortunately, there is a considerable modality gap between the semantic-related images and texts in terms of data form and semantic expression. How to fully incorporate visual information into texts to enhance the performance of machine translation is one of the critical issues for MNMT. However, the initial visual and textual features are generally extracted with their modality-specific models; Consequently, there is a considerable representation gap between images and texts. Most previous MNMT works prefer only to adopt the feature-level fusion strategies to learn multi-modal representation, while the modality representation gap is often ignored. To this end, this article proposes a progressive multi-modal Transformer (ProMul-Trans) with Modality Difference-Aware (MDA) to address the visual-to-textual fusion problem raised in MNMT. We first employ MDA to capture the modality-consistency information by taking visual and textual representations as inputs in each Transformer layer. Then a layer-level progressive fusion (Layer-ProFusion) strategy is adopted to progressively align visual and textual representations layer-by-layer to enhance machine translation performance. Experiment results on the Multi30 k dataset are conducted, and the results show that the proposed approach outperforms the compared state-of-the-art (SOTA) methods on English $\to$ German (En $\to$ De), English $\to$ French (En $\to$ Fr) and English $\to$ Czech (En $\to$ Cs) tasks. We release the code at https://github.com/JunjieYe-MMT/HierProMul-Trans.","2329-9304","","10.1109/TASLP.2023.3301210","National Key Research and Development Program of China(grant numbers:2020AAA0107904); National Natural Science Foundation of China(grant numbers:62241604,61866020); Natural Science Foundation Project of Yunnan Science and Technology Department(grant numbers:202301AT070444); Yunnan Provincial Major Science and Technology Special Plan Projects(grant numbers:202103AA080015); Yunnan Key Research Projects(grant numbers:202203AA080004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202177","Layer-level progressive fusion;modality difference-aware;multi-modal neural machine translation;multi-modal transformer","Visualization;Feature extraction;Machine translation;Transformers;Speech processing;Semantics;Task analysis","feature extraction;image fusion;language translation;learning (artificial intelligence);natural language processing","considerable modality gap;considerable representation gap;feature-level fusion strategies;fully incorporate visual information;initial visual features;layer-level progressive fusion strategy;Layer-level progressive Transformer;Layer-ProFusion;machine translation performance;Modality Difference awareness;Modality Difference-Aware;modality representation gap;modality-consistency information;modality-specific models;Multimodal neural machine translation;multimodal representation;previous MNMT works;progressive multimodal Transformer;representations layer-by-layer;semantic-related images;textual features;textual representations;Transformer layer;visual representations;visual-to-textual fusion problem","","","","49","IEEE","2 Aug 2023","","","IEEE","IEEE Journals"
"Machine translation from Japanese and French to Vietnamese, the difference among language families","T. -N. -D. Do; M. Utiyama; E. Sumita","Inter. Research Institute MICA, HUST, Hanoi, Vietnam; National Institute of Information & Communications Technology, Kyoto, Japan; National Institute of Information & Communications Technology, Kyoto, Japan","2015 International Conference on Asian Language Processing (IALP)","14 Apr 2016","2015","","","17","20","Although Vietnamese is spoken language of more than 90 million people in the world (in 2014), Vietnamese language is still considered as a low-resourced language. Vietnamese NLP still lacks of resources for text and speech processing, especially research on machine translation for Vietnamese is very rare. This paper presents our first attempt to collect and construct French-Vietnamese and Japanese-Vietnamese statistical machine translation systems. These two different languages, French and Japanese, are less focused in Vietnamese-related machine translation research. The differences between these two languages in comparison with Vietnamese can bring out interesting observations.","","978-1-4673-9596-0","10.1109/IALP.2015.7451521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451521","low-resourced language;statistical machine translation system;Vietnamese;French;Japanese","Color;Focusing","language translation;linguistics;natural language processing","low-resourced language;Vietnamese NLP;text processing resource;speech processing resource;French-Vietnamese statistical machine translation systems;Japanese-Vietnamese statistical machine translation systems","","1","","21","IEEE","14 Apr 2016","","","IEEE","IEEE Conferences"
"MLP emulation of N-gram models as a first step to connectionist language modeling","M. J. Castro; F. Prat; F. Casacuberta","Departament de Sistemes Informatics i Computaco, Universitat Polit√©cnica de Val√©ncia, Spain; NA; Departament de Sistemes Informatics i Computaco, Universitat Polit√©cnica de Val√©ncia, Spain","1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)","6 Aug 2002","1999","2","","910","915 vol.2","In problems such as automatic speech recognition and machine translation, where the system response must be a sentence in a given language, language models are employed in order to improve system performance. These language models are usually N-gram models (for instance, bigram or trigram models) which are estimated from large text databases using the occurrence frequencies of these N-grams. Nakamura and Shikano (1989) empirically showed how multilayer perceptrons can emulate trigram model predictive capabilities with additional generalization features. Our paper discusses Nakamura and Shikano's work, provides new empirical evidence on multilayer perceptron capability to emulate N-gram models, and proposes new directions for extending neural network-based language models. The experimental work we present here compares connectionist phonological bigram models with a conventional one using different measures, which include recognition performances in a Spanish acoustic-phonetic decoding task.","0537-9989","0-85296-721-7","10.1049/cp:19991228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=818053","","","speech recognition","N-gram models;connectionist language modeling;automatic speech recognition;machine translation;language models;trigram models;bigram models;large text databases;occurrence frequencies;predictive capabilities;generalization features;neural network-based language models;connectionist phonological bigram models;recognition performances;Spanish acoustic-phonetic decoding task","","1","","","","6 Aug 2002","","","IET","IET Conferences"
"Long sentence partitioning using top-down analysis for machine translation","B. Yin; J. Zuo; N. Ye","Knowledge Engineering Research Center, Shenyang Aerospace University, Shenyang, China; Knowledge Engineering Research Center, Shenyang Aerospace University, Shenyang, China; Knowledge Engineering Research Center, Shenyang Aerospace University, Shenyang, China","2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems","14 Nov 2013","2012","03","","1425","1429","Long sentence processing is an important part for English-Chinese machine translation systems. The system performance is directly affected by the correctness of long sentence processing. A basic thought for processing a long sentence is to partition it into short sub-sentences and to merge the sub-translations for the whole translation. In this paper, a rule-based top-down partitioning algorithm is provided. The rules are inducted from sentence patterns and use regular expressions as main part. Firstly, the algorithm reduces some sentence components to shorten the sentence; then coordinate sub-sentences are recognized and partitioned; finally, clauses within sub-sentences are processed. Experiment shows an approximate 85% accuracy and an over 90% recall rate of the algorithm.","2376-595X","978-1-4673-1857-0","10.1109/CCIS.2012.6664620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664620","machine translation;top-down analysis;long sentence partitioning;sentence pattern","Partitioning algorithms;Speech;Algorithm design and analysis;Pragmatics;Speech recognition;Pattern matching;Google","language translation;natural language processing","regular expressions;sentence patterns;rule-based top-down partitioning algorithm;short sub-sentences;English-Chinese machine translation systems;top-down analysis;long sentence partitioning","","2","","12","IEEE","14 Nov 2013","","","IEEE","IEEE Conferences"
"Mimicking Infants‚Äô Bilingual Language Acquisition for Domain Specialized Neural Machine Translation","C. Park; W. -Y. Go; S. Eo; H. Moon; S. Lee; H. Lim","Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; The Affiliated Institute of ETRI, Future Strategy Center, Daejeon, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea","IEEE Access","18 Apr 2022","2022","10","","38684","38693","Existing methods of training domain-specialized neural machine translation (DS-NMT) models are based on the pretrain-finetuning approach (PFA). In this study, we reinterpret existing methods based on the perspective of cognitive science related to cross language speech perception. We propose the cross communication method (CCM), a new DS-NMT training approach. Inspired by the learning method of infants, we perform DS-NMT training by configuring and training DC and GC concurrently in batches. Quantitative and qualitative analysis of our experimental results show that CCM can achieve superior performance compared to the conventional methods. Additionally, we conducted an experiment considering the DS-NMT service to meet industrial demands.","2169-3536","","10.1109/ACCESS.2022.3165572","Ministry of Science and ICT (MSIT), South Korea, through the Information Technology Research Center (ITRC) Support Program supervised; Institute of Information & Communications Technology Planning & Evaluation (IITP)(grant numbers:IITP-2018-0-01405); IITP Grant funded; Korea Government (MSIT) (A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques)(grant numbers:2020-0-00368); Basic Science Research Program through the National Research Foundation of Korea (NRF) funded; Ministry of Education(grant numbers:NRF-2021R1A6A1A03045425); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751075","Domain-specialized neural machine translation;cross communication method;deep learning;neural machine translation","Training data;Machine translation;Neural networks;Computational modeling;Brain modeling;Terminology;Deep learning;Domain specific languages","language translation;learning (artificial intelligence);natural language processing;neural nets","pretrain-finetuning approach;language speech perception;cross communication method;DS-NMT training approach;DS-NMT service;mimicking infants;domain specialized neural machine translation;training domain-specialized neural machine translation models","","2","","40","CCBY","7 Apr 2022","","","IEEE","IEEE Journals"
"Attending From Foresight: A Novel Attention Mechanism for Neural Machine Translation","X. Li; L. Liu; Z. Tu; G. Li; S. Shi; M. Q. . -H. Meng","Linguisitics, Ohio State University, Columbus, Ohio, USA; Tencent AI Lab, Shenzhen, Guangdong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Electronic Engineering, Chinese University of Hong Kong, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","17 Aug 2021","2021","29","","2606","2616","Machines translation¬†(MT) is an essential task in natural language processing or even in artificial intelligence. Statistical machine translation has been the dominant approach to MT for decades, but recently neural machine translation achieves increasing interest because of its appealing model architecture and impressive translation performance. In neural machine translation, an attention model is used to identify the aligned source words for the next target word, i.e., target foresight word, to select translation context. However, it does not make use of any information about this target foresight word at all. Previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrating substantial alignment tasks. However, this approach cannot be applied in machine translation tasks where the target foresight word is unavailable. This paper proposes several novel enhanced attention models by introducing hidden information¬†(such as part-of-speech) of the target foresight word for the translation task. We incorporate the novel enhanced attention employing hidden information about the target foresight word into both recurrent and self-attention-based neural translation models and theoretically justify that such hidden information can make translation prediction easier. Empirical experiments on four datasets further verify that the proposed attention models deliver significant improvements in translation quality.","2329-9304","","10.1109/TASLP.2021.3097939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490355","NMT;Attention;Word Alignment","Predictive models;Machine translation;Task analysis;Context modeling;Recurrent neural networks;Decoding;Unemployment","language translation;natural language processing;statistical analysis","statistical machine translation;impressive translation performance;attention model;target foresight word;translation context;machine translation tasks;translation prediction;translation quality;neural machine translation","","5","","62","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Mobile Speech Translation for Multilingual Requirements Meetings: A Preliminary Study","F. Calefato; F. Lanubile; D. Romita; R. Prikladnicki; J. H. Stocker Pinto","Dipartimento di Informatica, Universit√† di Bari, Bari, Italy; Dipartimento di Informatica, Universit√† di Bari, Bari, Italy; Dipartimento di Informatica, Universit√† di Bari, Bari, Italy; PUCRS, Pontif√≠cia Universidade Cat√≥lica do Rio Grande do Sul, Porto Alegre, Brazil; Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, BR","2014 IEEE 9th International Conference on Global Software Engineering","2 Oct 2014","2014","","","145","152","Communication in global software projects usually occurs between native and non-native English speakers with the drawback of an unequal ability to fully understand and contribute to discussions. In this paper, we investigate the adoption of combining speech recognition and machine translation in order to overcome language barriers among stakeholders who are remotely negotiating software requirements. We report our findings from a simulated study where stakeholders communicate speaking three different languages with the help of the Google mobile speech translation service.","2329-6313","978-1-4799-4360-9","10.1109/ICGSE.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915265","speech translation;language distance;distributed development;requirements engineering;simulation","Speech recognition;Speech;Accuracy;Software;Mobile communication;Google;Real-time systems","language translation;mobile computing;professional communication;project management;software development management;speech recognition;systems analysis","multilingual requirements meetings;global software project communication;nonnative English speakers;speech recognition;machine translation;language barriers;remote software requirement negotiation;stakeholder communication;Google mobile speech translation service","","4","","24","IEEE","2 Oct 2014","","","IEEE","IEEE Conferences"
"Hybrid machine translation for Javanese speech levels","A. P. Wibawa; A. Nafalski; J. Tweedale; N. Murray; A. E. Kadarisman","School of Electrical and Information Engineering, University of South Australia Adelaide, South Australia; School of Electrical and Information Engineering, University of South Australia Adelaide, South Australia; School of Electrical and Information Engineering, University of South Australia Adelaide, South Australia; Centre for Applied Linguistics, Warwick University Warwick, United Kingdom; Faculty of Letters, State University of Malang Malang, Indonesia","2013 5th International Conference on Knowledge and Smart Technology (KST)","4 May 2013","2013","","","64","69","Javanese is a local language with the biggest number of speakers in Indonesia. However, the fact that it is characterized by a complex system of politeness means that it is perceived negatively by Javanese teenagers, who have difficulty understanding it. The hybrid corpus-based machine translation described here is designed to address this by offering a means of translating speech levels appropriately. The system embeds statistical features into a memory-based machine translation to obtain the best performance of Javanese speech levels' translation. The evaluation shows satisfactory results; 0.83 and 90.4 for the average accuracy and quality of the translation, respectively.","","978-1-4673-4853-9","10.1109/KST.2013.6512789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6512789","Javanese speech levels;corpus;machine translation","Speech;Pragmatics;Databases;Educational institutions;Accuracy;Training;Vocabulary","","","","1","","31","IEEE","4 May 2013","","","IEEE","IEEE Conferences"
"Using Multiple Edit Distances to Automatically Grade Outputs From Machine Translation Systems","Y. Akiba; K. Imamura; E. Sumita; H. Nakaiwa; S. Yamamoto; H. G. Okuno","NTT Communication Science Laboratories, Kyoto, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan; NTT Communication Science Laboratories, Kyoto, Japan; ATR Spoken Language Communication Research Laboratories, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan","IEEE Transactions on Audio, Speech, and Language Processing","21 Feb 2006","2006","14","2","393","402","This paper addresses the challenging problem of automatically evaluating output from machine translation (MT) systems that are subsystems of speech-to-speech MT (SSMT) systems. Conventional automatic MT evaluation methods include BLEU, which MT researchers have frequently used. However, BLEU has two drawbacks in SSMT evaluation. First, BLEU assesses errors lightly at the beginning of translations and heavily in the middle, even though its assessments should be independent of position. Second, BLEU lacks tolerance in accepting colloquial sentences with small errors, although such errors do not prevent us from continuing an SSMT-mediated conversation. In this paper, the authors report a new evaluation method called ‚Äúg Rader based on Edit Distances (RED)‚Äù that automatically grades each MT output by using a decision tree (DT). The DT is learned from training data that are encoded by using multiple edit distances, that is, normal edit distance (ED) defined by insertion, deletion, and replacement, as well as its extensions. The use of multiple edit distances allows more tolerance than either ED or BLEU. Each evaluated MT output is assigned a grade by using the DT. RED and BLEU were compared for the task of evaluating MT systems of varying quality on ATR's Basic Travel Expression Corpus (BTEC). Experimental results show that RED significantly outperforms BLEU.","1558-7924","","10.1109/TSA.2005.860770","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1597245","BLEU;decision tree (DT);edit distances (EDs);machine translation evaluation;mWER;reference translations","Humans;Natural languages;Decision trees;Laboratories;Training data;Mobile communication;Informatics;Resource management;Speech processing","","BLEU;decision tree (DT);edit distances (EDs);machine translation evaluation;mWER;reference translations","","3","","34","IEEE","21 Feb 2006","","","IEEE","IEEE Journals"
"A study to find influential parameters on a Farsi-English statistical machine translation system","S. Bakhshaei; S. Khadivi; N. Riahi; H. Sameti","Department of Computer & Technology, Alzahra University, Tehran, Iran; Department of Computer & Technology, Amirkabir University of TechnologyÏä†, Tehran, Iran; Department of Computer & Technology, Alzahra University, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","2010 5th International Symposium on Telecommunications","17 Mar 2011","2010","","","985","991","The aim of this paper is to analyze the Farsi-English statistical machine translation systems as a useful communication tool. Improvement of the nation's communication increases the need of easier way of translating between different languages in front of expensive human translators. In this work, a statistical phrase-based system is run on Farsi - English pair languages and the effect of its parameters on the translation quality has been deeply studied. Using BLEU as a metric of translation accuracy, the system achieves an improvement of 1.84%, relative to the baseline accuracy, which is increment from 16.97% to 18.81% in the best case.","","978-1-4244-8183-5","10.1109/ISTEL.2010.5734165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734165","Statistical Machine Translation;SMT;Farsi - English;Farsi;Communication tool;Moses","Mathematical model;Decoding;Humans;Google;Speech;Computers;Accuracy","language translation;statistical analysis","influential parameters;Farsi-English statistical machine translation system;communication tool;expensive human translators;statistical phrase-based system;Farsi-English pair languages;translation quality","","","","17","IEEE","17 Mar 2011","","","IEEE","IEEE Conferences"
"Response generation based on statistical machine translation for speech-oriented guidance system","K. Nishimura; H. Kawanami; H. Saruwatari; K. Shikano","Nara Institute of science and Technology, Japan; Nara Institute of science and Technology, Japan; Nara Institute of science and Technology, Japan; Graduate School of Information Science","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","17 Jan 2013","2012","","","1","4","An example-based response generation is a robust and practical approach for a real-environment information guidance system. However, this framework cannot reflect differences in nuance, because the set of answer sentences are fixed beforehand. To overcome this issue, we have proposed response generation using a statistical machine translation technique. In this paper, we make use of N-best speech recognition candidates instead of manual transcription used in our previous study. As a result, the generation rate of appropriate response sentences was improved by using multiple recognition hypothesis.","","978-0-6157-0050-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411808","","Speech recognition;Training data;Training;Weather forecasting;Manuals;Analytical models","speech recognition;statistical analysis","response generation;speech-oriented guidance system;example-based response generation;real-environment information guidance system;statistical machine translation technique;N-best speech recognition;manual transcription;multiple recognition hypothesis","","","","5","","17 Jan 2013","","","IEEE","IEEE Conferences"
"Optimizing Instance Selection for Statistical Machine Translation with Feature Decay Algorithms","E. Bi√ßici; D. Yuret","Ko√ß University, Istanbul, Turkey; Ko√ß University, Istanbul, Turkey","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","2","339","350","We introduce FDA5 for efficient parameterization, optimization, and implementation of feature decay algorithms (FDA), a class of instance selection algorithms that use feature decay. FDA increase the diversity of the selected training set by devaluing features (i.e., n-grams) that have already been included. FDA5 decides which instances to select based on three functions used for initializing and decaying feature values and scaling sentence scores controlled with five parameters. We present optimization techniques that allow FDA5 to adapt these functions to in-domain and out-of-domain translation tasks for different language pairs. In a transductive learning setting, selection of training instances relevant to the test set can improve the final translation quality. In machine translation experiments performed on the 2 million sentence English-German section of the Europarl corpus, we show that a subset of the training set selected by FDA5 can gain up to 3.22 BLEU points compared to a randomly selected subset of the same size, can gain up to 0.41 BLEU points compared to using all of the available training data using only 15% of it, and can reach within 0.5 BLEU points to the full training set result by using only 2.7% of the full training data. FDA5 peaks at around 8M words or 15% of the full training set. In an active learning setting, FDA5 minimizes the human effort by identifying the most informative sentences for translation and FDA gains up to 0.45 BLEU points using 3/5 of the available training data compared to using all of it and 1.12 BLEU points compared to random training set. In translation tasks involving English and Turkish, a morphologically rich language, FDA5 can gain up to 11.52 BLEU points compared to a randomly selected subset of the same size, can achieve the same BLEU score using as little as 4% of the data compared to random instance selection, and can exceed the full dataset result by 0.78 BLEU points. FDA5 is able to reduce the time to build a statistical machine translation system to about half with 1M words using only 3% of the space for the phrase table and 8% of the overall space when compared with a baseline system using all of the training data available yet still obtain only 0.58 BLEU points difference with the baseline system in out-of-domain translation.","2329-9304","","10.1109/TASLP.2014.2381882","Koc University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987314","Domain adaptation;information retrieval;instance selection;machine translation;transductive learning","Training;Training data;Optimization;IEEE transactions;Speech;Speech processing;Adaptation models","feature extraction;language translation;natural language processing;random processes","FDA5;feature decay algorithms;training instance selection algorithms;decaying feature values;sentence scores;out-of-domain translation tasks;in-domain translation tasks;language pairs;transductive learning setting;machine translation experiments;English-German section;Europarl corpus;BLEU points;training data;active learning setting;random training set;statistical machine translation system","","14","","34","IEEE","18 Dec 2014","","","IEEE","IEEE Journals"
"An EM algorithm for SCFG in formal syntax-based translation","Songfang Huang; Bowen Zhou","Center of Speech Technology Research, University of Edinburgh, UK; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4813","4816","In this paper, we investigate the use of bilingual parsing on parallel corpora to better estimate the rule parameters in a formal syntax-based machine translation system, which are normally estimated from the inaccurate heuristics. We use an Expectation-Maximization (EM) algorithm to re-estimate the parameters of synchronous context-free grammar (SCFG) rules according to the derivation knowledge from parallel corpora based on maximum likelihood principle, rather than using only the heuristic information. The proposed algorithm produces significantly better BLEU scores than a state-of-the-art formal syntax-based machine translation system on the IWSLT 2006 Chinese to English task.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960708","Formal Syntax-based Translation;SCFG;Expectation-Maximization;Inside-Outside Algorithm","Parameter estimation;Production;Training data;Natural languages;Speech;Maximum likelihood estimation;Context modeling;Induction generators;Synchronous generators","context-free grammars;expectation-maximisation algorithm;language translation","formal syntax-based machine translation system;expectation-maximization algorithm;SCFG;synchronous context-free grammar;bilingual parsing;parallel corpora;maximum likelihood principle","","1","","8","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Human-computer Interactive Oral Translation Methods and Related Issues","M. Zou","Shanghai Industrial and Commercial Polytechnic, Shanghai, China","2022 IEEE 2nd International Conference on Mobile Networks and Wireless Communications (ICMNWC)","7 Feb 2023","2022","","","1","5","Nowadays, machine translation technology has penetrated into daily life and is a commonly used auxiliary tool for cross-language communication. Therefore, the translation for spoken language is particularly important. In addition to its scientific and engineering significance, oral translation also contains rich economic and social benefits. The purpose of this paper is to study human-machine interactive spoken language translation methods and related issues. Firstly, the concept of interpreting and related work at home and abroad are introduced, and then the machine translation system and interpreting system, which are important components of interpreting, are introduced respectively. The acoustic model and statistical language model based on HMM are established. In the process of creating a statistical language model, in order to solve the problem of sparse data, we adopted a modeling method combining word-based language model and part-of-speech-based language model. Finally, the system is tested by use cases, and the correct rate of oral translation is above 90%.","","978-1-6654-9111-2","10.1109/ICMNWC56175.2022.10031908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031908","human-computer interaction;spoken language translation;speech interaction;statistical understanding model","Wireless communication;Economics;Analytical models;Hidden Markov models;Manuals;Data models;Acoustics","hidden Markov models;interactive systems;language translation;natural language processing","commonly used auxiliary tool;computer interactive oral translation methods;cross-language communication;human-machine interactive spoken language translation methods;interpreting system;machine translation system;machine translation technology;modeling method combining word-based language model;part-of-speech-based language model;rich economic benefits;scientific engineering significance;social benefits;statistical language model","","","","18","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Script Translation System For Devnagari To English","J. A. Todase; S. Shelke","Electronics & Telecommunication Department, College of Engineering, Pune, India; Electronics & Telecommunication Department, Dr. D. Y. Patil Institute of Engineering, Management and Research, Pune, India","2018 International Conference on Information , Communication, Engineering and Technology (ICICET)","15 Nov 2018","2018","","","1","4","This paper presents a Machine Translation system for Devnagarito English language translation. Proposed system is able to translate more than one Devnagari sentences to English using rule base approach. The system accepts Marathi language which is derived from Devnagari and translates it to English language. This is done by identifying the parts of speech in Marathi sentence, tokenization and identifying English meaning of each word in bilingual dictionary and get a direct translation. Thesedirect translated words are linked by the English grammar rules to have meaningful translation in English.","","978-1-5386-5510-8","10.1109/ICICET.2018.8533734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533734","Machine Translation;Rule Based Translation;Language Translation","Dictionaries;Grammar;Databases;Communications technology;Knowledge based systems;Production","dictionaries;grammars;language translation;natural language processing","rule base approach;Marathi language;Marathi sentence;tokenization;English grammar rules;meaningful translation;Devnagarito English language translation;Devnagari sentences;machine translation system;script translation system;direct translated words","","4","","12","IEEE","15 Nov 2018","","","IEEE","IEEE Conferences"
"A new decoder for spoken language translation based on confusion networks","N. Bertoldi; M. Federico","ITC-irst-Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy; ITC-irst-Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","86","91","A novel approach to spoken language translation is proposed, which more tightly integrates automatic speech recognition (ASR) and statistical machine translation (SMT). SMT is directly applied on an approximation of the word graph produced by the ASR system, namely a confusion network. The decoding algorithm extends a conventional phrase-based decoder in that it can process at once a large number of source sentence hypotheses contained in the confusion network. Experimental results are presented on a Spanish-English large vocabulary task, namely the translation of the European Parliament plenary sessions. With respect to a conventional SMT decoder processing N-best lists, a slight improvement in the BLEU score is reported as well as a significantly lower decoding time","","0-7803-9478-X","10.1109/ASRU.2005.1566492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566492","","Decoding;Natural languages;Automatic speech recognition;Surface-mount technology;Vocabulary;Acoustic transducers;Polynomials;Entropy","decoding;language translation;natural languages;speech recognition;vocabulary;word processing","spoken language translation;automatic speech recognition;statistical machine translation;word graph;confusion network;decoding algorithm;source sentence hypotheses;Spanish-English large vocabulary;European Parliament plenary sessions","","14","1","12","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Innovative algorithms for Parts of Speech Tagging in hindi-english machine translation language","S. Mall; U. C. Jaiswal","Computer Science and Engineering: dept., Madan Mohan Malaviya University of Technology, Gorakhpur, India; Computer Science and Engineering: dept., Madan Mohan Malaviya University of Technology, Gorakhpur, India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","14 Jan 2016","2015","","","709","714","In this paper we develop and evaluate Parts Of Speech Tagging algorithm for parsing the Hindi text in Unicode format, it verifies the Hindi text according to the correct grammar. The accuracy for only Parts of Speech Tagging 93.6% window-3, 93.96% window-2 and 92.09% window-1. Accuracy of word with Parts of Speech Tagging for window-1- 94.45%, window-2- 95.17% and window-3- 94.75%. The accuracy for Parts of Speech Tagging was achieved by Conditional Random Fields algorithm.","","978-1-4673-7910-6","10.1109/ICGCIoT.2015.7380555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380555","component;formatting Hindi tokenizer;Conditional Random Fields;Parts of Speech Tagger","Dictionaries","computational linguistics;language translation;text analysis","Hindi-English machine translation language;innovative algorithm;part-of-speech tagging algorithm;unicode format;conditional random field algorithm","","3","","16","IEEE","14 Jan 2016","","","IEEE","IEEE Conferences"
"A Hierarchy-to-Sequence Attentional Neural Machine Translation Model","J. Su; J. Zeng; D. Xiong; Y. Liu; M. Wang; J. Xie","Xiamen University, Xiamen, China; Fujian Provincial Key Laboratory of Information Processing and Intelligent Control, Minjiang University, Fuzhou, China; Xiamen University, Xiamen, China; Provincial Key Laboratory for Computer Information Processing Technology, Soochow University, Suzhou, China; Fujian Provincial Key Laboratory of Information Processing and Intelligent Control, Minjiang University, Fuzhou, China; Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","17 Jan 2018","2018","26","3","623","632","Although sequence-to-sequence attentional neural machine translation (NMT) has achieved great progress recently, it is confronted with two challenges: learning optimal model parameters for long parallel sentences and well exploiting different scopes of contexts. In this paper, partially inspired by the idea of segmenting a long sentence into short clauses, each of which can be easily translated by NMT, we propose a hierarchy-to-sequence attentional NMT model to handle these two challenges. Our encoder takes the segmented clause sequence as input and explores a hierarchical neural network structure to model words, clauses, and sentences at different levels, particularly with two layers of recurrent neural networks modeling semantic compositionality at the word and clause level. Correspondingly, the decoder sequentially translates segmented clauses and simultaneously applies two types of attention models to capture contexts of interclause and intraclause for translation prediction. In this way, we can not only improve parameter learning, but also well explore different scopes of contexts for translation. Experimental results on Chinese-English and English-German translation demonstrate the superiorities of the proposed model over the conventional NMT model.","2329-9304","","10.1109/TASLP.2018.2789721","National Natural Science Foundation of China(grant numbers:61672440,61622209,61432013); Scientific Research Project of National Language Committee of China(grant numbers:YB135-49); Open Fund Project of Fujian Provincial Key Laboratory of Information Processing and Intelligent Control (Minjiang University)(grant numbers:MJUKF201742); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8246560","Hierarchy-to-sequence;neural machine translation;natural language processing","Decoding;Training;Context modeling;Semantics;Recurrent neural networks;Speech;Speech processing","grammars;language translation;learning (artificial intelligence);natural language processing;neural nets;recurrent neural nets;text analysis","conventional NMT model;hierarchy-to-sequence attentional neural machine translation model;sequence-to-sequence attentional neural machine translation;optimal model parameters;long parallel sentences;short clauses;hierarchy-to-sequence attentional NMT model;segmented clause sequence;hierarchical neural network structure;recurrent neural networks;clause level;segmented clauses;attention models;translation prediction;parameter learning;English-German translation;semantic compositionality modeling;Chinese-English translation","","55","","35","IEEE","4 Jan 2018","","","IEEE","IEEE Journals"
"An Empirical Analysis of PoS Tagging for Kannada Machine Translation","Jamuna; H. R. Mamatha","Computer Science and Engineering, PES University, Bangalore, India; Computer Science and Engineering, PES University, Bangalore, India","2023 International Conference on Applied Intelligence and Sustainable Computing (ICAISC)","9 Aug 2023","2023","","","1","5","Parts of Speech (POS) tagging process has emerged as one of the crucial and very basic preprocessing technique for any natural language processing tasks. In Kannada, a dominant language in southern India, morphologically rich and low resourced at the same time, PoS tagging process was difficult to achieve in the beginning. Later, studies show that Conditional Random Fields, Hidden Morkov Model and deep learning techniques have produced good accuracy. This paper investigates all the three above mentioned models and argues that deep learning model, which uses bidirectional Long Short Term Memory as a RNN unit, produces the highest accuracy of 93% in contrast to CRF and HMM model with a precision accuracy of 65% and 42% respectively. Also, the paper specifies how important a PoS tagging process is in the task of Machine Translation, which is booming in the world of computational linguistics.","","979-8-3503-2379-5","10.1109/ICAISC58445.2023.10201030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10201030","Conditional Random Fields (CRF);Word2Vec;Hidden Morkov Model (HMM);bidirectional Long Short Term Memory (BiLSTM)","Deep learning;Computational modeling;Hidden Markov models;Tagging;Conditional random fields;Planning;Machine translation","computational linguistics;conditional random fields;deep learning (artificial intelligence);hidden Markov models;language translation;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","bidirectional Long Short Term Memory;Conditional Random Fields;crucial preprocessing technique;deep learning model;deep learning techniques;dominant language;empirical analysis;Hidden Morkov Model;Kannada Machine Translation;mentioned models;natural language processing tasks;PoS tagging process;Speech tagging process;very basic preprocessing technique","","","","23","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"A first speech recognition system for Mandarin-English code-switch conversational speech","N. T. Vu; D. -C. Lyu; J. Weiner; D. Telaar; T. Schlippe; F. Blaicher; E. -S. Chng; T. Schultz; H. Li","Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; School of Computer Engineering, Nanyang Technological University, Singapore; Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; School of Computer Engineering, Nanyang Technological University, Singapore; Cognitive Systems Laboratory, Institute for Anthropomatics, Karlsruhe Institute of Technology, Germany; School of Computer Engineering, Nanyang Technological University, Singapore","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4889","4892","This paper presents first steps toward a large vocabulary continuous speech recognition system (LVCSR) for conversational Mandarin-English code-switching (CS) speech. We applied state-of-the-art techniques such as speaker adaptive and discriminative training to build the first baseline system on the SEAME corpus [1] (South East Asia Mandarin-English). For acoustic modeling, we applied different phone merging approaches based on the International Phonetic Alphabet (IPA) and Bhattacharyya distance in combination with discriminative training to improve accuracy. On language model level, we investigated statistical machine translation (SMT) - based text generation approaches for building code-switching language models. Furthermore, we integrated the provided information from a language identification system (LID) into the decoding process by using a multi-stream approach. Our best 2-pass system achieves a Mixed Error Rate (MER) of 36.6% on the SEAME development set.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289015","code-switching;multilingual speech recognition","Hidden Markov models;Speech;Training;Acoustics;Speech recognition;Speech coding;Merging","error statistics;language translation;speech recognition","first speech recognition system;conversational Mandarin-English code-switching speech;CS speech;large vocabulary continuous speech recognition system;LVCSR;speaker adaptive training;speaker discriminative training;baseline system;SEAME corpus;South East Asia Mandarin-English;phone merging approaches;International Phonetic Alphabet;Bhattacharyya distance;IPA;language model level;statistical machine translation;SMT based text generation approaches;code-switching language models;language identification system;LID;decoding process;multistream approach;2-pass system;mixed error rate;MER","","48","","18","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Multi-Head Attention for End-to-End Neural Machine Translation","I. Fung; B. Mak","The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong","2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP)","6 May 2019","2018","","","250","254","Inspired by the recent success of Google's Transformer model works have been done on borrowing the novel idea of multi-head attention to various applications under different architectures. Albeit latest works have adopted this idea using an end-to-end recurrent model on speech recognition and voice search, making use of a similar model on machine translation has not been attempted yet. In this work, we examine multi-head attention under the attention-based recurrent encoder-decoder frame-work, and conduct detailed analysis on the positional response of multiple heads. Through leveraging the essence of multi-head attention, we are capable of attaining a state-of-the-art result on IWSLT' 15 with 28.48 tokenized BLEU and 53.86% TER, which gives a 0.17 gain in BLEU and 0.37% reduction in TER. Similarly we achieve 25.58 tokenized BLEU and 55.03% TER on WMT'16, which provide a 0.40 gain in BLEU and 0.32% reduction in TER to the baseline model respectively. To the best of our knowledge, this is the first work1 that evaluates the concept of multi-head attention in an end-to-end recurrent network on machine translation tasks.","","978-1-5386-5627-3","10.1109/ISCSLP.2018.8706667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706667","neural machine translation;multi-head attention;end-to-end deep learning","Decoding;Magnetic heads;Training;Speech recognition;Testing;Task analysis;Vocabulary","decoding;encoding;language translation;natural language processing;recurrent neural nets","multihead attention;end-to-end recurrent model;end-to-end recurrent network;end-to-end neural machine translation;attention-based recurrent encoder-decoder framework;Google Transformer model","","1","","23","IEEE","6 May 2019","","","IEEE","IEEE Conferences"
"A bilingual machine translation system: English & Bengali","C. Adak","Dept. of Computer Science and Engineering, University of Kalyani, West Bengal, India","2014 First International Conference on Automation, Control, Energy and Systems (ACES)","1 May 2014","2014","","","1","4","Natural language is a fundamental thing of human-society to communicate and interact with one another. In this globalization era, we interact with different regional people as per our interest in social, cultural, economical, educational and professional domain. There are thousands of natural languages exist in our earth. It is quite tough, rather impossible to know all the languages. So we need a computerized approach to convert one natural language to another as per our necessity. This computerized conversion among multiple languages is known as multilingual machine translation. But in this paper we work with a bilingual model, where we concern with two languages: English and Bengali. We use soft computational approach where fuzzy If-Then rule is applied to choose a lemma from prior knowledge; Penn TreeBank PoS tags and HMM tagger are used as lexical class marker to each word in corpora.","","978-1-4799-3894-0","10.1109/ACES.2014.6808033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808033","Bilingual Model;Computational Linguistics;Machine Translation;Parts of Speech Tagging;Rule Based Model","Hidden Markov models;Natural language processing;Smoothing methods;Computational modeling;Tagging;Computational linguistics","computational linguistics;fuzzy logic;hidden Markov models;language translation;natural language processing","bilingual machine translation system;natural language;human-society;globalization era;regional people;social domain;cultural domain;economical domain;educational domain;professional domain;computerized conversion;multilingual machine translation;bilingual model;English language;Bengali language;soft computational approach;fuzzy if-then rule;lemma;Penn TreeBank PoS tags;HMM tagger;lexical class marker","","5","","14","IEEE","1 May 2014","","","IEEE","IEEE Conferences"
"Mutually-Constrained Monotonic Multihead Attention for Online ASR","J. Song; H. Shim; E. Yang","KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","6508","6512","Despite the feature of real-time decoding, Monotonic Multi-head Attention (MMA) shows comparable performance to the state-of-the-art offline methods in machine translation and automatic speech recognition (ASR) tasks. However, the latency of MMA is still a major issue in ASR and should be combined with a technique that can reduce the test latency at inference time, such as head-synchronous beam search decoding, which forces all non-activated heads to activate after a small fixed delay from the first head activation. In this paper, we remove the discrepancy between training and test phases by considering, in the training of MMA, the interactions across multiple heads that will occur in the test time. Specifically, we derive the expected alignments from monotonic attention by considering the boundaries of other heads and reflect them in the learning process. We validate our proposed method on the two standard benchmark datasets for ASR and show that our approach, MMA with the mutually-constrained heads from the training stage, provides better performance than baselines.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413862","National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413862","Online Speech Recognition;Transformer;Monotonic Multihead Attention;Head-Synchronous Beam Search Decoding","Training;Signal processing;Magnetic heads;Real-time systems;Decoding;Acoustic beams;Machine translation","decoding;language translation;learning (artificial intelligence);search problems;speech recognition;video signal processing","Monotonic multihead Attention;online ASR;real-time decoding;Monotonic Multihead Attention;MMA;state-of-the-art offline methods;machine translation;automatic speech recognition tasks;inference time;head-synchronous beam search decoding;head activation;multiple heads;monotonic attention","","1","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Image-Assisted Transformer in Zero-Resource Multi-Modal Translation","P. Huang; S. Sun; H. Yang","School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; 2012 Labs, Huawei Technologies CO., LTD, Beijing, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7548","7552","Humans learn language speaking and translation with the help of common knowledge of the external world, while standard machine translation depends only on parallel corpora. There-fore, zero-resource translation has been proposed to explore a way to make models learn to translate without any parallel corpora but with external knowledge. Current models in the field usually combine an image encoder with a textual decoder, which leads to extra efforts to make use of the original model in machine translation. On the other hand, Transformers have achieved great progress in neural language processing while they are rarely utilized in the field of zero-resource translation with image pivot. In this paper, we investigate how to use visual information as an auxiliary hint for a Transformer-based system in a zero-resource translation scenario. Our model achieves state-of-the-art BLEU scores in the field of zero-resource machine translation with image pivot.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413389","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413389","Multi-modal Transformer;Zero-resource;Machine Translation","Training;Visualization;Conferences;Signal processing;Acoustics;Image restoration;Decoding","language translation;learning (artificial intelligence);natural language processing","image-assisted Transformer;zero-resource multimodal translation;external world;standard machine translation;parallel corpora;external knowledge;image encoder;neural language processing;image pivot;Transformer-based system;zero-resource translation scenario;zero-resource machine translation","","5","","19","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Bilingual Continuous-Space Language Model Growing for Statistical Machine Translation","R. Wang; H. Zhao; B. -L. Lu; M. Utiyama; E. Sumita","Department of Computer Science and Engineering and Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering and Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering and Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China; Multilingual Translation Laboratory, National Institute of Information and Communications Technology, Kyoto; Multilingual Translation Laboratory, National Institute of Information and Communications Technology, Kyoto","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","7","1209","1220","Larger n-gram language models (LMs) perform better in statistical machine translation (SMT). However, the existing approaches have two main drawbacks for constructing larger LMs: 1) it is not convenient to obtain larger corpora in the same domain as the bilingual parallel corpora in SMT; 2) most of the previous studies focus on monolingual information from the target corpora only, and redundant n-grams have not been fully utilized in SMT. Nowadays, continuous-space language model (CSLM), especially neural network language model (NNLM), has been shown great improvement in the estimation accuracies of the probabilities for predicting the target words. However, most of these CSLM and NNLM approaches still consider monolingual information only or require additional corpus. In this paper, we propose a novel neural network based bilingual LM growing method. Compared to the existing approaches, the proposed method enables us to use bilingual parallel corpus for LM growing in SMT. The results show that our new method outperforms the existing approaches on both SMT performance and computational efficiency significantly.","2329-9304","","10.1109/TASLP.2015.2425220","National Natural Science Foundation of China(grant numbers:60903119,61170114,61272248); National Basic Research Program of China(grant numbers:2013CB329401); Science and Technology Commission of Shanghai Municipality(grant numbers:13511500200); European Union Seventh Framework Program(grant numbers:247619); Cai Yuanpei Program(grant numbers:201304490199,201304490171); Shanghai Jiao Tong University(grant numbers:14X190040031(14JCRZ04)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090970","Continuous-space language model;language model growing (LMG);neural network language model;statistical machine translation (SMT)","Joining processes;Probability;Decoding;Training;IEEE transactions;Speech;Speech processing","language translation;neural nets","bilingual continuous-space language model;statistical machine translation;SMT;bilingual parallel corpora;CSLM;neural network language model;NNLM;monolingual information;bilingual LM growing method;bilingual parallel corpus","","22","","66","IEEE","21 Apr 2015","","","IEEE","IEEE Journals"
"Phi DM-Dialog: an experimental speech-to-speech dialog translation system","H. Kitano","NEC Corporation Limited, Japan","Computer","6 Aug 2002","1991","24","6","36","50","Phi DM-Dialog, one of the first experimental speech-to-speech systems and the first to demonstrate simultaneous interpretation possibilities, is described. An overview is given of the model behind Phi DM-Dialog. It consists of a memory network for representing various knowledge levels and markers for inferencing. The markers have rich information content. The integration of speech and natural language processing in Phi DM-Dialog and its cost-based scheme of ambiguity resolution are discussed. Its simultaneous interpretation capability, which is made possible by an incremental parsing and generation algorithm, is examined. Prototype system results are reported.<>","1558-0814","","10.1109/2.86837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86837","","Speech recognition;Natural languages;Speech processing;Prototypes;National electric code;Artificial intelligence;Feedback;Parallel machines;Filters;Assembly systems","grammars;inference mechanisms;natural languages;speech analysis and processing;speech recognition","incremental parsing algorithm;speech processing system;dialog translation system;Phi DM-Dialog;memory network;knowledge levels;inferencing;natural language processing;generation algorithm","","17","173","12","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"Coping with out-of-vocabulary words: Open versus huge vocabulary asr","M. Gerosa; M. Federico","FBK-irst Fondazione Bruno Kessler, Trento, Italy; FBK-irst Fondazione Bruno Kessler, Trento, Italy","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4313","4316","This paper investigates methods for coping with out-of-vocabulary words in a large vocabulary speech recognition task, namely the automatic transcription of Italian broadcast news. Two alternative ways for augmenting a 64 K(thousand)-word recognition vocabulary and language model are compared: introducing extra words with their phonetic transcription up to 1.2 M (million) words, or extending the language model with so-called graphones, i.e. subword units made of phone-character sequences. Graphones and phonetic transcriptions of words are automatically generated by adapting an off-the-shelf statistical machine translation toolkit. We found that the word-based and graphone based extentions allow both for better recognition performance, with the former performing significantly better than the latter. In addition, the word-based extension approach shows interesting potential even under conditions of little supervision. In fact, by training the grapheme to phoneme translation system with only 2 K manually verified transcriptions, the final word error rate increases by just 3% relative, with respect to starting from a lexicon of 64 K words.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960583","Automatic Speech Recognition;Open-vocabulary speech recognition;OOV words","Vocabulary;Automatic speech recognition;Speech recognition;Broadcasting;Error analysis;Art;Robustness;Training data;Natural languages;Documentation","language translation;speech processing;speech recognition;statistical analysis","out-of-vocabulary word;vocabulary speech recognition;automatic transcription;Italian broadcast news;language model;phoneme translation system;word error rate;word recognition;phonetic transcription;statistical machine translation toolkit","","4","","13","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Gender aware spoken language translation applied to English-Arabic","M. Elaraby; A. Y. Tawfik; M. Khaled; H. Hassan; A. Osama",Microsoft AI & Research; Microsoft AI & Research; Microsoft AI & Research; Microsoft AI & Research; Microsoft AI & Research,"2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)","7 Jun 2018","2018","","","1","6","Spoken Language Translation (SLT) is becoming more widely used and becoming a communication tool that helps in crossing language barriers. One of the challenges of SLT is the translation from a language without gender agreement to a language with gender agreement such as English to Arabic. In this paper, we introduce an approach to tackle such limitation by enabling a Neural Machine Translation system to produce gender-aware unbiased translation. We show that NMT system can model the speaker/listener gender information to produce gender-aware translation that reduces the bias effect resulting from having training data dominated by particular gender forms. We propose a method to generate data used in adapting a NMT system to produce gender-aware and unbiased translation. The proposed approach can achieve significant improvement of the translation quality by 2 BLEU points.","","978-1-5386-4543-7","10.1109/ICNLSP.2018.8374387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374387","Speaker Gender;Unbiased Translation;Gender Aware Translation;Gender Agreement Neural Machine Translation System","Training;Support vector machines;Noise measurement;Training data;Runtime;Speech recognition;Labeling","language translation;natural language processing;neural nets","SLT;communication tool;language barriers;gender agreement;gender-aware unbiased translation;NMT system;translation quality;gender aware spoken language translation;English-Arabic language translation;neural machine translation system;speaker gender information;listener gender information;BLEU point","","5","","16","IEEE","7 Jun 2018","","","IEEE","IEEE Conferences"
"Interactive translation of conversational speech","A. Waibel",Carnegie Mellon University and University of Karlsruhe,"Computer","6 Aug 2002","1996","29","7","41","48","As communication becomes increasingly automated and transnational, the need for rapid, computer-aided speech translation grows. The Janus-II system uses paraphrasing and interactive error correction to boost performance. Janus-II operates on spontaneous conversational human dialogue in limited domains with vocabularies of 3,000 or more words. Current experiments involve 10,000 to 40,000 word vocabularies. It now accepts English, German, Japanese, Spanish, and Korean input, which it translates into any other of these languages. Beyond translating syntactically well-formed speech or carefully structured human-to-machine speech utterances, Janus-II research has focused on the more difficult task of translating spontaneous conversational speech between humans. This naturally requires a suitable database and task domain.","1558-0814","","10.1109/2.511967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=511967","","Speech recognition;Natural languages;Speech processing;Speech analysis;Humans;Telephony;Error correction;Error analysis;Databases;Europe","language translation;error correction;speech recognition","interactive translation;conversational speech;computer-aided speech translation;Janus-II system;paraphrasing;interactive error correction;spontaneous conversational human dialogue;vocabularies;English input;German input;Japanese input;Spanish input;Korean input;syntactically well-formed speech translation;carefully structured human-to-machine speech utterance translation","","30","133","14","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"On the Evaluation of Adaptive Machine Translation for Human Post-Editing","L. Bentivogli; N. Bertoldi; M. Cettolo; M. Federico; M. Negri; M. Turchi","Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy; Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy; Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy; Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy; Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy; Fondazione Bruno Kessler, HLT-MT research unit of the Center of Information Technology, Trento, Italy","IEEE/ACM Transactions on Audio, Speech, and Language Processing","11 Jan 2016","2016","24","2","388","399","We investigate adaptive machine translation (MT) as a way to reduce human workload and enhance user experience when professional translators operate in real-life conditions. A crucial aspect in our analysis is how to ensure a reliable assessment of MT technologies aimed to support human post-editing. We pay particular attention to two evaluation aspects: i) the design of a sound experimental protocol to reduce the risk of collecting biased measurements, and ii) the use of robust statistical testing methods (linear mixed-effects models) to reduce the risk of under/over-estimating the observed variations. Our adaptive MT technology is integrated in a web-based full-fledged computer-assisted translation (CAT) tool. We report on a post-editing field test that involved 16 professional translators working on two translation directions (English-Italian and English-French), with texts coming from two linguistic domains (legal, information technology). Our contrastive experiments compare user post-editing effort with static vs. adaptive MT in an end-to-end scenario where the system is evaluated as a whole. Our results evidence that adaptive MT leads to an overall reduction in post-editing effort (HTER) up to 10.6% (p <; 0.05). A follow-up manual evaluation of the MT outputs and their corresponding post-edits confirms that the gain in HTER corresponds to higher quality of the adaptive MT system and does not come at the expense of the final human translation quality. Indeed, adaptive MT shows to return better suggestions than static MT (p <; 0.01), and the resulting post-edits do not significantly differ in the two conditions.","2329-9304","","10.1109/TASLP.2015.2509241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358024","Computer-assisted translation;domain adaptation;human in the loop evaluation;machine translation;online adaptation;post-editing","Adaptation models;Data models;Adaptive systems;Terminology;IEEE transactions;Speech;Speech processing","Internet;language translation","adaptive machine translation;human post-editing;MT technologies;Web-based full-fledged computer-assisted translation;CAT tool;human translation quality","","10","","57","IEEE","17 Dec 2015","","","IEEE","IEEE Journals"
"Multilingual speech to speech MT based chat system","A. Gopi; Shobana Devi P; Sajini T; J. Stephen; Bhadhran VK","Center for Development of Advanced computing, Trivandrum; Center for Development of Advanced computing, Trivandrum; Center for Development of Advanced computing, Trivandrum; Center for Development of Advanced computing, Trivandrum; Center for Development of Advanced computing, Trivandrum","2015 International Conference on Computing and Network Communications (CoCoNet)","18 Feb 2016","2015","","","771","776","This paper presents a chat application that make use of speech as interaction mode between users in their mother tongue. This system is based on Machine Translation (MT) which automatically translates a sentence from one language to other. Now-a-days, Chat has become increasingly important as an effective means to communicate in social networks. But in existing personal chat systems, chat services are text-based only. To circumvent this issue, innovative, practical XMPP-based chat system architecture is proposed in this paper that creates a Speech based chat services by integrating state-of-the-art technologies like Automatic Speech Recognition (ASR), Text to Speech (TTS) and Machine Translation (MT). The proposed system eliminates the need to have a common language for chat communication. In this chat system the user speak in their own language after selecting the language of their comfort. The audio will be recognized in real time. The text is translated to the language opted by end user and the system synthesizes it. Here, we discuss an interface prototype that can overcome the language barrier to some extent.","","978-1-4673-7309-8","10.1109/CoCoNet.2015.7411277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411277","chat;speech to speech transaltion;XMPP;ASR;MT;TTS;CMU Sphinx;Festival","Speech;Servers;Speech recognition;Google;Engines;Data models;Vocabulary","language translation;natural language processing;protocols;social networking (online);user interfaces","multilingual speech to speech MT based chat system;machine translation;social networks;personal chat systems;XMPP-based chat system architecture;speech based chat services;chat communication;audio recognition;interface prototype;language barrier;communications protocol","","1","","28","IEEE","18 Feb 2016","","","IEEE","IEEE Conferences"
"Recognition and translation of code-switching speech utterances","S. Nakayama; T. Kano; A. Tjandra; S. Sakti; S. Nakamura","Nara Institute of Science & Technology, Japan; Nara Institute of Science & Technology, Japan; Nara Institute of Science & Technology, Japan; Advanced Intelligence Project AIP, Nara Institute of Science & Technology, RIKEN, Japan; Advanced Intelligence Project AIP, Nara Institute of Science & Technology, RIKEN, Japan","2019 22nd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","23 Apr 2020","2019","","","1","6","Code-switching (CS), a hallmark of worldwide bilingual communities, refers to a strategy adopted by bilinguals (or multilinguals) who mix two or more languages in a discourse often with little change of interlocutor or topic. The units and the locations of the switches may vary widely from single-word switches to whole phrases (beyond the length of the loanword units). Such phenomena pose challenges for spoken language technologies, i.e., automatic speech recognition (ASR), since the systems need to be able to handle the input in a multilingual setting. Several works constructed a CS ASR on many different language pairs. But the common aim of developing a CS ASR is merely for transcribing CS-speech utterances into CS-text sentences within a single individual. In contrast, in this study, we address the situational context that happens during dialogs between CS and non-CS (monolingual) speakers and support monolingual speakers who want to understand CS speakers. We construct a system that recognizes and translates from codeswitching speech to monolingual text. We investigated several approaches, including a cascade of ASR and a neural machine translation (NMT), a cascade of ASR and a deep bidirectional language model (BERT), an ASR that directly outputs monolingual transcriptions from CS speech, and multi-task learning. Finally, we evaluate and discuss these four ways on a Japanese- English CS to English monolingual task.","2472-7695","978-1-7281-2449-0","10.1109/O-COCOSDA46868.2019.9060847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060847","code-switching;speech recognition;speech andtext translation;BERT;multi-task learning","","language translation;learning (artificial intelligence);natural language processing;speaker recognition;speech recognition;text analysis","code-switching speech utterances;single-word switches;multi-task learning;monolingual speakers;bilingual communities;English CS;deep bidirectional language model;neural machine translation;monolingual text;CS speakers;CS-text sentences;CS-speech utterances;CS ASR;automatic speech recognition;spoken language technologies;loanword units","","1","","","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"Evaluation for POS tagger, chunk and resolving issues in word sense disambiguate in machine translation for Hindi to English languages","S. Mall; U. C. Jaiswal","Computer Science and Engineering: dept., Madan Mohan Malaviya University of Technology, Gorakhpur, INDIA; Computer Science and Engineering: dept., Madan Mohan Malaviya University of Technology, Gorakhpur, INDIA","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","31 Oct 2016","2016","","","14","18","Our paper develops innovative algorithms for machine translation system based on the innovative algorithms for parts of speech tagger, chunking, word sense disambiguate and word translation in English. Parts of speech tagging and chunking for 1657 tokens with 990 phrases for Hindi languages and to calculate the accuracy we created confusion matrix an evaluate Precision, Recall, F-score, Accuracy for chunk accuracy: 81.23%; precision: 66.57%; recall: 73.03%; F-score: 69.65, 90.31, POS accuracy: 94.75%; precision: 90.67%; recall: 93.23%; F-score 91.93. Rule based and learning algorithm (Conditional Random Fields) is used to develop the system.","","978-9-3805-4421-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724218","Parts of speech tagging;chunking;word sense disambiguate;English word translation Rule based","Decision support systems;Handheld computers;Hafnium","knowledge based systems;language translation;learning (artificial intelligence);natural language processing","POS tagger;word sense disambiguate;machine translation;Hindi languages;English languages;parts of speech tagger;rule based algorithm;learning algorithm","","","","15","","31 Oct 2016","","","IEEE","IEEE Conferences"
"Integrating translation technologies towards a powerful translation Web service","V. Antonopoulos; I. Demiros; G. Carayannis; S. Piperidis","Institute for Language and Speech Processing Athens, Athens, Greece; Institute for Language and Speech Processing Athens, Athens, Greece; Institute for Language and Speech Processing Athens, Greece; Institute for Language and Speech Processing Athens, Athens, Greece","IEEE Conference on Cybernetics and Intelligent Systems, 2004.","5 Jul 2005","2004","1","","526","531 vol.1","Rapid changes in the global marketplace have given rise to new demands and have provided new opportunities for the translation industry. The need for multilinguality in the presentation and business logic layers of most modern systems, applications and services is a great challenge that the translation industry now faces. But even after many years of intense research and many commercial attempts of related products, translation systems of today still fail to completely meet the above needs. Within this framework, an architecture of a modern automatic translation system exploiting current infrastructure and covering today and future needs is proposed in this paper.","","0-7803-8643-4","10.1109/ICCIS.2004.1460470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460470","","Web services;Search engines;Internet;Appropriate technology;Natural languages;Speech processing;Business;Logic;XML;Simple object access protocol","Internet;XML;access protocols;language translation;linguistics;knowledge based systems;globalisation","global marketplace;multilinguality;business logic layers;automatic translation system;Web service;machine translation;translation memories;XML;SOAP;bilingual concordancing","","","","8","IEEE","5 Jul 2005","","","IEEE","IEEE Conferences"
"Multi-Source Neural Machine Translation With Missing Data","Y. Nishimura; K. Sudoh; G. Neubig; S. Nakamura","Nara Institute of Science and Technology, Ikoma, Japan; Nara Institute of Science and Technology, Ikoma, Japan; Carnegie Mellon University, Pittsburgh, USA; Nara Institute of Science and Technology, Ikoma, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","22 Jan 2020","2020","28","","569","580","Machine translation is rife with ambiguities in word ordering and word choice, and even with the advent of machine-learning methods that learn to resolve this ambiguity based on statistics from large corpora, mistakes are frequent. Multi-source translation is an approach that attempts to resolve these ambiguities by exploiting multiple inputs (e.g. sentences in three different languages) to increase translation accuracy. These methods are trained on multilingual corpora, which include the multiple source languages and the target language, and then at test time uses information from both source languages while generating the target. While there are many of these multilingual corpora, such as multilingual translations of TED talks or European parliament proceedings, in practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages. Existing studies on multi-source translation did not explicitly handle such situations, and thus are only applicable to complete corpora that have all of the languages of interest, severely limiting their practical applicability. In this article, we examine approaches for multi-source neural machine translation (NMT) that can learn from and translate such incomplete corpora. Specifically, we propose methods to deal with incomplete corpora at both training time and test time. For training time, we examine two methods: (1) a simple method that simply replaces missing source translations with a special NULL symbol, and (2) a data augmentation approach that fills in incomplete parts with source translations created from multi-source NMT. For test-time, we examine methods that use multi-source translation even when only a single source is provided by first translating into an additional auxiliary language using standard NMT, then using multi-source translation on the original source and this generated auxiliary language sentence. Extensive experiments demonstrate that the proposed training-time and test-time methods both significantly improve translation performance.","2329-9304","","10.1109/TASLP.2019.2959224","JSPS KAKENHI(grant numbers:JP16H05873,JP17H06101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930957","Neural machine translation (NMT);multi-linguality;data augmentation","Training;Decoding;Standards;Speech processing;Europe;Probability;Training data","language translation;learning (artificial intelligence);natural language processing;neural nets","multilingual translations;multilingual corpora;multisource neural machine translation;incomplete corpora;training time;source translations;multisource NMT;test-time;translation performance;machine-learning methods;translation accuracy;multiple source languages;missing data;word ordering;word choice;missing source translations;NULL symbol","","9","","27","IEEE","11 Dec 2019","","","IEEE","IEEE Journals"
"Usted: Improving ASR with a Unified Speech and Text Encoder-Decoder","B. Yusuf; A. Gandhe; A. Sokolov","Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","8297","8301","Improving end-to-end speech recognition by incorporating external text data has been a longstanding research topic. There has been a recent focus on training E2E ASR models that get the performance benefits of external text data without incurring the extra cost of evaluating an external language model at inference time. In this work, we propose training ASR model jointly with a set of text-to-text auxiliary tasks with which it shares a decoder and parts of the encoder. When we jointly train ASR and masked language model with the 960-hour Librispeech and Opensubtitles data respectively, we observe WER reductions of 16% and 20% on test-other and test-clean respectively over an ASR-only baseline without any extra cost at inference time, and reductions of 6% and 8% compared to a stronger MUTE-L baseline which trains the decoder with the same text data as our model. We achieve further improvements when we train masked language model on Librispeech data or when we use machine translation as the auxiliary task, without significantly sacrificing performance on the task itself.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746554","sequence-to-sequence;multitask;end-to-end ASR;masked language model;machine translation","Training;Costs;Computational modeling;Speech recognition;Signal processing;Data models;Decoding","decoding;language translation;natural language processing;speech recognition;statistical analysis","improving ASR;improving end-to-end speech recognition;external text data;longstanding research topic;extra cost;external language model;inference time;ASR model;text-to-text auxiliary tasks;decoder;Opensubtitles data;ASR-only baseline;Librispeech data","","4","","24","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Semantic translation error rate for evaluating translation systems","K. Subramanian; D. Stallard; R. Prasad; S. Saleem; P. Natarajan","BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA; BBN Technologies, Inc., Cambridge, MA, USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","390","395","In this paper, we introduce a new metric which we call the semantic translation error rate, or STER, for evaluating the performance of machine translation systems. STER is based on the previously published translation error rate (TER) (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) metrics. Specifically, STER extends TER in two ways: first, by incorporating word equivalence measures (WordNet and Porter stemming) standardly used by METEOR, and second, by disallowing alignments of concept words to non-concept words (aka stop words). We show how these features make STER alignments better suited for human-driven analysis than standard TER. We also present experimental results that show that STER is better correlated to human judgments than TER. Finally, we compare STER to METEOR, and illustrate that METEOR scores computed using the STER alignments have similar statistical properties to METEOR scores computed using METEOR alignments.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430144","Automated Metric;Statistical Machine Translation","Error analysis;Humans;Costs;Natural languages;Surface-mount technology;Information technology;Web sites;Globalization;Investments;Information analysis","error statistics;language translation;performance evaluation;software metrics","semantic translation error rate;translation systems evaluation;machine translation systems;word equivalence measures;WordNet;Porter stemming;STER alignments;human-driven analysis;METEOR","","3","","10","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Combining multiple translation systems for Spoken Language Understanding portability","F. Garc√≠a; L. F. Hurtado; E. Segarra; E. Sanchis; G. Riccardi","Department Sistemes Inform√†tics i Computaci√≥, Universitat Polit√®cnica de Val√®ncia, Spain; Department Sistemes Inform√†tics i Computaci√≥, Universitat Polit√®cnica de Val√®ncia, Spain; Department Sistemes Inform√†tics i Computaci√≥, Universitat Polit√®cnica de Val√®ncia, Spain; Department Sistemes Inform√†tics i Computaci√≥, Universitat Polit√®cnica de Val√®ncia, Spain; Department of Information Engineering and Computer science, University of Trento, Italy","2012 IEEE Spoken Language Technology Workshop (SLT)","31 Jan 2013","2012","","","194","198","We are interested in the problem of learning Spoken Language Understanding (SLU) models for multiple target languages. Learning such models requires annotated corpora, and porting to different languages would require corpora with parallel text translation and semantic annotations. In this paper we investigate how to learn a SLU model in a target language starting from no target text and no semantic annotation. Our proposed algorithm is based on the idea of exploiting the diversity (with regard to performance and coverage) of multiple translation systems to transfer statistically stable word-to-concept mappings in the case of the romance language pair, French and Spanish. Each translation system performs differently at the lexical level (wrt BLEU). The best translation system performances for the semantic task are gained from their combination at different stages of the portability methodology. We have evaluated the portability algorithms on the French MEDIA corpus, using French as the source language and Spanish as the target language. The experiments show the effectiveness of the proposed methods with respect to the source language SLU baseline.","","978-1-4673-5126-3","10.1109/SLT.2012.6424221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424221","Spoken Language Understanding;Statistical Models;Language Portability","Semantics;Media;Speech;Training;Speech recognition;Stochastic processes;Automata","language translation;learning (artificial intelligence);natural language processing","spoken language understanding portability;multiple translation systems;SLU model learning problem;annotated corpora;parallel text translation;semantic annotations;statistically stable word-to-concept mappings;romance language pair;French language;Spanish language;translation system performances;portability algorithms;French MEDIA corpus;target languages","","6","","12","IEEE","31 Jan 2013","","","IEEE","IEEE Conferences"
"Clustering: a technique for search space reduction in example-based machine translation","L. Cranias; H. Papageorgiou; S. Piperidis","Institute of Language and Speech Processing, Greece; Institute of Language and Speech Processing, Greece; Institute of Language and Speech Processing, Greece","Proceedings of IEEE International Conference on Systems, Man and Cybernetics","6 Aug 2002","1994","1","","1","6 vol.1","This paper addresses an important problem in example-based machine translation (EBMT), namely how to make retrieval of the example that best matches the input more efficient. The use of clustering is proposed, to enable the application of the same similarity metric to first limit the search space and then locate the best available match in a database. Evaluation results are presented on a large number of test cases.<>","","0-7803-2129-4","10.1109/ICSMC.1994.399803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=399803","","Databases;Information retrieval;Impedance matching;Natural languages;Speech processing;Testing;Large-scale systems;Natural language processing;Knowledge acquisition;Probability distribution","natural languages;language translation;search problems;database management systems","search space reduction;example-based machine translation;translation example retrieval;clustering;similarity metric;textual database;natural language","","1","12","16","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Phone-to-word decoding through statistical machine translation and complementary system combination","D. Falavigna; M. Gerosa; R. Gretter; D. Giuliani","Human Language Technology Research Unit, FBK Fondazione Bruno Kessler, Trento, Italy; Human Language Technology Research Unit, FBK Fondazione Bruno Kessler, Trento, Italy; Human Language Technology Research Unit, FBK Fondazione Bruno Kessler, Trento, Italy; Human Language Technology Research Unit, FBK Fondazione Bruno Kessler, Trento, Italy","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","519","524","In this paper, phone-to-word transduction is first investigated by coupling a speech recognizer, generating for each speech segment a phone sequence or a phone confusion network, with the efficient decoder of confusion networks adopted by MOSES, a popular statistical machine translation toolkit. Then, system combination is investigated by combining the outputs of several conventional ASR systems with the output of a system embedding phone-to-word decoding through statistical machine translation. Experiments are carried out in the context of a large vocabulary speech recognition task consisting of transcription of speeches delivered in English during the European Parliament Plenary Sessions (EPPS). While only a marginal performance improvements is achieved in system combination experiments when the output of the phone-to-word transducer is included in the combination, partial results show a great potential for improvements.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373281","ASR system combination;phone-to-word transducer;word graph rescoring","Decoding;Automatic speech recognition;Transducers;Adaptation model;Audio recording;Speech recognition;Vocabulary;Decision trees;Mel frequency cepstral coefficient;Humans","language translation;speech coding;speech recognition equipment","phone-to-word decoding;statistical machine translation;complementary system combination;speech recognizer;phone confusion network;confusion networks decoder;MOSES;ASR systems;large vocabulary speech recognition task;European parliament plenary sessions","","1","","21","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Answering English Queries in Automatically Transcribed Arabic Speech","A. F. A. Nwesri; S. M. M. Tahaghoghi; F. Scholer","School of Computer Science and Information Technology, RMIT University, Melbourne, Australia; School of Computer Science and Information Technology, RMIT University, Melbourne, Australia; School of Computer Science and Information Technology, RMIT University, Melbourne, Australia","6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)","23 Jul 2007","2007","","","11","16","There are several well-known approaches to parsing Arabic text in preparation for indexing and retrieval. Techniques such as stemming and stopping have been shown to improve search results on written newswire dispatches, but few comparisons are available on other data sources. In this paper, we apply several alternative stemming and stopping approaches to Arabic text automatically extracted from the audio soundtrack of news video footage, and compare these with approaches that rely on machine translation of the underlying text. Using the TRECVID video collection and queries, we show that normalisation, stopword- removal, and light stemming increase retrieval precision, but that heavy stemming and trigrams have a negative effect. We also show that the choice of machine translation engine plays a major role in retrieval effectiveness.","","0-7695-2841-4","10.1109/ICIS.2007.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276350","Arabic information retrieval;Cross-language;information retrieval;Machine translation.","Information retrieval;Data mining;Automatic speech recognition;Indexing;Acoustic noise;Shape;Computer science;Information technology;Australia;Engines","grammars;indexing;language translation;natural languages;speech recognition;text analysis;video retrieval","English query answering;automatically transcribed Arabic speech;parsing;indexing;audio soundtrack;news video footage;machine translation engine;Arabic information retrieval;Arabic text","","","","18","IEEE","23 Jul 2007","","","IEEE","IEEE Conferences"
"The IBM 2009 GALE Arabic speech transcription system","B. Kingsbury; H. Soltau; G. Saon; S. Chu; H. -K. Kuo; L. Mangu; S. Ravuri; N. Morgan; A. Janin","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; International Computer Science Institute, Berkeley, CA, USA; International Computer Science Institute, Berkeley, CA, USA; International Computer Science Institute, Berkeley, CA, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","4672","4675","We describe the Arabic broadcast transcription system fielded by IBM in the GALE Phase 4 machine translation evaluation. Key advances over our Phase 3.5 system include improvements to context-dependent modeling in vowelized Arabic acoustic models; the use of neural-network features provided by the International Computer Science Institute; Model M language models; a neural network language model that uses syntactic and morphological features; and improvements to our system combination strategy. These advances were instrumental in achieving a word error rate of 8.9% on the Phase 4 evaluation set, and an absolute improvement of 1.6% word error rate over our 2008 system on the unsequestered Phase 3.5 evaluation data.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947397","large vocabulary speech recognition","Computational modeling;Hidden Markov models;Artificial neural networks;Acoustics;Context modeling;Syntactics;Error analysis","IBM compatible machines;language translation;neural nets;speech recognition","IBM 2009 GALE Arabic speech transcription system;GALE phase 4 machine translation evaluation;context-dependent modeling;vowelized Arabic acoustic model;International Computer Science Institute;model M language model;neural network language model;syntactic feature;morphological feature;word error rate","","7","2","17","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Confidence estimation for spoken language translation based on Round Trip Translation","D. Yu; W. Wei; L. Jia; B. Xu","Digital Media Content Technology Research Center, Institute of Automation, Chinese Academy and Sciences, Beijing, China; Digital Media Content Technology Research Center, Institute of Automation, Chinese Academy and Sciences, Beijing, China; Digital Media Content Technology Research Center, Institute of Automation, Chinese Academy and Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","2010 7th International Symposium on Chinese Spoken Language Processing","10 Jan 2011","2010","","","426","429","In this paper we propose a Round Trip Translation (RTT) based approach to sentence-level confidence estimation (CE) for spoken language translation without the assistant of reference translations generated by human. A number of novel RTT based features are introduced to reflect the quality of spoken language translation in more detail. After combing various kinds of features together, support vector regression (SVR) method is employed to learn human's assessment patterns of translation quality. Experimental results show that RTT based features could improve the accuracy of CE significantly and SVR method could model human's assessment pattern accurately and robustly. In the final CE task of spoken language translation from Chinese to English, our system achieves comparable performance with that of BLEU, which needs the assistance of human's reference, even with small training data.","","978-1-4244-6246-9","10.1109/ISCSLP.2010.5684855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5684855","confidence estimation;spoken language translation;Round Trip Translation;SVR","Feature extraction;Humans;Training data;Probability;Strontium;Training;Subspace constraints","language translation;natural language processing;regression analysis;speech processing;support vector machines","spoken language translation;round trip translation;sentence level confidence estimation;support vector regression method;human assessment patterns;translation quality;Chinese;English;BLEU;training data","","","1","10","IEEE","10 Jan 2011","","","IEEE","IEEE Conferences"
"Targeted Adversarial Attacks Against Neural Machine Translation","S. Sadrizadeh; A. D. Aghdam; L. Dolamic; P. Frossard","√âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne, Switzerland; University of Tehran, Tehran, Iran; Armasuisse S+T, Thun, Switzerland; √âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne, Switzerland","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Neural Machine Translation (NMT) systems are used in various applications. However, it has been shown that they are vulnerable to very small perturbations of their inputs, known as adversarial attacks. In this paper, we propose a new targeted adversarial attack against NMT models. In particular, our goal is to insert a predefined target keyword into the translation of the adversarial sentence while maintaining similarity between the original sentence and the perturbed one in the source domain. To this aim, we propose an optimization problem, including an adversarial loss term and a similarity term. We use gradient projection in the embedding space to craft an adversarial sentence. Experimental results show that our attack outperforms Seq2Sick, the other targeted adversarial attack against NMT models, in terms of success rate and decrease in translation quality. Our attack succeeds in inserting a keyword into the translation for more than 75% of sentences while similarity with the original sentence stays preserved1.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095342","Adversarial attack;deep neural network;natural language processing;neural machine translation;targeted attack","Perturbation methods;Force;Signal processing;Acoustics;Machine translation;Iterative methods;Task analysis","","","","","","22","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"An empirical machine translation framework for translating bangla imperative, optative and exclamatory sentences into English","T. Alamgir; M. S. Arefin; M. M. Hoque","Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology, Chittagong, Bangladesh; Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology, Chittagong, Bangladesh; Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology, Chittagong, Bangladesh","2016 5th International Conference on Informatics, Electronics and Vision (ICIEV)","1 Dec 2016","2016","","","932","937","A set of Context Sensitive Grammar (CSG) rules to translate Bangla imperative, optative and exclamatory sentences into English are introduced in this paper. In this paper, sentences are considered according to the function and purpose of the user rather than structure of the sentence. Three algorithms are implemented to complete major three steps of machine translation system (i.e., parsing, transfer and generation). The experimental results shows that the performance of the proposed machine translation framework is quite appeasement and efficiency is compared with Google Translator for some selected sentences which are quite satisfactory.","","978-1-5090-1269-5","10.1109/ICIEV.2016.7760137","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760137","Machine Translation;context sensitive grammar;parsing;transfer;generation","Grammar;Context;Computers;Speech;Generators;Natural languages;Information and communication technology","language translation;natural language processing","Bangla;imperative sentences;optative sentences;exclamatory sentences;English;context sensitive grammar rules;CSG rules;machine translation system;Google translator","","4","","16","IEEE","1 Dec 2016","","","IEEE","IEEE Conferences"
"Lexical Gap in English - Vietnamese Machine Translation: What to Do?","L. M. Hai; P. T. Tuoi","Department of Computer Science, HUTECH, Vietnam; Department of Computer Science, Ho Chi Minh City University of Technology, Vietnam","2010 International Conference on Asian Language Processing","6 Jan 2011","2010","","","265","269","In English - Vietnamese machine translation (EVMT) project at Ho Chi Minh City University of Technology there are some problems that cause the system to malfunction. One of the most undesired phenomena is lexical gap. A lexical gap occurs in case of lacking Vietnamese equivalent word to English word. There are some approaches to this obstacle. Some researchers prefer replacing lexical gap by its nearest meaning word. However, in out project there isn't a conception database for Vietnamese word that helps to find acceptable word to lexical gap. Our approach for lexical gap establishes a word - to - phrase model for EVMT. Naive replacements of word by phrase keep semantic information as much as possible, however, lead translated sentences to syntactic mismatch, and therefore ungrammatical text. We restructure target sentence using phrase in dictionary as template of formal phrase structure in the model with some algorithms to keep target sentence be grammatical. This approach improves rule - based English Vietnamese machine translation.","","978-1-4244-9063-9","10.1109/IALP.2010.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681570","English - Vietnamese machine translation;lexical gap","Dictionaries;Syntactics;Grammar;Databases;Cities and towns;Speech;Semantics","dictionaries;language translation;natural language processing;text analysis;word processing","lexical gap;English-Vietnamese machine translation;Ho Chi Minh City University of Technology;malfunction system;Vietnamese equivalent word;English word;nearest meaning word;conception database;acceptable word;word to phrase model;EVMT;naive replacement;semantic information;translated sentence;syntactic mismatch;ungrammatical text;target sentence restructure;dictionary phrase;formal phrase structure","","2","","15","IEEE","6 Jan 2011","","","IEEE","IEEE Conferences"
"Transonics: a speech to speech system for English-Persian interactions","S. Narayanan; S. Ananthakrishnan; R. Belvin; E. Ettaile; S. Ganjavi; P. G. Georgiou; C. M. Hein; S. Kadambe; K. Knight; D. Marcu; H. E. Neely; N. Srinivasamurthy; D. Traum; D. Wang","University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; HRL Laboratories, Malibu, California, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; HRL Laboratories, Malibu, California, USA; HRL Laboratories, Malibu, California; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; HRL Laboratories, Malibu, California, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","670","675","In this paper, we describe the first phase of development of our speech-to-speech system between English and Modem Persian under the DARPA Babylon program. We give an overview of the various system components: the front end ASR, the machine translation system and the speech generation system. Challenges such as the sparseness of available spoken language data and solutions that have been employed to maximize the obtained benefits from using these limited resources are examined. Efforts in the creation of the user interface and the underlying dialog management system for mediated communication are described.","","0-7803-7980-2","10.1109/ASRU.2003.1318520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318520","","Natural languages;Automatic speech recognition;Graphical user interfaces;Modems;User interfaces;Broadcasting;Switches;Laboratories;Loudspeakers;Writing","language translation;speech recognition equipment;speech synthesis;speech-based user interfaces","English-Persian translation system;transonics;speech to speech system;English-Persian interactions;front end ASR;machine translation system;speech generation system;spoken language data sparseness;user interface;dialog management system;mediated communication","","7","7","7","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Feedforward sequential memory networks based encoder-decoder model for machine translation","J. Hou; S. Zhang; L. Dai; H. Jiang","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, Anhui, P. R. China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, Anhui, P. R. China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, Anhui, P. R. China; Department of Electrical Engineering and Computer Science, York University, Toronto, Ontario, Canada","2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","8 Feb 2018","2017","","","622","625","Recently recurrent neural networks based encoder-decoder model is a popular approach to sequence to sequence mapping problems, such as machine translation. However, it is time-consuming to train the model since symbols in a sequence can not be processed parallelly by recurrent neural networks because of the temporal dependency restriction. In this paper we present a sequence to sequence model by replacing the recurrent neural networks with feedforward sequential memory networks in both encoder and decoder, which enables the new architecture to encode the entire source sentence simultaneously. We also modify the attention module to make the decoder generate outputs simultaneously during training. We achieve comparable results in WMT'14 English-to-French translation task with 1.4 to 2 times faster during training because of temporal independency in feedforward sequential memory networks based encoder and decoder.","","978-1-5386-1542-3","10.1109/APSIPA.2017.8282100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282100","","Decoding;Mathematical model;Training;Feedforward neural networks;Computational modeling;Recurrent neural networks;Task analysis","decoding;encoding;language translation;learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition","feedforward sequential memory networks;encoder-decoder model;machine translation;recurrent neural networks;sequence mapping problems;sequence model","","1","","19","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"English to Japanese spoken lecture translation system by using DNN-HMM and phrase-based SMT","N. Goto; K. Yamamoto; S. Nakagawa","Toyohashi University of Technology, Toyohashi, Aichi, Japan; Toyohashi University of Technology, Toyohashi, Aichi, Japan; Toyohashi University of Technology, Toyohashi, Aichi, Japan","2015 2nd International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","23 Nov 2015","2015","","","1","6","This paper presents our scheme to translate spoken English lectures into Japanese that consists of an English automatic speech recognition system (ASR) that utilizes a deep neural network (DNN) and an English to Japanese phrase-based statistical machine translation system (SMT). We utilized an existing Wall Street Journal corpus for our acoustic model and adapted it with MIT OpenCourseWare lectures whose transcriptions we also utilized to create our language model. For the parallel corpus of our SMT system, we used TED Talks and Japanese News Article Alignment Data. Our ASR system achieved a word error rate (WER) of 21.0%, and our SMT system achieved a 3-gram base bilingual evaluation understudy (BLEU) of 16.8 for text input and 14.6 for speech input, respectively. These scores outperformed our previous system : WER = 32.1% and BLEU = 11.0.","","978-1-4673-8143-7","10.1109/ICAICTA.2015.7335357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335357","automatic speech recognition;statistical machine translation;classroom lecture;DNN-HMM;phrase-based translation","Hidden Markov models;Data models;Adaptation models;Acoustics;Speech;Speech recognition;Computational modeling","language translation;neural nets;speech recognition","English to Japanese spoken lecture translation;DNN-HMM;phrase-based SMT;English automatic speech recognition system;ASR;deep neural network;English to Japanese phrase-based statistical machine translation system;SMT;OpenCourseWare;word error rate;3-gram base bilingual evaluation understudy","","2","","21","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"Scalable Multilingual Frontend for TTS","A. Conkie; A. Finch",Apple; Apple,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","6684","6688","This paper describes progress towards making a Neural Text-to-Speech (TTS) Frontend that works for many languages and can be easily extended to new languages. We take a Machine Translation (MT) inspired approach to constructing the frontend, and model both text normalization and pronunciation on a sentence level by building and using sequence-to-sequence (S2S) models. We experimented with training normalization and pronunciation as separate S2S models and with training a single S2S model combining both functions. For our language-independent approach to pronunciation we do not use a lexicon. Instead all pronunciations, including context-based pronunciations, are captured in the S2S model. We also present a language-independent chunking and splicing technique that allows us to process arbitrary-length sentences. Models for 18 languages were trained and evaluated. Many of the accuracy measurements are above 99%. We also evaluated the models in the context of end-to-end synthesis against our current production system.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054560","speech synthesis;machine learning","Training;Production systems;Splicing;Data models;Iron;Speech processing;Context modeling","language translation;natural language processing;neural nets;speech recognition;speech synthesis;text analysis","TTS;machine translation;model both text normalization;pronunciation;training normalization;single S2S model;context-based pronunciations;language-independent chunking;splicing technique;arbitrary-length sentences;text-to-speech frontend","","2","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"IBM Mastor: Multilingual Automatic Speech-To-Speech Translator","Yuqing Gao; Bowen Zhou; Liang Gu; R. Sarikaya; Hong-kwang Kuo; A. . -V. I. Rosti; M. Afify; Weizhong Zhu","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","In this paper, we describe the IBM MASTOR systems which handle spontaneous free-form speech-to-speech translation on both laptop and hand-held PDAs. Challenges include speech recognition and machine translation in adverse environments, lack of data and linguistic resources for under-studied languages, and the need to rapidly develop capabilities for new languages. Importantly, the code and models must fit within the limited memory and computational resources of hand-held devices. We describe our approaches, experience, and success in building working free-form S2S systems that can handle two language pairs (including a low-resource language).","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661498","","Natural languages;Speech recognition;Dictionaries;Automatic speech recognition;Handheld computers;Portable computers;Personal digital assistants;Loudspeakers;Decoding","language translation;linguistics;natural languages","IBM MASTOR;multilingual automatic speech-to-speech translator;speech recognition;machine translation;linguistic resources;hand-held devices","","5","1","11","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Groupwise learning for ASR k-best list reranking in spoken language translation","R. W. M. Ng; K. Shah; L. Specia; T. Hain","The University of Sheffield, Sheffield, Sheffield, GB; Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","6120","6124","Quality estimation models are used to predict the quality of the output from a spoken language translation (SLT) system. When these scores are used to rerank a k-best list, the rank of the scores is more important than their absolute values. This paper proposes groupwise learning to model this rank. Groupwise features were constructed by grouping pairs, triplets or M-plets among the ASR k-best outputs of the same sentence. Regression and classification models were learnt and a score combination strategy was used to predict the rank among the k-best list. Regression models with pairwise features give a bigger gain over other model and feature constructions. Groupwise learning is robust to sentences with different ASR-confidence. This technique is also complementary to linear discriminant analysis feature projection. An overall BLEU score improvement of 0.80 was achieved on an in-domain English-to-French SLT task.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472853","groupwise learning;spoken language translation","Hidden Markov models;Feature extraction;Data models;Predictive models;Training;Adaptation models;Support vector machines","language translation;learning (artificial intelligence);regression analysis;speech recognition","groupwise learning;ASR k-best list reranking;quality estimation models;spoken language translation system;classification models;regression models;linear discriminant analysis feature projection;in-domain English-to-French SLT task;automatic speech recognition","","3","","32","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Multi-class Model M","A. Emami; S. F. Chen","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5516","5519","Model M, a novel class-based exponential language model, has been shown to significantly outperform word n-gram models in state-of-the-art machine translation and speech recognition systems. The model was motivated by the observation that shrinking the sum of the parameter magnitudes in an exponential language model leads to better performance on unseen data. Being a class-based language model, Model M makes use of word classes that are found automatically from training data. In this paper, we extend Model M to allow for different clusterings to be used at different word positions. This is motivated by the fact that words play different roles depending on their position in an n-gram. Experiments on standard NIST and GALE Arabic-to-English development and test sets show improvements in machine translation quality as measured by automatic evaluation metrics.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947608","Language Modeling;Machine Translation;Maximum-Entropy Models","Clustering algorithms;Prediction algorithms;Data models;Adaptation models;Decoding;Training data;Speech recognition","language translation;speech recognition","multiclass model M;class-based exponential language model;machine translation;speech recognition system;n-gram model;GALE Arabic-to-English development;NIST","","2","","11","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Speech Recognition in Human Mediated Translation Scenarios","M. Paulik; S. Stuker; C. Fugen","Interactive Systems Laboratories, Universit√§t Karlsruhe, Germany; Interactive Systems Laboratories, Universit√§t Karlsruhe, Germany; Interactive Systems Laboratories, Universit√§t Karlsruhe, Germany","MELECON 2006 - 2006 IEEE Mediterranean Electrotechnical Conference","24 Jul 2006","2006","","","1232","1235","Human-mediated translation refers to situations in which a human interpreter translates between a source and a target language using either a written or a spoken representation of the source language. In this work we improve the recognition performance on the (English) speech of the human translator and, in case of a spoken source language representation, at the same time on the (Spanish) speech of the source language speaker. To do so, machine translation techniques are used within an iterative system design to translate between the source and target language resources. The used ASR and MT systems are then recursively biased towards the gained knowledge. In the case of a written source language representation we are able to reduce the word error rate of our English baseline system by 35.8% relative. In case of a spoken source language representation we are able to reduce the word error rate by 29.9% relative for English and 20.9% relative for Spanish","2158-8481","1-4244-0087-2","10.1109/MELCON.2006.1653325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1653325","","Speech recognition;Humans;Natural languages;Automatic speech recognition;Error analysis;Vocabulary;Interactive systems;Laboratories;Decoding","iterative methods;language translation;speaker recognition","speech recognition;human mediated translation scenarios;human interpreter;English;human translator;spoken source language representation;Spanish;machine translation techniques;iterative system design;target language resources","","2","","7","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Learning online alignments with continuous rewards policy gradient","Y. Luo; C. -C. Chiu; N. Jaitly; I. Sutskever",Tsinghua University; Google Brain; Google Brain; Open AI,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","2801","2805","Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7952667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952667","Automatic Speech Recognition;End-to-End Speech Recognition;Very Deep Convolutional Neural Networks","Mathematical model;Speech recognition;Predictive models;Entropy;Stochastic processes;Training;Computational modeling","decision theory;gradient methods;language translation;speech recognition;stochastic processes","continuous rewards policy gradient method;machine translation;instantaneous translation;hard online alignments;online sequence-to-sequence model;voice-based instantaneous translator;hard binary stochastic decisions;TIMIT speech recognition datasets;Wall Street Journal speech recognition datasets","","11","1","25","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Machine Translation Approach for Vietnamese Diacritic Restoration","T. N. Diep Do; D. B. Nguyen; D. K. Mac; D. Dat Tran","MICA Institute, Hanoi University of Science and Technology - CNRS/UMI2954/Grenoble INP, Hanoi, Vietnam; MICA Institute, Hanoi University of Science and Technology - CNRS/UMI2954/Grenoble INP, Hanoi, Vietnam; MICA Institute, Hanoi University of Science and Technology - CNRS/UMI2954/Grenoble INP, Hanoi, Vietnam; MICA Institute, Hanoi University of Science and Technology - CNRS/UMI2954/Grenoble INP, Hanoi, Vietnam","2013 International Conference on Asian Language Processing","24 Oct 2013","2013","","","103","106","The diacritic marks exist in many languages such as French, German, Slovak, Vietnamese, etc. However for some reasons, sometime they are omitted in writing. This phenomenon may lead to the ambiguity for reader when reading a non-diacritic text. The automatic diacritic restoration problem has been proposed and resolved in several languages using the character-based approach, word-based approach, point-wise approach, etc. However, these approaches lean heavily on the linguistics information, size of training corpus and sometime they are language dependent. In this paper, a simple and effective restoration method will be presented. The machine translation approach will be used as a new solution for this problem. The restoration method has been applied for Vietnamese language, and integrated in an Android application named VIVA (Vietnamese Voice Assistant) that reads out the content of incoming text messages on mobile phone. Our experiments show that the proposed restoration method can recover diacritic marks with a 99.0% accuracy rate.","","978-0-7695-5063-3","10.1109/IALP.2013.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646014","diacritics restoration;vietnamese;statistical machine translation;text message","Training;Accuracy;Writing;Speech;Smart phones;Training data","language translation;natural language processing","machine translation;Vietnamese diacritic restoration;diacritic mark;automatic diacritic restoration problem;linguistics information;Vietnamese language;Android application;VIVA;Vietnamese Voice Assistant","","4","","17","IEEE","24 Oct 2013","","","IEEE","IEEE Conferences"
"Bilingual Corpus Research on Chinese English Machine Translation in Computer Centres of Chinese Universities","C. -J. Liu; S. Han","Department of Basic Courses, Shandong University of Science and Technology, Taian, China; Department of Basic Courses, Shandong University of Science and Technology, Taian, China","2012 International Conference on Computer Science and Service System","31 Dec 2012","2012","","","1720","1723","In recent years, monolingual or multilingual (primarily bilingual) corpora are viewed as key resources in language information processing and language engineering projects. A Chinese English parallel corpus is being set up. This paper gives a brief discussion on construction, annotation, and alignment of the parallel corpus. And how it is used in Chinese English Machine translation.","","978-0-7695-4719-0","10.1109/CSSS.2012.430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6394749","College Bilingual Corpus;Corpus annotation;Corpus alignment;Machine Translation","Tagging;Speech;Encoding;Pragmatics;Educational institutions;Standards","language translation","bilingual corpus research;Chinese english machine translation;Chinese university computer centres;monolingual corpora;multilingual corpora;language information processing;language engineering projects;Chinese english parallel corpus;parallel corpus construction;parallel corpus annotation;parallel corpus alignment","","2","","4","IEEE","31 Dec 2012","","","IEEE","IEEE Conferences"
"Robust analysis of spoken input combining statistical and knowledge-based information sources","R. Cattoni; M. Federico; A. Lavie","ITC IRST, Trento, Italy; ITC IRST, Trento, Italy; Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","347","350","The paper is concerned with the analysis of automatic transcription of spoken input into an interlingua formalism for a speech-to-speech machine translation system. This process is based on two sub-tasks: (1) the recognition of the domain action (a speech act and a sequence of concepts); (2) the extraction of arguments consisting of feature-value information. Statistical models are used for the former, while a knowledge-based approach is employed for the latter. The paper proposes an algorithm that improves the analysis in terms of robustness and performance; it combines the scores of the statistical models with the extracted arguments, taking into account the well-formedness constraints defined by the interlingua formalism.","","0-7803-7343-X","10.1109/ASRU.2001.1034658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034658","","Robustness;Information analysis;Natural languages;Speech processing;Speech analysis;Data mining;Performance analysis;Algorithm design and analysis;Speech recognition;Automatic speech recognition","language translation;speech recognition;speech processing;text analysis;linguistics;natural languages;statistical analysis;knowledge based systems","spoken input analysis;knowledge-based information sources;automatic transcription;interlingua formalism;speech-to-speech machine translation;concept sequence;speech act;argument extraction;feature-value information;statistical models","","2","","4","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Implicit language identification system based on random forest and support vector machine for speech","M. Gupta; S. S. Bharti; S. Agarwal","Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India","2017 4th International Conference on Power, Control & Embedded Systems (ICPCES)","23 Nov 2017","2017","","","1","6","Speech uttered by the human beings contains the information about speakers, languages and contents. Language of uttered speech can easily be identified by extracting the language specific information from it. Identification of language of speech is known as Language Identification (LID). Identification of language from speech is helpful in its translation, speech recognition and speech activated automatic systems. LID system may also play an important role in speaker recognition as identification of language can be used to reduce search space. In this paper an approach based on Linear Predictive Coding (LPC) and Mel Frequency Cepstral Coefficients (MFCCs) features for language identification is proposed using SVM and Random Forest (RF) classification techniques. Both LPC and MFCC features are vocal tract features. LPC and MFCC features extracted from uttered speech contain language as well as speaker related informations. Identification of language highly depends upon extraction of language specific features. Both these vocal tract parameters of speech contain lot of information about languages spoken compared to other parameters like excitation source parameters and prosodic parameters. Hence combination of these features performs better than individual. Experiments have been performed on the database obtained from IIIT-Hyderabad consisting of 5000 multilingual clean speech signals (Hindi, Bengali, Telugu, Tamil, Marathi and Malayalam). For training the proposed model, 600 speech signals are taken arbitrarily from the above database. Language model are created for each language. Evaluation of the proposed models has been made using other 300 speech signals from same database. Language models are evaluated using individual features as well as combined features. Experiments performed by taking both features at a time give better result as compared to taking individual features one at a time. Using these features, the accuracy of language identification is not more than 80% so far as claimed by other researchers. In the proposed approach, the accuracy of language identification is improved to 92.6% using combination of same features and random forest model.","","978-1-5090-4426-9","10.1109/ICPCES.2017.8117624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117624","MFCCs;SVM;LPC;Random Forest(RF)","Speech;Support vector machines;Mel frequency cepstral coefficient;Feature extraction;Databases;Hidden Markov models;Training","cepstral analysis;encoding;feature extraction;linear predictive coding;natural language processing;signal classification;speaker recognition;speech recognition;support vector machines","language specific information;speech recognition;MFCC;speech signals;language identification system;speech activated automatic systems;linear predictive coding;LPC;mel frequency cepstral coefficients;SVM;random forest classification;RF classification;support vector machine;LID","","9","","14","IEEE","23 Nov 2017","","","IEEE","IEEE Conferences"
"Automatic Question-Answer Generation from Video Lecture using Neural Machine Translation","S. Sonia; P. Kumar; A. Saha","Amity University, Uttar Pradesh, India; Amity University, Uttar Pradesh, India; SGT University, Uttar Pradesh, India","2021 8th International Conference on Signal Processing and Integrated Networks (SPIN)","19 Oct 2021","2021","","","661","665","Video lectures perform very crucial role in learning nowadays, from video or online lectures a learner can retrieve content or gain knowledge anytime and anywhere. The paper presents Question and Answer Generation from video or online lectures automatically, from such videos the learner can be able to self-assist himself. The main goal of the presented project is to convert video speech into text, and that text is applied for Question-and-Answer Generation Automatically after selecting only highly instructive sentences from the text. Conventional approaches have mainly used heuristic rules that convert a sentence into questions. The proposed Encoder-decoder neural network model which generates significant as well as different questions from the sentences. The encoder is used to read the input text with the position of answer. The output of encoder is fed to the decoder to generates question with answers using SQuAD dataset.","2688-769X","978-1-6654-3564-2","10.1109/SPIN52536.2021.9566139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566139","Recurrent Neural network;Automatic Question Generation;Long-short term memory;Natural language Processing;Sequence to Sequence","Recurrent neural networks;Education;Signal processing algorithms;Prototypes;Signal processing;Decoding;Machine translation","computer aided instruction;decoding;language translation;learning (artificial intelligence);neural nets","input text;different questions;Encoder-decoder neural network model;highly instructive sentences;video speech;presented project;online lectures;neural machine translation;video lecture;automatic Question-Answer Generation","","","","17","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"Efficient Estimation of Language Model Statistics of Spontaneous Speech Via Statistical Transformation Model","Y. Akita; T. Kawahara","Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan; Academic Center for Computing and Media Studies, Kyoto University, Kyoto, Japan","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","1","","I","I","One of the most significant problems in language modeling of spontaneous speech such as meetings and lectures is that only limited amount of matched training data, i.e. faithful transcript for the relevant task domain, is available. In this paper, we propose a novel transformation approach to estimate language model statistics of spontaneous speech from a document-style text database, which is often available with a large scale. The proposed statistical transformation model is designed for modeling characteristic linguistic phenomena in spontaneous speech and estimating their occurrence probabilities. These contextual patterns and probabilities are derived from a small amount of parallel aligned corpus of the faithful transcripts and their document-style texts. To realize wide coverage and reliable estimation, a model based on part-of-speech (POS) is also prepared to provide a back-off scheme from a word-based model. The approach has been successfully applied to estimation of the language model for National Congress meetings from their minute archives, and significant reduction of test-set perplexity is achieved","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1660204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1660204","","Natural languages;Statistics;Automatic speech recognition;Databases;Probability;Minutes;Speech recognition;Differential equations;Training data;Large-scale systems","language translation;natural languages;speech processing;statistical analysis","language model statistics;spontaneous speech;statistical transformation model;document-style text database;linguistic phenomena;part-of-speech;back-off scheme;word-based model;test-set perplexity;statistical machine translation","","10","","8","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"Machine Translation of LATEX Based Mathematical Equations to Spoken Mathematics","M. M. Thinn; Y. K. Thu; H. M. Nwe; N. N. Yee; T. Myint; H. A. Thant; T. Supnithi","Department of Information Science, University of Technology (Yatanarpon Cyber City), Pyin Oo Lwin, Myanmar; Language and Semantic Technology Research Team (LST), National Electronic and Computer Technology Center (NECTEC), Pathum Thani, Thailand; Department of Information Science, University of Technology (Yatanarpon Cyber City), Pyin Oo Lwin, Myanmar; Department of Information Science, University of Technology (Yatanarpon Cyber City), Pyin Oo Lwin, Myanmar; Department of Information Science, University of Technology (Yatanarpon Cyber City), Pyin Oo Lwin, Myanmar; Department of Information Science, University of Technology (Yatanarpon Cyber City), Pyin Oo Lwin, Myanmar; Language and Semantic Technology Research Team (LST), National Electronic and Computer Technology Center (NECTEC), Pathum Thani, Thailand","2020 24th International Computer Science and Engineering Conference (ICSEC)","15 Mar 2021","2020","","","1","6","This paper describes the machine translation of LATEX encoded mathematical equations to spoken mathematical sentences. A LATEX- Spoken math parallel corpus (5,600 sentences) was developed. In this paper, the 10-fold cross-validation experiments were carried out by applying Phrase-based Statistical Machine Translation (PBSMT), Weighted Finite-State Transducers (WFST) and Ripple Down Rules (RDR) based tagging approaches. The BLEU, RIBES, F1 and WER evaluation scoring metrics are used for measuring translation performance. The experimental results show that the PBSMT approach achieved the highest translation performance for LATEX mathematical equations to spoken mathematical sentences translation. Moreover, we found that the translation performance of RDR approach is comparable with PBSMT.","","978-1-6654-0339-9","10.1109/ICSEC51790.2020.9375339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9375339","Phrase-based Statistical Machine Translation (PBSMT);Ripple Down Rules (RDR);Weighted Finite State Transducers (WFST)","Measurement;Computer science;Transducers;Error analysis;Tagging;Mathematics;Machine translation","language translation;natural language processing;speech recognition;statistical analysis","Spoken math parallel corpus;cross-validation experiments;applying Phrase-based Statistical Machine Translation;Weighted Finite-State Transducers;WER evaluation scoring metrics;PBSMT approach;highest translation performance;LATEX mathematical equations;spoken mathematical sentences translation;RDR approach;Spoken mathematics","","","","22","IEEE","15 Mar 2021","","","IEEE","IEEE Conferences"
"Applying example-based error correction selectively","T. Yamaguchi; S. Sako; H. Yamamoto; G. Kikui","Graduate School of Engineering, University of Tokushima, Japan; Graduate School of Engineering, Nagoya Institute of Technology, Japan; ATR Spoken Language Translation Research Laboratories, Japan; ATR Spoken Language Translation Research Laboratories, Japan","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","162","167","This paper presents a supervised approach to combining detection and correction of speech recognition errors. For each word in a recognition result, our example-based correction algorithm generates a correction candidate by aligning the recognition result and an example sentence in the corpus. The distance between the aligned sentences is regarded as the reliability of the candidate. Then, an SVM (support vector machine) classifier judges whether the correction candidate should chosen by referring to the reliability score of the candidate and multiple confidence measures that are obtained from the recognition result. Experiments carried out on a travel task corpus have shown that the proposed approach achieved a 20 % reduction (from 10 % to 8 % absolute) in WER.","","0-7803-7980-2","10.1109/ASRU.2003.1318422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318422","","Error correction;Speech recognition;Context modeling;Natural languages;Support vector machines;Support vector machine classification;Laboratories;Entropy;Costs;Target recognition","support vector machines;error correction;speech recognition;learning by example;error statistics","example-based error correction;supervised approach;error detection;speech recognition;support vector machine;multiple confidence measures;reliability score;WER;SVM classifier","","1","1","10","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition","J. Pan; T. Lei; K. Kim; K. J. Han; S. Watanabe","ASAPP Inc., Mountain View, CA, USA; ASAPP Inc., Mountain View, CA, USA; ASAPP Inc., Mountain View, CA, USA; ASAPP Inc., Mountain View, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","7872","7876","The Transformer architecture has been well adopted as a dominant architecture in most sequence transduction tasks including automatic speech recognition (ASR), since its attention mechanism excels in capturing long-range dependencies. While models built solely upon attention can be better parallelized than regular RNN, a novel network architecture, SRU++, was recently proposed. By combining the fast recurrence and attention mechanism, SRU++ exhibits strong capability in sequence modeling and achieves near-state-of-the-art results in various language modeling and machine translation tasks with improved compute efficiency. In this work, we present the advantages of applying SRU++ in ASR tasks by comparing with Conformer across multiple ASR benchmarks and study how the benefits can be generalized to long-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model achieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive performances compared with the state-of-the-art Conformer encoder under the same set-up. Specifically, SRU++ can surpass Conformer on long-form speech input with a large margin, based on our analysis.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746187","speech recognition;SRU++;attention;re-current neural network","Computational modeling;Neural networks;Computer architecture;Benchmark testing;Signal processing;Transformers;Data models","language translation;natural language processing;recurrent neural nets;speech recognition","attention mechanism;long-range dependencies;regular RNN;novel network architecture;sequence modeling;achieves near-state-of-the-art results;language modeling;machine translation tasks;improved compute efficiency;ASR tasks;long-form speech input;SRU++ model;state-of-the-art Conformer encoder;pioneering fast recurrence;Transformer architecture;dominant architecture;sequence transduction tasks;automatic speech recognition","","2","","17","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"A Turkish-English speech translation system with speaker adaptation","C. Mermer; C. Demir; H. Kaya; M. U. Dogan","Ulusal Elektronik ve Kriptoloji Arastirma Enstitusu (UEKAE), Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK), Turkiye; Ulusal Elektronik ve Kriptoloji Arastirma Enstitusu (UEKAE), Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK), Turkiye; Ulusal Elektronik ve Kriptoloji Arastirma Enstitusu (UEKAE), Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK), Turkiye; Ulusal Elektronik ve Kriptoloji Arastirma Enstitusu (UEKAE), Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK), Turkiye","2009 IEEE 17th Signal Processing and Communications Applications Conference","26 Jun 2009","2009","","","608","611","In this paper, we introduce a Turkish-to-English speech translation system trained with a parallel corpus recently developed at TUBITAK-UEKAE and we investigate the performance improvement obtained by using speaker adaptation. We describe the two sub-modules of the system, namely, the speech recognition and machine translation modules, and we investigate the relationship between their individual performances and the overall system performance using various metrics. Furthermore, we present the improvements in recognition error rate and recognition time obtained by applying speaker adaptation.","2165-0608","978-1-4244-4435-9","10.1109/SIU.2009.5136469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5136469","","Decoding;Viterbi algorithm;Speech recognition;System performance;Error analysis","language translation;linguistics;speaker recognition","Turkish-English speech translation system;speaker adaptation;TUBITAK-UEKAE;speech recognition;machine translation module","","","","15","IEEE","26 Jun 2009","","","IEEE","IEEE Conferences"
"Does joint decoding really outperform cascade processing in English-to-Chinese transliteration generation? The role of syllabification","Y. Song; C. Kit","Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong, China; Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong, China","2010 International Conference on Machine Learning and Cybernetics","20 Sep 2010","2010","6","","3323","3328","Transliteration is a challengeable task aimed at converting a proper name into another language with phonetic equivalence. Since the conversion relates to the phonetic aspect of a text, syllabification is considered a major factor affecting the performance of a transliteration system. In grapheme-based approaches, there are two routines to transliterate, one is to perform in a pipeline of separate syllabification and other components in generation process step by step, the other is to synchronously segment syllables and generating transliteration options. Usually, joint decoding outperforms the cascade processing in many natural language processing missions, however, syllabification is a special component in transliteration task. Thus in this paper, we investigate the two routines with a systematic analysis and compare their results to illustrate the strength of syllabification. A phrase-based statistical machine translation framework for joint decoding and a conditional random field syllabification system are used in this work for our investigation, which shows a different scenario on the issue of joint decoding versus cascade processing in transliteration.","2160-1348","978-1-4244-6527-9","10.1109/ICMLC.2010.5580674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580674","Transliteration;syllabification;joint decoding;statistical machine translation;log-linear model","Decoding;Testing;Feature extraction;Training;Accuracy;Joints","decoding;language translation;natural language processing;speech processing","joint decoding;cascade processing;English to Chinese transliteration;phonetic equivalence;syllabification;grapheme based approach;natural language processing;phrase based statistical machine translation","","","","17","IEEE","20 Sep 2010","","","IEEE","IEEE Conferences"
"A Bilingual Machine Transliteration System for Sanskrit-English Using Rule-Based Approach","N. Sethi; A. Dev; P. Bansal","Information Technology, Indira Gandhi Delhi Technical Univaersity for Women, Delhi, India; Information Technology, Indira Gandhi Delhi Technical Univaersity for Women, Delhi, India; Artificial Intelligence & Data Science, Indira Gandhi Delhi Technical University for Women, Delhi, India","2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)","17 Mar 2023","2022","","","1","5","Machine Transliteration is a big challenging area in an increasingly multilingual ecosphere due to its dangerous position in various downstream natural language processing application systems. When a word is transliterated, it is shifted from one script to another. In contrast to a translation, which clarifies the meaning of a word written in a different language, a transliteration only conveys the word's pronunciation by utilizing a familiar alphabet. This paper proposes a technique for creating a bilingual automated tool to type Sanskrit using English orthography/alphabets and converting Sanskrit text into the script of English which helps in reading the Sanskrit text for those who are not aware about the orthography of Sanskrit language. The system receives input via the QWERTY keyboard, which produces the equivalent Sanskrit text and inversely user can give Sanskrit text as input to get equivalent text using the script of English language. The goal is to create an easy-to-use and robust automated solution that allows end-users to effortlessly type Sanskrit shlokas or sentences using an English keyboard. The suggested approach is unique in that it is based on the language's Unicode and works for the low-resource ancient language Sanskrit. The primary applications of the designed tool are to help user to read Sanskrit text using the script/orthography of English, to create parallel corpora for the Sanskrit language translation process, to create e-versions of manuscripts written in Sanskrit language as the majority of ancient knowledge is not available on the internet, and to make it easy to learn Sanskrit language by any individual or students at the school level. This tool encourages humans to use their original language.","","978-1-6654-9902-6","10.1109/AIST55798.2022.10064993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064993","Low-resource;Sanskrit;Unicode‚Äôs;Transliteration;Orthography;Shlokas;E-learning tool;Natural language processing","Buildings;Keyboards;Oral communication;Machine translation;Speech processing;Artificial intelligence;Testing","Internet;keyboards;language translation;linguistics;natural language processing;text analysis","bilingual automated tool;bilingual machine transliteration system;downstream natural language processing application systems;English keyboard;English language;equivalent Sanskrit text;low-resource ancient language Sanskrit;QWERTY keyboard;rule-based approach;Sanskrit language translation process;Sanskrit shlokas;Sanskrit-English","","","","23","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Direct Text to Speech Translation System Using Acoustic Units","V. Mingote; P. Gimeno; L. Vicente; S. Khurana; A. Laurent; J. Duret","ViVoLab - Arag√≥n Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza, Spain; ViVoLab - Arag√≥n Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza, Spain; ViVoLab - Arag√≥n Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza, Spain; MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA; LIUM, Le Mans University, Le Mans, France; LIA, Avignon University, Avignon, France","IEEE Signal Processing Letters","14 Sep 2023","2023","30","","1262","1266","This letter proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.","1558-2361","","10.1109/LSP.2023.3313513","GENCI‚ÄìIDRIS; HPC resources(grant numbers:2022-AD011012565,AD011012527); European Union's Horizon 2020 Research and Innovation Program; Marie Sk≈Çodowska-Curie Grant(grant numbers:101007666); JSALT 2022 at JHU; Amazon, Microsoft, and Google; MCIN/AEI/10.13039/501100011033 and European Union NextGenerationEU/PRTR(grant numbers:PDC2021-120846-C41,PDI2021-126061OB-C44); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246390","Acoustic units;CVSS corpus;direct text to speech translation;mBART","Acoustics;Task analysis;Vocoders;Training;Machine translation;Computer architecture;Spectrogram","","","","","","23","IEEE","11 Sep 2023","","","IEEE","IEEE Journals"
"Domain adaptation of a speech translation system for lectures by utilizing frequently appearing parallel phrases in-domain","N. Goto; K. Yamamoto; S. Nakagawa","Toyohashi University of Technology, Aichi, Japan; Toyohashi University of Technology, Aichi, Japan; Toyohashi University of Technology, Aichi, Japan","2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","19 Jan 2017","2016","","","1","4","This paper describes our scheme to translate spoken English lectures into Japanese consisting of an English automatic speech recognition system (ASR) that utilizes a deep neural network (DNN) and an English to Japanese phrase-based statistical machine translation system (SMT). We focused on domain adaptation of the acoustic and translation models. For domain adaptation of the translation model, frequently appearing English-phrases consisting of multiple words are extracted from transcripts of in-domain lectures based on n-gram words or a part of syntax tree. Then we translated the English phrases into Japanese-phrases by hand semi-automatically. These phrase pairs of source and target language are used to learn an SMT model for domain adaptation. An adaptation method directly inserts these phrase pairs into a phrase table or adds them to a parallel corpus. In the experiments, n-gram and syntax tree based methods are compared whilst extracting frequent English-phrases. Furthermore, the adapted phrase table and the parallel corpus are compared. When the frequent English and Japanese phrase pairs based on syntax tree were added to the phrase table, the baseline model was improved.","","978-9-8814-7682-1","10.1109/APSIPA.2016.7820896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820896","","Adaptation models;Speech;Syntactics;Data models;Acoustics;Speech recognition;Mathematical model","natural language processing;neural nets;speech recognition","domain adaptation;speech translation system;parallel phrases in-domain;automatic speech recognition system;deep neural network;phrase-based statistical machine translation system;n-gram methods;syntax tree based methods","","1","","13","","19 Jan 2017","","","IEEE","IEEE Conferences"
"Listwise Ranking Functions for Statistical Machine Translation","M. Zhang; Y. Liu; H. Luan; M. Sun","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","8","1464","1472","Decision rules play an important role in the tuning and decoding steps of statistical machine translation. The traditional decision rule selects the candidate with the greatest potential from a candidate space by examining each candidate individually. However, viewing each candidate as independent imposes a serious limitation on the translation task. We instead view the problem from a ranking perspective that naturally allows the consideration of an entire list of candidates as a whole through the adoption of a listwise ranking function. Our shift from a pointwise to a listwise perspective proves to be a simple yet powerful extension to current modeling that allows arbitrary pairwise functions to be incorporated as features, whose weights can be estimated jointly with traditional ones. We further demonstrate that our formulation encompasses the minimum Bayes risk (MBR) approach, another decision rule that considers restricted listwise information, as a special case. Experiments show that our approach consistently outperforms the baseline and MBR methods across the considered test sets.","2329-9304","","10.1109/TASLP.2016.2560527","863 Program(grant numbers:2015AA011808); National Natural Science Foundation of China(grant numbers:61522204,61432013,61303075); Singapore National Research Foundation; IDM Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462217","Statistical machine translation;listwise ranking function;discriminative reranking;Discriminative reranking;listwise ranking function;statistical machine translation","Measurement;Tuning;IEEE transactions;Speech;Speech processing;Maximum likelihood decoding","Bayes methods;language translation;risk management;statistical analysis","MBR approach;minimum Bayes risk approach;arbitrary pairwise functions;candidate space;decoding step;tuning step;decision rules;statistical machine translation;listwise ranking functions","","4","","34","IEEE","28 Apr 2016","","","IEEE","IEEE Journals"
"A new verb based approach for English to Bangla machine translation","M. Rabbani; K. M. R. Alam; M. Islam","Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh; Khulna University of Engineering and Technology, Khulna, BD; Department of Computer Science and Engineering, Khulna University of Engineering and Technology, Khulna, Bangladesh","2014 International Conference on Informatics, Electronics & Vision (ICIEV)","10 Jul 2014","2014","","","1","6","This paper proposes verb based machine translation (VBMT), a new approach of machine translation (MT) from English to Bangla (EtoB). For translation, it simplifies any form (i.e. simple, complex, compound, active and passive form) of English sentence into the simplest form of English sentence i.e. subject plus verb plus object. When compared with existing rule based EtoB MT schemes, VBMT doesn't employ exclusive or individual structural rules of various English sentences; it only detects the main verb from any form of English sentence and then transforms it into the simplest form of English sentence. Thus VBMT can translate from EtoB very simply, correctly and efficiently. Rule based EtoB MT is tough because it requires the matching of sentences with the stored rules. Moreover, many existing EtoB MT schemes which deploy rules are almost inefficient to translate complex or complicated sentences because it is difficult to match them with well-established rules of English grammar. VBMT is efficient because after identifying the main verb of any form of English sentence, it binds the remaining parts of speech (POS) as subject and object. VBMT has been successfully implemented for the MT of Assertive, Interrogative, Imperative, Exclamatory, Active-Passive, Simple, Complex, and Compound form of English sentences applicable in both desktop and mobile applications.","","978-1-4799-5180-2","10.1109/ICIEV.2014.6850684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850684","Rule based Machine Translation;Natural Language Processing;English to Bangla;Human Language Technology","Tagging;Compounds;Informatics;Conferences;Natural languages;Knowledge based systems;Databases","language translation;natural language processing","English to Bangla machine translation;VBMT;verb based machine translation;EtoB MT schemes;natural language processing;human language technology","","11","","18","IEEE","10 Jul 2014","","","IEEE","IEEE Conferences"
"One decade of statistical machine translation: 1996-2005","H. Ney","Human Language Technology and Pattern Recognition Lehrstuhl fur Informatik VI-Computer Science Department, RWTH Aachen University, Aachen, Germany","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","2","11","In the last decade, the statistical approach has found widespread use in machine translation both for written and spoken language and has had a major impact on the translation accuracy. The goal of this paper is to cover the state of the art in statistical machine translation. We would re-visit the underlying principles of the statistical approach to machine translation and summarize the progress that has been made over the last decade","","0-7803-9478-X","10.1109/ASRU.2005.1566466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566466","","Natural languages;Context modeling;Hidden Markov models;Speech;Humans;Pattern recognition;Computer science;Transducers;Tides;Vocabulary","language translation;natural languages;statistical analysis","statistical machine translation;written language;spoken language","","4","","","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"A Rule-Based Source-Side Reordering on Phrase Structure Subtrees","F. Liang; L. Chen; M. Li; Nasun-urtu","Institute of Intelligent Machines, Chinese Academy of Sciences, University of Science and Technology of China, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; Network Center, Inner Mongolia University, Hohhot, China","2011 International Conference on Asian Language Processing","2 Jan 2012","2011","","","173","176","Since different languages put words in different orders, reordering is an important issue in statistical machine translation. The paper proposes a rule-based reordering method at the source side as a preprocessing step, which applies some syntactic reordering rules on the phrase structure subtree to reorder source language. The reordering rules integrate the phrase structure tree with part-of-speech tags, which can implement the reordering not only between words but also between words and phrases. And the problems of long-distance reordering and translation errors can be partly solved. Meanwhile, the interference between reordering rules of this method has been significantly reduced in this method. Experiments shows that our method can improve the performance of the state-of-the-art phrase translation models, achieving 1.71 BLEU score increase over the standard phrase-based machine translation system.","","978-1-4577-1733-8","10.1109/IALP.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121496","reordering;phrase structure trees;part-of-speech tags;statistical machine translation","Pragmatics;Syntactics;Interference;Training;Context;Decoding;Entropy","computational linguistics;language translation;natural language processing;trees (mathematics)","rule-based source side reordering method;phrase structure subtrees;statistical machine translation;syntactic reordering rules;part-of-speech tags;long-distance reordering;1.71 BLEU score;standard phrase-based machine translation system","","4","","17","IEEE","2 Jan 2012","","","IEEE","IEEE Conferences"
"Automatic acoustic segmentation in N-best list rescoring for lecture speech recognition","P. Shen; X. Lu; H. Kawai","National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan","2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)","4 May 2017","2016","","","1","5","Speech segmentation is important in automatic speech recognition (ASR) and machine translation (MT). Particularly in N-best list rescoring processing, generalizing N-best lists consisting of as many as candidates from a decoding lattice requires proper utterance segmentation. In lecture speech recognition, only long audio recordings are provided without any utterance segmentation information. In addition, rather than only speech event, other acoustic events, e.g., laugh, applause, etc., are included in the recordings. Traditional speech segmentation algorithms for ASR focus on acoustic cues in segmentation, while in MT, speech text segmentation algorithms pay much attention to linguistic cues. In this study, we propose a three-stage speech segmentation framework by integrating both the acoustic and linguistic cues. We tested the segmentation framework for lecture speech recognition. Our results showed the effectiveness of the proposed segmentation algorithm.","","978-1-5090-4294-4","10.1109/ISCSLP.2016.7918409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918409","Acoustic segmentation;acoustic event detection;language model;N-best list rescore","Speech;Acoustics;Decoding;Speech recognition;Algorithm design and analysis;Hidden Markov models;Lattices","acoustic signal processing;computational linguistics;language translation;speech recognition;text analysis","automatic acoustic segmentation;lecture speech recognition;automatic speech recognition;ASR;machine translation;MT;N-best list rescoring processing;N-best list generalization;lattice decoding;utterance segmentation;audio recordings;acoustic cues;speech text segmentation algorithms;linguistic cues;three-stage speech segmentation framework","","","","18","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Multilanguage voice dictionary for ubiquitous environment","A. Rai; U. Shrawankar","G. H. Raisoni College of Engineering, Nagpur, MS, India; G. H. Raisoni College of Engineering, Nagpur, MS, India","2014 International Conference on Information Systems and Computer Networks (ISCON)","24 Nov 2014","2014","","","45","49","Languages in India play an important role as a communication medium. As the person is traveling from one state to another s/he faces difficulty to communicate in other language with other community. So, the Multilanguage Voice Dictionary is applying for developing Indian language Machine Translation system. This application comprises of two algorithms. The word based translation model with the rule-based model is used as the main technique. The word based translation model is implementing for verb and other type of words. The rule based method is particularly used for Out Of Vocabulary (OOV) words which have to be used as it can't be translated. This is performing by extending a lexicon and writing a set of sample words. The translation is doing through templates associated with the lexicon with the word in other language. The speech processing such as input and output in voice form is to be implemented using speech simulator. For the alphabets of a language, the language word library is using in this application.","","978-1-4799-2981-8","10.1109/ICISCON.2014.6965216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965216","Multilanguage dictionary;natural language processing;word based translation model;rule based translation model;voice recognition;lexicons","Speech;Dictionaries;Speech processing;Speech recognition;Vocabulary;Information processing;Educational institutions","knowledge based systems;language translation;natural language processing;speech processing;ubiquitous computing","multilanguage voice dictionary;ubiquitous environment;Indian language machine translation system;word-based translation model;out-of-vocabulary words;OOV words;speech processing;speech simulator;language word library;natural language processing","","","","18","IEEE","24 Nov 2014","","","IEEE","IEEE Conferences"
"A Sequence-to-Sequence Pronunciation Model for Bangla Speech Synthesis","A. Ahmad; M. Raihan Hussain; M. Reza Selim; M. Zafar Iqbal; M. Shahidur Rahman","Department of Computer Science and Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh; Department of Computer Science and Engineering, Leading University, Sylhet, Bangladesh; Department of Computer Science and Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh; Department of Computer Science and Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh; Department of Computer Science and Engineering, Shahjalal University of Science and Technology, Sylhet, Bangladesh","2018 International Conference on Bangla Speech and Language Processing (ICBSLP)","2 Dec 2018","2018","","","1","4","Extracting pronunciation from written text is necessary in many application areas, especially in text-to-speech synthesis. `Bangla' is not completely a phonetic language, meaning there is not always direct mapping from orthography to pronunciation. It mainly suffers from `schwa deletion' problem, along with some other ambiguous letters and conjuncts. Rule-based approaches cannot completely solve this problem. In this paper, we propose to adopt an Encoder-Decoder based neural machine translation (NMT) model for determining pronunciations of Bangla words. We mapped the pronunciation problem into a sequence-to-sequence problem and used two `Gated Recurrent Unit Recurrent Neural Network's (GRU-RNNs) for our model. We fed the model with two types of input data. In one model we used `raw' words and in other model we used `pre-processed' words (normalized by hand-written rules) as input. Both experiments showed promising results and can be used in any practical application.","","978-1-5386-8207-4","10.1109/ICBSLP.2018.8554507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554507","encoder-decoder;sequence-to-sequence;gru-rnn;pronunciation model","Hidden Markov models;Decoding;Computer science;Data models;Training;Neural networks;Speech synthesis","decoding;encoding;language translation;natural language processing;recurrent neural nets;speech synthesis;text analysis","sequence-to-sequence pronunciation model;Bangla speech synthesis;written text;text-to-speech synthesis;phonetic language;direct mapping;rule-based approaches;Bangla words;pronunciation problem;gated recurrent unit recurrent neural network;orthography;encoder-decoder based neural machine translation model;handwritten rules","","2","","12","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Neural network joint modeling via context-dependent projection","Y. -C. Tam; Y. Lei","Speech Technology and Research Laboratory, SRI International, Menlo Park, CA, USA; Speech Technology and Research Laboratory, SRI International, Menlo Park, CA, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5356","5360","Neural network joint modeling (NNJM) has produced huge improvement in machine translation performance. As in standard neural network language modeling, a context-independent linear projection is applied to project a sparse input vector into a continuous representation at each word position. Because neighboring words are dependent on each other, context-independent projection may not be optimal. We propose a context-dependent linear projection approach which considers neighboring words. Experimental results showed that the proposed approach further improves NNJM by 0.5 BLEU for English-Iraqi Arabic translation in N-best rescoring. Compared to a baseline using hierarchical phrases and sparse features, NNJM with our proposed approach has achieved a 2 BLEU improvement.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7178994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178994","Neural network joint modeling;context-dependent linear projection;position-dependent linear projection;statistical machine translation","Artificial neural networks;Pragmatics;Syntactics;History","language translation;neural nets;speech recognition","neural network joint modeling;machine translation;context-independent linear projection;sparse input vector;English-Iraqi Arabic translation;N-best rescoring","","","","19","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Joint Speech-Text Embeddings with Disentangled Speaker Features","M. G. Gonzales; P. Corcoran; N. Harte; M. Schukat","College of Science and Engineering, University of Galway, Galway, Ireland; College of Science and Engineering, University of Galway, Galway, Ireland; School of Engineering, Trinity College Dublin, Dublin, Ireland; College of Science and Engineering, University of Galway, Galway, Ireland","2023 34th Irish Signals and Systems Conference (ISSC)","3 Jul 2023","2023","","","1","5","This paper presents a novel model architecture for speech processing that takes advantage of a joint speech-text embedding space and disentangled speaker features. Here unsupervised representation learning extracts latent features from the input without labels which results in task-agnostic, but information-entangled embeddings. On the other hand, a unified embedding space of speech and text aims to leverage acoustics and semantic knowledge from the two modalities, respectively. The model was trained on 4 speakers from the CMU Arctic dataset and evaluated on three downstream tasks: speaker recognition, automatic speech recognition (ASR), and text-to-speech (TTS). Results show 96.87% speaker classification accuracy, 11.57% Word Error Rate, and 8.91 Mel Cepstral Distortion mean on the evaluation set.","2688-1454","979-8-3503-4057-0","10.1109/ISSC59246.2023.10162046","Science Foundation Ireland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10162046","speech-text;speech recognition;speech synthesis","Training;Representation learning;Error analysis;Semantics;Feature extraction;Speaker recognition;Machine translation","cepstral analysis;feature extraction;learning (artificial intelligence);speaker recognition;speech processing;speech recognition;unsupervised learning","96.87% speaker classification accuracy;automatic speech recognition;disentangled speaker features;information-entangled embeddings;joint speech-text embedding space;joint speech-text embeddings;model architecture;speaker recognition;speech processing;task-agnostic;text-to-speech;unified embedding space;unsupervised representation learning extracts latent features","","","","32","IEEE","3 Jul 2023","","","IEEE","IEEE Conferences"
"P-Transformer: Towards Better Document-to-Document Neural Machine Translation","Y. Li; J. Li; J. Jiang; S. Tao; H. Yang; M. Zhang","Key Laboratory of Linguistic and Cultural Computing of Ministry of Education, Northwest Minzu University, Lanzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Key Laboratory of Linguistic and Cultural Computing of Ministry of Education, Northwest Minzu University, Lanzhou, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2023","PP","99","1","13","Directly training a document-to-document (Doc2Doc) neural machine translation (NMT) via Transformer from scratch, especially on small datasets, usually fails to converge. Our dedicated probing tasks show that 1) both the absolute position and relative position information gets gradually weakened or even vanished once it reaches the upper encoder layers, and 2) the vanishing of absolute position information in encoder output causes the training failure of Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer (P-Transformer) to enhance both the absolute and relative position information in both self-attention and cross-attention. Specifically, we integrate absolute positional information, i.e., position embeddings, into the query-key pairs both in self-attention and cross-attention through a simple yet effective addition operation. Moreover, we also integrate relative position encoding in self-attention. The proposed P-Transformer utilizes sinusoidal position encoding and does not require any task-specified position embedding, segment embedding, or attention mechanism. Through the above methods, we build a Doc2Doc NMT model with P-Transformer, which ingests the source document and completely generates the target document in a sequence-to-sequence (seq2seq) way. In addition, P-Transformer can be applied to seq2seq-based document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translations. Extensive experimental results of Doc2Doc NMT show that P-Transformer significantly outperforms strong baselines on the widely-used 9 document-level datasets in 7 language pairs, covering small-, middle-, and large-scales, and achieves a new state-of-the-art. Experimentation on discourse phenomena shows that our Doc2Doc NMT models improve the translation quality in both BLEU and discourse coherence. We make our code available on Github.","2329-9304","","10.1109/TASLP.2023.3313445","NSFC(grant numbers:6203600); National Natural Science Foundation of China(grant numbers:62266038); Fundamental Research Funds for the Central Universities of Northwest Minzu University(grant numbers:31920230122); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255243","Neural machine translation;document-level NMT;document-to-document translation;position information;sequence-to-sequence","Transformers;Task analysis;Training;Encoding;Context modeling;Training data;Machine translation","","","","","","","IEEE","19 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Back-Translation-Style Data Augmentation for end-to-end ASR","T. Hayashi; S. Watanabe; Y. Zhang; T. Toda; T. Hori; R. Astudillo; K. Takeda","Nagoya University, Nagoya, JAPAN; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Google, Inc., USA; Nagoya University, Nagoya, JAPAN; Mitsubishi Electric Research Laboratories (MERL), Cambridge MA, USA; Spoken Language Systems Lab, INESC-ID Lisboa, Portugal; Nagoya University, Nagoya, JAPAN","2018 IEEE Spoken Language Technology Workshop (SLT)","14 Feb 2019","2018","","","426","433","In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is retrained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.","","978-1-5386-4334-1","10.1109/SLT.2018.8639619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8639619","automatic speech recognition;end-to-end;data augmentation;back-translation","Data models;Acoustics;Decoding;Feature extraction;Training data;Hidden Markov models;Training","acoustic signal processing;language translation;learning (artificial intelligence);neural nets;speech coding;speech recognition","attention-based end-to-end automatic speech recognition;speech signals;back-translation technique;machine translation;pre-trained E2E-ASR encoder;acoustic features;E2E-ASR decoder;back-translation-style data augmentation;data augmentation method;neural text-to-encoder model;attention learning","","35","","32","IEEE","14 Feb 2019","","","IEEE","IEEE Conferences"
"Exploring the use of acoustic embeddings in neural machine translation","S. Deena; R. W. M. Ng; P. Madhyastha; L. Specia; T. Hain","Speech and Hearing Research Group, University of Sheffield, UK; Speech and Hearing Research Group, University of Sheffield, UK; Natural Language Processing Research Group, The University of Sheffield, UK; Natural Language Processing Research Group, The University of Sheffield, UK; Speech and Hearing Research Group, University of Sheffield, UK","2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","25 Jan 2018","2017","","","450","457","Neural Machine Translation (NMT) has recently demonstrated improved performance over statistical machine translation and relies on an encoder-decoder framework for translating text from source to target. The structure of NMT makes it amenable to add auxiliary features, which can provide complementary information to that present in the source text. In this paper, auxiliary features derived from accompanying audio, are investigated for NMT and are compared and combined with text-derived features. These acoustic embeddings can help resolve ambiguity in the translation, thus improving the output. The following features are experimented with: Latent Dirichlet Allocation (LDA) topic vectors and GMM subspace i-vectors derived from audio. These are contrasted against: skip-gram/Word2Vec features and LDA features derived from text. The results are encouraging and show that acoustic information does help with NMT, leading to an overall 3.3% relative improvement in BLEU scores.","","978-1-5090-4788-8","10.1109/ASRU.2017.8268971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268971","Neural Machine Translation;LDA topics;Acoustic Embeddings","Feature extraction;Acoustics;Training;Decoding;Data mining;Resource management;Recurrent neural networks","language translation;natural language processing;neural nets;text analysis","auxiliary features;NMT;acoustic embeddings;Latent Dirichlet Allocation topic vectors;LDA features;acoustic information;neural machine translation;source text;text translation","","4","","24","IEEE","25 Jan 2018","","","IEEE","IEEE Conferences"
"Automatic translation from parallel speech: Simultaneous interpretation as MT training data","M. Paulik; A. Waibel","Interactive Systems Laboratories interACT, Carnegie Mellon University, USA; Interactive Systems Laboratories interACT, Universit√§t Karlsruhe, Germany","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","496","501","State-of-the art statistical machine translation depends heavily on the availability of domain-specific bilingual parallel text. However, acquiring large amounts of bilingual parallel text is costly and, depending on the language pair, sometimes impossible. We propose an alternative to parallel text as machine translation (MT) training data; audio recordings of parallel speech (pSp) as it occurs in any scenario where interpreters are involved. Although interpretation (pSp) differs significantly from translation (parallel text), we achieve surprisingly strong translation results with our pSp-trained MT and speech translation systems.We argue that the presented approach is of special interest for developing speech translation in the context of resource-deficient languages where even monolingual resources are scarce.","","978-1-4244-5478-5","10.1109/ASRU.2009.5372880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372880","","Training data;Natural languages;Audio recording;Automatic speech recognition;Interactive systems;Laboratories;Art;Large-scale systems;Dictionaries;Books","language translation;linguistics;speech processing;statistical analysis;text analysis","automatic speech translation;parallel speech;MT training data;statistical machine translation;domain-specific bilingual parallel text;audio recording;resource-deficient language","","3","","16","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Domain Sensitive Chinese-English Example Based Machine Translation","M. Yang; H. Jiang; Z. Tiejun; S. Li; D. Liu","MOE-MS Joint Key Lab of NLP&Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP&Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP&Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; MOE-MS Joint Key Lab of NLP&Speech, School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Engineering of Technology, Harbin, China","2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery","5 Nov 2008","2008","2","","453","456","This paper proposes a method of domain sensitive example based machine translation system between Chinese and English. First, the example base is automatically extracted according to the relative position of word alignments, which is a language independent approach. Then, through combining the text-classification technique, the EBMT system manages to select the most appropriate example base for the input text. It is indicated by the experiments that the method is sensitive to the translation domains and capable of improving the translation performance in a multi-domain environment.","","978-0-7695-3305-6","10.1109/FSKD.2008.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666158","","Natural languages;Computer science;Knowledge acquisition;Fuzzy systems;Knowledge engineering;Manuals;System testing;Dictionaries;Information analysis","language translation;text editing","Chinese-English translation;domain sensitive example-based machine translation;word alignments;text classification","","1","","9","IEEE","5 Nov 2008","","","IEEE","IEEE Conferences"
"Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation","N. Bhandari; P. -Y. Chen",RV College of Engineering; IBM Research,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution1 to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094630","adversarial robustness;machine translation","Measurement;Semantics;Signal processing algorithms;Signal processing;Robustness;Acoustics;Machine translation","","","","","","23","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Dynamic Multi-Branch Layers for On-Device Neural Machine Translation","Z. Tan; Z. Yang; M. Zhang; Q. Liu; M. Sun; Y. Liu","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Huawei Noah‚Äôs Ark Lab, Shenzhen, China; Huawei Noah‚Äôs Ark Lab, Shenzhen, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","7 Mar 2022","2022","30","","958","967","With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications, such as neural machine translation (NMT), from cloud to mobile devices. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.5 times faster with the same number of parameters.","2329-9304","","10.1109/TASLP.2022.3153257","National Key R&D Program of China(grant numbers:2018YFB1005103); National Natural Science Foundation of China(grant numbers:62006138,61925601,61772302); Huawei Noah‚Äôs Ark Lab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729651","Conditional computation;decoding;machine translation;natural language processing;transformers","Training;Performance evaluation;Transformers;Market research;Mobile handsets;Hardware;Machine translation","language translation;mobile computing;natural language processing;neural nets","shared-private reparameterization;artificial intelligence;WMT20 Chinese-English translation task;WMT14 English-German translation task;on-device NMT systems;mobile devices;AI;on-device neural machine translation;dynamic multibranch layers","","2","","25","IEEE","7 Mar 2022","","","IEEE","IEEE Journals"
"Probabilistic finite-state machines - part I","E. Vidal; F. Thollard; C. de la Higuera; F. Casacuberta; R. C. Carrasco","Departamento de Sistemas Inform√°ticos y Computaci√≥n and Instituto Tecnol√≥gico de Inform√°tica, Universidad Polit√©cnica de Valencia, Valencia, Spain; Facult√© des Sciences et Techniques, EURISE, Saint-Etienne, France; Facult√© des Sciences et Techniques, EURISE, Saint-Etienne, France; Departamento de Sistemas Inform√°ticos y Computaci√≥n and Instituto Tecnol√≥gico de Inform√°tica, Universidad Polit√©cnica de Valencia, Valencia, Spain; Departamento de Lenguajes y Sistemas Inform√°ticos, Universidad de Alicante, Alicante, Spain","IEEE Transactions on Pattern Analysis and Machine Intelligence","23 May 2005","2005","27","7","1013","1025","Probabilistic finite-state machines are used today in a variety of areas in pattern recognition, or in fields to which pattern recognition is linked: computational linguistics, machine learning, time series analysis, circuit testing, computational biology, speech recognition, and machine translation are some of them. In Part I of this paper, we survey these generative objects and study their definitions and properties. In Part II, we study the relation of probabilistic finite-state automata with other well-known devices that generate strings as hidden Markov models and n-grams and provide theorems, algorithms, and properties that represent a current state of the art of these objects.","1939-3539","","10.1109/TPAMI.2005.147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432736","Index Terms- Automata;classes defined by grammars or automata;machine learning;language acquisition;language models;language parsing and understanding;machine translation;speech recognition and synthesis;structural pattern recognition;syntactic pattern recognition.","Pattern recognition;Computational linguistics;Machine learning;Circuit analysis;Pattern analysis;Speech analysis;Time series analysis;Circuit testing;Computational biology;Speech recognition","finite state machines;pattern recognition;probabilistic automata","probabilistic finite-state machines;pattern recognition;computational linguistics;machine learning;time series analysis;circuit testing;computational biology;speech recognition;machine translation;probabilistic finite-state automata;hidden Markov models","Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Information Storage and Retrieval;Models, Statistical;Natural Language Processing;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Sequence Alignment;Sequence Analysis;Signal Processing, Computer-Assisted","158","1","62","IEEE","23 May 2005","","","IEEE","IEEE Journals"
"TSTMT: step towards an accurate Thai sign translation","T. Ditcharoen; N. Cercone; K. Naruedomkul; B. Tipakorn","Institute for Innovation and Development of Learning Process, Mahidol University, Bangkok, Thailand; Department of Mathematics, Mahidol University, Bangkok, Thailand; Department of Computer Science, Dalhousie University, Halifax, Canada; Department of Computer Engineering, King Mongkut''s University of Technology, Bangkok, Thailand","Fourth International Conference on Machine Learning and Applications (ICMLA'05)","20 Mar 2006","2005","","","6 pp.","","We propose Thai sign to Thai machine translation (TSTMT), an alternative approach to machine translation which is used for translating from Thai sign language into Thai text. TSTMT performs the translation by recognizing an image sequence of signs as sign words and synthesizing a target sentence from those sign words. TSTMT represents a hybrid of image recognition and natural language processing techniques. This approach is comprised of seven modules which are straightforward and extendable. The applications of TSTMT can be used to facilitate learning in deaf people and to help them to communicate with hearing people via their own language. In this paper the idea and initial experiment are presented.","","0-7695-2495-8","10.1109/ICMLA.2005.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1607467","","Deafness;Speech synthesis;Handicapped aids;Auditory system;Natural languages;Automatic speech recognition;Machine learning;Image recognition;Internet;Avatars","natural languages;language translation;image recognition;image sequences;text analysis","Thai sign translation;Thai machine translation;Thai sign language;Thai text;image sequence recognition;sign word;sentence synthesis;natural language processing;deaf people communication learning","","2","","17","IEEE","20 Mar 2006","","","IEEE","IEEE Conferences"
"Bidirectional Sign Language Translation","A. Kanvinde; A. Revadekar; M. Tamse; D. R. Kalbande; N. Bakereywala","Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India","2021 International Conference on Communication information and Computing Technology (ICCICT)","12 Aug 2021","2021","","","1","5","Communication is a tedious task for people who have hearing and speech impairment. Such people are assisted by a sign language translator for communicating with people. This however, is a slow process of translation, and many a times, it leads to loss of context in conversations, further leading to lack of involvement in social events. This research work proposes a portable and 24x7 available system with support for bidirectional translation i.e. from sign language to speech and speech to sign language. The portability of this system is assured with the use of a smartphone application, which is constantly available with the user, with no requirement of additional hardware. The mobile application will give normal speech output as audio and text and sign language output as a 3D animated video sequence, with the help of Unity3D. With the help of Machine Learning, the system will utilize a pre-trained model made with a combination of Convolution and Recurrent Neural Networks.","","978-1-6654-0430-3","10.1109/ICCICT50803.2021.9510146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510146","Sign language translation;Computer Vision;Mobile application;Unity3D;Machine Learning;Video sequences","Three-dimensional displays;Recurrent neural networks;Assistive technology;Video sequences;Gesture recognition;Machine learning;Tools","computer animation;handicapped aids;language translation;learning (artificial intelligence);mobile computing;neural nets;recurrent neural nets;smart phones;video signal processing","hearing;speech impairment;sign language translator;social events;portable x7 available system;24x7 available system;bidirectional translation i.e;normal speech output;sign language output;bidirectional sign language translation","","4","","12","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"The Multilingual Hive: Neural Machine Assistance As A Linguistic Expertise","A. Chakole; D. Gawande; S. Wazalwar","Department of Information Technology, G H Raisoni College of Engineering, Nagpur, India; Department of Information Technology, G H Raisoni College of Engineering, Nagpur, India; Center of Excellence Information Security, G H Raisoni College of Engineering, Nagpur, India","2023 11th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)","19 Jun 2023","2023","","","1","5","The fundamental of translation of language is natural processing. This paper talks about the detection, translation and as well as the speech translation and detection of a language into the desired language. In the past years, there has been work done upon such translation projects. Multilingual Hive holds the advantage of translating a multilingual sentence into a monolingual sentence with accurate results, also it can the same in context of speech. In the past era, statistical methods were used to translate the monolingual text. Multilingual hive compares the textual content from the source language while translating it to the destination language. Nowadays, in this busy era sometimes all of us may enter a sentence without spaces and this sentence without spaces can also be translated into a readable sentence with accuracy. It grasps the textual content to replicate the text from source to target language. Comparing and then grasping the meanings of the textual content are the basic steps in the translation.","2157-0485","979-8-3503-4842-2","10.1109/ICETET-SIP58143.2023.10151611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10151611","Hive;Multilingual;Monolingual;Speech;Detection;Translation","Backpropagation;Image segmentation;Statistical analysis;Grasping;Speech recognition;Information processing;Linguistics","computational linguistics;language translation;linguistics;natural language processing;statistical analysis;text analysis","desired language;destination language;linguistic expertise;monolingual sentence;monolingual text;multilingual hive;multilingual sentence;natural processing;neural machine assistance;paper talks;readable sentence;source language;speech translation;textual content;translation projects","","","","29","IEEE","19 Jun 2023","","","IEEE","IEEE Conferences"
"A Hybrid Translation System from Turkish Spoken Language to Turkish Sign Language","D. Kayahan; T. G√ºng√∂r","Computer Engineering, Boƒüazi√ßi University, ƒ∞stanbul, Turkey; Computer Engineering, Boƒüazi√ßi University, ƒ∞stanbul, Turkey","2019 IEEE International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)","29 Jul 2019","2019","","","1","6","Sign language is the primary tool of communication for deaf and mute people. It employs hand gestures, facial expressions, and body movements to state a word or a phrase. Like spoken languages, sign languages also vary among the regions and the cultures. The aim of this study is to implement a machine translation system to convert Turkish spoken language into Turkish Sign Language (TID). The advantages of rule-based and statistical machine translation techniques are combined into a hybrid translation system.","","978-1-7281-1862-8","10.1109/INISTA.2019.8778347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778347","sign language;TiD;hybrid translation;rule-based translation;statistical translation","Assistive technology;Gesture recognition;Natural languages;Avatars;Dictionaries;Visualization;Speech recognition","handicapped aids;language translation;natural language processing;sign language recognition","Turkish spoken language;Turkish Sign Language;statistical machine translation techniques;hybrid translation system;deaf people;mute people;rule-based machine translation techniques","","8","","12","IEEE","29 Jul 2019","","","IEEE","IEEE Conferences"
"Name-aware language model adaptation and sparse features for statistical machine translation","W. Wang; H. Li; H. Ji",SRI International; Nuance; Rensselaer Polytechnic Institute,"2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","11 Feb 2016","2015","","","324","330","We propose approaches improving statistical machine translation (SMT) performance, by developing name-aware language model adaptations and sparse features, in addition to extracting name-aware translation grammar and rules, adding name phrase table, and name translation driven decoding. Chinese-English translation experiments showed that our proposed approaches produce an absolute gain of +2.3 BLEU on top of our previous high-performing, name-aware machine translation system.","","978-1-4799-7291-3","10.1109/ASRU.2015.7404812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404812","statistical machine translation;name translation;sparse features;language model adaptation","Decoding;Adaptation models;Training;Computational modeling;Recurrent neural networks;Feature extraction;Grammar","language translation;natural language processing","name-aware language model adaptation;sparse features;statistical machine translation;SMT;name-aware translation grammar;name-aware translation rules;name phrase table;name translation driven decoding;Chinese-English translation experiments","","","","27","IEEE","11 Feb 2016","","","IEEE","IEEE Conferences"
"English to Turkish Example-Based Machine Translation with Synchronous SSTC","N. Deniz ALP; C. Turhan","Software Development Department, Tepe Teknolojik Servisler A. ≈ü, Ankara, Turkey; Department of Computer Engineering, Atilim University, Ankara, Turkey","Fifth International Conference on Information Technology: New Generations (itng 2008)","18 Apr 2008","2008","","","674","679","Example Based Machine Translation (EBMT) is a corpus-based method which is based on using previously translated text. In this project, English to Turkish EBMT system has been developed which uses the Synchronous SSTC for the representation of the sentences in the parallel corpora. This method proves to be effective especially in structurally different languages such as English and Turkish.","","978-0-7695-3099-4","10.1109/ITNG.2008.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492559","Machine Translation;Example Based Machine Translation;Synchronous SSTC","Natural languages;Synchronous generators;Tree data structures;Information technology;Programming;Character generation;Computational linguistics;Software;Speech","natural languages","English-to-Turkish example-based machine translation;synchronous SSTC;example based machine translation;corpus-based method;translated text;parallel corpora","","2","","13","IEEE","18 Apr 2008","","","IEEE","IEEE Conferences"
"On difficulties of Chinese-English machine translation","Xiao-Shen Zheng; Pi-Lian He; Mei Tian; Zhong Wang; Yue-Heng Sun","Department of Computer Science and Technology, Tianjin University, TJU, China; Department of Computer Science and Technology, Tianjin University, TJU, China; Tianjin Institute of Science and Technology Information, China; Department of Computer Science and Technology, Tianjin University, TJU, China; Department of Computer Science and Technology, Tianjin University, TJU, China","Proceedings. International Conference on Machine Learning and Cybernetics","19 Feb 2003","2002","4","","1747","1750 vol.4","This paper analyses the difficulties in the process of Chinese-English machine translation, especially in Chinese grammar parsing and Chinese-English transform, and gives some suggestions.","","0-7803-7508-4","10.1109/ICMLC.2002.1175336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175336","","Dictionaries;Natural languages;Helium;Sun;Computer science;Application software;Consumer electronics;Information retrieval;Speech recognition;Character recognition","language translation;grammars;trees (mathematics)","semantics tree;Chinese lexical analysis;syntactic analysis;machine translation;Chinese English translation;Chinese grammar parsing","","","","7","IEEE","19 Feb 2003","","","IEEE","IEEE Conferences"
"Designing a primary science education support system based on Super Function","Yusuke Konishi; Rui-Fan Li; Fu-Ji Ren","School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China","2009 International Conference on Machine Learning and Cybernetics","25 Aug 2009","2009","4","","2409","2413","In this paper, a new method for constructing a primary science education support system based on Super Function is presented. And the outline of an experimental system based on the proposed method is described. In this system, the input sentence is analyzed and the science problem statements are generated automatically using Super Function from the field of machine translation. The system can give more interesting problems and answers so that students can immerse themselves in a more pleased learning environment. Moreover, speech recognition and voice synthesis are embedded to be used for the system interface.","2160-1348","978-1-4244-3702-3","10.1109/ICMLC.2009.5212191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212191","Learning system;Natural language processing;Educational support system","Natural languages;Machine learning;Cybernetics;Computer science education;Systems engineering education;Speech recognition;Speech synthesis;Displays;Conference management;Computer science","educational administrative data processing;language translation;speech recognition;speech synthesis","primary science education support system;super function;machine translation;speech recognition;voice synthesis","","","","13","IEEE","25 Aug 2009","","","IEEE","IEEE Conferences"
"MT-based artificial hypothesis generation for unsupervised discriminative language modeling","E. Dikici; M. Sara√ßlar","Department of Electrical and Electronics Engineering, Bogazici University, Bebek, Istanbul, Turkey; Department of Electrical and Electronics Engineering, Bogazici University, Bebek, Istanbul, Turkey","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","1401","1405","Discriminative language modeling (DLM) is used as a postprocessing step to correct automatic speech recognition (ASR) errors. Traditional DLM training requires a large number of ASR N-best lists together with their reference transcriptions. It is possible to incorporate additional text data into training via artificial hypothesis generation through confusion modeling. A weighted finite-state transducer (WFST) or a machine translation (MT) system can be used to generate the artificial hypotheses. When the reference transcriptions are not available, training can be done in an unsupervised way via a target output selection scheme. In this paper we adapt the MT-based artificial hypothesis generation approach to un-supervised discriminative language modeling, and compare it with the WFST-based setting. We achieve improvements in word error rate of up to 0.7% over the generative baseline, which is significant at p <; 0.001.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362614","Discriminative language model;confusion model;machine translation;unsupervised training","Training;Adaptation models;Data models;Europe;Signal processing;Speech;Manuals","finite state machines;language translation;speech recognition","unsupervised discriminative language modeling;automatic speech recognition errors;ASR errors;DLM training;confusion modeling;weighted finite-state transducer;WFST;machine translation system;MT system;target output selection scheme;MT-based artificial hypothesis generation approach","","","","22","","28 Dec 2015","","","IEEE","IEEE Conferences"
"A hybrid part-of-speech tagger for Marathi sentences","M. M. Deshpande; S. D. Gore","Department of Computer Science, Savitribai Phule Pune University, Pune, India; Department of Statistics, Savitribai Phule Pune University, Pune, India","2018 International Conference on Communication information and Computing Technology (ICCICT)","29 Mar 2018","2018","","","1","10","With thousands of languages in the world, and the increasing speed and quantity of information being distributed across the world, automatic translation between languages by computers, Machine Translation, has become an increasingly important area of research. For a machine to translate text in one natural language to target text in another language, it requires an understanding of the language, its grammar (syntax), its meaning (semantics) and the ability to use this knowledge for making inferences. Words have definite meaning(s) making them deterministic and finite. Words are not ambiguous in their meaning. Context dependency arises when a word is used with a group of words (bag-of-words) in a specific way that causes its meaning to be dependent on the group of words. In this paper, we present a hybrid, multi-pass Part-Of-Speech (POS) tagger developed for Marathi sentences which builds feature vector for each word in a sentence by referring to the previous and next word preceding and succeeding the current word that is being tagged. The analysis of the Marathi input sentence is done first by tokenizing each word in the sentence and finding the stem for each token. Every token is analyzed for its POS tag, the tense, mood and aspect. This process is POS tagging. Ambiguities may arise in the process of tagging.","","978-1-5386-2051-9","10.1109/ICCICT.2018.8325898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325898","POS Ambiguity;Machine Translation;Marathi Grammar rules;Part-of-Speech (POS);Root/Stem of a Word;Stemming;Tagger;YASS","Grammar;Dictionaries;Computers;Tagging;Silicon;Mood;Feature extraction","language translation;natural language processing","Marathi input sentence;part-of-speech tagger;automatic translation;natural language;bag-of-words;machine translation;POS tagging;tagging process","","3","","","IEEE","29 Mar 2018","","","IEEE","IEEE Conferences"
"Improving Phrase-Based Statistical Machine Translation with Preprocessing Techniques","S. Yashothara; R. T. Uthayasanker; S. Jayasena","Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka; Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka","2018 International Conference on Asian Language Processing (IALP)","31 Jan 2019","2018","","","322","327","This work presents an improvement to phrase-based statistical machine translation models which incorporates linguistic knowledge, namely parts-of-speech information and preprocessing techniques. Any Statistical Machine Translation (SMT) System needs large parallel corpora for exact performance. So, non-availability of corpora limits the success achievable in machine translation to and from those languages. In this study, we choose Sinhala to Tamil translation which gains importance since both of them are acknowledged as official languages of Sri Lanka and also resource-poor languages. Even though findings presented here is for Sinhala to Tamil translation, the concept of pre-processing is language neutral and can be transcended to any other language pair with different parameters. To overcome the translation challenges in traditional SMT, preprocessing techniques are used. Preprocessing described in the research is related to generating phrasal units, Parts of Speech (POS) integration and segmentation. At the end, automatic evaluation of the system is performed by using BLEU as evaluation metrics. We observed all preprocessing techniques outperform the baseline system. The best performance is reported with PMI based chunking for Sinh ala to Tamil translation. We could improve performance by 12% BLEU (3.56) using a small Sinhala to Tamil corpus with the help of proposed PMI based preprocessing. Notably, this increase is significantly higher compared to the increase shown by prior approaches for the same language pair.","","978-1-7281-1175-9","10.1109/IALP.2018.8629203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629203","Statistical Machine Translation;Preprocessing techniques;Sinhala to Tamil translation;Chunking;Segmentation;Natural language Processing","Linguistics;Syntactics;Tokenization;Standards;Semantics;Tagging;Data mining","language translation;natural language processing;statistical analysis","language neutral;language pair;preprocessing techniques;phrase-based statistical machine translation models;parts-of-speech information;parallel corpora;corpora limits;official languages;resource-poor languages;statistical machine translation system;PMI based chunking;Sinhala to Tamil translation","","1","","21","IEEE","31 Jan 2019","","","IEEE","IEEE Conferences"
"Impact of Machine Learning on Regional Languages Processing: A Survey","G. Spandan; S. H. Brahmananda","Dept. of Computer Science and Engineering, GITAM University Bengaluru Campus, Bengaluru, India; Dept. of Computer Science and Engineering, GITAM University Bengaluru Campus, Bengaluru, India","2023 2nd International Conference for Innovation in Technology (INOCON)","19 Apr 2023","2023","","","1","8","Machine Translation System refers to the process of automatically or with minimal human involvement converting text from one language form to another using computer technology (MTS). Rich data and increased processing speed by infrastructure have led to appealing solutions for complex issues in a variety of fields, including Speech Processing, Image Processing, Natural Language Processing, Machine Learning, and others languages.Regional languages of India are said to have limited resources.Traditional MT methodologies are challenging to use in the Indian context because to a lack of data and the rich morphological structure of Indian languages. In this study, several strategies and techniques for processing Indian language presented by researchers are examined.","","979-8-3503-2092-3","10.1109/INOCON57975.2023.10101023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101023","Machine Translation;Natural Language Processing","Technological innovation;Image processing;Machine learning;Machine translation;Speech processing","language translation;learning (artificial intelligence);natural language processing;speech processing","appealing solutions;complex issues;computer technology;Image Processing;Indian language;language form;Machine Learning;Machine Translation System;minimal human involvement converting text;Natural Language Processing;others languages;regional languages Processing;rich data;rich morphological structure;Speech Processing;traditional MT methodologies","","","","49","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"Extreme Learning Machine for Automatic Language Identification Utilizing Emotion Speech Data","M. A. Abbood Albadr; S. Tiun; M. Ayob; F. T. Al-Dhief; T. -A. N. Abdali; A. F. Abbas","CAIT, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; CAIT, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; CAIT, Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; School of Electrical Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; Centre for Cyber Security, Faculty of Information Science and Technology (FTSM), Universiti Kebangsaan Malaysia, Bangi, Malaysia; School of Electrical Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia","2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)","27 Aug 2021","2021","","","1","6","The technique used for recognizing a language by utilizing pronounced speech is called spoken Language Identification (LID). This field has a high significance in the interaction between human and computer. Besides, it can be implemented in several applications such as call centers, speaker diarization in multilingual environments, and in translation systems using a speech-to-speech manner. However, most studies that used LID systems are used and focused on neutral speech only. Moreover, the application of emotional speech in LID systems is crucial in real applications. Therefore, this study aims to investigate the performance of Extreme Learning Machine (ELM) in LID system by utilizing emotional speech. The system is evaluated based on two different languages (Germany and English language). This study has used the Berlin Emotional Speech Dataset (BESD) for the Germany language while the Ryerson Audio-Visual Dataset of Emotional Speech and Song (RAVDESS) for the English language. Four different evaluation scenarios (All Dataset (AD), Normal-Speech Dependent (N-SD), Gender-Female Dependent (G-FD), and Gender-Male Dependent (G-MD) scenario) have been conducted in order to evaluate the system. The experiments results have shown that the highest performance was achieved an accuracy of 99.08%, 100.00%, 98.22%, and 99.37% for AD, N-SD, G-FD, and G-MD scenario, respectively.","","978-1-6654-3897-1","10.1109/ICECCE52056.2021.9514107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9514107","Language Identification;ELM;Emotional Speech Dataset;Classification","Extreme learning machines;Speech recognition","emotion recognition;learning (artificial intelligence);speaker recognition;speech processing;speech recognition","Extreme Learning Machine;automatic Language Identification utilizing emotion Speech data;pronounced speech;translation systems;speech-to-speech manner;used LID systems;neutral speech;LID system;English language;Berlin Emotional Speech Dataset;Germany language","","3","","33","IEEE","27 Aug 2021","","","IEEE","IEEE Conferences"
"A Loss-Augmented Approach to Training Syntactic Machine Translation Systems","T. Xiao; D. F. Wong; J. Zhu","College of Computer Science and Engineering, Northeastern University, Shenyang, China; Department of Computer and Information Science, University of Macau, Macau, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","11","2069","2083","Current syntactic machine translation (MT) systems implicitly use beam-width unlimited search in learning model parameters (e.g., feature values for each translation rule). However, a limited beam-width has to be adopted in decoding new sentences, and the MT output is in general evaluated by various metrics, such as BLEU and TER. In this paper, we address: 1) the mismatch of adopted beam-widths between training and decoding; and 2) the mismatch of training criteria and MT evaluation metrics. Unlike previous work, we model the two problems in a single training paradigm simultaneously. We design a loss-augmented approach that explicitly considers the limited beam-width and evaluation metric in training, and present a simple but effective method to learn the model. By using beam search and BLEU-related losses, our approach improves a state-of-the-art syntactic MT system by +1.0 BLEU on Chinese-to-English and English-to-Chinese translation tasks. It even outperforms seven previous training approaches over 0.8 BLEU points. More interestingly, promising improvements are observed when our approach works with TER.","2329-9304","","10.1109/TASLP.2016.2594383","National Science Foundation of China(grant numbers:61272376,61300097,61432013); Science and Technology Development Fund; FDCT; University of Macau(grant numbers:057/2014/A,MYRG2015-00175-FST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523295","Loss-augmented training;machine translation;syntax-based model","Training;Syntactics;Measurement;Speech;Speech processing;Maximum likelihood decoding","language translation","English-to-Chinese translation task;Chinese-to-English translation task;BLEU-related losses;beam search;MT evaluation metric;TER metric;BLEU metric;MT system;syntactic machine translation system;loss-augmented approach","","4","","62","IEEE","27 Jul 2016","","","IEEE","IEEE Journals"
"Spoken language translation","A. Waibel; C. Fugen","Technical Faculty ‚ÄúMihajlo Pupin‚Äù, University of Novi Sad, Zrenjanin, Serbia; Technical Faculty ‚ÄúMihajlo Pupin‚Äù, University of Novi Sad, Zrenjanin, Serbia","IEEE Signal Processing Magazine","18 Apr 2008","2008","25","3","70","79","In this article we have reviewed state-of-the-art speech translation systems. We have discussed issues of performance as well as deployment, and we reviewed the history and technical underpinnings of this growing and challenging research area. The field provides a plethora of fascinating research challenges for scientists as well as opportunities for true impact in the society of tomorrow.","1558-0792","","10.1109/MSP.2008.918415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4490203","","Natural languages;Automatic speech recognition;Speech synthesis;Error analysis;Speech processing;Humans;Speech enhancement;Face recognition;Microphones;Convergence","language translation;linguistics;speech recognition;speech synthesis","spoken language translation;speech translation system;cross-lingual human-human communication;automatic speech recognition;text-to-speech synthesis;machine translation","","10","5","34","IEEE","18 Apr 2008","","","IEEE","IEEE Magazines"
"Factored phrase-based statistical machine translation","D. Tufis; A. Ceausu","Research Institute for Artificial Intelligence, Romanian Academy of Sciencies, Bucharest, Romania; Research Institute for Artificial Intelligence, Romanian Academy of Sciencies, Bucharest, Romania","2009 Proceedings of the 5-th Conference on Speech Technology and Human-Computer Dialogue","6 Jul 2009","2009","","","1","7","We describe the results of a short-term SEE-ERAnet project the aim of which was to investigate the feasibility of machine translation (MT) research and development for several South Slavic and Balkan languages. The major tasks of the project were: compilation of a multilingual parallel corpus for the concerned languages, the XML mark-up of the corpus (tokenization, lemmatization, tagging), the sentence and word alignment of the corpus and the building of the statistical translation models. Additionally, based on the created resources and models, we conducted preliminary experiments on building prototype MT systems for Romanian ‚â™-‚â´ English, Greek ‚â™-‚â´ English and Slovene ‚â™-‚â´ English. We argue that by investing efforts in building accurate language resources, larger the better, as well as in fine-tuning of the statistical parameters, the current machine-learning technologies can be successfully used for a quick development of acceptable MT prototypes, valuable starting points in implementing working systems. We substantiate this claim with recent results from a follow-up national project, aiming at the development of a Romanian‚â™-‚â´ English translation system.","","978-1-4244-4727-5","10.1109/SPED.2009.5156180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156180","alignment;machine translation;MOSES decoder;language model;translation model","Natural languages;XML;Tagging;Prototypes;Artificial intelligence;Research and development;Decoding;Training data;Learning systems","language translation;learning (artificial intelligence);natural language processing","factored phrase-based machine translation;statistical machine translation;multilingual parallel corpus;machine-learning technologies;national project;language translation;South Slavic languages;Balkan languages","","","","15","","6 Jul 2009","","","IEEE","IEEE Conferences"
"Improving Structural Statistical Machine Translation for Sign Language With Small Corpus Using Thematic Role Templates as Translation Memory","H. -Y. Su; C. -H. Wu","Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Transactions on Audio, Speech, and Language Processing","14 Jul 2009","2009","17","7","1305","1315","This paper presents a structural statistical machine translation (SSMT) model to deal with the data sparseness problem that occurs as a result of the necessarily small corpus to translate Chinese into Taiwanese Sign Language (TSL). A parallel bilingual corpus was developed, and linguistic information from the Sinica Treebank is adopted for Chinese sentence analysis. The synchronous context free grammar (SCFG) was adopted to convert a Chinese structure to the corresponding TSL structure and then extract a translation memory which comprises the thematic relations between the grammar rules of both structures. In structural translation, the statistical MT (SMT) approach was used to align the thematic roles in the grammar rules and the translation memory provides the reference templates for TSL structure translation. Finally, the agreement information for TSL verbs was labeled for enriching the expressiveness of the translated TSL sequence. Several experiments were conducted to evaluate the translation performance and the communication effectiveness for the deaf. The evaluation results demonstrate that the proposed approach outperforms a baseline statistical MT system using the same small corpus, especially for the translation of long sentences.","1558-7924","","10.1109/TASL.2009.2016234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165114","Sign language;small corpus;structural statistical machine translation (SSMT);thematic relation","Handicapped aids;Deafness;Natural languages;Surface-mount technology;Auditory system;Avatars;Information analysis;Data mining;Communication effectiveness;Employment","computational linguistics;context-free grammars;language translation;natural language processing;statistical analysis","structural statistical machine translation model;Taiwanese sign language;small corpus;thematic role template;translation memory;data sparseness problem;parallel bilingual corpus;linguistic information;Sinica Treebank;Chinese sentence analysis;synchronous context free grammar","","17","","35","IEEE","14 Jul 2009","","","IEEE","IEEE Journals"
"On automatic voice casting for expressive speech: Speaker recognition vs. speech classification","N. Obin; A. Roebel; G. Bachman","STMS, IRCAM-CNRS-UPMC, Paris, France; STMS, IRCAM-CNRS-UPMC, Paris, France; ExeQuo - The Production & Localization Company, Paris, France","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","950","954","This paper presents the first large-scale automatic voice casting system, and explores the adaptation of speaker recognition techniques to measure voice similarities. The proposed system is based on the representation of a voice by classes (e.g., age/gender, voice quality, emotion). First, a multi-label system is used to classify speech into classes. Then, the output probabilities for each class are concatenated to form a vector that represents the vocal signature of a speech recording. Finally, a similarity search is performed on the vocal signatures to determine the set of target actors that are the most similar to a speech recording of a source actor. In a subjective experiment conducted in the real-context of voice casting for video games, the multi-label system clearly outperforms standard speaker recognition systems. This indicates evidence that speech classes successfully capture the principal directions that are used in the perception of voice similarity.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6853737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853737","voice casting;voice similarity;speaker recognition;speech classification","Speech;Speaker recognition;Speech recognition;Support vector machines;Casting;Vectors;Acoustics","computer games;language translation;natural language processing;probability;signal classification;speaker recognition;speech processing","large-scale automatic voice casting system;expressive speech;speaker recognition technique;speech classification;voice similarity measurement;multilabel system;output probabilities;vocal signatures;speech recording;similarity search;video games;voice similarity perception;voice representation","","6","","32","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Improved statistical models for SMT-based speaking style transformation","G. Neubig; Y. Akita; S. Mori; T. Kawahara","School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; School of Informatics, Kyoto University, Sakyo, Kyoto, Japan; School of Informatics, Kyoto University, Sakyo, Kyoto, Japan","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5206","5209","Automatic speech recognition (ASR) results contain not only ASR errors, but also disfluencies and colloquial expressions that must be corrected to create readable transcripts. We take the approach of statistical machine translation (SMT) to ‚Äútranslate‚Äù from ASR results into transcript-style text. We introduce two novel modeling techniques in this framework: a context-dependent translation model, which allows for usage of context to accurately model translation probabilities, and log-linear interpolation of conditional and joint probabilities, which allows for frequently observed translation patterns to be given higher priority. The system is implemented using weighted finite state transducers (WFST). On an evaluation using ASR results and manual transcripts of meetings of the Japanese Diet (national congress), the proposed methods showed a significant increase in accuracy over traditional modeling techniques.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5494997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494997","speaking style transformation;log-linear models;weighted finite state transducers","Automatic speech recognition;Context modeling;Surface-mount technology;Probability;Transducers;Parameter estimation;Informatics;Error correction;Interpolation;Manuals","finite state machines;language translation;speech recognition","SMT based speaking style transformation;automatic speech recognition;statistical machine translation;context dependent translation model;weighted finite state transducer;log linear interpolation","","7","","10","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Effective Exploitation of Posterior Information for Attention-Based Speech Recognition","J. Tang; J. Hou; Y. Song; L. -R. Dai; I. Mcloughlin","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","IEEE Access","18 Jun 2020","2020","8","","108988","108999","End-to-end attention-based modeling is increasingly popular for tackling sequence-to-sequence mapping tasks. Traditional attention mechanisms utilize prior input information to derive attention, which then conditions the output. However, we believe that knowledge of posterior output information may convey some advantage when modeling attention. A recent technique proposed for machine translation called the posterior attention model (PAM) demonstrates that posterior output information can be used in that way for machine translation. This paper explores the use of posterior information for attention modeling in an automatic speech recognition (ASR) task. We demonstrate that direct application of PAM to ASR is unsatisfactory, due to two deficiencies; Firstly, PAM adopts attention based weighted single-frame output prediction by assuming a single focused attention variable, whereas wider contextual information from acoustic frames is important for output prediction in ASR. Secondly, in addition to the well-known exposure bias problem, PAM introduces additional mismatches in attention training and inference calculations. We present extensive experiments combining a number of alternative approaches to solving these problems, leading to a high performance technique which we call extended PAM (EPAM). To counter the first deficiency, EPAM modifies the encoder to introduce additional context information for output prediction. The second deficiency is overcome in EPAM through a two part solution of a mismatch penalty term and an alternate learning strategy. The former applies a divergence-based loss to correct the mismatch bias distribution, while the latter employs a novel update strategy which relies on introducing iterative inference steps alongside each training step. In experiments with both WSJ-80hrs and Switchboard-300hrs datasets we found significant performance gains. For example, the full EPAM system model achieved a word error rate (WER) of 10.6% on the WSJ eval92 test set, compared to 11.6% for traditional prior-attention modeling. Meanwhile, on the Switchboard eval2000 test set, we achieved 16.3% WER compared to the traditional method WER of 17.3%.","2169-3536","","10.1109/ACCESS.2020.3001636","National Key R&D Program of China(grant numbers:2017YFB1002202); National Natural Science Foundation of China(grant numbers:U1836219); Key Science and Technology Project of Anhui Province(grant numbers:18030901016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9114877","Speech recognition;posterior attention;divergence penalty;exposure bias;alternate learning strategy","Training;Decoding;Speech recognition;Acoustics;Optimization;Task analysis","inference mechanisms;language translation;learning (artificial intelligence);speech recognition","EPAM system model;divergence-based loss;inference calculations;attention training;single-frame output prediction;ASR;automatic speech recognition;PAM;posterior attention model;machine translation;posterior output information;sequence-to-sequence mapping tasks;attention-based speech recognition","","2","","47","CCBY","11 Jun 2020","","","IEEE","IEEE Journals"
"Exploring Multi-Stage Information Interactions for Multi-Source Neural Machine Translation","Z. Lu; X. Li; Y. Liu; C. Zhou; J. Cui; B. Wang; M. Zhang; J. Su","School of Informatics, Xiamen University, Xiamen, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Tsinghua University, Beijing, China; School of Informatics, Xiamen University, Xiamen, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Soochow University, Suzhou, China; School of Informatics, Xiamen University, Xiamen, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Feb 2022","2022","30","","562","570","Existing studies for multi-source neural machine translation (NMT) either separately model different source sentences or resort to the conventional single-source NMT by simply concatenating all source sentences. However, there exist two drawbacks in these approaches. First, they ignore the explicit word-level semantic interactions between source sentences, which have been shown effective in the embeddings of multilingual texts. Second, multiple source sentences are simultaneously encoded by an NMT model, which is unable to fully exploit the semantic information of each source sentence. In this paper, we explore multi-stage information interactions for multi-source NMT. Specifically, we first propose a multi-source NMT model that performs information interactions at the encoding stage. Its encoder contains multiple semantic interaction layers, each of which sequentially consists of (1) monolingual semantic interaction sub-layer, which is based on the self-attention mechanism and used to learn word-level monolingual contextual representations of source sentences, and (2) cross-lingual semantic interaction sub-layer, which leverages word alignments to perform fine-grained semantic transitions among hidden states of different source sentences. Furthermore, at the training stage, we introduce a mutual distillation based training framework, where single-source models and ours perform information interactions. Such framework can fully exploit the semantic information of each source sentence to enhance our model. Extensive experimental results on the WMT14 English-German-French dataset show our method exhibits significant improvements upon competitive baselines.","2329-9304","","10.1109/TASLP.2021.3120592","National Natural Science Foundation of China(grant numbers:62036004,61672440); Natural Science Foundation of Fujian Province of China(grant numbers:2020J06001); Youth Innovation Fund of Xiamen(grant numbers:3502Z20206059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576577","Information interactions;mutual distillation;natural language processing;neural machine translation","Semantics;Training;Machine translation;Encoding;Speech processing;Adaptation models;Transformers","information retrieval;language translation;learning (artificial intelligence);natural language processing;neural nets;text analysis","multistage information interactions;multisource NMT model;performs information interactions;multiple semantic interaction layers;source sentence;cross-lingual semantic interaction sub-layer;single-source models;semantic information;multisource neural machine translation;explicit word-level semantic interactions","","1","","43","IEEE","15 Oct 2021","","","IEEE","IEEE Journals"
"Open morphological machine translation: Bangla to English","M. S. Rahman; Muhammad Firoz Mridha; Sangita Rani Poddar; Mohammad Nurul Huda","Department of Computer Science, Stamford University, Dhaka, Bangladesh; Department of Computer Science, Stamford University, Dhaka, Bangladesh; Department of Computer Science, Stamford University, Dhaka, Bangladesh; Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh","2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)","22 Nov 2010","2010","","","460","465","The increase of Internet users all over the world and the subsequent growth of available multilingual information on the Web have brought new challenges to machine translation systems. It is really difficult to build up a complete Machine Translation System for natural languages. This paper represents a new solution that can be useful for building a MT system for converting the Bangla sentences into English sentences. In this paper we deal with the root word of Bangla sentence. We show here how to use the root word to translate the Bangla sentence into English Sentence. We deal here first to find out the root word from the database. Here is also an alternative to use the morphological analyzing. After that we define the parts of speech of that sentence. Then we detect the proper grammatical structure of that sentence. Then we find the grammar for the targeted language. The system will be able to translate error-free and ambiguous sentences correctly and will indicate if there is any error. Hopefully this paper will be helpful for the MT researchers to build and efficient MT System for Bangla Language.","","978-1-4244-7818-7","10.1109/CISIM.2010.5643495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643495","Machine Translation (MT);Source Sentence (SS);Target Sentence (TS);Syntactic Transfer","Grammar;Computer architecture;Birds;Computer science;Algorithm design and analysis;Computers;Information systems","Internet;language translation;natural languages","open morphological machine translation;multilingual information;Web;natural languages;Bangla sentences;English sentences;grammatical structure;error-free sentences;ambiguous sentences","","4","","14","IEEE","22 Nov 2010","","","IEEE","IEEE Conferences"
"Bilingual Recurrent Neural Networks for improved statistical machine translation","B. Zhao; Y. -C. Tam","SRI International; SRI International, Menlo Park, CA, USA","2014 IEEE Spoken Language Technology Workshop (SLT)","2 Apr 2015","2014","","","66","70","Recurrent Neural Networks (RNN) have been successfully applied for improved speech recognition and statistical machine translation (SMT) for N-best list re-ranking. In SMT, we investigate using bilingual word-aligned sentences to train a bilingual recurrent neural network model. We employ a bag-of-word representation of a source sentence as additional input features in model training. Experimental results show that our proposed approach performs consistently better than recurrent neural network language model trained only on target-side text in terms of machine translation performance. We also investigate other input representation of a source sentence based on latent semantic analysis.","","978-1-4799-7129-9","10.1109/SLT.2014.7078551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7078551","Bilingual recurrent neural network model;statistical machine translation","Abstracts;Engines;Artificial neural networks;Joints","language translation;natural language processing;recurrent neural nets","statistical machine translation;SMT;bilingual word-aligned sentences;bilingual recurrent neural network model;bag-of-word representation;recurrent neural network language model;neural network training;target side text;latent semantic analysis","","4","","23","IEEE","2 Apr 2015","","","IEEE","IEEE Conferences"
"Dysarthric Speech Enhancement Based on Convolution Neural Network","S. Wang; Y. Tsao; W. Zheng; H. Yeh; P. Li; S. Fang; Y. Lai","Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Department of Biomedical Engineering, National Yang Ming Chiao Tung university, Taipei, Taiwan; Department of Biomedical Engineering, National Yang Ming Chiao Tung university, Taipei, Taiwan; Department of Audiology and speech language pathology, Macky Medical College, New Taipei City, Taiwan; Department of Electrical Engineering, Yuan Ze University, Taoyuan, Taiwan; Department of Biomedical Engineering, National Yang Ming Chiao Tung university, Taipei, Taiwan","2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","8 Sep 2022","2022","","","60","64","Generally, those patients with dysarthria utter a distorted sound and the restrained intelligibility of a speech for both human and machine. To enhance the intelligibility of dysarthric speech, we applied a deep learning-based speech enhancement (SE) system in this task. Conventional SE approaches are used for shrinking noise components from the noise-corrupted input, and thus improve the sound quality and intelligibility simultaneously. In this study, we are focusing on reconstructing the severely distorted signal from the dysarthric speech for improving intelligibility. The proposed SE system prepares a convolutional neural network (CNN) model in the training phase, which is then used to process the dysarthric speech in the testing phase. During training, paired dysarthric-normal speech utterances are required. We adopt a dynamic time warping technique to align the dysarthric-normal utter-ances. The gained training data are used to train a CNN - based SE model. The proposed SE system is evaluated on the Google automatic speech recognition (ASR) system and a subjective listening test. The results showed that the proposed method could notably enhance the recognition performance for more than 10% in each of ASR and human recognitions from the unprocessed dysarthric speech. Clinical Relevance‚Äî This study enhances the intelligibility and ASR accuracy from a dysarthria speech to more than 10%","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871531","","Training;Time-frequency analysis;Neural networks;Training data;Focusing;Speech enhancement;Internet","convolutional neural nets;deep learning (artificial intelligence);speech enhancement;speech intelligibility;speech recognition","dysarthric speech enhancement;convolution neural network;distorted sound;restrained intelligibility;deep learning-based speech enhancement system;conventional SE approaches;noise-corrupted input;severely distorted signal;SE system;convolutional neural network model;paired dysarthric-normal speech utterances;dysarthric-normal utter-ances;SE model;Google automatic speech recognition system;unprocessed dysarthric speech;dysarthria speech","Auditory Perception;Dysarthria;Humans;Neural Networks, Computer;Sound;Speech","","","36","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Spell My Name: Keyword Boosted Speech Recognition","N. Jung; G. Kim; J. S. Chung","Naver Corporation, South Korea; Naver Corporation, South Korea; Korea Advanced Institute of Science and Technology, South Korea","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6642","6646","Recognition of uncommon words such as names and technical terminology is important to understanding conversations in context. However, the ability to recognise such words remains a challenge in modern automatic speech recognition (ASR) systems.In this paper, we propose a simple but powerful ASR decoding method that can better recognise these uncommon keywords, which in turn enables better readability of the results. The method boosts the probabilities of given keywords in a beam search based on acoustic model predictions. The method does not require any training in advance.We demonstrate the effectiveness of our method on the LibriSpeeech test sets and also internal data of real-world conversations. Our method significantly boosts keyword accuracy on the test sets, while maintaining the accuracy of the other words, and as well as providing significant qualitative improvements. This method is applicable to other tasks such as machine translation, or wherever unseen and difficult keywords need to be recognised in beam search.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747714","contextual biasing;speech recognition;keyword boosting;keyword score;beam search","Training;TV;Terminology;Training data;Signal processing algorithms;Speech recognition;Acoustics","speech recognition","keyword boosted speech recognition;automatic speech recognition systems;beam search;acoustic model predictions;LibriSpeeech test sets;ASR decoding method;machine translation","","3","","26","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Semantic based orthographic with prepositional phrase for English-Tamil translation","S. Suganthi; P. Bamarukmani; K. G. Srinivasagan; M. Saravanan","National Engineering College, Computer Science & Engineering, Kovilpatti, India; National Engineering College, Computer Science & Engineering, Kovilpatti, India; National Engineering College, Computer Science & Engineering, Kovilpatti, India; National Engineering College, Computer Science & Engineering, Kovilpatti, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","21 Mar 2013","2012","","","1","6","Machine translation quality has improved substantially in recent years. Many researchers have contributed and developed quite good algorithms, frameworks and models for various languages. Processing of natural language involves various levels, complexities, ambiguities arise at each of those levels. Some pragmatic and semantic approaches can be used to tackle these issues. Generally, prepositions are plays sound role in meaningful translation for any languages. While translating from English to Tamil, preposition in English sentences should be translated to postposition to have meaningful sentences. Thus the prepositional phase errors and orthographic errors are the major issue in machine translation. The main goal of this paper is to improve the translation quality. To achieve the goal, we use some semantic rule to correct the prepositional and orthographic errors in this English-Tamil translation. The proposed approach takes as input an English sentence containing a preposition and the correct postposition and correct spelling in Tamil for that particular sentence context as output. The rules are evaluated with corpus data and the performance is good. The outcomes of the results are compared with Google Translate.","","978-1-4673-4369-5","10.1109/IHCI.2012.6481845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481845","Rule-Based Machine Translation;POS Tagging;Word Reordering;PENN Tree;Prepositions;Orthographic","Semantics;Google;Speech;Computer science;Educational institutions;Accuracy;Pragmatics","computational linguistics;grammars;language translation;natural language processing;semantic networks","semantic based orthographic;Google translate;corpus data;sentence context;spelling;semantic rule;orthographic errors;prepositional phase errors;meaningful sentences;English sentences;semantic approach;pragmatic approach;natural language;machine translation quality;English-Tamil translation;prepositional phrase","","2","","14","IEEE","21 Mar 2013","","","IEEE","IEEE Conferences"
"Progress of artificial intelligence in Japan","M. Nagao","Department of Electrical Engineering, Kyoto University, Sakyo, Kyoto, Japan","Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications","6 Aug 2002","1988","","","3","6","The different approaches to artificial intelligence in Japan and the United States, as connected with the national characters of these two countries, are examined. The state of the art of information processing in Japan is reviewed, with attention given to character recognition, picture and image processing, robot vision, speech recognition and synthesis, machine translation, and speech translation. Research themes for the development of an automatic speech translation telephone system are considered.<>","","","10.1109/AIIA.1988.13261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=13261","","Artificial intelligence;Speech synthesis;Information processing;Character recognition;Image processing;Robot vision systems;Robotics and automation;Speech recognition;Speech processing;Telephony","artificial intelligence;character recognition;computer vision;computerised picture processing;language translation;speech recognition;speech synthesis","picture processing;speech synthesis;artificial intelligence;Japan;United States;information processing;character recognition;image processing;robot vision;speech recognition;machine translation;speech translation;automatic speech translation telephone system","","","","4","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Exploring Diverse Features for Statistical Machine Translation Model Pruning","M. Tu; Y. Zhou; C. Zong","Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China; Chinese Academy of Sciences, Institute of Automation, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","11","1847","1857","In phrase-based and hierarchical phrase-based statistical machine translation systems, translation performance depends heavily on the size and quality of the translation table. To meet the requirements of making a real-time response, some research has been performed to filter the translation table. However, most existing methods are always based on one or two constraints that act as hard rules, such as not allowing phrase-pairs with low translation probabilities. These approaches sometimes make constraints rigid because they consider only a single factor instead of composite factors. Based on the considerations above, in this paper, we propose a machine learning-based framework that integrates multiple features for translation model pruning. Experimental results show that our framework is effective by pruning 80% of the phrase-pairs and 70% of the hierarchical rules, while retaining the quality of the translation models when using the BLEU evaluation metric. Our study further shows that our method can select the most useful phrase-pairs and rules, including those that are low in frequency but still very useful.","2329-9304","","10.1109/TASLP.2015.2456413","Natural Science Foundation of China(grant numbers:61333018,61403379); International Science and Technology Cooperation Program of China(grant numbers:2014DFA11350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156075","Classification;statistical machine translation (SMT);syntactic constraints;translation model pruning","Syntactics;Decoding;Training data;Training;Data models;Bidirectional control;IEEE transactions","filtering theory;language translation;learning (artificial intelligence)","statistical machine translation model pruning;diverse features;hierarchical phrase;real-time response;translation table filter;hard rules;composite factors;machine learning;BLEU evaluation metric;phrase-pairs","","3","","34","IEEE","14 Jul 2015","","","IEEE","IEEE Journals"
"An Empirical Study on Task-Oriented Dialogue Translation","S. Liu",Macao Polytechnic Institute,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7558","7562","Translating conversational text, in particular task-oriented dialogues, is an important application task for machine translation technology. However, it has so far not been extensively explored due to its inherent characteristics including data limitation, discourse, informality and personality. In this paper, we systematically investigate advanced models on the task-oriented dialogue translation task, including sentence-level, document-level and non-autoregressive NMT models. Be-sides, we explore existing techniques such as data selection, back/forward translation, larger batch learning, finetuning and domain adaptation. To alleviate low-resource problem, we transfer general knowledge from four different pre-training models to the downstream task. Encouragingly, we find that the best model with mBART pre-training pushes the SOTA performance on WMT20 English-German and IWSLT DIALOG Chinese-English datasets up to 62.67 and 23.21 BLEU points, respectively.1","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413521","Task-Oriented Dialogue;Neural Machine Translation;Multilingual;Pre-Training;Discourse.","Adaptation models;Systematics;Conferences;Training data;Signal processing;Data models;Acoustics","interactive systems;language translation;learning (artificial intelligence);natural language processing","conversational text;particular task-oriented dialogues;important application task;machine translation technology;data limitation;task-oriented dialogue translation task;nonautoregressive NMT models;different pre-training models;downstream task","","","","29","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning","H. Zhang; N. Si; Y. Chen; W. Zhang; X. Yang; D. Qu; W. -Q. Zhang","School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","27 Feb 2023","2023","31","","1075","1086","The end-to-end speech translation (E2E-ST) model has gradually become a mainstream paradigm due to its low latency and less error propagation. However, it is non-trivial to train such a model well due to the task complexity and data scarcity. The speech-and-text modality differences result in the E2E-ST model performance usually inferior to the corresponding machine translation (MT) model. Based on the above observation, existing methods often use sharing mechanisms to carry out implicit knowledge transfer by imposing various constraints. However, the final model often performs worse on the MT task than the MT model trained alone, which means that the knowledge transfer ability of this method is also limited. To deal with these problems, we propose the FCCL (Fine- and Coarse- Granularity Contrastive Learning) approach for E2E-ST, which makes explicit knowledge transfer through cross-modal multi-grained contrastive learning. A key ingredient of our approach is applying contrastive learning at both sentence- and frame-level to give the comprehensive guide for extracting speech representations containing rich semantic information. In addition, we adopt a simple whitening method to alleviate the representation degeneration in the MT model, which adversely affects contrast learning. Experiments on the MuST-C benchmark show that our proposed approach significantly outperforms the state-of-the-art E2E-ST baselines on all eight language pairs. Further analysis indicates that FCCL can free up its capacity from learning grammatical structure information and force more layers to learn semantic information.","2329-9304","","10.1109/TASLP.2023.3244521","National Natural Science Foundation of China(grant numbers:62171470,62276153); Central Plains Science and Technology Innovation Leading Talent Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10042965","Speech translation;contrastive learning;end-to-end","Task analysis;Semantics;Training;Data mining;Feature extraction;Data models;Training data","language translation;learning (artificial intelligence);natural language processing","-text modality differences;contrast learning;corresponding machine translation model;cross-modal multigrained contrastive learning;data scarcity;E2E-ST model performance;end-to-end speech translation;makesexplicit knowledge;MT model;MT task;outimplicit knowledge;speech representations;state-of-the-art E2E-ST baselines;task complexity","","2","","70","IEEE","13 Feb 2023","","","IEEE","IEEE Journals"
"A special parser for learning English composition - Error analysis & learners' model for ILTS","L. Chen; N. Tokuda","Computer Science Department, University of Northern British Columbia, Prince George, BC, Canada; R&D Center, SunFlare Company, Tokyo, Japan","2009 International Conference on Machine Learning and Cybernetics","25 Aug 2009","2009","6","","3696","3700","By embedding the function of automatically correcting nearly free format English translations of given Japanese sentences, we have developed a new ILTS(intelligent language tutoring system) for improving English Writing Skills in a L2 tutoring environment. In addition to a diagnostic engine capable of identifying grammatical, lexical and word usage errors of students' input translations and returning error contingent feedback, we have developed a simple table look-up parser for displying the grammar trees. The table look-up parser parses a user input, which is always erronous, by simply matching the extended part-of-speech tag sequence of a closing sentence in a template of ILTS to the entries of a look-up-table, in which each extended part-of-speech tag array corresponds to one grammar tree. The complexity of the table look-up parser is O(n), where n denotes the length of a sentence. This shows a marked improvement over the general purpose parser of which the complexity is O(n3).","2160-1348","978-1-4244-3702-3","10.1109/ICMLC.2009.5212797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5212797","intelligent language learning system;machine learning;machine translation;natural language processing","Error analysis;Machine learning;Cybernetics","computational complexity;grammars;intelligent tutoring systems;language translation;learning (artificial intelligence);linguistics;natural language processing;trees (mathematics)","learning English composition;error analysis;learners model;English translations;Japanese sentences;intelligent language tutoring system;English Writing Skills;L2 tutoring environment;error contingent feedback;table look-up parser;grammar trees;part-of-speech tag sequence;machine learning;machine translation;natural language processing","","1","","7","IEEE","25 Aug 2009","","","IEEE","IEEE Conferences"
"Potential of embedded vision platforms in development of spatial AI enabled CPS","B. Brkljaƒç; B. Antiƒá; Z. Mitroviƒá","Department of Power, Electronic and Telecommunication Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Department of Power, Electronic and Telecommunication Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Department of Power, Electronic and Telecommunication Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia","2022 11th Mediterranean Conference on Embedded Computing (MECO)","21 Jun 2022","2022","","","1","4","Motivated by the recent trends in the field of em-bedded vision platforms, we discuss potential of such solutions in providing foundations for the next generation of Cyber-Physical Systems (CPS). Improved capabilities and reduced price of these platforms will have profound effect on their everyday usage and applications. In comparison to speech and natural language processing, which have established speech recognition and machine translation applications as indispensable in many contemporary CPSs, the vision community is still searching for an application that would be so necessary and desirable to make most of the consumers buy specific vision hardware just to run it. That would be the ultimate proof of the core value of the technology in the market. Thus, also vision problems come with a longstanding tradition and history of numerous solutions, it is still hard to point out a single application that would incorporate many specific vision tasks into one device, and which would be ubiquitously useful and affordable to all (e.g. like smartphone has done in the fields of communication and personal computing). However, with development of new miniaturization technologies and spatial AI it is reasonable to expect that there will be more possibilities for designing CPS with capabilities of visual understanding of outdoor, dynamic and uncontrolled environments. One step in such direction are embedded vision platforms that besides powerful computing capabilities also provide multimodal perception, and thus improve the algorithm performance. As an example, we will discuss stereo depth perception in the context of new spatial AI platforms like OAK-D lite, and point out some possibilities for its improvement and integration into future CPS.","2637-9511","978-1-6654-6828-2","10.1109/MECO55406.2022.9797078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797078","CPS;embedded vision;multimodal perception;spatial AI;depth from stereo;OAK-D lite","Performance evaluation;Visualization;Speech recognition;Market research;Machine translation;History;Artificial intelligence","computer vision;embedded systems;knowledge based systems;language translation;natural language processing;speech recognition;stereo image processing","embedded vision platforms;CPS;em-bedded vision platforms;Cyber-Physical Systems;everyday usage;natural language processing;speech recognition;machine translation applications;vision community;specific vision hardware;vision problems;single application;specific vision tasks;spatial AI platforms","","","","36","IEEE","21 Jun 2022","","","IEEE","IEEE Conferences"
"Phoneme-based English-Amharic Statistical Machine Translation","M. G. Teshome; L. Besacier; G. Taye; D. Teferi","IT Doctoral Program, Addis Ababa University, Addis Ababa, Ethiopia; University Joseph Fourier, Grenoble, France; Addis Ababa University, Addis Ababa, Ethiopia; Addis Ababa University, Addis Ababa, Oromia, ET","AFRICON 2015","19 Nov 2015","2015","","","1","5","This research considers the application of Statistical method to automatic Machine Translation (MT) from English to Amharic. The research focuses on improving the translation quality by applying phonemic transcription on the target side, which is Amharic. Accordingly, the BLEU score results for the phoneme-based EASMT system is 37.53% a gain of 2.21 BLEU point from another baseline phrase-based EASMT with a BLEU score result of 35.32%. This clearly shows that phoneme-based translation outperforms the baseline system.","2153-0033","978-1-4799-7498-6","10.1109/AFRCON.2015.7331921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331921","","Vocabulary;Training;Electronic mail;Computers;Morphology;Tuning;Medical services","language translation;speech processing;statistical analysis","phoneme-based English-Amharic statistical machine translation;automatic machine translation;translation quality;phonemic transcription;phoneme-based EASMT system;baseline phrase-based EASMT;phoneme-based translation","","6","","10","IEEE","19 Nov 2015","","","IEEE","IEEE Conferences"
"Improve User Experience on Web for Machine Translation System Using Storm","R. Ahmad; P. Kumar; A. Kumar; M. K. Sinha; B. D. Chaudhary","Language Technology Research Center (LTRC), International Institute of Information Technology, Hyderabad, India; Expert Software Consultants Ltd, New Delhi, India; Expert Software Consultants Ltd, New Delhi, India; Expert Software Consultants Ltd, New Delhi, India; School of Computing and Electrical Engineering, Indian Institute of Technology, Mandi, India","2014 IEEE Fourth International Conference on Big Data and Cloud Computing","9 Feb 2015","2014","","","243","248","A transfer based Machine Translation (MT) system is a large complex functional application where the job completion time is proportional to job size. When these applications are deployed on web with increasing translation load web user experience degrades. The end user has to wait unusually longer to get his first visible response. Generic layer 3 load balancing techniques does not help to improve the response time as each job is assigned similar compute resources irrespective of job size. This paper presents an engineering approach to deploy MT system on cloud platform using Storm, a distributed computing framework. This scheme, by utilizing the inherent parallelism of a functional application, not only reduces the job completion time considerably but it, also as a web application, gives very good user experience, viz., the first visible response time within an acceptable time limit, and the subsequent responses well before the user finishes perusing the preceding response. Using Storm framework a translation job is split into multiple job partitions and is streamed into the storm cluster such that first job partitions of all jobs are processed before the second partitions, i.e., All nth partitions of all jobs are processed before (n+1)th partitions. Thus machine translation system is able to produce translation output as a continuous stream, sentence by sentence, as soon as each sentence gets translated. The system maintains flow rate of translated sentences stream high enough so that the next translated sentence is produced well before the end user finishes reading the previous sentence, thereby providing very good user experience. There is a class of natural language processing (NLP) applications, viz., machine translation systems, text to speech systems, speech recognition systems, etc., that are functional in nature, and this engineering approach would be equally applicable to them as well.","","978-1-4799-6719-3","10.1109/BDCloud.2014.88","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034793","NLP;Machine Translation;Storm Framework;Stream Processing;Interactive Response;Throughput;Performance","Storms;Time factors;Topology;Fasteners;Parallel processing;Speech recognition;Natural language processing","cloud computing;language translation;natural language processing;pattern clustering","transfer based machine translation system;MT system;user experience improvement;complex functional application;job completion time;job size;distributed computing framework;Web application;cloud platform;first visible response time;acceptable time limit;translation job;multiple job partitions;storm cluster;end user;natural language processing applications;NLP applications;text to speech systems;speech recognition systems;engineering approach","","1","","15","IEEE","9 Feb 2015","","","IEEE","IEEE Conferences"
"Studies on International Discourse Construction of Chinese Literary Classics with the Approach of Computer-assisted Translation","C. Yang; D. Xu; X. Shen","School of Foreign Languages, Wuhan Business University, WBU, Wuhan; College of Civil Engineering, Hubei Urban Construction Vocational and Technological College, Wuhan; School of Foreign Languages, Wuhan Business University, WBU, Wuhan","2021 2nd International Conference on Information Science and Education (ICISE-IE)","29 Mar 2022","2021","","","1123","1126","To show more of the Chinese literary classics to the outside world, boost their influences and improve speech rights of Chinese nation in the new era, this dissertation, with the method of computer-assisted translation (CAT), concentrates on international discourse construction of Chinese literary classics by analyzing successful translation cases of Chinese literary classics as well as their common reasons for translation success, and strategies of discourse construction of Chinese Literary Classics. The study finds out scholars are dedicating to publicizing Chinese culture for long, that studies on medio-translatology have been paid much attention since 2015 in China, that The Analects of Confucius , Lv Xun's works, Shih Chi, Book of Poetry etc. are some successful translation cases we can refer to; that more elements like political background, cultural belief, potential readers' needs should be covered while choosing a source text and that some strategies of discourse construction are recommended.","","978-1-6654-3829-2","10.1109/ICISE-IE53922.2021.00254","Wuhan Business University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9742404","international discourse construction;Chinese literary classics;international rights of speech;CAT","Information science;Education;Linguistics;Machine translation;Cultural differences","Internet;language translation;literature;politics;text analysis","Chinese Literary Classics;computer-assisted translation;Chinese literary classics;international discourse construction;successful translation cases","","","","8","IEEE","29 Mar 2022","","","IEEE","IEEE Conferences"
"Translating natural language utterances to search queries for SLU domain detection using query click logs","D. Hakkani-T√ºr; G. Tur; R. Iyer; L. Heck","Speech Laboratories, Microsoft, Mountain View, CA, USA; Speech Laboratories, Microsoft, Mountain View, CA, USA; Speech Laboratories, Microsoft, Mountain View, CA, USA; Speech Laboratories, Microsoft, Mountain View, CA, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4953","4956","Logs of user queries from a search engine (such as Bing or Google) together with the links clicked provide valuable implicit feedback to improve statistical spoken language understanding (SLU) models. However, the form of natural language utterances occurring in spoken interactions with a computer differs stylistically from that of keyword search queries. In this paper, we propose a machine translation approach to learn a mapping from natural language utterances to search queries. We train statistical translation models, using task and domain independent semantically equivalent natural language and keyword search query pairs mined from the search query click logs. We then extend our previous work on enriching the existing classification feature sets for input utterance domain detection with features computed using the click distribution over a set of clicked URLs from search engine query click logs of user utterances with automatically translated queries. This approach results in significant improvements for domain detection, especially when detecting the domains of user utterances that are formulated as natural language queries and effectively complements to the earlier work using syntactic transformations.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289031","domain detection;spoken language understanding;search query click logs;machine translation","Natural languages;Feature extraction;Keyword search;Search engines;Web search;Syntactics;Error analysis","feedback;language translation","natural language utterances;search queries;SLU domain detection;query click logs;search engine;Bing;Google;valuable implicit feedback;statistical spoken language understanding;SLU models;machine translation approach;URLs;syntactic transformations","","2","8","15","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Topic-Based Coherence Modeling for Statistical Machine Translation","D. Xiong; M. Zhang; X. Wang","Institute for Infocomm Research, Singapore; Institute for Infocomm Research, Singapore; Provincial Key Laboratory for Computer Information Processing Technology, Soochow University, Suzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","26 Feb 2015","2015","23","3","483","493","Coherence that ties sentences of a text into a meaningfully connected structure is of great importance to text generation and translation. In this paper, we propose topic-based coherence models to produce coherence for document translation, in terms of the continuity of sentence topics in a text. We automatically extract a coherence chain for each source text to be translated. Based on the extracted source coherence chain, we adopt a maximum entropy classifier to predict the target coherence chain that defines a linear topic structure for the target document. We build two topic-based coherence models on the predicted target coherence chain: 1) a word level coherence model that helps the decoder select coherent word translations and 2) a phrase level coherence model that guides the decoder to select coherent phrase translations. We integrate the two models into a state-of-the-art phrase-based machine translation system. Experiments on large-scale training data show that our coherence models achieve substantial improvements over both the baseline and models that are built on either document topics or sentence topics obtained under the assumption of direct topic correspondence between the source and target side. Additionally, further evaluations on translation outputs suggest that target translations generated by our coherence models are more coherent and similar to reference translations than those generated by the baseline.","2329-9304","","10.1109/TASLP.2015.2395254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050388","Text coherence;text analysis;coherence chain;topic modeling;statistical machine translation (SMT);natural language processing","Coherence;Predictive models;Hidden Markov models;Training data;Data models;NIST;Decoding","language translation;natural language processing;pattern classification;statistical analysis;text analysis","topic-based coherence modeling;statistical machine translation;text generation;text translation;document translation;sentence topics continuity;source text;extracted source coherence chain;maximum entropy classifier;linear topic structure;target coherence chain;word level coherence model;coherent word translations;phrase level coherence model;coherent phrase translations;phrase-based machine translation system;large-scale training data","","16","","33","IEEE","26 Feb 2015","","","IEEE","IEEE Journals"
"An algorithm for the better assessment of machine translation","P. Malik; A. S. Baghel","Computer Science Department, Gautam Buddha University, Greater Noida, India; Computer Science Department, Gautam Buddha University, Greater Noida, India","2017 International Conference on Computing, Communication and Automation (ICCCA)","21 Dec 2017","2017","","","395","399","Machine Translation, sometimes referred by the acronym MT, is one of the important fields of study of computational linguistics that investigates the use of computer software to translate text or speech from one natural language to another. At its basic level, MT performs simple substitution of atomic words in one natural language for words in another language. Around the world, numerous systems are available in the market for the assessment of the translation being done by the various translation systems. Even within India, a large number of such evaluation systems are available and a lot of research is still going on to develop a better evaluation system which can beat the results produced by Human Evaluators. Even the main challenge before Indian Researchers is that the evaluation systems which are giving unbeatable results for the translation of Foreign languages (such as German, French, Chinese, etc.) are not even giving considerable results for the translation of Indian Languages (Hindi, Tamil, Telugu, Punjabi, etc.). So at par these evaluation systems cannot be applied as it is to evaluate Machine Translations of Indian Languages. Indian languages require a novel approach because of the relatively unrestricted order of words within a word group. In this paper, we are presenting an algorithm (by incorporating different modules of language models like synonym replacement, root word extraction and shallow parsing) which when applied upon the translation of English to Hindi text gives better evaluation results as compared to those algorithms which do not incorporate all these modules. Moreover, our study is limited to English to Hindi language pair and the testing is being with the corpora of agriculture domain.","","978-1-5090-6471-7","10.1109/CCAA.2017.8229833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229833","","Agriculture;Algorithm design and analysis;Automation;Engines;Speech;Natural languages;Measurement","agriculture;computational linguistics;language translation;natural languages;text analysis","numerous systems;translation systems;evaluation system;Human Evaluators;Indian Researchers;Foreign languages;Indian Languages;machine translation;language models;natural language;atomic words;computational linguistics","","","","18","IEEE","21 Dec 2017","","","IEEE","IEEE Conferences"
"A Remedy For Distributional Shifts Through Expected Domain Translation","J. -C. Gagnon-Audet; S. Shahtalebi; F. Rudzicz; I. Rish","Mila - Qu√©bec AI Institute, Montr√©al, QC, Canada; Vector Institute for Artificial Intelligence, Toronto, Canada; Vector Institute for Artificial Intelligence, Toronto, Canada; Mila - Qu√©bec AI Institute, Montr√©al, QC, Canada","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","4523","4527","Machine learning models often fail to generalize to unseen domains due to the distributional shifts. A family of such shifts, ‚Äúcorrelation shifts,‚Äù is caused by spurious correlations in the data. It is studied under the overarching topic of ‚Äúdomain generalization.‚Äù In this work, we employ multi-modal translation networks to tackle the correlation shifts that appear when data is sampled out-of-distribution. Learning a generative model from training domains enables us to translate each training sample under the special characteristics of other possible domains. We show that by training a predictor solely on the generated samples, the spurious correlations in training domains average out, and the invariant features corresponding to true correlations emerge. Our proposed technique, Expected Domain Translation (EDT), is benchmarked on the Colored MNIST dataset and drastically improves the state-of-the-art classification accuracy by 38% with train-domain validation model selection.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746434","machine learning;domain generalization;out-of-distribution generalization;correlation shift","Training;Correlation;Conferences;Machine learning;Signal processing;Benchmark testing;Acoustics","generalisation (artificial intelligence);language translation;learning (artificial intelligence);pattern classification","distributional shifts;Expected Domain Translation;machine learning models;unseen domains;correlation shifts;spurious data correlations;domain generalization;multimodal translation networks;generative model;training domains;train-domain validation model selection;out-of-distribution sampling;training sample translation;Colored MNIST dataset","","","","21","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Machine translation in the year 2004","K. Knight; D. Marcu","Information Sciences Institute and Department of Computer Science, The Viterbi School of Engineering, University of Southern California, Marina del Rey, CA, USA; Information Sciences Institute and Department of Computer Science, The Viterbi School of Engineering, University of Southern California, Marina del Rey, CA, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","5","","v/965","v/968 Vol. 5","Machine translation (MT) accuracy has recently increased, due to better techniques and to the availability of larger parallel training sets. Statistical MT systems are now able to translate across a wide variety of language pairs. This paper covers the basic elements of state-of-the-art, statistical MT, including modeling, decoding, evaluation, and data preparation.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1416466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416466","","Decoding;Humans;NIST;Page description languages;Resumes;Security;Councils;Computer science;Viterbi algorithm;Dictionaries","statistical analysis;language translation","human language machine translation;automatic statistical training;machine translation accuracy;parallel training sets;statistical machine translation;data preparation;language decoding;language modeling","","8","","19","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Rule-Based and Example-Based Machine Translation from English to Arabic","M. F. Alawneh; T. M. Sembok","Faculty of Information Science and Technology, National University of Malaysia, Bangi, Malaysia; Faculty of Information Science and Technology, National University of Malaysia, Bangi, Malaysia","2011 Sixth International Conference on Bio-Inspired Computing: Theories and Applications","17 Oct 2011","2011","","","343","347","Machine Translation has been defined as the process that utilizes computer software to translate text from one natural language to another. This definition involves accounting for the grammatical structure of each language and using rules, examples and grammars to transfer the grammatical structure of the source language (SL) into the target language (TL). This paper presents English to Arabic approach for translating well-structured English sentences into well-structured Arabic sentences, using a Grammar based and example-translation techniques to handle the problems of ordering and agreement. The proposed methodology is flexible and scalable, the main advantages are: first, a hybrid-based approach combined advantages of rule-based (RBMT) with advantages example-based (EBMT), and second, it can be applied on some other languages with minor modifications. The OAK Parser is used to analyze the input English text to get the part of speech (POS) for each word in the text as a pre-translation process using the C# language, validation rules have been applied in both the database design and the programming code in order to ensure the integrity of data. A major design goal of this system is that it will be used as a stand-alone tool, and can be very well integrated with a general machine translation system for English sentences.","","978-1-4577-1092-6","10.1109/BIC-TA.2011.76","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046925","MT;Agreement;Word reorder;Rule-Based;Example-based;Hybrid-based OAK;Parser;POS","Google;Databases;Grammar;Pragmatics;Educational institutions;Helium;Syntactics","computer software;data integrity;database management systems;grammars;language translation;natural languages","example-based machine translation;rule-based machine translation;computer software;natural language;grammatical structure;source language;target language;English to Arabic approach;English sentences;Arabic sentences;hybrid-based approach;OAK parser;part of speech;pretranslation process;C# language;validation rules;database design;programming code;data integrity;general machine translation system","","5","","11","IEEE","17 Oct 2011","","","IEEE","IEEE Conferences"
"Arabic‚ÄìChinese Neural Machine Translation: Romanized Arabic as Subword Unit for Arabic-sourced Translation","F. Aqlan; X. Fan; A. Alqwbani; A. Al-Mansoub","School of Computer Science and Engineering, Central South University (CSU), Changsha, China; School of Computer Science and Engineering, Central South University (CSU), Changsha, China; School of Computer Science and Engineering, Central South University (CSU), Changsha, China; School of Computer Science and Engineering, South China University of Technology (SCUT), Guangzhou, China","IEEE Access","24 Sep 2019","2019","7","","133122","133135","Morphologically rich and complex languages such as Arabic, pose a major challenge to neural machine translation (NMT) due to the large number of rare words and the inability of NMT to translate them. Unknown word (UNK) symbols are used to represent out-of-vocabulary words because NMT typically operates with a fixed vocabulary size. These rare words can be effectively encoded as sequences of subword units by using algorithms, such as byte pair encoding (BPE), to tackle the UNK problem. However, for languages with highly inflected and morphological variations, such as Arabic, the aforementioned method has its own limitations that make it not effective enough for translation quality. To alleviate the UNK problem and address the inconvenient behavior of BPE when translating the Arabic language, we propose to utilize a romanization system that converts Arabic scripts to subword units. We investigate the effect of our approach on NMT performance under various segmentation scenarios and compare the results with systems trained on original Arabic form. In addition, we integrate Romanized Arabic as an input factor for Arabic-sourced NMT compared with well-known factors, namely, lemma, part-of-speech tags, and morph features. Extensive experiments on Arabic-Chinese translation demonstrate that the proposed approaches can effectively tackle the UNK problem and significantly improve the translation quality for Arabic-sourced translation. Additional experiments in this study focus on developing the NMT system on Chinese-Arabic translation. Before implementing our experiments, we first propose standard criteria for the data filtering of a parallel corpus, which helps in filtering out its noise.","2169-3536","","10.1109/ACCESS.2019.2941161","National Natural Science Foundation of China(grant numbers:61876190,61802120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835016","Arabic morphology;Arabic romanization;BPE;data filtering;linguistic feature;morphological segmentation;neural machine translation","Filtering;Vocabulary;Linguistics;Standards;Training;Economics","language translation;natural language processing;text analysis;vocabulary;word processing","Arabic-Chinese neural machine translation;Romanized Arabic;subword unit;Arabic-sourced translation;rich languages;complex languages;rare words;unknown word symbols;out-of-vocabulary words;fixed vocabulary size;byte pair encoding;UNK problem;highly inflected variations;morphological variations;translation quality;Arabic language;romanization system;Arabic scripts;NMT performance;original Arabic form;Arabic-sourced NMT;Arabic-Chinese translation;NMT system;Chinese-Arabic translation;BPE","","10","","46","CCBY","12 Sep 2019","","","IEEE","IEEE Journals"
"A Tool for Scholarly Phonemic Transcription of Ukrainian Texts","M. Vakulenko","Institute of Problems of Artificial Intelligence, Kyjiv, Ukraine","2022 12th International Conference on Advanced Computer Information Technologies (ACIT)","12 Oct 2022","2022","","","516","519","This article aims to present an instrument for phonemic transcription of Ukrainian texts that is necessary for phonetic studies, lexicographic work, and in text-to-speech conversion, machine translation, information retrieval, automatic speech recognition, language model transfer, and other natural language processing tasks. The tool provides an automatic scholarly transcription of Ukrainian texts into the International Phonetic Alphabet (IPA) and the Extended Speech Assessment Methods Phonetic Alphabet (X-SAMPA).","2770-5226","978-1-6654-1050-2","10.1109/ACIT54803.2022.9913111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913111","phonemic transcription;grapheme;phoneme;IPA;X-SAMPA;voice output communication aids;machine translation;information retrieval;text-to-speech","Instruments;Computational modeling;Phonetics;Information retrieval;Machine translation;Reliability;Task analysis","natural language processing;speech processing;speech recognition;text analysis","international phonetic alphabet;language model transfer;natural language processing tasks;Ukrainian texts;scholarly phonemic transcription;text-to-speech conversion;automatic speech recognition;IPA;international phonetic alphabet;X-SAMPA","","","","19","IEEE","12 Oct 2022","","","IEEE","IEEE Conferences"
"Searching Better Architectures for Neural Machine Translation","Y. Fan; F. Tian; Y. Xia; T. Qin; X. -Y. Li; T. -Y. Liu","School of Computer Science, University of Science and Technology of China, Hefei, China; Facebook Inc, Menlo Park, USA; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; School of Computer Science, University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Jun 2020","2020","28","","1574","1585","Neural architecture search (NAS) has played important roles in the evolution of neural architectures. However, no much attention has been paid to improve neural machine translation (NMT) through NAS approaches. In this work, we propose a gradient-based NAS algorithm for NMT, which automatically discovers architectures with better performances. Compared with previous NAS work, we jointly search the network operations (e.g., LSTM, CNN, self-attention etc) as well as dropout rates to ensure better results. We show that with reasonable resources it is possible to discover novel neural network architectures for NMT, which achieve consistently better performances than Transformer [1], the state-of-the-art NMT model, across different tasks. On WMT'14 English-to-German translation, IWSLT'14 German-to-English translation and WMT'18 Finnish-to-English translation tasks, our discovered architectures could obtain 30.1, 36.1 and 26.4 BLEU scores, which are great improvement over Transformer baselines. We also empirically verify that the discovered model on one task can be transferred to other tasks.","2329-9304","","10.1109/TASLP.2020.2995270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095246","Neural architecture search (NAS);neural machine translation","Computer architecture;Decoding;Task analysis;Speech processing;Network architecture;Neural networks;Optimization","language translation;natural language processing;neural net architecture","Finnish-to-English translation tasks;German-to-English translation;English-to-German translation;NMT model;neural network architectures;gradient-based NAS algorithm;neural machine translation","","11","","43","IEEE","18 May 2020","","","IEEE","IEEE Journals"
"Break prediction of prosody for Hakka'S TTS systems based on data mining approaches","F. -L. Huang; N. -H. Pan; M. -S. Yu; J. -Y. Wu","Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan; Department of Information Management, Chienkuo Technology University, Changhua, Taiwan; Department of Computer Science and Engineering, National Ching-Hsing University, Taichung, Taiwan; Department of Computer Science and Engineering, National Ching-Hsing University, Taichung, Taiwan","2011 International Conference on Machine Learning and Cybernetics","12 Sep 2011","2011","1","","51","55","This paper aims at the prosody generation for Hakka's language based on the data mining approaches, and implement the TTS system on Internet. Our system is composed of the following four components: 1) Text analysis, 2) Mandarin to Hakka word translation, 3) Prosody prediction, and 4) Speech generation module. More than 2427 monosyllabic speech units and 2234 word speech units of Hakka and several silences with various durations have been recorded as basic units for speech synthesis. We focus on adding breaks to speeches, with emphasis on predicting the types of break. There are three kinds of breaks: major break, minor break and no-break between words. We train a break model and predict break based on the data mining approaches - Bayesian network (BN) and CART classifier. The best precision rate for testing achieves 80.17% based on the CART. Fourteen students familiar with Hakka joined to evaluate the prosody quality of synthesized speeches. The results with 10 scale achieves 7.54 score in average. Based on the comprehensive evaluation, it is obvious that our system can synthesize the clear and natural Hakka's speeches.","2160-1348","978-1-4577-0308-9","10.1109/ICMLC.2011.6016704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016704","Hakka Language;Text-to-speech (TTS) system;Prosody Prediction;Data Mining","Speech;Data mining;Machine learning;Cybernetics;Bayesian methods;Testing;Speech processing","Bayes methods;data mining;natural language processing;pattern classification;speech synthesis;text analysis","break prediction;prosody;Hakka TTS systems;text to speech system;data mining;Hakka language;text analysis;word translation;speech generation;monosyllabic speech units;speech synthesis;Bayesian network;CART classifier","","1","","11","IEEE","12 Sep 2011","","","IEEE","IEEE Conferences"
"Effect of linguistic information in neural machine translation","N. Nakamura; H. Isahara","Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan; Information and Media Center, Toyohashi University of Technology, Toyohashi, Japan","2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)","2 Nov 2017","2017","","","1","6","Deep Neural Networks(DNNs) outperform previous works in many fields such as in natural language processing. Neural Machine Translation(NMT) also outperforms Statistical Machine Translation(SMT) which has complex features and rules. However, NMT requires a large corpus and a long calculation time. In order to suppress calculation cost, recent researches replaced low frequency words with symbols. However, the symbols make sentences ambiguous and deteriorates translation accuracy. To solve this problem, sub-word units such as Byte Pair Encoding(BPE) and Wordpiece Model(WPM) creating vocabularies in a prespecified vocabulary size has been proposed. Nevertheless, these tokenize methods break words and treat them as symbols. Words as symbols are compatible with neural networks and NMT performance has increased. This result shows that linguistic correctness is not necessarily important in NMT. If that is the case, we wonder to what extent linguistic correctness contributes to NMT accuracy. In this research, we experiment to incorporate linguistic information into sub-word units. Experimentally, we demonstrate that morpheme as linguistic information is a helpful factor for sub-word units.","","978-1-5386-3001-3","10.1109/ICAICTA.2017.8090975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090975","","Pragmatics;Vocabulary;Decoding;Computer architecture;Mathematical model;Telescopes","language translation;learning (artificial intelligence);linguistics;natural language processing;neural nets;speech recognition;statistical analysis;text analysis","calculation cost;low frequency words;symbols;translation accuracy;sub-word units;prespecified vocabulary size;NMT performance;NMT accuracy;linguistic information;natural language processing;Byte Pair Encoding;BPE;Wordpiece Model;WPM;linguistic correctness;Deep Neural Networks;DNN;Neural Machine Translation;NMT;Statistical Machine Translation;SMT","","1","","16","IEEE","2 Nov 2017","","","IEEE","IEEE Conferences"
"Research and Application on Intelligent Translation Technology for Korean Learning","L. Xuesi","Urban Vocational College of Sichuan, Chengdu, China","2020 13th International Conference on Intelligent Computation Technology and Automation (ICICTA)","13 Sep 2021","2020","","","165","168","In this paper, under the intelligent learning environment, the design of mobile phone English and Korean learning software is studied. First, through the understanding of intelligent learning, smart phone software and the development characteristics of teaching paradigm, this paper explores the possibility of applying mobile phone software to Korean learning based on intelligent learning environment. Then the key technologies of intelligent translation are mainly studied: phrase based statistical machine translation and image-based real-time translation module. The architecture, component model and component configuration of Android system are also studied. For the research of architecture, the basis of hierarchical division and the functions of each level are described, and the concrete implementation method of translation module is provided. Finally, by edge experiments we verify the correct implementation of the real-time translation function, and demonstrate the feasibility and usability of the system.","","978-1-7281-8666-5","10.1109/ICICTA51737.2020.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526724","Korean learning;android;APP;intelligent translation;phrase","Operating systems;Image edge detection;Computer architecture;Speech recognition;Signal processing;Software;Real-time systems","language translation;learning (artificial intelligence);mobile computing;mobile handsets;smart phones;teaching","intelligent translation technology;Korean learning;intelligent learning environment;mobile phone English;smart phone software;mobile phone software;phrase based statistical machine translation;image-based real-time translation module;component model;component configuration;real-time translation function","","","","9","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"Smart Translation for Physically Challenged People Using Machine Learning","R. Hegde; R. M. Chitrashree; N. Dimple; H. G. Shet; J. Deekshitha; S. M. Soumyasri","Dept. of Computer Science and Engineering, Vidyavardhaka College of Engg., Mysore, India; Dept. of Computer Science and Engineering, Vidyavardhaka College of Engg., Mysore, India; Dept. of Computer Science and Engineering, Vidyavardhaka College of Engg., Mysore, India; Dept. of Computer Science and Engineering, Vidyavardhaka College of Engg., Mysore, India; Dept. of Computer Science and Engineering, Vidyavardhaka College of Engg., Mysore, India; Dept. of Computer Science, JSS College of Arts, Commerce and Science, Mysore, India","2022 IEEE International Conference on Data Science and Information System (ICDSIS)","14 Oct 2022","2022","","","1","5","In sign language, hand motions are one of the nonverbal communication modalities employed. It is most typically used by deaf and hard of hearing persons who have hearing or speech impairments. Difficulties communicating with one another or with regular people folks. Several sign language systems have been created by There are numerous producers all throughout the world, but they are neither versatile nor adaptable, end-user-friendly in terms of price. Our project's goal is to create a feasible system for communication for deaf people. This project is divided into two sections. (1) It translates an audio message into sign language, and (2) it translates images/video into text/speech. The first category we'll take input as audio, turns the audio recorded message into text message, and later displays predetermined Indian Sign Language(ISL) visuals or GIFs. The use of this technique facilitates communication between hearing and deaf persons. In the second category, we will gather photographs and train images with CNN and present the results.","","978-1-6654-9801-2","10.1109/ICDSIS55133.2022.9915882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915882","Sign Language;Text/Video Speech;Conversion;CNN algorithm","Visualization;Webcams;Image processing;Neural networks;Gesture recognition;Auditory system;Machine learning","gesture recognition;handicapped aids;learning (artificial intelligence);natural language processing;sign language recognition;speech synthesis;video signal processing","smart translation;physically challenged people;machine learning;hand motions;nonverbal communication modalities;hearing persons;speech impairments;regular people folks;sign language systems;numerous producers;end-user-friendly;project;feasible system;deaf people;audio message;text message;deaf persons","","1","","15","IEEE","14 Oct 2022","","","IEEE","IEEE Conferences"
"Machine translation: a hybrid view","Y. Wilks","University of Sheffield, UK","IEEE Expert","6 Aug 2002","1996","11","2","12","14","MT works, in the sense that everyday MT systems produce fully automatic translations that many people use with apparent benefits. The failure of intellectual breakthroughs to produce high quality, fully automatic MT is also apparent. The author considers statistical and knowledge-based translation.","2374-9407","","10.1109/64.491310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=491310","","Artificial intelligence;Speech;System recovery;Research and development;Laboratories;Natural languages;Hidden Markov models;Microelectronics;Ontologies;Large-scale systems","language translation;humanities;expert systems;statistical analysis","hybrid machine translation;fully automatic translations;statistical translation;knowledge-based translation","","1","","5","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"Extract reordering rules of sentence structure using neuro-fuzzy machine learning system","S. P. Singh; A. Kumar; H. Darbari; A. Rastogi; S. Jain","AAI, Center for development of Advanced Computing, Pune, India; AAI, Center for development of Advanced Computing, Pune, India; AAI, Center for development of Advanced Computing, Pune, India; Banasthali Vidyapith, Banasthali, Rajasthan, India; Banasthali Vidyapith, Banasthali, Rajasthan, India","2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon)","14 May 2018","2017","","","173","177","Recent development and enhancement in the field of soft computing which is the cutting edge of Neural network and fuzzy logic. Here by we are proposing a solution using Deep Neural Network (DNN) and Fuzzy logics for Natural Language Processing (NLP) with is the sub part of Artificial Intelligence (AI). If we look India we have 22 constitutional human languages and in the world around 6909 living human languages. Building cross Machine Translation (MT) system using these all language which currently not possible because of lack of resources, knowledge and etc. If we have huge corpus of source and target language this propose system will automatically find out the grammatical structure of source sentence matching with the target language using Neural network and fuzzy logic and it is flexible machine learning technique it deals with uncertainly or vagueness existing in a system and formulating fuzzy rules to find grammar structure to a given new sentences.","","978-1-5386-0569-1","10.1109/SmartTechCon.2017.8358364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8358364","Neural Network (NN);Fuzzy Logic;POS (Parts of speech) Tagging;word2vec;Natural Language Processing (NLP);Machine Translation (MT)","Databases;Tagging;Machine learning;Fuzzy logic;Biological neural networks;Tools;Natural language processing","fuzzy logic;fuzzy neural nets;fuzzy set theory;language translation;learning (artificial intelligence);natural language processing","grammatical structure;source sentence;fuzzy logic;formulating fuzzy rules;sentence structure;soft computing;natural language processing;artificial intelligence;reordering rules;neuro-fuzzy machine learning system;deep neural network;constitutional human languages;cross machine translation system","","1","","16","IEEE","14 May 2018","","","IEEE","IEEE Conferences"
"Grammar-based and example-based techniques in machine translation from English to Arabic","M. f. Alawneh; T. M. Sembok; M. Mohd","Information Science and Technology, National University of Malaysia, Bangi, Malaysia; Information and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Information Science and Technology, National University of Malaysia, Bangi, Malaysia","2013 5th International Conference on Information and Communication Technology for the Muslim World (ICT4M)","23 May 2013","2013","","","1","6","In the modern world, there is an increased need for language translation. This paper presents English to Arabic approach for translating well-structured English sentences into well-structured Arabic sentences, using a grammar-based and example-translation techniques to handle the problems of ordering and agreement. This technique combines rule-based MT (RBMT) and example-based MT (EBMT) which is called hybrid-based MT (HERBMT). The proposed methodology is flexible and scalable. The main advantages of HERBMT are that it combines the advantages of RBMT and EBMT, and it can be applied to other languages with minor modifications. EBMT extracts an example of target language sentences that are analogous to input source language sentences. The extraction of appropriate translated sentences is preceded by an analysis stage for the decomposition of input sentences into appropriate fragments. RBMT is used when examples of the source language to be translated into the target language are not found in the machine database. The OAK Parser is used to analyze the input English text to get the part of speech (POS) for each word in the text as a pre-translation process. A major design goal of this system is that it will be used as a stand-alone tool, and can be integrated with a general machine translation system for English sentences. The evaluation is carried out on 250 independent test suites, and the analysis indicates that HERBMT achieved good performance with an average of 97.2% precision.","","978-1-4799-0136-4","10.1109/ICT4M.2013.6518910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6518910","MT;Agreement;Word reorder;Rule-Based;Example-based;Hybrid-based;OAK Parser","Grammar;Databases;Syntactics;Google;Educational institutions;Data mining;Pragmatics","knowledge based systems;language translation;natural language processing","grammar-based technique;example-based technique;machine translation;English-to-Arabic translation;language translation;English sentence;Arabic sentence;example-translation technique;rule-based MT;example-based MT;RBMT;EBMT;hybrid-based MT;HERBMT;language sentence;part-of-speech;OAK Parser;POS","","","","12","IEEE","23 May 2013","","","IEEE","IEEE Conferences"
"A maximum entropy based reordering model for Mongolian-Chinese SMT with morphological information","Z. Yang; M. Li; Z. Zhu; L. Chen; L. Wei; S. Wang","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China","2014 International Conference on Asian Language Processing (IALP)","4 Dec 2014","2014","","","175","178","Different order between Mongolian and Chinese and the scarcity of parallel corpus are the main problems in Mongolian-Chinese statistical machine translation (SMT). We propose a method that adopts morphological information as the features of the maximum entropy based phrase reordering model for Mongolian-Chinese SMT. By taking advantage of the Mongolian morphological information, we add Mongolian stem and affix as phrase boundary information and use a maximum entropy model to predict reordering of neighbor blocks. To some extent, our method can alleviate the influence of reordering caused by the data sparseness. In addition, we further add part-of-speech (POS) as the features in the reordering model. Experiments show that the approach outperforms the maximum entropy model using only boundary words information and provides a maximum improvement of 0.8 BLEU score increment over baseline.","","978-1-4799-5330-1","10.1109/IALP.2014.6973484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973484","morphological;machine translation;reordering;maximum entropy","Entropy;Training;Feature extraction;Decoding;Educational institutions;Morphology;Pragmatics","language translation;maximum entropy methods;natural language processing","maximum entropy;Mongolian-Chinese SMT;morphological information;parallel corpus;Mongolian-Chinese statistical machine translation;phrase reordering model;Mongolian stem;Mongolian affix;phrase boundary information;reordering prediction;data sparseness;part-of-speech;POS;boundary words information;BLEU score","","3","","16","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"Data Augmentation for end-to-end Code-Switching Speech Recognition","C. Du; H. Li; Y. Lu; L. Wang; Y. Qian","Department of Computer Science and Engineering AI Institute, MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering AI Institute, MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering AI Institute, MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China; CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology; Department of Computer Science and Engineering AI Institute, MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","194","200","Training a code-switching end-to-end automatic speech recognition (ASR) model normally requires a large amount of data, while code-switching data is often limited. In this paper, three novel approaches are proposed for code-switching data augmentation. Specifically, they are audio splicing with the existing code-switching data, and TTS with new code-switching texts generated by word translation or word insertion. Our experiments on 200 hours Mandarin-English code-switching dataset show that all the three proposed approaches yield significant improvements on code-switching ASR individually. Moreover, all the proposed approaches can be combined with recent popular SpecAugment, and an addition gain can be obtained. WER is significantly reduced by relative 24.0% compared to the system without any data augmentation, and still relative 13.0% gain compared to the system with only SpecAugment.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383620","end-to-end speech recognition;code-switching;data augmentation;text-to-speech","Training;Speech coding;Splicing;Conferences;Data models;Automatic speech recognition","natural language processing;speech recognition","end-to-end code-switching speech recognition;code-switching end-to-end automatic speech recognition model;code-switching data augmentation;code-switching texts;word translation;Mandarin-English code-switching dataset;ASR;SpecAugment;WER;time 200.0 hour","","3","","34","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"The NESPOLE! System for multilingual speech communication over the Internet","A. Lavie; F. Pianesi; L. Levin","Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Istituto Trentino di Cultura-Centro per la Ricerca Scientifica e Tecnologica(ITC-irst), Trento, Italy; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Audio, Speech, and Language Processing","21 Aug 2006","2006","14","5","1664","1673","The NESPOLE! System is a speech communication system designed to support multilingual interaction between common users and providers of e-commerce services over the Internet. The core of the system is a distributed interlingua-based speech-to-speech translation system, which is supported by multimodal capabilities that allow the two parties participating in the communication to share Web pages and graphical content which can be annotated using gestures. We describe the unique features and considerations behind the design and implementation of this system, and evaluate these within the context of a constructed full prototype of the system that was developed for the domain of travel planning","1558-7924","","10.1109/TSA.2005.858520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677986","Distributed processing;machine translation;multimodal interfaces;speech communication","Oral communication;Natural languages;Speech analysis;Robustness;Web and internet services;Web pages;Prototypes;Computer architecture;Personal communication networks;Web server","Internet;language translation;natural languages;speech processing","NESPOLE! system;multilingual speech communication;Internet;e-commerce services;speech-to-speech translation system;multimodal capabilities;Web pages;graphical content;gestures;distributed interlingua","","7","1","29","IEEE","21 Aug 2006","","","IEEE","IEEE Journals"
"Exploring the Role of Language Families for Building Indic Speech Synthesisers","A. Prakash; H. A. Murthy","Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai, India; Department of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India","IEEE/ACM Transactions on Audio, Speech, and Language Processing","13 Jan 2023","2023","31","","734","747","Building end-to-end speech synthesisers for Indian languages is challenging, given the lack of adequate clean training data and multiple grapheme representations across languages. This work explores the importance of training multilingual and multi-speaker text-to-speech (TTS) systems based on language families. The objective is to exploit the phonotactic properties of language families, where small amounts of accurately transcribed data across languages can be pooled together to train TTS systems. These systems can then be adapted to new languages belonging to the same family in extremely low-resource scenarios. TTS systems are trained separately for Indo-Aryan and Dravidian language families, and their performance is compared to that of a combined Indo-Aryan+Dravidian voice. We also investigate the amount of training data required for a language in a multilingual setting. Same-family and cross-family synthesis and adaptation to unseen languages are analysed. The analyses show that language family-wise training of Indic systems is the way forward for the Indian subcontinent, where a large number of languages are spoken.","2329-9304","","10.1109/TASLP.2022.3230453","Department of Science and Technology, Government of India(grant numbers:IMP/2018/000986); Natural Language Translation Mission(grant numbers:11(1)/2020-HCC(TDIL)); Ministry of Electronics and Information Technology (MeitY), GoI; Speech to Speech Machine Translation(grant numbers:SA/NL/2018(G)&(C)); Office of the Principal Scientific Adviser (PSA), GoI; Speech Technologies in Indian Languages(grant numbers:SP/21-22/1960/CSMEIT/003119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9992058","End-to-end speech synthesis;Indian languages;language families;low-resource","Training;Training data;Synthesizers;Hidden Markov models;Buildings;Databases;Adaptation models","learning (artificial intelligence);natural language processing;speech processing;speech synthesis","combined Indo-Aryan+Dravidian voice;cross-family synthesis;Dravidian language families;end-to-end speech synthesisers;Indian languages;Indian subcontinent;Indic speech synthesisers;Indic systems;language family-wise training;multilingual systems;multiple grapheme representations;multispeaker text-to-speech systems;phonotactic properties;same-family synthesis;TTS systems;unseen languages","","2","","75","IEEE","19 Dec 2022","","","IEEE","IEEE Journals"
"Joint Training and Decoding for Multilingual End-to-End Simultaneous Speech Translation","W. Huang; R. Jin; W. Zhang; J. Luan; B. Wang; D. Xiong","Xiaomi AI Lab, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; Xiaomi AI Lab, Beijing, China; College of Intelligence and Computing, Tianjin University, Tianjin, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Recent studies on end-to-end speech translation(ST) have facilitated the exploration of multilingual end-to-end ST and end-to-end simultaneous ST. In this paper, we investigate end-to-end simultaneous speech translation in a one-to-many multilingual setting which is closer to applications in real scenarios. We explore a separate decoder architecture and a unified architecture for joint synchronous training in this scenario. To further explore knowledge transfer across languages, we propose an asynchronous training strategy on the proposed unified decoder architecture. A multi-way aligned multilingual end-to-end ST dataset was curated as a benchmark testbed to evaluate our methods. Experimental results demonstrate the effectiveness of our models on the collected dataset. Our codes and data are available at: https://github.com/XiaoMi/TED-MMST.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095811","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095811","End-to-End ST;Simultaneous Machine Translation;Multilingual","Training;Codes;Signal processing;Benchmark testing;Acoustics;Decoding;Speech processing","","","","","","23","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Word-Region Alignment-Guided Multimodal Neural Machine Translation","Y. Zhao; M. Komachi; T. Kajiwara; C. Chu","Tokyo Metropolitan University, Hino, Japan; Tokyo Metropolitan University, Hino, Japan; Ehime University, Matsuyama, Japan; Kyoto University, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","11 Jan 2022","2022","30","","244","259","We propose word-region alignment-guided multimodal neural machine translation (MNMT), a novel model for MNMT that links the semantic correlation between textual and visual modalities using word-region alignment (WRA). Existing studies on MNMT have mainly focused on the effect of integrating visual and textual modalities. However, they do not leverage the semantic relevance between the two modalities. We advance the semantic correlation between textual and visual modalities in MNMT by incorporating WRA as a bridge. This proposal has been implemented on two mainstream architectures of neural machine translation (NMT): the recurrent neural network (RNN) and the transformer. Experiments on two public benchmarks, English‚ÄìGerman and English‚ÄìFrench translation tasks using the Multi30k dataset and English‚ÄìJapanese translation tasks using the Flickr30kEnt-JP dataset prove that our model has a significant improvement with respect to the competitive baselines across different evaluation metrics and outperforms most of the existing MNMT models. For example, 1.0 BLEU scores are improved for the English‚ÄìGerman task and 1.1 BLEU scores are improved for the English‚ÄìFrench task on the Multi30k test2016 set; and 0.7 BLEU scores are improved for the English‚ÄìJapanese task on the Flickr30kEnt-JP test set. Further analysis demonstrates that our model can achieve better translation performance by integrating WRA, leading to better visual information use.","2329-9304","","10.1109/TASLP.2021.3138719","Grant-in-Aid for Young Scientists(grant numbers:#19K20343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664333","Multi30k;multimodal machine translation;semantic correlation;vision and language;word-region alignment","Graphics;Magnetization;Magnetostatics;Speech processing;Permeability;Image color analysis;Guidelines","language translation;natural language processing;recurrent neural nets","word-region alignment-guided multimodal neural machine translation;semantic correlation;textual modalities;visual modalities;WRA;English-French translation tasks;English-Japanese translation tasks;BLEU scores;MNMT models;Flickr30kEnt-JP test set;visual information use;Multi30k test2016 set","","6","","63","CCBY","28 Dec 2021","","","IEEE","IEEE Journals"
"On Arabic-English cross-language information retrieval: a machine translation approach","M. Aljlayl; O. Frieder; D. Grossman","Information Retrieval Laboratory, Illinois Institute of Technology, USA; Information Retrieval Laboratory, Illinois Institute of Technology, USA; Information Retrieval Laboratory, Illinois Institute of Technology, USA","Proceedings. International Conference on Information Technology: Coding and Computing","7 Aug 2002","2002","","","2","7","Machine translation (MT) is an automatic process that translates from one human language to another by using context information. We empirically evaluate the use of an MT-based approach for query translation in an Arabic-English cross-language information retrieval (CLIR) system, called ALKAFI, using the TREC-7 and TREC-9 topics and collections. The effect of the query length on the MT performance is also investigated in order to explore how much context is actually required for successful MT processing.","","0-7695-1506-1","10.1109/ITCC.2002.1000351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1000351","","Information retrieval;Natural languages;Dictionaries;Writing;Humans;Laboratories;Demography;Tongue;Shape;Speech","language translation;information retrieval;software performance evaluation","Arabic-English cross-language information retrieval;machine translation system;language translation;context information;query translation;ALKAFI;TREC-7 topics;TREC-9 topics;TREC-7 collections;TREC-9 collections;query length;translation performance","","5","","15","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"Korean-Thai Lexicon for Natural Language Processing","S. Mahahing; P. Seresangtakul","Natural Language and Speech Processing Laboratory (NLSP), Department of Computer Science, Faculty of Science, Khon Kaen University, Khon Kaen, Thailand; Natural Language and Speech Processing Laboratory (NLSP), Department of Computer Science, Faculty of Science, Khon Kaen University, Khon Kaen, Thailand","2013 International Conference on Information Science and Applications (ICISA)","15 Aug 2013","2013","","","1","4","This paper presents Korean-Thai lexicon. This research aims to study and collect necessary features to construct the Korean-Thai lexicon for natural language processing (NLP) and speech processing researches. The research method used for study was that of (1) creating Korean-Thai lexicon consisting of 7 parts : Korean words, Korean Revised Romanization, part of speech, sub part of speech, special characteristic, Thai meaning and description of meaning (2) Korean transcription. According to lack of useful tools for the Korean- Thai machine translation, therefore we have a proposal for creating Korean-Thai lexicon for machine translation. The Korean-Thai lexicon consists of 36,000 Korean words. As it would take a lot of time and effort to gather enough Korean words to cover all domains, Korean Revised Romanization was applied for some words such as terminology, names and places.","2162-9048","978-1-4799-0604-8","10.1109/ICISA.2013.6579435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579435","","Speech;Educational institutions;Databases;Printing;Encyclopedias;Natural language processing;Speech processing","language translation;natural language processing;speech processing;word processing","Korean-Thai lexicon;natural language processing;NLP;Korean words;Korean Revised Romanization;subpart-of-speech;special characteristic;Thai meaning description;Korean transcription;Korean-Thai machine translation;terminology;names;places","","1","","13","IEEE","15 Aug 2013","","","IEEE","IEEE Conferences"
"Syntactic Based Machine Translation from English to Malayalam","A. T. Nair; S. M. Idicula","Department of Computer Science, Federal Institute of Science and Technology, Ernakulam, India; Department of Computer Science, Cochin University of Science and Technology, Cochin, India","2012 International Conference on Data Science & Engineering (ICDSE)","27 Aug 2012","2012","","","198","202","Due to the emergence of multiple language support on the Internet, machine translation (MT) technologies are indispensable to the communication between speakers using different languages. Recent research works have started to explore tree-based machine translation systems with syntactical and morphological information. This work aims the development of Syntactic Based Machine Translation from English to Malayalam by adding different case information during translation. The system identifies general rules for various sentence patterns in English. These rules are generated using the Parts Of Speech (POS) tag information of the texts. Word Reordering based on the Syntax Tree is used to improve the translation quality of the system. The system used Bilingual English-Malayalam dictionary for translation.","","978-1-4673-2149-5","10.1109/ICDSE.2012.6282326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6282326","morphology;POS;SBMT","Syntactics;Dictionaries;Helium;Generators;Morphology;Conferences;Pattern recognition","language translation;natural language processing","syntactic based machine translation;multiple language support;Internet;machine translation technology;tree based machine translation system;morphological information;tag information;word reordering;syntax tree;translation quality;bilingual English Malayalam dictionary","","5","","15","IEEE","27 Aug 2012","","","IEEE","IEEE Conferences"
"Why Do Neural Dialog Systems Generate Short and Meaningless Replies? a Comparison between Dialog and Translation","B. Wei; S. Lu; L. Mou; H. Zhou; P. Poupart; G. Li; Z. Jin","Software Institute, Peking University, China; Software Institute, Peking University, China; University of Waterloo; Toutiao AI Lab; University of Waterloo; Software Institute, Peking University, China; Software Institute, Peking University, China","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","7290","7294","This paper addresses the question: In neural dialog systems, why do sequence-to-sequence (Seq2Seq) neural networks generate short and meaningless replies for open-domain response generation? We conjecture that in a dialog system, due to the randomness of spoken language, there may be multiple equally plausible replies for one utterance, causing the deficiency of a Seq2Seq model. To evaluate our conjecture, we propose a systematic way to mimic the dialog scenario in machine translation systems with both real datasets and toy datasets generated elaborately. Experimental results show that we manage to reproduce the phenomenon of generating short and meaningless sentences in the translation setting.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682634","Dialog system;machine translation;sequence-to-sequence;short replies","Toy manufacturing industry;Entropy;Training;Measurement;Neural networks;Adaptation models;Decoding","interactive systems;language translation;natural language processing;neural nets","neural dialog systems;sequence-to-sequence neural networks;open-domain response generation;Seq2Seq model;machine translation systems;spoken language","","3","","20","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Adaptive Adapters: An Efficient Way to Incorporate BERT Into Neural Machine Translation","J. Guo; Z. Zhang; L. Xu; B. Chen; E. Chen","Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Alibaba Damo Academy, Hangzhou, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science, and Technology of China, Hefei, China; Alibaba Damo Academy, Hangzhou, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","31 May 2021","2021","29","","1740","1751","Large-scale pre-trained language models (e.g., BERT) have attracted great attention in recent years. It is straightforward to fine-tune them on natural language understanding tasks such as text classification, however, effectively and efficiently incorporating them into natural language generation tasks such as neural machine translation remains a challenging problem. In this paper, we integrate two pre-trained BERT models from the source and target language domains into a sequence-to-sequence model by introducing light-weight adapter modules. The adapters are inserted between BERT layers and tuned on downstream tasks, while the parameters of BERT models are fixed during fine-tuning. As pre-trained language models are usually very deep, inserting adapters into all layers will result in a considerable scale of new parameters. To deal with this problem, we introduce latent variables to decide whether using adapters or not in each layer, which are learned during fine-tuning. In this way, the model is able to automatically determine which adapters to use, therefore hugely promoting the parameter efficiency and decoding speed. We evaluate the proposed framework on various neural machine translation tasks. Equipped with parallel sequence decoding, our model consistently outperforms autoregressive baselines while reducing the inference latency by half. With automatic adapter selection, the proposed model further achieves 20% speedup while still outperforming autoregressive baselines. When applied to autoregressive decoding, the proposed model can also achieve comparable performance with the state-of-the-art baseline models.","2329-9304","","10.1109/TASLP.2021.3076863","National Natural Science Foundation of China(grant numbers:U20A20229); Natural Science Foundation of Anhui Province(grant numbers:2008085J31); Alibaba Group through Alibaba Research Intern Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420282","Pre-trained language model;adapter;neural machine translation","Adaptation models;Bit error rate;Task analysis;Decoding;Machine translation;Natural languages;Training","decoding;language translation;learning (artificial intelligence);natural language processing;neural nets;text analysis","sequence-to-sequence model;light-weight adapter modules;BERT;downstream tasks;parameter efficiency;neural machine translation tasks;parallel sequence decoding;automatic adapter selection;adaptive adapters;pre-trained language models;natural language understanding tasks;natural language generation tasks","","2","","62","IEEE","30 Apr 2021","","","IEEE","IEEE Journals"
"Hierarchical Transfer Learning Architecture for Low-Resource Neural Machine Translation","G. Luo; Y. Yang; Y. Yuan; Z. Chen; A. Ainiwaer","Xinjiang Laboratory of Minority Speech and Language Information Processing, The Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, The Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, The Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, The Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, The Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China","IEEE Access","5 Nov 2019","2019","7","","154157","154166","Neural Machine Translation(NMT) has achieved notable results in high-resource languages, but still works poorly on low-resource languages. As times goes on, It is widely recognized that transfer learning methods are effective for low-resource language problems. However, existing transfer learning methods are typically based on the parent-child architecture, which does not adequately take advantages of helpful languages. In this paper, inspired by human transitive inference and learning ability, we handle this issue by proposing a new hierarchical transfer learning architecture for low-resource languages. In the architecture, the NMT model is trained in the unrelated high-resource language pair, the similar intermediate language pair and the low-resource language pair in turn. Correspondingly, the parameters are transferred and fine-tuned layer by layer for initialization. In this way, our hierarchical transfer learning architecture simultaneously combines the data volume advantages of high-resource languages and the syntactic similarity advantages of cognate languages. Specially, we utilize Byte Pair Encoding(BPE) and character-level embedding for data pre-processing, which effectively solve the problem of out of vocabulary(OOV). Experimental results on Uygur-Chinese and Turkish-English translation demonstrate the superiorities of the proposed architecture over the NMT model with parent-child architecture.","2169-3536","","10.1109/ACCESS.2019.2936002","National Natural Science Foundation of China(grant numbers:U1703133); Chinese Academy of Sciences(grant numbers:2017-XBQNXZ-A-005); Chinese Academy of Sciences(grant numbers:2017472); Major Science and Technology Project of Xinjiang Uygur Autonomous Region(grant numbers:2016A03007-3); Xinjiang Uygur Autonomous Region Level Talent Introduction Project(grant numbers:Y839031201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805098","Hierarchical transfer learning;low-resource problem;neural machine translation","Computer architecture;Learning systems;Training;Computational modeling;Syntactics;Solid modeling;Decoding","language translation;learning (artificial intelligence);natural language processing;neural nets;vocabulary","OOV;out of vocabulary problem;existing transfer learning methods;low-resource language problems;low-resource neural machine translation;parent-child architecture;cognate languages;high-resource languages;low-resource language pair;similar intermediate language pair;high-resource language pair;NMT model;low-resource languages;hierarchical transfer learning architecture;learning ability;human transitive inference;helpful languages","","9","","56","CCBY","19 Aug 2019","","","IEEE","IEEE Journals"
"Robust Latent Representations Via Cross-Modal Translation and Alignment","V. Rajan; A. Brutti; A. Cavallaro","Centre for Intelligent Sensing, Queen Mary University of London, UK; Fondazione Bruno Kessler, Trento, Italy; Centre for Intelligent Sensing, Queen Mary University of London, UK","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","4315","4319","Multi-modal learning relates information across observation modalities of the same physical phenomenon to leverage complementary information. Most multi-modal machine learning methods require that all the modalities used for training are also available for testing. This is a limitation when signals from some modalities are unavailable or severely degraded. To address this limitation, we aim to improve the testing performance of uni-modal systems using multiple modalities during training only. The proposed multi-modal training framework uses cross-modal translation and correlation-based latent space alignment to improve the representations of a worse performing (or weaker) modality. The translation from the weaker to the better performing (or stronger) modality generates a multi-modal intermediate encoding that is representative of both modalities. This encoding is then correlated with the stronger modality representation in a shared latent space. We validate the proposed framework on the AVEC 2016 dataset (RECOLA) for continuous emotion recognition and show the effectiveness of the framework that achieves state-of- the-art (uni-modal) performance for weaker modalities.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413456","Cross-modal knowledge transfer;multi-modal training uni-modal testing;emotion recognition","Training;Emotion recognition;Speech recognition;Machine learning;Signal processing;Encoding;Task analysis","emotion recognition;language translation;learning (artificial intelligence)","multimodal intermediate encoding;stronger modality representation;robust latent representations;cross-modal translation;multimodal machine learning methods;uni-modal systems;multiple modalities;multimodal training framework;correlation-based latent space alignment;multiple modalities","","2","","19","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Leveraging Large Language Models With Vocabulary Sharing For Sign Language Translation","H. Lee; J. -H. Kim; E. J. Hwang; J. Kim; J. C. Park","NLP*CL Laboratory, School of Computing, KAIST, South Korea; NLP*CL Laboratory, School of Computing, KAIST, South Korea; NLP*CL Laboratory, School of Computing, KAIST, South Korea; NLP*CL Laboratory, School of Computing, KAIST, South Korea; NLP*CL Laboratory, School of Computing, KAIST, South Korea","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","Sign language translation (SLT) is a task that provides translation between spoken and sign languages used in the same country, which tend to show high lexical similarity but low syntactic similarity. The recent emergence of large language models (LLMs) has been remarkable for all downstream tasks in natural language processing, but they have yet to be applied to SLT. In this paper, we explore how to use an LLM with vocabulary sharing for two gloss-based SLT tasks (text-to-gloss (T2G) and gloss-to-text (G2T)) on the NIASL2021 dataset, which consists of 180,848 preprocessed Korean and Korean Sign Language (KSL) sentence pairs. The experimental results showed that Ko-GPT-Trinity-1.2B+VS, a GPT-3-based SLT model with vocabulary sharing, outperformed other SLT models, achieving BLEU-4 scores of 22.06 and 45.89 on T2G and G2T tasks, respectively. We expect that the adoption of an LLM with vocabulary sharing will significantly lessen the resource scarcity problem of SLT.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193533","Institute for Information and communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193533","machine translation;sign language translation;large language model;vocabulary sharing","Vocabulary;Conferences;Gesture recognition;Assistive technologies;Syntactics;Signal processing;Natural language processing","language translation;natural language processing;text analysis","downstream tasks;gloss-to-text;GPT-3-based SLT model;high lexical similarity;Ko-GPT-Trinity-1.2B+VS,;language models;low syntactic similarity;natural language processing;Sign Language translation;Sign language translation;SLT models;SLT tasks;spoken sign languages;text-to-gloss;vocabulary sharing","","","","26","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Computational modelling of personal pronouns for English to Yor√πb√† machine translation system","I. E. Safiriyu; O. A. Akindeji; O. Azeez Isau","Dept of Computer Science & Engineering, Obafemi Awolowo University, Ile-Ife, Osun, NG; Dept of Computer Science & Engineering, Obafemi Awolowo University, Ile-Ife, Osun, NG; Dept of Computer Science & Engineering, Obafemi Awolowo University, Ile-Ife, Osun, NG","2015 SAI Intelligent Systems Conference (IntelliSys)","21 Dec 2015","2015","","","733","741","Translating third person singular pronouns from English to Yor√∫b√° is yet to receive research attention. The issue can be understood if the translation is speech to speech. In the literature, the authors translate sentence without considering the gender of the author or doer as it is in the English (source language) sentence. This does pose a problem to the reader of English to Yor√∫b√° translated sentences. We envisaged that there is need to represent she/he/it differently unlike the way we are translating it now. Presently, she/he/it are translated as √ì. We proposed ways of representing the three third person singular pronouns; b√¨nrin, k√∫nrin and unk√†n. Feminine (She) is b√¨nrin, masculine (he) is k√∫nrin and non-human (it) is unk√†n. We developed English to Yor√∫b√° machine translation (IFEMT1) system that can translate simple English sentence text to Yor√∫b√° sentence text using the existing and proposed third person singular pronouns translation processes. We used rule based approach and python programming language to implement the IFEMT1 system. The system graphical user interface (GUI) can display the English sentences (to be translated), translated Yor√∫b√° sentences for the existing and our proposed translations.","","978-1-4673-7606-8","10.1109/IntelliSys.2015.7361222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361222","Yor√∫b√°;personal-pronoun;b√¨nrin;k√πnrin;unk√†n;rule-based","Databases;Speech;Intelligent systems;Graphical user interfaces;Computer science;Sociology;Statistics","graphical user interfaces;language translation;linguistics;natural language processing;programming languages","system GUI;system graphical user interface;IFEMT1 system;python programming language;third-person singular pronouns translation process;Yoruba machine translation system;English;computational modelling","","2","","21","IEEE","21 Dec 2015","","","IEEE","IEEE Conferences"
"Grounded Sequence to Sequence Transduction","L. Specia; L. Barrault; O. Caglayan; A. Duarte; D. Elliott; S. Gella; N. Holzenberger; C. Lala; S. J. Lee; J. Libovicky; P. Madhyastha; F. Metze; K. Mulligan; A. Ostapenko; S. Palaskar; R. Sanabria; J. Wang; R. Arora","Department of Computing, Imperial College London, London, U.K.; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Department of Computing, Imperial College London, London, U.K.; Department of Signal Theory and Communications, Universitat Politcnica de Catalunya, Barcelona, Spain; Department of Computer Science, University of Copenhagen, Kobenhavn, Denmark; Institute for Language, Cognition and Computation, University of Edinburgh, Edinburgh, U.K.; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Department of Computer Science, University of Sheffield, Sheffield, U.K.; University of Pennsylvania, Philadelphia, USA; Center for Information and Language Processing, LMU Munich, Munich, Germany; Department of Computing, Imperial College London, London, U.K.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA; Johns Hopkins University, Baltimore, USA; Worcester Polytechnic Institute, Worcester, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA; Department of Computing, Imperial College London, London, U.K.; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA","IEEE Journal of Selected Topics in Signal Processing","25 Jun 2020","2020","14","3","577","591","Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality - either speech or text - as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset , a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset , and the general direction of multimodal learning with other datasets as well.","1941-0484","","10.1109/JSTSP.2020.2998415","NSF BIGDATA(grant numbers:IIS-1546482); NSF CRCNS(grant numbers:IIS-1822575); NSF CAREER(grant numbers:IIS-1943251); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103248","Grounding;multimodal machine learning;speech recognition;machine translation;representation learning;summarization","Visualization;Feature extraction;Speech recognition;Task analysis;Training;Adaptation models","language translation;learning (artificial intelligence);speech recognition","machine translation;multimodal architectures;multimodal learning;grounded sequence to sequence transduction;speech recognition","","1","","81","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"A bootstrapping approach for SLU portability to a new language by inducting unannotated user queries","T. Misu; E. Mizukami; H. Kashioka; S. Nakamura; H. Li","National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; Institute for Infocomm Research, Singapore","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4961","4964","This paper proposes a bootstrapping method of constructing a new spoken language understanding (SLU) system in a target language by utilizing statistical machine translation given an SLU module in some source language. The main challenge in this work is to induct unannotated automatic speech recognition results of user queries in the source language collected through a spoken dialog system, which is under public test. In order to select candidate expressions from among erroneous translation results stemming from problems with speech recognition and machine translation, we use back-translation results to check whether the translation result maintains the semantic meaning of the original sentence. We demonstrate that the proposed scheme can effectively prefer suitable sentences for inclusion in the training data as well as help improve the SLU module for the target language.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289033","Spoken language understanding;Language portability;Statistical machine translation","Training data;Manuals;Training;Tagging;Semantics;Automatic speech recognition;Data models","speech recognition","bootstrapping approach;SLU portability;inducting unannotated user queries;spoken language understanding module;statistical machine translation;SLU module;unannotated automatic speech recognition;source language;spoken dialog system","","4","","12","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Preordering Encoding on Transformer for Translation","Y. Kawara; C. Chu; Y. Arase","Graduate school of Information Science and Technology, Osaka University, Suita, Japan; Kyoto University, Kyoto, Japan; Graduate school of Information Science and Technology, Osaka University, Suita, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Jan 2021","2021","29","","644","655","The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel preordering encoding that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese-English and WMT 2015 English-German, English-Czech, and English-Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by 1.34 points in the Japanese-to-English task, 2.19 points in the English-to-German task, 0.15 points in the Czech-to-English task, and 1.48 points in the English-to-Russian task.","2329-9304","","10.1109/TASLP.2020.3042001","NTT Communication Science Laboratories and Grant-in-Aid for Young Scientists(grant numbers:#19K20343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9290425","Neural machine translation;preordering;word-order;transformer","Encoding;Task analysis;Speech processing;Decoding;Mathematical model;Licenses;Indexes","language translation;natural language processing;recurrent neural nets;statistical analysis","word orders;target languages;serious hurdle;preordering methods;source sentence;similar word ordering;target language;statistical machine translation;preordering position;translation quality;recurrent neural network-based models;preordering information;Transformer model;Transformer-based neural machine translation;novel preordering encoding;reordering information;target sentences;positional encoding;ASPEC Japanese-English;WMT 2015 English-German;English-Czech;English-Russian translation tasks;Japanese-to-English task;English-to-German task;Czech-to-English task;English-to-Russian task","","6","","39","CCBY","10 Dec 2020","","","IEEE","IEEE Journals"
"Synchronous Inference for Multilingual Neural Machine Translation","Q. Wang; J. Zhang; C. Zong","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","7 Jun 2022","2022","30","","1827","1839","Multilingual neural machine translation allows a single model to translate between multiple language pairs, which greatly reduces the cost of model training and receives much attention recently. Previous studies mainly focus on training stage optimization and improve positive knowledge transfer among languages with different levels of parameter sharing, but ignore the multilingual knowledge transfer during inference although the translation in one language may help the generation of other languages. This work enhances knowledge sharing among multiple target languages in the inference phase. To achieve this, we propose a synchronous inference method that can simultaneously generate translations in multiple languages. During generation, the model that predicts the next word of each language not only based on source sentence and previously predicted segments, but also based on predicted words of other target languages. To maximize the inference stage knowledge sharing, we design a cross-lingual attention module which allows the model to dynamically selects the most relevant information from multiple target languages. The synchronous inference model requires multi-way parallel training data which is scarce. We therefore propose to adopt multi-task learning to incorporate large-scale bilingual data. We evaluate our method on three multilingual translation datasets and prove that the proposed method significantly improve the translation quality and the decoding efficiency compared to strong bilingual and multilingual baselines.","2329-9304","","10.1109/TASLP.2022.3178241","National Natural Science Foundation of China(grant numbers:62122088,U1836221,62006224); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783207","Multilingualism;neural machine translation;synchronous inference","Interference;Array signal processing;Noise reduction;Attenuation;Sensors;Sensor arrays;Speech processing","information retrieval;language translation;learning (artificial intelligence);natural language processing","parameter sharing;multilingual knowledge transfer;multiple target languages;inference phase;synchronous inference method;predicted words;inference stage knowledge sharing;cross-lingual attention module;synchronous inference model;multiway parallel training data;multilingual translation datasets;translation quality;strong bilingual baselines;multilingual baselines;multilingual neural machine translation;multiple language pairs;model training;positive knowledge transfer","","1","","49","IEEE","27 May 2022","","","IEEE","IEEE Journals"
"Translation of Multilingual Text into Speech for Visually Impaired Person","A. Arun Kumar; B. Senthilvasudevan; H. Ulfath Farhan","Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, Tamil Nadu, India; Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, Tamil Nadu, India; Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, Tamil Nadu, India","2022 7th International Conference on Communication and Electronics Systems (ICCES)","29 Jul 2022","2022","","","60","64","Today‚Äôs human contact is primarily through voice and text. A person must have a proper vision to view and access the information available in the form of text. In any case, individuals who are blind can gather information by using their hearing ability. The proposed method is a camera-based assistive message reading system that allows blind people and travellers to read the E-messages, printed notes, and other materials in their native languages. It integrates the ideas of optical character recognition (OCR), text to voice synthesizer (TTS), and interpreter in the Raspberry Pi. The proposed model coverts the text from verified records or a caption overlay available on an image into machine-readable text. Text-to-Speech conversion has been enabled by employing the OCR operation to sweep and read any language characters and numbers present in an image and interpret the information in any desired language, and lastly produce a voice output of the decoded text. Voice is the output obtained through the Raspberry Pi‚Äôs sound jack, which may be heard by using speakers or headphones. The main goal of this research work is to develop and implement a novel model to transcribe the Text to Speech in many dialects.","","978-1-6654-9634-6","10.1109/ICCES54183.2022.9835819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835819","Visually Impaired Person (VIP);Character Recognition;Speech Recognition.","Integrated optics;Headphones;Text recognition;Synthesizers;Optical recording;Speech recognition;Optical imaging","cameras;handicapped aids;optical character recognition;speech synthesis","multilingual Text;visually impaired person;human contact;proper vision;hearing ability;camera-based assistive message reading system;blind people;E-messages;printed notes;native languages;optical character recognition;voice synthesizer;interpreter;verified records;machine-readable text;Text-to-Speech conversion;OCR operation;language characters;desired language;voice output;decoded text;Raspberry Pi's sound jack","","","","6","IEEE","29 Jul 2022","","","IEEE","IEEE Conferences"
"Alleviating Exposure Bias for Neural Machine Translation via Contextual Augmentation and Self Distillation","Z. Liu; J. Li; M. Zhu","School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Meituan Inc., Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","31 May 2023","2023","31","","2079","2089","In neural machine translation (NMT), most sequence-to-sequence (seq2seq) models are trained only with the teacher-forcing paradigm, where the ground truth history is used to predict the next ground truth word. At the inference stage, however, the decoder predicts the next token solely based on history generated from scratch. Both using ground truth history and predicting ground truth words potentially lead to exposure bias. On the one hand, to alleviate the issue of exposure bias caused by using ground truth history, we propose contextual augmentation by allowing substitution, insertion, and deletion of words. The contextual augmentation applies to target sequence to generate non-ground truth and natural history when predicting next words. On the other hand, to alleviate the exposure bias caused by predicting ground truth words, we further apply self distillation to guide the model to carry out optimization according to smoothed prediction distribution, i.e, enable the model to predict not only ground truth words, but also other potentially correct and reasonable words. Experimental results on WMT14 English $\leftrightarrow$ German and IWSLT14 German $\rightarrow$ English translation tasks demonstrate that our approach achieves significant improvements over Transformer on standard benchmarks. Detailed experimental analyses further reveal the effectiveness of our proposed approach in improving translation quality.","2329-9304","","10.1109/TASLP.2023.3277245","National Natural Science Foundation of China(grant numbers:61876120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128685","Contextual augmentation;exposure bias;neural machine translation;self distillation","History;Predictive models;Training;Decoding;Task analysis;Machine translation;Perturbation methods","inference mechanisms;language translation;natural language processing","alleviating exposure bias;contextual augmentation;ground truth history;ground truth word;neural machine translation;nonground truth;potentially correct words;predicting ground truth words;reasonable words;seq2seq;sequence-to-sequence models;smoothed prediction distribution","","","","45","IEEE","17 May 2023","","","IEEE","IEEE Journals"
"A Cascaded Framework for Statistical Machine Translation System Combination","J. Du; W. Wei; Z. Yang; B. Xu","Institute of Automation, Chinese Academy and Sciences, Beijing, China; Institute of Automation, Chinese Academy and Sciences, Beijing, China; Institute of Automation, Chinese Academy and Sciences, Beijing, China; Institute of Automation, Chinese Academy and Sciences, Beijing, China","2007 International Conference on Natural Language Processing and Knowledge Engineering","29 Oct 2007","2007","","","285","292","This paper investigates an extensive evaluation of combination techniques and presents a cascaded framework for combining multiple machine translation (MT) system outputs. A word transition network (WTN) is constructed from an N -best list by aligning the hypotheses against an alignment reference, where the alignment is based on minimising an modified translation edit rate (TER) with word or phrase reordering. The minimum Bayes risk (MBR) decoding techinque is inverstigated for the selection of an appropriate alignment reference. Pairwise word alignment is created by an enhanced statistical alignment algorithm that explicitly models word reordering. Experimental results are presented based on three MT systems of Chinese-English translation outputs. It is shown that worthwhile improvements in translation performance can be obtained using the proposed framework.","","978-1-4244-1610-3","10.1109/NLPKE.2007.4368045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4368045","","Automatic speech recognition;Decoding;Automation;System performance;Surface-mount technology;Algorithm design and analysis;Error analysis","Bayes methods;decoding;language translation;natural languages","statistical machine translation system;combination techniques;cascaded framework;word transition network;WTN;modified translation edit rate;TER;phrase reordering;minimum Bayes risk;MBR decoding techinque;pairwise word alignment;statistical alignment algorithm;Chinese-English translation","","","2","21","IEEE","29 Oct 2007","","","IEEE","IEEE Conferences"
"Phonetic name matching for cross-lingual Spoken Sentence Retrieval","Heng Ji; R. Grishman; Wen Wang","City University of New York, USA; New York University, USA; SRI International, Inc., USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","281","284","Cross-lingual Spoken Sentence Retrieval (CLSSR) remains a challenge, especially for queries including OOV words such as person names. This paper proposes a simple method of fuzzy matching between query names and phones of candidate audio segments. This approach has the advantage of avoiding some word decoding errors in Automatic Speech Recognition (ASR). Experiments on Mandarin-English CLSSR show that phone-based searching and conventional translation-based searching are complementary. Adding phone matching achieved 26.29% improvement on F-measure over searching on state-of-the-art Machine Translation (MT) output and 8.83% over Entity Translation (ET) output.","","978-1-4244-3471-8","10.1109/SLT.2008.4777895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777895","Speech Recognition;Information Retrieval","Automatic speech recognition;Decoding;Information retrieval;Error analysis;Broadcasting;Natural languages;Speech recognition;Content based retrieval;Text recognition;Pipelines","fuzzy systems;language translation;natural language processing;pattern matching;query processing;speech processing;speech recognition","phonetic name matching;cross-lingual spoken sentence retrieval;OOV words;fuzzy matching;query names;candidate audio segments;word decoding errors;automatic speech recognition;Mandarin-English CLSSR;machine translation;entity translation","","","","15","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Espresso: A Fast End-to-End Neural Speech Recognition Toolkit","Y. Wang; T. Chen; H. Xu; S. Ding; H. Lv; Y. Shao; N. Peng; L. Xie; S. Watanabe; S. Khudanpur","Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; ASLP@NPU, School of Computer Science, Northwestern Polytechnical University, Xian, China; Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Information Sciences Institute, University of Southern California, Los Angeles, CA, USA; ASLP@NPU, School of Computer Science, Northwestern Polytechnical University, Xian, China; Center of Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD, USA","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","136","143","We present Espresso, an open-source, modular, extensible end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit FAIRSEQ. ESRESSO supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard data sets among other end-to-end systems without data augmentation, and is 4-11x faster for decoding than similar systems (e.g. ESPNET).","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003968","automatic speech recognition;end-to-end;parallel decoding;language model fusion","Decoding;Speech recognition;Training;Computational modeling;Gold;Python;Smoothing methods","natural language processing;neural nets;speech recognition","Espresso;end-to-end neural automatic speech recognition toolkit;deep learning library PyTorch;word-based language model fusion;ASR performance;end-to-end systems;neural machine translation toolkit FAIRSEQ;WSJ data sets;Switchboard data sets;LibriSpeech data sets","","25","","60","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"The GALE project: A description and an update","J. Cohen","Jordan Cohen is the Principal Investigator for GALE at SRI, SRI International, Inc., USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","237","237","Summary form only given. The GALE (global autonomous language exploitation) program is a DARPA program to develop and apply computer software technologies to absorb, translate, analyze, and interpret huge volumes of speech and text in multiple languages This program has been active for two years, and the GALE contractors have been engaged in developing highly robust speech recognition, machine translation, and information delivery systems in Chinese and Arabic. Several GALE-developed talks will be given in this workshop. This overview talk will review the program goals, the technical highlights, and the technical issues remaining in the GALE project.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430115","","Natural languages;Speech analysis;Software;Information analysis;Robustness;Speech recognition;Web pages","language translation;natural languages;speech recognition","GALE project;global autonomous language exploitation;DARPA program;speech recognition;machine translation;information delivery system;natural language","","2","","1","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"Multi-Teacher Distillation With Single Model for Neural Machine Translation","X. Liang; L. Wu; J. Li; T. Qin; M. Zhang; T. -Y. Liu","Soochow University, Suzhou, China; Microsoft Research, Beijing, China; Soochow University, Suzhou, China; Microsoft Research, Beijing, China; Soochow University, Suzhou, China; Microsoft Research, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 Mar 2022","2022","30","","992","1002","Knowledge distillation (KD) is an effective strategy for neural machine translation (NMT) to improve the performance of a student model. Usually, the teacher can guide the student to be better by distilling the soft label or data knowledge from the teacher itself. However, the data diversity and teacher knowledge are limited with only one teacher model. Though a natural solution is to adopt multiple randomized teacher models, one big shortcoming is that the model parameters and training costs are largely increased with the number of teacher models. In this work, we explore to mimic multiple teacher distillation from the sub-network space and permuted variants of one single teacher model. Specifically, we train a teacher by multiple sub-network extraction paradigms: sub-layer reordering, layer-drop, and dropout variants. In doing so, one teacher model can provide multiple outputs variants and causes neither additional parameters nor much extra training cost. Experiments on 8 IWSLT datasets: IWSLT14 En $\leftrightarrow$ De, En $\leftrightarrow$ Es and IWSLT17 En $\leftrightarrow$ Fr, En $\leftrightarrow$ Zh and the large WMT14 EN $\to$ DE translation tasks show that our method even achieves nearly comparable performance with multiple teacher models with different randomized parameters, both word-level and sequence-level knowledge distillation. Our code is available online at https://github.com/dropreg/RLD.","2329-9304","","10.1109/TASLP.2022.3153264","National Natural Science Foundation of China(grant numbers:62036004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722996","Dropout;knowledge distillation;multiple teachers;neural machine translation;sub-networks","Data models;Transformers;Training;Costs;Task analysis;Machine translation;Decoding","language translation;learning (artificial intelligence);natural languages;neural nets;random processes","multiteacher distillation;neural machine translation;multiple randomized teacher models;sequence-level knowledge distillation;sublayer reordering;multiple subnetwork extraction;data diversity;teacher knowledge;layer-drop variant;dropout variant;word-level knowledge distillation;IWSLT datasets","","1","","51","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"Reducing over-smoothness in speech synthesis using Generative Adversarial Networks","L. Sheng; E. N. Pavlovskiy","Department of Mathematics and Mechanics, Novosibirsk State University, Novosibirsk, Russia; Stream Data Analytics and Machine Learning laboratory, Novosibirsk State University, Novosibirsk, Russia","2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)","16 Jan 2020","2019","","","0972","0974","Speech synthesis is widely used in many practical applications. In recent years, speech synthesis technology has developed rapidly. However, one of the reasons why synthetic speech is unnatural is that it often has over-smoothness. In order to improve the naturalness of synthetic speech, we first extract the mel-spectrogram of speech and convert it into a real image, then take the over-smooth mel-spectrogram image as input, and use image-to-image translation Generative Adversarial Networks(GANs) framework to generate a more realistic mel-spectrogram. Finally, the results show that this method greatly reduces the over-smoothness of synthesized speech and is more close to the mel-spectrogram of real speech.","","978-1-7281-4401-6","10.1109/SIBIRCON48586.2019.8957862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957862","Speech synthesis;over-smoothness;mel-spectrogram;GANs","Generative adversarial networks;Speech synthesis;Spectrogram;Generators;Hidden Markov models;Training","feature extraction;image sampling;speech synthesis","speech synthesis technology;synthetic speech;over-smooth mel-spectrogram image;realistic mel-spectrogram;image-to-image translation generative adversarial networks","","","","18","IEEE","16 Jan 2020","","","IEEE","IEEE Conferences"
"A fixed-point decoding approach for statistical machine translation on mobile terminals","X. Li; J. Xu; W. Jiang; Q. Liu; Y. L√º","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Key Laboratory of Intelligent Information Processing, Chinese Academy and Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Chinese Academy and Sciences, Beijing, China; Key Laboratory of Intelligent Information Processing, Chinese Academy and Sciences, Beijing, China","2010 4th International Universal Communication Symposium","13 Dec 2010","2010","","","268","272","The demand for statistical machine translation on mobile terminals is increasing rapidly, but translation speed is restricted by the embedded processors without a floating-point unit. This paper proposes an approach to convert floating-point numbers into fixed-point numbers for SMT decoding on mobile terminals in order to reduce the impact of the processors without a floating-point unit on translation speed. The experiments based on PC and mobile terminal show that this approach ensures the quality of translation and the speed of fixed-point arithmetic operations is 135.6% faster than that of floating-point arithmetic operations. Therefore, this approach can efficiently improve translation speed of SMT systems on mobile terminals with weak ability in floating-point arithmetic operations.","","978-1-4244-7820-0","10.1109/IUCS.2010.5666031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666031","SMT;fixed-point;mobile terminals","Decoding;Mobile communication;Speech;Floating-point arithmetic;Program processors;Computers;Speech recognition","code convertors;decoding;fixed point arithmetic;floating point arithmetic;language translation","fixed point decoding;mobile terminal;statistical machine translation;embedded processor;floating point number;fixed point number conversion;arithmetic operation","","1","","26","IEEE","13 Dec 2010","","","IEEE","IEEE Conferences"
"Constructing shallow-transfer rule for Indonesian-English machine translation","E. Yulianti; M. Adriani; H. M. Manurung; I. Budi; A. N. Hidayanto","Laboratory of Information Retrieval, Faculty of Computer Science, Universitas Indonesia, Indonesia; Laboratory of Information Retrieval, Faculty of Computer Science, Universitas Indonesia, Indonesia; Laboratory of Information Retrieval, Faculty of Computer Science, Universitas Indonesia, Indonesia; Laboratory of Information Retrieval, Faculty of Computer Science, Universitas Indonesia, Indonesia; Laboratory of Information Retrieval, Faculty of Computer Science, Universitas Indonesia, Indonesia","2012 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","25 Feb 2013","2012","","","143","148","Shallow-transfer rule is the main component of shallow-transfer machine translation that matches the pattern of source language and then performs required action to translate them into the target language. However, high development cost is required to build it because it needs much of linguistic knowledge. This paper focuses on the process to automatically generate the shallow-transfer rule for Indonesian-to-English that is followed with manual post-editing to improve the quality of automatic rule. The result shows that the automatic transfer rule can improve the performance of the word-for-word translation. Furthermore, manual post-editing process was also shown to increase the quality of automatic transfer rule. According to this result, using automatic approach is an effective way to build shallow-transfer rule at starting point that can be further enhanced by performing rule post-editing.","","978-1-4673-3026-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468732","","Dictionaries;Speech;Pattern matching;Training;Testing;Morphology;Speech processing","language translation;natural language processing","shallow transfer rule;Indonesian-English machine translation;source language;target language;linguistic knowledge;automatic rule quality;word-for-word translation","","","","14","","25 Feb 2013","","","IEEE","IEEE Conferences"
"The IBM 2006 Gale Arabic ASR System","H. Soltau; G. Saon; B. Kingsbury; J. Kuo; L. Mangu; D. Povey; G. Zweig","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-349","IV-352","This paper describes the advances made in IBM's Arabic broadcast news transcription system which was fielded in the 2006 GALE ASR and machine translation evaluation. These advances were instrumental in lowering the word error rate by 42% relative over the course of one year and include: training on additional LDC data, large-scale discriminative training on 1800 hours of unsupervised data, automatic vowelization using a flat-start approach, use of a large vocabulary with 617K words and 2 million pronunciations and lastly, a system architecture based on cross-adaptation between unvowelized and vowelized acoustic models.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.366921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218109","Speech recognition","Automatic speech recognition;Vocabulary;Broadcasting;Speech recognition;Error analysis;Large-scale systems;Natural languages;Acoustic testing;System testing;Instruments","language translation;natural language processing;speech processing;speech recognition","IBM 2006 GALE Arabic ASR system;Arabic broadcast news transcription system;machine translation evaluation;large-scale discriminative training;automatic vowelization;unsupervised data;flat-start approach;unvowelized acoustic models","","31","2","9","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"The RWTH Arabic-to-English spoken language translation system","O. Bender; E. Matusov; S. Hahn; S. Hasan; S. Khadivi; H. Ney","Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany; Human Language Technology and Pattern Recognition, Lehrstuhl f√ºr Informatik 6-Computer Science Department, RWTH Aachen University, Aachen, Germany","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","396","401","We present the RWTH phrase-based statistical machine translation system designed for the translation of Arabic speech into English text. This system was used in the Global Autonomous Language Exploitation (GALE) Go/No-Go Translation Evaluation 2007. Using a two-pass approach, we first generate n-best translation candidates and then rerank these candidates using additional models. We give a short review of the decoder as well as of the models used in both passes. We stress the difficulties of spoken language translation, i.e. how to combine the recognition and translation systems and how to compensate for missing punctuation. In addition, we cover our work on domain adaptation for the applied language models. We present translation results for the official GALE 2006 evaluation set and the GALE 2007 development set.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430145","speech to text;adjustment of ASR and MT vocabularies;LM adaptation;punctuation prediction","Natural languages;Automatic speech recognition;Surface-mount technology;Vocabulary;Decoding;Speech analysis;Broadcasting;Humans;Pattern recognition;Computer science","language translation;natural languages;speech recognition;statistical analysis","RWTH phrase-based statistical machine translation system;Arabic-to-English spoken language translation system;Global Autonomous Language Exploitation;applied language model;official GALE 2006 evaluation set;GALE 2007 development set","","3","1","20","IEEE","14 Jan 2008","","","IEEE","IEEE Conferences"
"An Automatic Speech Segmentation Algorithm of Portuguese based on Spectrogram Windowing","L. M. Hoi; Y. Sun; S. K. Im","Engineering Research Centre of Applied Technology on Machine Translation and Artificial Intelligence, Ministry of Education Macao Polytechnic University, Macao, China; Engineering Research Centre of Applied Technology on Machine Translation and Artificial Intelligence, Ministry of Education Macao Polytechnic University, Macao, China; Engineering Research Centre of Applied Technology on Machine Translation and Artificial Intelligence, Ministry of Education Macao Polytechnic University, Macao, China","2022 IEEE World AI IoT Congress (AIIoT)","13 Jul 2022","2022","","","290","295","Sentence segmentation is important for improving the human readability of Automatic Speech Recognition (ASR) systems. Although it has been explored through numerous interdisciplinary studies, segmentation of Portuguese is still time-consuming due to the lack of efficient automatic segmentation methods and the reliance on qualified phonetic experts. This paper presents a novel algorithm that efficiently segments speech into sentences by learning the spectrogram of sentences through windows using a classification model developed with an Artificial Neural Network (ANN). Based on our experiments, the beginning part of a European Portuguese (EP) sentence enables better identification of the sentence's boundaries. In addition, a window frame of spectrogram constructed by the previous ending of 100 milliseconds (ms) and the subsequent beginning of 300 ms presents the best performance in the automatic sentence segmentation. As a result, the proposed algorithm can automatically segment Portuguese speech into sentences by analyzing its spectrogram without knowing the speech semantics.","","978-1-6654-8453-4","10.1109/AIIoT54504.2022.9817299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817299","sentence segmentation;Portuguese speech;spectrogram;natural language processing","Semantics;Europe;Artificial neural networks;Manuals;Learning (artificial intelligence);Phonetics;Classification algorithms","natural language processing;neural nets;speech recognition","Automatic Speech segmentation algorithm;spectrogram;human readability;Automatic Speech Recognition systems;qualified phonetic experts;Artificial Neural Network;European Portuguese sentence;window frame;speech semantics;Portuguese speech;ANN;spectrogram windowing;ASR systems","","","","52","IEEE","13 Jul 2022","","","IEEE","IEEE Conferences"
"Recovery of acronyms, out-of-lattice words and pronunciations from parallel multilingual speech","J. Miranda; J. P. Neto; A. W. Black","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; INESC-ID / Instituto Superior T√©cnico, Lisboa, Portugal; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2012 IEEE Spoken Language Technology Workshop (SLT)","31 Jan 2013","2012","","","348","353","In this work we present a set of techniques which explore information from multiple, different language versions of the same speech, to improve Automatic Speech Recognition (ASR) performance. Using this redundant information we are able to recover acronyms, words that cannot be found in the multiple hypotheses produced by the ASR systems, and pronunciations absent from their pronunciation dictionaries. When used together, the three techniques yield a relative improvement of 5.0% over the WER of our baseline system, and 24.8% relative when compared with standard speech recognition, in an Europarl Committee dataset with three different languages (Portuguese, Spanish and English). One full iteration of the system has a parallel Real Time Factor (RTF) of 3.08 and a sequential RTF of 6.44.","","978-1-4673-5126-3","10.1109/SLT.2012.6424248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424248","speech recognition;machine translation;pronunciation;out-of-lattice;acronyms","Speech;Lattices;Speech recognition;Acoustics;Dictionaries;Europe","language translation;natural language processing;performance evaluation;speech recognition","acronym recovery;out-of-lattice word recovery;parallel multilingual speech;out-of-lattice pronunciation recovery;automatic speech recognition performance improvement;ASR performance improvement;ASR systems;pronunciation dictionaries;Europarl Committee dataset;Portuguese language;Spanish language;English language;parallel real time factor;parallel RTF;machine translation","","2","","11","IEEE","31 Jan 2013","","","IEEE","IEEE Conferences"
"Parts-of-Speech Tagger for Gujarati Language using Long-short-Term-Memory","C. Jobanputra; N. Parikh; V. Vora; S. K. Bharti","Computer Science Department, Pandit Deendayal Energy University, Gandhinagar, India; Computer Science Department, Pandit Deendayal Energy University, Gandhinagar, India; Computer Science Department, Pandit Deendayal Energy University, Gandhinagar, India; Computer Science Department, Pandit Deendayal Energy University, Gandhinagar, India","2021 International Conference on Artificial Intelligence and Machine Vision (AIMV)","10 Jan 2022","2021","","","1","5","Parts-of-Speech (POS) tagging is a crucial step to process the natural languages. It is a state-of-art method of providing the lexicon category such as noun, verb, adjective, etc. to each word that best suits the context of the sentence in which it is used. Being a part of pre-processing makes this task an important step in linguistics and semantics. Gujarati is an Indian language widely spoken in Asia and across the world. Part-of-Speech tagging can be used in word sense disambiguation, Information retrieval, machine translation and parsing. In this paper, we proposed Long-short-Term-Memory (LSTM) based Part-of-Speech tagger for Gujarati language. With our proposed approach, this paper envisions achieving accuracy of 95.34% and 96% precision with the help of this novel & efficient gradient based method.","","978-1-6654-4211-4","10.1109/AIMV53313.2021.9670996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670996","component;formatting;style;styling;insert","Error analysis;System performance;Machine vision;Semantics;Asia;Tagging;Linguistics","gradient methods;grammars;information retrieval;language translation;natural language processing","Gujarati language;long-short-term-memory;natural languages;state-of-art method;lexicon category;adjective;Indian language;part-of-speech tagging;word sense disambiguation;part-of-speech tagger;LSTM;information retrieval;machine translation;gradient based method","","","","12","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Extended word similarity based clustering on unsupervised PoS induction to improve English-Indonesian statistical machine translation","H. Sujaini; A. Purwarianti; A. A. Arman; Kuspriyanto","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia","2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)","11 Jan 2014","2013","","","1","4","In this paper, we present the unsupervised Part-of-Speech (PoS) induction algorithm to improve translations quality on statistical machine translation. The proposed algorithm is an extension of the algorithm Word-Similarity-Based (WSB) clustering. In the clustering, the similarity between words is measured by its grammatical relation with other words. The grammatical relation is represented as the n-gram relation. We extend the WSB clustering by take into account for the previous words in measuring the grammatical relation. The clustering results are then used in the English-Indonesia statistical machine translation. The experiments were conducted using MOSES as the machine translation decoder, and were evaluated by its BLEU score. Using 14.000 English-Indonesian sentence pairs, the clustering improved the BLEU score of 2.07%.","","978-1-4799-2378-6","10.1109/ICSDA.2013.6709880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6709880","Unsupervised PoS Induction;Word Clustering;English-Indonesian","Clustering algorithms;Hidden Markov models;Computational linguistics;Computational modeling;Equations;Accuracy;Tagging","language translation;natural language processing;statistical analysis;unsupervised learning","extended word similarity based clustering;unsupervised PoS induction;English-Indonesian statistical machine translation;unsupervised part-of-speech induction algorithm;grammatical relation;MOSES;machine translation decoder","","","","24","IEEE","11 Jan 2014","","","IEEE","IEEE Conferences"
"Part-of-Speech Tagging of English-Translation Bilingual Parallel Corpus Based on Convolutional Neural Network","X. Gou; R. Li; P. Zhang","College of Foreign Languages, Bohai University, Jinzhou, China; College of Foreign Languages, Bohai University, Jinzhou, China; College of Foreign Languages, Bohai University, Jinzhou, China","2022 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)","18 Jan 2023","2022","","","1300","1304","Annotation is the process of adding information to a corpus. The key to realizing the machine-reading of corpus and improving the value of corpus utilization is effective annotation. The convolutional neural network and its mathematical model are constructed, and the network structure, convolutional layer and pooling layer, activation function, loss function and optimization are studied in detail; a bilingual parallel corpus part-of-speech tagging model is constructed, including defining the objective function, determining the parameters and parameter space; the training process of the convolutional neural network is designed, including the forward propagation stage and the back propagation stage. Based on the convolutional neural network annotation, the corresponding features are learned from a large number of samples, which reduces the complexity of the network model, reduces the number of weights, avoids the complex feature extraction process, and promotes the research and application of machine translation.","","978-1-6654-7053-7","10.1109/TOCS56154.2022.10016187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016187","convolutional neural network;bilingual parallel corpus;part-of-speech tagging;mathematical model;training process","Annotations;Computational modeling;Biological system modeling;Manuals;Tagging;Feature extraction;Mathematical models","feature extraction;language translation;learning (artificial intelligence);natural language processing;neural nets;text analysis","convolutional layer;convolutional neural network annotation;corpus part-of-speech tagging model;corpus utilization;effective annotation;english-translation bilingual parallel corpus;loss function;network model;network structure;optimization","","","","10","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"New methods and evaluation experiments on translating TED talks in the IWSLT benchmark","A. Axelrod; X. He; L. Deng; A. Acero; M. -Y. Hwang","Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4945","4948","The IWSLT benchmark task is an annual evaluation campaign on spoken language translation held by the International Workshop on Spoken Language Processing (IWSLT). The task is to translate TED talks (www.ted.com). This task presents two unique challenges: Firstly, the underlying topic switches sharply from talk to talk, and each one contains only tens to hundreds of utterances. The translation system therefore needs to adapt to the current topic quickly and dynamically. Secondly, unlike other machine translation benchmark tasks, only a very small relevant parallel corpus (transcripts of TED talks) is available. Therefore, it is necessary to perform accurate translation model estimation with limited data. In this paper, we present our recent progress and two new methods on the IWSLT TED talk translation task from Chinese into English. In particular, to address the first problem, we use unsupervised topic modeling to select additional topic-dependent parallel data from a globally irrelevant corpus. These additional data slices can then be used to build an unsupervised topic-adapted machine translation system. For the second problem, we develop a discriminative training method to estimate the translation models more accurately. Our experimental evaluation results show that both methods improve the translation quality over a state-of-the-art baseline.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289029","spoken language translation;topic adaptation;discriminative training;IWSLT","Adaptation models;Training;Hidden Markov models;Data models;Estimation;Benchmark testing;Helium","language translation;natural language processing;speech processing","IWSLT benchmark task;spoken language translation system;International Workshop on Spoken Language Processing;machine translation benchmark tasks;IWSLT TED talk translation task;unsupervised topic modeling;topic-dependent parallel data;data slices;unsupervised topic-adapted machine translation system;discriminative training method","","","","18","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Performance improvement of Machine Translation system using LID and post-editing","K. Mrinalini; G. Sangavi; P. Vijayalakshmi","Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai, India; Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai, India; Department of Electronics and Communication Engineering, SSN College of Engineering, Chennai, India","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","2134","2137","Machine Translation systems are fast replacing human translators to aid us in translation between any pair of natural languages. Conversational sentences form the fundamental structure of a natural language. These sentences help in understanding the basic conversations in a language and hence improving translation of these sentences is considered more essential to improve the quality of a good translation systems in various real-life applications. This paper focuses on machine translation for travel conversations between any pair of languages namely, English, Hindi, and Tamil, with minimal intervention of linguistics. The translation system is trained using various text corpus sizes (1K, 5K, 13K sentences) of conversational and complex sentences for comparison. To improve the performance of a machine translation system a language identification system coupled with post-editing approach that applies n-best translation list analysis and language models are used. An improvement of translation performance by 3-5% in bilingual evaluation understudy (BLEU) score is observed without incorporating linguistic information.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848403","","Pragmatics;Training;Grammar;Natural languages;Speech;Context;Training data","language translation;natural language processing;text analysis","machine translation system performance improvement;LID;post-editing;travel conversations;English language;Hindi language;Tamil language;text corpus;complex sentences;conversational sentences;language identification system;bilingual evaluation understudy;BLEU score;language models","","2","","12","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"A survey paper on performance improvement of word alignment in English to Hindi translation system","K. K. Yadav; U. C. Jaiswal","Dept. of Computer Science and Engineering, Madan Mohan Malaviya University of Technology, Gorakhpur, U.P., India; Dept. of Computer Science and Engineering, Madan Mohan Malaviya University of Technology, Gorakhpur, U.P., India","2017 International Conference on Intelligent Computing and Control (I2C2)","22 Mar 2018","2017","","","1","4","This survey paper emphasis on the word alignment of English-Hindi language pair. Word alignment defines the process of establishing the better translation relationship between the words of a parallel or bilingual corpus. Word alignment's primary step is SML. A few word alignment approaches have been progressed in NLP. Word corpus is applied to fetch multiword phrases with linguistics sense. This survey paper uses a combined technique to progress the implementation of word alignment for English-Hindi language pair with limited resources. This paper introduces the different types of techniques of word alignment for English to Hindi translation system.","","978-1-5386-0374-1","10.1109/I2C2.2017.8321821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8321821","Machine translation;Statistical Machine Translation;Natural Language Processing;Word Alignment","Speech;Hidden Markov models;Natural language processing;Dictionaries;Computational modeling;Tools;Computational linguistics","language translation;natural language processing;text analysis","Hindi translation system;English-Hindi language pair;word alignment approaches;word corpus","","","","17","IEEE","22 Mar 2018","","","IEEE","IEEE Conferences"
"Lexical-semantic divergence in Urdu-to-English Example Based Machine Translation","A. Saboor; M. A. Khan","Department of Computer Science, University of Peshawar, Peshawar, Pakistan; Department of Computer Science, University of Peshawar, Peshawar, Pakistan","2010 6th International Conference on Emerging Technologies (ICET)","15 Nov 2010","2010","","","316","320","Divergence is one of the common obstacles in Example Based Machine Translation (EBMT). It needs to be identified, categorized and resolved to obtain correct translation for any pair of language. The focus in this research is on lexical semantic divergence and six different types are identified and generalizations are made on the basis of examples, for Urdu to English translation. Strategies are also presented for the identification of these types. This work helps in efficient retrieval of the examples from a bilingual corpus, which in turn will help in the alignment and recombination stages of EBMT, to produce good quality translation.","","978-1-4244-8058-6","10.1109/ICET.2010.5638469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638469","","Helium;Syntactics;Semantics;Books;Sleep;Toxicology;Speech","language translation","lexical semantic divergence;example based machine translation;bilingual corpus;urdu to english translation","","2","","10","IEEE","15 Nov 2010","","","IEEE","IEEE Conferences"
"Extension Schemes for the Alignment Model of English-Malayalam Statistical Machine Translator","M. P. Sebastian; S. Kurian K.; G. S. Kumar","Department of Computer Science and Engineering, Rajagiri School of Engg. & Technology, Kochi, Kerala, India; Department of Computer Science and Engineering, KMEA Engineering College, Edathala, Aluva, Kerala, India; Department of Computer Science, Cochin University of Science and Technology, Kerala, India","2012 International Conference on Advances in Computing and Communications","20 Sep 2012","2012","","","86","89","In Statistical Machine Translation from English to Malayalam, an unseen English sentence is translated into its equivalent Malayalam sentence using statistical models. A parallel corpus of English-Malayalam is used in the training phase. Word to word alignments has to be set among the sentence pairs of the source and target language before subjecting them for training. This paper deals with certain techniques which can be adopted for improving the alignment model of SMT. Methods to incorporate the parts of speech information into the bilingual corpus has resulted in eliminating many of the insignificant alignments. Also identifying the name entities and cognates present in the sentence pairs has proved to be advantageous while setting up the alignments. Presence of Malayalam words with predictable translations has also contributed in reducing the insignificant alignments. Moreover, reduction of the unwanted alignments has brought in better training results. Experiments conducted on a sample corpus have generated reasonably good Malayalam translations and the results are verified with F measure, BLEU and WER evaluation metrics.","","978-1-4673-1911-9","10.1109/ICACC.2012.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6305560","alignment;training;machine translation;English Malayalam translation","Training;Probability;Vectors;Decoding;Speech;Hidden Markov models;Tagging","language translation;natural language processing;parallel processing;statistical analysis","extension schemes;word to word alignment model;English-Malayalam statistical machine translator;English sentence;Malayalam sentence;statistical models;parallel corpus;training phase;source language;target language;SMT;parts of speech information;bilingual corpus;unwanted alignment reduction;F measure;BLEU;WER evaluation metrics","","","","12","IEEE","20 Sep 2012","","","IEEE","IEEE Conferences"
"Audio Synthesis Translation and Auto-Summarization (ASTA)","J. Varghese; P. Ranawat; R. Rodrigues; P. Shaikh","Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India; Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India; Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India; Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India","2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)","12 Dec 2022","2022","","","1","5","Availability of time has been a major issue in recent years for mankind. There has always been a huge demand for automation, since it can tremendously decrease time for doing menial tasks. This proposed project focuses on automation of text translation, summarization and speech synthesis which could reduce time required for reading books. In this paper, we present multiple machine learning models that synthesize text into speech and also into summarized text of Devanagari script. The main objective of the project is to conduct proper examination of the existing architecture of the text translation and summarization methodologies and to provide a robust system which is a cumulation of converting PDF files to audio files and also summarization of the PDF and translating into Devanagari text of the summarized English narrative. The architecture is called Audio Synthesis Translation and Auto-summarization (ASTA) and uses multiple models such as RNN sequence to sequence, NMT, Tacotron 2 and Waveglow. In addition to this, we use Google Vision OCR for text extraction from PDF. This system is an integration of multiple machine learning models and works as a pipelined system.","","978-1-6654-6855-8","10.1109/GCAT55367.2022.9971977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9971977","RNN;LSTM;Keras;TensorFlow;Neural Networks;Tacotron2;NMT;Vision OCR;WaveGlow","Automation;Pipelines;Optical character recognition;Machine learning;Internet;Speech synthesis;Task analysis","learning (artificial intelligence);natural language processing;optical character recognition;speech synthesis;text analysis","ASTA;audio files;Audio Synthesis Translation;Auto-summarization;Devanagari script;Devanagari text;menial tasks;multiple machine learning models;PDF files;reading books;speech synthesis;summarization methodologies;summarized English narrative;summarized text;synthesize text;text extraction;text translation","","","","10","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"Improving Low-Resource Neural Machine Translation With Teacher-Free Knowledge Distillation","X. Zhang; X. Li; Y. Yang; R. Dong","Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, Urumqi, China","IEEE Access","23 Nov 2020","2020","8","","206638","206645","Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. However, in low-resource neural machine translation, a stronger teacher model is not available. To counteract this, We therefore propose a novel Teacher-free Knowledge Distillation framework for low-resource neural machine translation, where the model learns from manually designed regularization distribution as a virtual teacher model. The prior distribution of artificial design can not only obtain the similarity information between words, but also provide effective regularity for model training. Experimental results show that the proposed method has improved performance in low-resource language effectively.","2169-3536","","10.1109/ACCESS.2020.3037821","Western Light of the Chinese Academy of Sciences(grant numbers:2017-XBQNXZ-A-005); National Natural Science Foundation of China(grant numbers:U1703133); Subsidy of the Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017472); Major Science and Technology Project of Xinjiang Uygur Autonomous Region(grant numbers:2016A03007-3); Tianshan Excellent Young Scholars of Xinjiang(grant numbers:2019Q031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257421","Neural machine translation;knowledge distillation;prior knowledge","Training;Decoding;Vocabulary;Task analysis;Standards;Knowledge engineering;Computational modeling","knowledge acquisition;language translation;learning (artificial intelligence);natural language processing;neural nets;text analysis","low resource neural machine translation;student model;virtual teacher model;model training;low resource language;teacher-free knowledge distillation framework;deep learning;natural language text","","4","","31","CCBY","12 Nov 2020","","","IEEE","IEEE Journals"
"Neural Machine Translation With Noisy Lexical Constraints","H. Li; G. Huang; D. Cai; L. Liu","Natural Language Processing Center, Tencent AI Lab, Shenzhen, China; Natural Language Processing Center, Tencent AI Lab, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong; Natural Language Processing Center, Tencent AI Lab, Shenzhen, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Jun 2020","2020","28","","1864","1874","In neural machine translation, lexically constrained decoding generates translation outputs strictly including the constraints predefined by users, and it is beneficial to improve translation quality at the cost of more decoding overheads if the constraints are perfect. Unfortunately, those constraints may contain mistakes in real-world situations and incorrect constraints will undermine lexically constrained decoding. In this article, we propose a novel framework that is capable of improving the translation quality even if the constraints are noisy. The key to our framework is to treat the lexical constraints as external memories. More concretely, it encodes the constraints by a memory encoder and then leverages the memories by a memory integrator. Experiments demonstrate that our framework can not only deliver substantial BLEU gains in handling noisy constraints, but also achieve speedup in decoding. These results motivate us to apply our models to a new scenario where the constraints are generated without the help of users. Experiments show that our models can indeed improve the translation quality with the automatically generated constraints.","2329-9304","","10.1109/TASLP.2020.2999724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108255","Neural machine translation;lexical constraints","Memory management;Noise measurement;Decoding;Encoding;Standards;Speech processing","language translation;natural language processing;neural nets","translation quality;external memories;memory encoder;memory integrator;neural machine translation;noisy lexical constraints;lexical constrained decoding;BLEU","","","","54","IEEE","4 Jun 2020","","","IEEE","IEEE Journals"
"Recovering from Chinese-English machine translation parser failures","Liu Ying","Laboratory of Computational Linguistics, Department of Chinese Language and Literature, Tsinghua University, Beijing, China","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)","6 Aug 2002","2001","2","","910","914 vol.2","The goal of the analysis stage of a Chinese-English machine translation is produce a correct tree. A correct tree may not be guaranteed to produce for any sentence when there is the wrong input, or excluded vocabulary words, or a wrong rule, etc. It is very important for machine translation to be able to recover from partial analytical results. The recovery process from partial analytical results is presented using a hybrid statistical-linguistic based approach.","1062-922X","0-7803-7087-2","10.1109/ICSMC.2001.973033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973033","","Statistical analysis;Computational linguistics;Natural languages;Vocabulary;Statistics;Speech;Information resources;Information analysis;Dictionaries","language translation;natural languages;trees (mathematics);statistical analysis;grammars","Chinese-English machine translation parser failures;correct tree;sentence;vocabulary words;machine translation;partial analytical results;recovery process;hybrid statistical-linguistics based approach","","","","5","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Incremental language modeling for broadcast news","K. Ohtsuki; L. Nguyen","NTT Cyber Space Laboratories, NTT Corporation, Yokosuka, Kanagawa, Japan; BBN Technologies, GTE, Cambridge, MA, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","139","144","In this paper, we address the task of incremental language modeling for automatic transcription of broadcast news speech. Daily broadcast news naturally contains new words that are not in the lexicon of the speech recognition system but are important for downstream applications such as information retrieval or machine translation. To recognize those new words, the lexicon and the language model of the speech recognition system need to be updated periodically. We propose a method of estimating a list of words to be added to the lexicon based on some time-series text data. The experimental results on the RT04 broadcast news data and other TV audio data showed that this method provided a decent and stable reduction in both out-of-vocabulary rates and speech recognition word error rates","","0-7803-9478-X","10.1109/ASRU.2005.1566531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566531","","Broadcasting;Speech recognition;Vocabulary;Natural languages;Frequency;Space technology;Information retrieval;Error analysis;World Wide Web;Laboratories","information retrieval;language translation;speech recognition","incremental language modeling;automatic transcription;broadcast news speech;speech recognition system;information retrieval;machine translation;speech recognition word error rates","","","","12","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Towards incorporating language morphology into statistical machine translation systems","P. Karageorgakis; A. Potamianos; I. Klasinas","Department of Electronics and Computer Engineering, Technical University of Crete, Chania, Greece; Department of Electronics and Computer Engineering, Technical University of Crete, Chania, Greece; Department of Electronics and Computer Engineering, Technical University of Crete, Chania, Greece","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","80","85","In this paper, a novel algorithm for incorporating morphological knowledge into statistical machine translation (SMT) systems is proposed. First, word stems are acquired automatically for the source and target languages using an unsupervised morphological acquisition algorithm. Then a word-stem based SMT system is built and combined with a phrase-based word level SMT system using a general statistical framework. The combined lexical and morphological SMT system is implemented using late integration and lattice re-scoring. The system is then evaluated on the Europarl corpus, using automatic evaluation methods for various training corpus sizes. It is shown, that both the BLEU and NIST scores of the lexical-morphological system improve by about 14% over the baseline English to Greek translation system when using a 1M word training corpus.","","0-7803-9478-X","10.1109/ASRU.2005.1566533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566533","","Morphology;Surface-mount technology;Natural languages;Lattices;Robustness;Knowledge engineering;NIST;Humans;Training data;Technological innovation","language translation;natural languages","language morphology;statistical machine translation systems;source language;target languages;lattice rescoring;English to Greek translation system","","1","1","16","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Dual Word Embedding for Robust Unsupervised Bilingual Lexicon Induction","H. Cao; L. Li; C. Zhu; M. Yang; T. Zhao","Machine Intelligence and Translation Laboratory, Faculty of Computing, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, Faculty of Computing, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, Faculty of Computing, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, Faculty of Computing, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, Faculty of Computing, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","13 Jul 2023","2023","31","","2606","2615","The word embedding models such as Word2vec and FastText simultaneously learn dual representations of input vectors and output vectors. In contrast, almost all existing unsupervised bilingual lexicon induction (UBLI) methods use only input vectors without utilizing output vectors. In this article, we propose a novel approach to making full use of both input and output vectors for more robust and strong UBLI. We discover the Common Difference Property that one orthogonal transformation can connect not only the input vectors of two languages but also the output vectors. Therefore, we can learn just one transformation to induce two different dictionaries from the input and output vectors, respectively. Between these two quite different dictionaries, a more accurate lexicon with less noise can be induced by taking the intersection of them in UBLI procedure. Extensive experiments show that our method achieves much more robust and strong results than state-of-the-art methods in distant language pairs, while reserving comparable performances in similar language pairs.","2329-9304","","10.1109/TASLP.2023.3290425","National Natural Science Foundation of China(grant numbers:U1908216); Key R&D Program of Yunnan(grant numbers:202203AA080004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167848","Word embedding;input vector;output vector;unsupervised bilingual lexicon induction","Dictionaries;Linear programming;Training;Task analysis;Speech processing;Mathematical models;Transforms","dictionaries;language translation;natural language processing;unsupervised learning","common difference property;dual Word embedding;input vectors;output vectors;robust unsupervised bilingual lexicon induction;UBLI methods;unsupervised bilingual lexicon induction methods;word embedding models","","","","36","IEEE","28 Jun 2023","","","IEEE","IEEE Journals"
"Thai-Isarn Dialect Parallel Corpus Construction for Machine Translation","P. Seresangtakul; P. Unlee","Department of Computer Science, Khon Kaen University, Khon Kaen, Thailand; Department of Innovation and Computer Education, Sakon Nakhon Rajabhat University, Thailand","2019 11th International Conference on Knowledge and Smart Technology (KST)","11 Apr 2019","2019","","","121","125","Parallel corpus is an essential resource in Natural Language Processing (NLP) research, especially machine language translation. This paper presents the construction process of the Thai language and Isarn dialect bilingual parallel corpus, which includes word segmentation, translation and word alignment, part of speech (POS) tagging, and the parallel corpus design and construction. In the study, source sentences in Thai are segmented into a sequence of words by applying a Conditional Random Field (CRF) approach. We used the example and rule based Thai-Isarn machine translation system as a tool to generate the corresponding target sentence (Isarn dialect). The POS of each word is tagged using Hidden Markov Modeling (HMM). The source and target sentences with their POSs are validated by Isarn native speakers, who are expert in both Thai language and Isarn dialects. Lastly, the validated data were collected into the Thai Isarn parallel corpus.","2374-314X","978-1-5386-7512-0","10.1109/KST.2019.8687534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8687534","Thai;Isarn dialect;parallel corpus;POS tagging","","hidden Markov models;language translation;natural language processing;text analysis","Thai-Isarn dialect parallel corpus construction;machine language translation;Thai language;Isarn dialect bilingual parallel corpus;word segmentation;speech tagging;POS;parallel corpus design;Conditional Random Field approach;Thai-Isarn machine translation system;Isarn native speakers;Thai Isarn parallel corpus;natural language processing research;hidden Markov modeling;HMM","","1","","12","IEEE","11 Apr 2019","","","IEEE","IEEE Conferences"
"Part of Speech Tagging in Urdu: Comparison of Machine and Deep Learning Approaches","W. Khan; A. Daud; K. Khan; J. A. Nasir; M. Basheri; N. Aljohani; F. S. Alotaibi","Department of Computer Science and Software Engineering, International Islamic University Islamabad, Islamabad, Pakistan; Department of Computer Science and Software Engineering, International Islamic University Islamabad, Islamabad, Pakistan; Department of Computer Science, University of Science and Technology, Bannu, Pakistan; Department of Computer Science and Software Engineering, International Islamic University Islamabad, Islamabad, Pakistan; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Access","2 Apr 2019","2019","7","","38918","38936","In Urdu, part of speech (POS) tagging is a challenging task as it is both inflectionally and derivationally rich morphological language. Verbs are generally conceived a highly inflected object in Urdu comparatively to nouns. POS tagging is used as a preliminary linguistic text analysis in diverse natural language processing domains such as speech processing, information extraction, machine translation, and others. It is a task that first identifies appropriate syntactic categories for each word in running text and second assigns the predicted syntactic tag to all concerned words. The current work is the extension of our previous work. Previously, we presented conditional random field (CRF)-based POS tagger with both language dependent and independent feature set. However, in the current study, we offer: 1) the implementation of both machine and deep learning models for Urdu POS tagging task with well-balanced language-independent feature set and 2) to highlight diverse challenges which cause Urdu POS task a challenging one. In this research, we demonstrated the effectiveness of machine learning and deep learning models for Urdu POS task. Empirically, we have evaluated the performance of all models on two benchmark datasets. The core models evaluated in this study are CRF, support vector machine (SVM), two variants of the deep recurrent neural network (DRNN), and a variant of n-gram Markov model the bigram hidden Markov model (HMM). The two variants of DRRN models evaluated include forward long short-term memory (LSTM)-RNN and LSTM-RNN with CRF output.","2169-3536","","10.1109/ACCESS.2019.2897327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636191","Urdu;part of speech (POS);conditional random field (CRF);support vector machine (SVM);recurrent neural network (RNN);hidden Markov model (HMM)","Task analysis;Hidden Markov models;Support vector machines;Tagging;Deep learning;Training data;Linguistics","computational linguistics;hidden Markov models;information retrieval;learning (artificial intelligence);natural language processing;natural languages;recurrent neural nets;speech processing;support vector machines;text analysis","preliminary linguistic text analysis;diverse natural language processing;speech processing;machine translation;appropriate syntactic categories;predicted syntactic tag;conditional random field-based POS tagger;deep learning models;well-balanced language-independent feature;diverse challenges;Urdu POS task;machine learning;support vector machine;deep recurrent neural network;speech tagging;deep learning approaches;rich morphological language;highly inflected object;Urdu comparatively;POS tagging","","24","","77","OAPA","6 Feb 2019","","","IEEE","IEEE Journals"
"Generating phonetic cognates to handle named entities in English-Chinese cross-language spoken document retrieval","H. M. Meng; Wai-Kit Lo; Berlin Chen; K. Tang","Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; National Taiwan University, Taiwan; Princeton University, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","311","314","We have developed a technique for automatic transliteration of named entities for English-Chinese cross-language spoken document retrieval (CL-SDR). Our retrieval system integrates machine translation, speech recognition and information retrieval technologies. An English news story forms a textual query that is automatically translated into Chinese words, which are mapped into Mandarin syllables by pronunciation dictionary lookup. Mandarin radio news broadcasts form spoken documents that are indexed by word and syllable recognition. The information retrieval engine performs matching in both word and syllable scales. The English queries contain many named entities that tend to be out-of-vocabulary words for machine translation and speech recognition, and are omitted in retrieval. Names are often transliterated across languages and are generally important for retrieval. We present a technique that takes in a name spelling and automatically generates a phonetic cognate in terms of Chinese syllables to be used in retrieval. Experiments show consistent retrieval performance improvement by including the use of named entities in this way.","","0-7803-7343-X","10.1109/ASRU.2001.1034649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034649","","Information retrieval;Natural languages;Speech recognition;Radio broadcasting;Dictionaries;Indexing;Engines;Digital multimedia broadcasting;Audio recording;Broadcast technology","information retrieval;speech processing;language translation;speech recognition;text analysis;dictionaries;indexing;pattern matching;natural language interfaces","phonetic cognate generation;named entities;English-Chinese spoken document retrieval;cross-language spoken document retrieval;automatic transliteration;machine translation;speech recognition;information retrieval;textual query;English news story;Chinese words;Mandarin syllables;pronunciation dictionary lookup;Mandarin radio news broadcasts;indexing;word recognition;syllable recognition;matching;out-of-vocabulary words;name spelling;performance improvement","","21","156","5","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Recursive annotations for attention-based neural machine translation","S. Ye; W. Guo","National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Laboratory for Speech and Language Information Processing, University of Science and Technology of China, Hefei, China","2017 International Conference on Asian Language Processing (IALP)","22 Feb 2018","2017","","","164","167","The last few years have witnessed the success of attention-based Neural Machine Translation (NMT), and many of variant models have been used to improve the performance. Most of the proposed attention-based NMT models encode the source sentence into a sequence of annotations which are kept fixed for the following steps. In this paper, we conjecture that the use of fixed annotations is the bottleneck in improving the performance ofconventional attention-based NMT. To tackle this shortcoming, we propose a novel model for attention-based NMT, which is intended to update the source annotations recursively when generating the target word at each time step. Experimental results show that the proposed approach achieves significant performance improvement over multiple test sets.","","978-1-5386-1981-0","10.1109/IALP.2017.8300570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300570","Recursive Annotations;Attention-Based;NMT","","language translation;neural nets","fixed annotations;source annotations;recursive annotations;NMT models;attention-based neural machine translation;conventional attention-based NMT","","2","","18","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Syntax encapsulated phrase model for statistical machine translation","H. Liang; T. Zhao; Y. Xue","MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; Department of New Media and Art, Harbin Institute of Technology, Harbin, China","2012 IEEE International Conference on Information Science and Technology","21 Jun 2012","2012","","","495","499","In the past few years, much attention has been paid on extending phrase-based statistical machine translation with syntactic structures. In this paper we introduce a novel syntax encapsulated phrase(SEP) model, in which treebank tag sequences are employed to decorate the bilingual phrase pairs. We use tag sequences, instead of phrase pairs, to train the lexicalized reordering model. Since the number of treebank tags is much smaller than the number of words, the tag sequence based reordering model is smaller and more accurate than the phrase based reordering model. Experiments were carried out on four types of models: the phrase model, the hierarchical phrase model, the POS tag encapsulated phrase(PTEP) model and the syntactic tag encapsulated phrase(STEP) model. The STEP model obtained higher BLEU-4 score than other models on NIST 2005 MT task.","2164-4357","978-1-4577-0345-4","10.1109/ICIST.2012.6221696","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221696","","Syntactics;Training;Vectors;Computational modeling;NIST;Data models;Mathematical model","computational linguistics;data encapsulation;language translation;statistical analysis;text analysis;trees (mathematics)","syntax encapsulated phrase model;phrase-based statistical machine translation;syntactic structures;SEP model;treebank tag sequences;bilingual phrase pairs;lexicalized reordering model;treebank tags;tag sequence based reordering model;phrase based reordering model;hierarchical phrase model;POS tag encapsulated phrase model;PTEP model;syntactic tag encapsulated phrase model;STEP model;BLEU-4 score;NIST 2005 MT task","","","","19","IEEE","21 Jun 2012","","","IEEE","IEEE Conferences"
"Application-Oriented Comparison and Evaluation of Six Semantic Similarity Measures Based on Wordnet","P. -y. Liu; T. -j. Zhao; X. -f. Yu","MOE MS Key Laboratory of NLP & Speech,School of Computer Science, Harbin Institute of Technology, Harbin, Heilongjiang, China; MOE MS Key Laboratory of NLP & Speech,School of Computer Science, Harbin Institute of Technology, Harbin, Heilongjiang, China; MOE MS Key Laboratory of NLP & Speech,School of Computer Science, Harbin Institute of Technology, Harbin, Heilongjiang, China","2006 International Conference on Machine Learning and Cybernetics","4 Mar 2009","2006","","","2605","2610","In the task of auto-building a Chinese-English semantic lexicon for translation selection, this research presents a method, which introduces WordNet similarity measures to wash out misaligned Chinese-English word pairs. Six different proposed measures of similarity based on WordNet were experimentally compared and evaluated by using WordNet and the software package WordNet::Similarity. It was found that the leader measure is res and lch, then wup, lin takes the fourth place, then jcn, and random is the tailender","2160-1348","1-4244-0061-9","10.1109/ICMLC.2006.258857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4028503","WordNet;WordNet::Similarity;Synset;semantic;similarity;relatedness","Particle measurements;Laboratories;Speech;Software measurement;Software packages;Machine learning;Computer science;Computational linguistics;Text categorization;Data mining","language translation;natural languages;programming language semantics;word processing","Chinese-English semantic lexicon;machine translation;WordNet::Similarity package;semantic similarity measure","","9","","17","IEEE","4 Mar 2009","","","IEEE","IEEE Conferences"
"Injecting a semantic objective function into early stage learning of spoken language translation","M. Beloucif; D. Wu","Human Language Technology Center, Hong Kong University of Science and Technology, Hong Kong; Human Language Technology Center, Hong Kong University of Science and Technology, Hong Kong","2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)","4 May 2017","2016","","","141","146","We describe a new approach for semantically training spoken language translation systems, in which we inject a crosslingual semantic frame based objective function directly into inversion transduction grammar (ITG) induction. This represents an ambitious jump from recent work on improving translation adequacy by using a semantic frame based objective function to drive the tuning of loglinear mixture weights in the final stage of statistical machine translation training. In contrast, our new approach propagates a semantic frame based objective function back into much earlier stages of the pipeline, during the actual learning of the translation model, biasing learning toward semantically more accurate alignments. This approach is motivated by the fact that ITG alignments have empirically been shown to fully cover crosslingual semantic frame alternations, even though they rule out an overwhelming majority of the space of possible alignments. We show that directly driving ITG induction with a crosslingual semantic based objective function not only helps to further sharpen the ITG constraints, but still avoids excising relevant portions of the search space, and leads to better performance than either conventional ITG or GIZA++ based approaches.","2472-7695","978-1-5090-3516-8","10.1109/ICSDA.2016.7919000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919000","","Decision support systems;Conferences;Standardization;Speech;Databases","language translation;pipeline processing","spoken language translation systems;crosslingual semantic frame based objective function;inversion transduction grammar induction;loglinear mixture weights;statistical machine translation training;ITG alignments;ITG constraints;search space","","","","33","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Recent efforts in spoken language translation","F. Casacuberta; M. Federico; H. Ney; E. Vidal",NA; NA; NA; NA,"IEEE Signal Processing Magazine","18 Apr 2008","2008","25","3","80","88","Spoken language translation (SLT) is of great relevance in our increasingly globalized world, both from a social and economic point of view. It is one of the major challenges in automatic speech recognition (ASR) and machine translation (MT), driving an intense research activity in these areas. Speech translation is useful to assist person-to-person communication in limited domains like tourism and traveling and to translate foreign parliamentary speeches and broadcast news. Speech translation is based on a suitable combination of two independent technologies, namely ASR and MT of written language. Thus, the important question is how to pass on the ASR ambiguities to the MT process. A unifying framework for this ASR-MT interface is provided by applying the Bayes decision rule to the speech translation tasks as whole rather than to each task individually. Depending on the MT approaches used, such as finite-state transducers or phrase-based modeling, various types of ASR-MT interfaces have been studied, ranging from N-best lists through word lattices to confusion networks. We have discussed experimental results on various tasks, ranging from limited to unrestricted domains. Despite the significant advances and the large number of experimental studies, it is still an open question what type of interface provides a suitable compromise between translation accuracy and computational cost.","1558-0792","","10.1109/MSP.2008.917989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4490204","","Natural languages;Automatic speech recognition;Speech processing;Robustness;Surveillance;Europe;Broadcasting;Image converters;Decision theory","Bayes methods;decision theory;language translation;natural language processing;speech recognition","spoken language translation;automatic speech recognition;machine translation;person-to-person communication;Bayes decision rule;finite-state transducer;phrase-based modeling;statistical approach;foreign parliamentary speech;word lattice;confusion network","","15","2","30","IEEE","18 Apr 2008","","","IEEE","IEEE Magazines"
"Improving Statistical Machine Translation Using Bayesian Word Alignment and Gibbs Sampling","C. Mermer; M. Saraclar; R. Sarikaya","T√úBƒ∞TAK Bƒ∞LGEM, Kocaeli, Turkey; Department of Electrical and Electronics Engineering, Boƒüazi√ßi University, Istanbul, Turkey; Microsoft Corporation, Redmond, WA, USA","IEEE Transactions on Audio, Speech, and Language Processing","25 Feb 2013","2013","21","5","1090","1101","We present a Bayesian approach to word alignment inference in IBM Models 1 and 2. In the original approach, word translation probabilities (i.e., model parameters) are estimated using the expectation-maximization (EM) algorithm. In the proposed approach, they are random variables with a prior and are integrated out during inference. We use Gibbs sampling to infer the word alignment posteriors. The inferred word alignments are compared against EM and variational Bayes (VB) inference in terms of their end-to-end translation performance on several language pairs and types of corpora up to 15 million sentence pairs. We show that Bayesian inference outperforms both EM and VB in the majority of test cases. Further analysis reveals that the proposed method effectively addresses the high-fertility rare word problem in EM and unaligned rare word problem in VB, achieves higher agreement and vocabulary coverage rates than both, and leads to smaller phrase tables.","1558-7924","","10.1109/TASL.2013.2244087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6425427","Bayesian methods;Gibbs sampling;statistical machine translation (SMT);word alignment","Hidden Markov models;Bayesian methods;Random variables;Inference algorithms;Speech;Speech processing;Computational modeling","belief networks;expectation-maximisation algorithm;inference mechanisms;language translation;natural language processing;probability;random processes;sampling methods;word processing","vocabulary coverage rates;phrase tables;variational Bayes inference;expectation-maximization algorithm;VB;unaligned rare word problem;EM;high-fertility rare word problem;sentence pairs;corpora types;language pairs;random variables;IBM models;Gibbs sampling;Bayesian word alignment posterior inference;statistical machine translation performance improvement","","6","","57","IEEE","1 Feb 2013","","","IEEE","IEEE Journals"
"Incremental Response Generation Using Prefix-to-Prefix Model for Dialogue System","R. Yahagi; Y. Chiba; T. Nose; A. Ito","Graduate School of Engineering, Tohoku University, Japan; Graduate School of Engineering, Tohoku University, Japan; Graduate School of Engineering, Tohoku University, Japan; Graduate School of Engineering, Tohoku University, Japan","2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)","21 Dec 2020","2020","","","349","350","A spoken dialogue system that is currently deployed in many devices cannot respond to a user with a natural switching pause. One of the reasons is that the conventional system generates the response with the pipe-line of several processes, such as speech recognition, response generation, and speech synthesis. The dialogue system should process the user's utterance and generate the response incrementally to achieve natural turn-taking as human-being. In this paper, we examined an incremental response generation method based on a Prefix-to-Prefix model, which is proposed for simultaneous machine translation. This model has a similar structure with the Sequence-to-Sequence model, which is successfully applied to the response generation. We conducted several experiments to confirm the effectiveness of the Prefix-to-Prefix model for incremental response generation.","2378-8143","978-1-7281-9802-6","10.1109/GCCE50665.2020.9291883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291883","spoken dialogue system;response generation","Conferences;Switches;Speech recognition;Data models;Speech synthesis;Consumer electronics","interactive systems;language translation;natural language interfaces;natural language processing;speech processing;speech recognition;speech synthesis","natural switching pause;natural turn-taking;incremental response generation method;sequence-to-sequence model;spoken dialogue system;prefix-to-prefix model;speech recognition;speech synthesis;user utterance;machine translation","","","","7","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Syntactically-informed models for comma prediction","B. Favre; D. Hakkani-Tur; E. Shriberg","International Computer Science Institute, Berkeley, USA; International Computer Science Institute, Berkeley, USA; SRI International, Inc., Menlo Park, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4697","4700","Providing punctuation in speech transcripts not only improves readability, but it also helps downstream text processing such as information extraction or machine translation. In this paper, we improve by 7% the accuracy of comma prediction in English broadcast news by introducing syntactic features inspired by the role of commas as described in linguistics studies. We conduct an analysis of the impact of those features on other subsets of features (prosody, words...) when combined through CRFs. The syntactic cues can help characterizing large syntactic patterns such as appositions and lists which are not necessarily marked by prosody.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960679","Speech Processing;Punctuation;Machine Learning","Predictive models;Broadcasting;Speech processing;Data mining;Computer science;Classification tree analysis;Boosting;Neural networks;Decision trees;Testing","linguistics;natural language processing;speech recognition;text analysis","speech transcription;downstream text processing;information extraction;machine translation;English broadcast news;linguistics;automatic speech recognition systems","","4","","11","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"LegoNN: Building Modular Encoder-Decoder Models","S. Dalmia; D. Okhonko; M. Lewis; S. Edunov; S. Watanabe; F. Metze; L. Zettlemoyer; A. Mohamed","Carnegie Mellon University, Pittsburgh, PA, USA; Samaya AI., Mountain View, CA, USA; Meta Platforms Inc., Menlo Park, CA, USA; Meta Platforms Inc., Menlo Park, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Meta Platforms Inc., Menlo Park, CA, USA; Rembrand Inc., Palo Alto, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","16 Aug 2023","2023","31","","3112","3126","State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or automatic speech recognition (ASR)) are constructed and trained end-to-end as an atomic unit. No component of the model can be (re-)used without the others, making it impossible to share parts, e.g. a high resourced decoder, across tasks. We describe LegoNN, a procedure for building encoder-decoder architectures in a way so that its parts can be applied to other tasks without the need for any fine-tuning. To achieve this reusability, the interface between encoder and decoder modules is grounded to a sequence of marginal distributions over a pre-defined discrete vocabulary. We present two approaches for ingesting these marginals; one is differentiable, allowing the flow of gradients across the entire network, and the other is gradient-isolating. To enable the portability of decoder modules between MT tasks for different source languages and across other tasks like ASR, we introduce a modality agnostic encoder which consists of a length control mechanism to dynamically adapt encoders' output lengths in order to match the expected input length range of pre-trained decoders. We present several experiments to demonstrate the effectiveness of LegoNN models: a trained language generation LegoNN decoder module from German-English (De-En) MT task can be reused without any fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT tasks, matching or beating the performance of baseline. After fine-tuning, LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5% relative WER reduction on the Europarl ASR task. To show how the approach generalizes, we compose a LegoNN ASR model from three modules ‚Äì each has been learned within different end-to-end trained models on three different datasets ‚Äì achieving an overall WER reduction of 19.5%.","2329-9304","","10.1109/TASLP.2023.3296019","FAIR, Meta Platforms Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184317","End-to-end;encoder-decoder models;modularity;speech recognition;machine translation","Task analysis;Decoding;Vocabulary;Predictive models;Training;Artificial intelligence;Machine translation","decoding;encoding;language translation;natural language processing;neural nets;speech recognition","atomic unit;automatic speech recognition;De-En;decoder modules;encoder-decoder architectures;end-to-end trained models;Europarl ASR task;Europarl English ASR;German-English;gradient-isolating;high resourced decoder;LegoNN ASR model;length control mechanism;machine translation;marginal distributions;modality agnostic encoder;modular encoder-decoder models;MT task;pre-defined discrete vocabulary;pre-trained decoders;Romanian-English MT tasks;source languages;state-of-the-art encoder-decoder models","","","","73","IEEE","17 Jul 2023","","","IEEE","IEEE Journals"
"Pronunciation variants generation using SMT-inspired approaches","P. Karanasou; L. Lamel","Spoken Language Processing Group, LIMSI-CNRS, Orsay, France; Spoken Language Processing Group, LIMSI-CNRS, Orsay, France","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","4908","4911","Enriching a pronunciation dictionary with phonological variation is a challenging task, not yet solved despite several decades of research, in particular for speech-to-text transcription of real world data where it is important to cover different pronunciation variants. This paper proposes two alternative methods, inspired by machine translation, to derive pronunciation variants from an initial lexicon with limited variations. In the first case, an n-best pronunciation list is extracted directly from a machine translation tool, used as a grapheme-to-phoneme (g2p) converter. The second is a novel method based on a pivot approach, previously used for the paraphrase extraction task, and here applied as a post-processing step to the g2p converter. Some preliminary speech recognition experiments with the automatically generated pronunciation variants are reported using Quaero development data.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947456","pronunciation variants;SMT;pivot paraphrasing;g2p conversion;English","Dictionaries;Speech recognition;Training;Hidden Markov models;Maximum likelihood decoding;Error analysis;Context","language translation;speech recognition","pronunciation variants generation;SMT-inspired approach;phonological variation;speech-to-text transcription;machine translation;grapheme-to-phoneme converter;g2p converter;speech recognition;Quaero development data","","4","","15","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Syntax-Aware Data Augmentation for Neural Machine Translation","S. Duan; H. Zhao; D. Zhang","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Microsoft Research Asia, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","10 Aug 2023","2023","31","","2988","2999","Data augmentation is an effective method for the performance enhancement of neural machine translation (NMT) by generating additional bilingual data. In this article, we propose a novel data augmentation strategy for neural machine translation. Unlike existing data augmentation methods that simply modify words with the same probability across different sentences, we introduce a sentence-specific probability approach for word selection based on the syntactic roles of words in the sentence. Our motivation is to consider a linguistics-motivated method to obtain more ingenious language generation rather than relying on computation-motivated approaches only. We argue that high-quality aligned bilingual data is crucial for NMT, and only computation-motivated data augmentation is insufficient to provide good enough extra enhancement data. Our approach leverages dependency parse trees of input sentences to determine the selection probability of each word in the sentence using three different functions to calculate probabilities for words with different depths. Besides, our method also revises the probability for words considering the sentence length. We evaluate our methods on multiple translation tasks. The experimental results demonstrate that our proposed data augmentation method does effectively boost existing sentence-independent methods for significant improvement of performance on translation tasks. Furthermore, an ablation study shows that our method does select fewer essential words and preserves the syntactic structure.","2329-9304","","10.1109/TASLP.2023.3301214","National Key Research and Development Program of China(grant numbers:2017YFB0304100); Key Projects of National Natural Science Foundation of China(grant numbers:U1836222,61733011); Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202198","Natural language processing;neural machine translation;data augmentation;dependency parsing","Data augmentation;Transformers;Training;Task analysis;Data models;Syntactics;Linguistics","computational linguistics;data augmentation;grammars;language translation;natural language processing;probability;trees (mathematics)","additional bilingual data;computation-motivated approaches;data augmentation method;different sentences;existing sentence-independent methods;extra enhancement data;fewer essential words;high-quality aligned bilingual data;input sentences;linguistics-motivated method;multiple translation tasks;neural machine translation;NMT;novel data augmentation strategy;selection probability;sentence length;sentence-specific probability approach;syntax-aware data augmentation;word selection","","","","58","IEEE","2 Aug 2023","","","IEEE","IEEE Journals"
"Dependency-to-Dependency Neural Machine Translation","S. Wu; D. Zhang; Z. Zhang; N. Yang; M. Li; M. Zhou","Department of Computer Science, Harbin Institute of Technology, Harbin, China; Microsoft Research Asia, Beijing, China; Department of Computer Science, University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 Aug 2018","2018","26","11","2132","2141","Recent research has proven that syntactic knowledge is effective to improve the performance of neural machine translation (NMT). Most previous work focuses on leveraging either source or target syntax in the recurrent neural network (RNN) based encoder‚Äìdecoder model. In this paper, we simultaneously use both source and target dependency tree to improve the NMT model. First, we propose a simple but effective syntax-aware encoder to incorporate source dependency tree into NMT. The new encoder enriches each source state with dependence relations from the tree. Then, we propose a novel sequence-to-dependence framework. In this framework, the target translation and its corresponding dependence tree are jointly constructed and modeled. During decoding, the tree structure is used as context to facilitate word generations. Finally, we extend the sequence-to-dependence framework with the syntax-aware encoder to build a dependence-NMT model and apply the dependence-based framework to the Transformer. Experimental results on several translation tasks show that both source and target dependence structures can improve the translation quality and their effects can be accumulated.","2329-9304","","10.1109/TASLP.2018.2855968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410795","Syntax;neural machine translation;dependence parsing","Decoding;Syntactics;Computational modeling;Recurrent neural networks;Magnetic heads;Predictive models;Task analysis","decoding;encoding;language translation;recurrent neural nets;trees (mathematics)","neural machine translation;syntax-aware encoder;encoder-decoder model;dependence tree;recurrent neural network;NMT;target translation;novel sequence-to-dependence framework;translation quality;translation tasks;tree structure","","44","","46","IEEE","13 Jul 2018","","","IEEE","IEEE Journals"
"Time Series Neural Networks for Real Time Sign Language Translation","S. S Kumar; T. Wangyal; V. Saboo; R. Srinath","Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India; Computer Science and Engineering Department, PES Institute of Technology, Bangalore, India","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","243","248","Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614068","Sign Language Translation, Computer Vision, Long Short Term Memory Networks, Neural Machine Translation, Attention Neural Networks, Deep Neural Networks, American Sign Language","Assistive technology;Gesture recognition;Videos;Recurrent neural networks;Time series analysis;Shape","computer vision;handicapped aids;language translation;learning (artificial intelligence);natural language processing;neural nets;real-time systems;sign language recognition;time series;video signal processing","time series neural networks;Neural Machine Translation;American Sign Language gloss recognition;hearing impaired;speech impaired;end to end neural network system;real time sign language translation system;ASL gloss recognition;computer vision;sign language communication;continuous sign language videos","","10","","23","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"HMM-based transmodal mapping from audio speech to talking faces","S. Nakamura","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","Neural Networks for Signal Processing X. Proceedings of the 2000 IEEE Signal Processing Society Workshop (Cat. No.00TH8501)","6 Aug 2002","2000","1","","33","42 vol.1","Describes a transmodal mapping from audio speech to talking faces based on hidden Markov models (HMMs). If face movements are synthesized well enough for natural communication, a lot of benefits will be brought to human-machine communication. This paper describes an HMM-based speech-driven lip movement synthesis. The paper also describes its improvement by audio-visual joint estimation and its extension to talking face generation. The results of evaluation experiments show that the proposed method generates natural and accurate talking faces from audio speech inputs.","1089-3555","0-7803-6278-0","10.1109/NNSP.2000.889360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=889360","","Hidden Markov models;Speech synthesis;Signal synthesis;Face;Signal mapping;Image converters;Speech processing;Natural languages;Laboratories;Electronic mail","speech processing;computer animation;hidden Markov models;audio-visual systems;speech-based user interfaces","hidden Markov models;transmodal mapping;audio speech inputs;talking face generation;face movement synthesis;natural communication;human-machine communication;HMM-based speech-driven lip movement synthesis;audio-visual joint estimation","","4","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Time-Restricted Self-Attention Layer for ASR","D. Povey; H. Hadian; P. Ghahremani; K. Li; S. Khudanpur","Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD, USA","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","5874","5878","Self-attention - an attention mechanism where the input and output sequence lengths are the same - has recently been successfully applied to machine translation, caption generation, and phoneme recognition. In this paper we apply a restricted self-attention mechanism (with multiple heads) to speech recognition. By ‚Äúrestricted‚Äù we mean that the mechanism at a particular frame only sees input from a limited number of frames to the left and right. Restricting the context makes it easier to encode the position of the input - we use a I-hot encoding of the frame offset. We try introducing attention layers into TDNN architectures, and replacing LSTM layers with attention layers in TDNN+LSTM architectures. We show experiments on a number of ASR setups. We observe improvements compared to the TDNN and TDNN+LSTM baselines. Attention layers are also faster than LSTM layers in test time, since they lack recurrence.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462497","ASR;attention;lattice-free MMI;neural network;LSTM","Switches;Speech recognition;Indexes;Decoding;Encoding;Neural networks;Task analysis","neural nets;speech recognition","LSTM layers;time-restricted self-attention layer;output sequence lengths;phoneme recognition;speech recognition;TDNN architectures;ASR;input sequence lengths;machine translation;caption generation;I-hot encoding","","65","","16","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"On the use of machine translation for spoken language understanding portability","C. Servan; N. Camelin; C. Raymond; F. B√©chet; R. De Mori","Universit√© d'Avignon et des Pays de Vaucluse, LIA-CERI, Avignon, France; Universit√© d'Avignon et des Pays de Vaucluse, LIA-CERI, Avignon, France; Universit√© Europ√©enne de Bretagne, IRISA-INSA, Rennes, France; Aix Marseille Universite, LIF-CNRS, Marseille, France; Universit√© d'Avignon et des Pays de Vaucluse, LIA-CERI, Avignon, France","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5330","5333","Across language portability of a spoken language understanding system (SLU) deals with the possibility of reusing with moderate effort in a new language knowledge and data acquired for another language. The approach proposed in this paper is motivated by the availability of the fairly large MEDIA corpus carefully transcribed in French and semantically annotated in terms of constituents. A method is proposed for manually translating a portion of the training set for training an automatic machine translation (MT) system to be used for translating the remaining data. As the source language is annotated in terms of concept tags, a solution is presented for automatically transferring these tags to the translated corpus. Experimental results are presented on the accuracy of the translation expressed with the BLEU score as function of the size of the training corpus. It is shown that the process leads to comparable concept error rates in the two languages making the proposed approach suitable for SLU portability across languages.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5494960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494960","Spoken Language Understanding;Portability across languages;Dialog Systems","Natural languages;Error analysis;Classification tree analysis;Stochastic processes;Contracts;Availability;Humans;Size measurement;Testing","language translation;natural language processing;text analysis","spoken language understanding portability;MEDIA training corpus;French;automatic machine translation;BLEU score;SLU portability","","9","","10","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"SBSim: A Sentence-BERT Similarity-Based Evaluation Metric for Indian Language Neural Machine Translation Systems","K. Mrinalini; P. Vijayalakshmi; T. Nagarajan","Sri Sivasubramaniya Nadar College of Engineering, Chennai, Tamil Nadu, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, Tamil Nadu, India; Shiv Nadar University, Chennai, Tamil Nadu, India","IEEE/ACM Transactions on Audio, Speech, and Language Processing","12 Apr 2022","2022","30","","1396","1406","Machine translation (MT) outputs are widely scored using automatic evaluation metrics and human evaluation scores. The automatic evaluation metrics are expected to be easily computable and a reflection of human evaluation. Traditional string-based metrics such as BLEU, ChrF++ scores, are widely used to evaluate MT systems, but fail to account for synonyms that appear in the state-of-the-art neural machine translation (NMT) systems, owing to their inability to evaluate paraphrases. While similarity-based metrics such as Yisi, BERTScore address this issue, these metrics need to be modified to better evaluate morphologically rich Indian languages such as, Tamil and Hindi. The current work proposes a novel and individual sentence-BERT based similarity (SBSim) metric, that makes use of a paraphrase-BERT model and sentence-level embedding to evaluate NMT outputs. The effectiveness of the BLEU, ChrF++, Yisi, BERTScore, and the proposed SBSim are evaluated on English-to-Tamil and English-to-Hindi NMT outputs. The sentence-level metric correlation of the proposed SBSim metric with respect to human scores is observed to outperform the existing metrics with a correlation of 0.9123 and 0.9052 for English-to-Tamil and English-to-Hindi NMT systems, respectively. Further, the average metric correlation of the SBSim metric is also observed to be the highest with a value of 0.9801 and 0.9836 for these NMT systems, respectively. The proposed metric is also evaluated on WMT2020 dataset and reports the highest correlation of 0.7129 with the human scores.","2329-9304","","10.1109/TASLP.2022.3161160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739828","Automatic evaluation;human correlation;individual metric;paraphrase-BERT;sentence-BERT","Measurement;Speech processing;Correlation;Computational modeling;Surface morphology;Machine translation;Linguistics","language translation;natural language processing;neural nets;text analysis","English-to-Hindi NMT systems;SBSim metric;sentence-BERT similarity-based evaluation metric;Indian language neural machine translation systems;automatic evaluation metrics;human evaluation scores;paraphrase-BERT model;sentence-level metric correlation;BLEU;BERTScore;WMT2020 dataset;English-to-Tamil NMT systems;sentence-level embedding;Yisi scores;ChrF++ scores","","8","","27","IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Improving Neural Text Normalization with Partial Parameter Generator and Pointer-Generator Network","W. Jiang; J. Li; M. Chen; J. Ma; S. Wang; J. Xiao",Ping An Technology; Ping An Technology; Ping An Technology; Ping An Technology; Ping An Technology; Ping An Technology,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7583","7587","Text Normalization (TN) is an essential part in conversational systems like text-to-speech synthesis (TTS) and automatic speech recognition (ASR). It is a process of transforming non-standard words (NSW) into a representation of how the words are to be spoken. Existing approaches to TN are mainly rule-based or hybrid systems, which require abundant hand-crafted rules. In this paper, we treat TN as a neural machine translation problem and present a pure data-driven TN system using Transformer framework. Partial Parameter Generator (PPG) and Pointer-Generator Network (PGN) are combined in our model to improve accuracy of normalization and act as auxiliary modules to reduce the number of simple errors. The experiments demonstrate that our proposed model reaches remarkable performance on various semiotic classes.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9415113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415113","Text Normalization;Parameter Genera-tor;Pointer-Generator Network;Text-To-Speech","Error analysis;Conferences;Signal processing;Generators;Acoustics;Semiotics;Grammar","language translation;natural language processing;neural nets;speech recognition;speech synthesis;text analysis","neural text Normalization;partial Parameter Generator;Pointer-Generator Network;conversational systems;automatic speech recognition;transforming nonstandard words;abundant hand-crafted rules;neural machine translation problem;pure data-driven TN system;Transformer framework;Partial Parameter Generator","","5","","22","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Convolutional Dropout and Wordpiece Augmentation for End-to-End Speech Recognition","H. Xu; Y. Huang; Y. Zhu; K. Audhkhasi; B. Ramabhadran","Google Inc, New York, NY, U.S.A.; Google Inc, New York, NY, U.S.A.; Google Inc, New York, NY, U.S.A.; Google Inc, New York, NY, U.S.A.; Google Inc, New York, NY, U.S.A.","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","5984","5988","Regularization and data augmentation are crucial to training end-to-end automatic speech recognition systems. Dropout is a popular regularization technique, which operates on each neuron independently by multiplying it with a Bernoulli random variable. We propose a generalization of dropout, called ""convolutional dropout"", where each neuron‚Äôs activation is replaced with a randomly-weighted linear combination of neuron values in its neighborhood. We believe that this formulation combines the regularizing effect of dropout with the smoothing effects of the convolution operation. In addition to convolutional dropout, this paper also proposes using random word-piece segmentations as a data augmentation scheme during training, inspired by results in neural machine translation. We adopt both these methods during the training of transformer-transducer speech recognition models, and show consistent WER improvements on Librispeech as well as across different languages.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9415004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415004","End-to-end speech recognition;dropout;regularization;Transformer;RNN-transducer","Training;Smoothing methods;Convolution;Conferences;Neurons;Signal processing algorithms;Random variables","natural language processing;neural nets;speech recognition;statistical distributions","convolutional dropout;data augmentation scheme;wordpiece augmentation;end to end speech recognition;regularization technique;Bernoulli random variable;smoothing effects;neuron activation;neural machine translation;Librispeech;random word piece segmentations;weighted linear combination;transformer transducer speech recognition models","","3","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Tuning statistical machine translation parameters using perplexity","A. R. Nabhan; A. Rafea","Department of Mathematics, Faculty of Science, Fayoum Branch, Cairo University, Egypt, Egypt; Computer Science Department, American University, Cairo, Cairo, Egypt","IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.","12 Sep 2005","2005","","","338","343","Statistical machine translation (SMT) involves many tasks including modeling, training, decoding, and evaluation. In this work, we present a methodology for optimizing the training process to get better translation quality using the well known GIZA ++ SMT toolkit. The methodology is based on adjusting the parameters of GIZA ++ that affect the generation of the translation model. When applying the methodology, an average improvement of 7% has been achieved in the translation quality.","","0-7803-9093-8","10.1109/IRI-05.2005.1506496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1506496","","Surface-mount technology;Decoding;Mathematics;Computer science;Optimization methods;Parameter estimation;Speech recognition;Data mining;Testing;Measurement","language translation;statistical analysis","tuning statistical machine translation;GIZA++ SMT toolkit","","8","","15","IEEE","12 Sep 2005","","","IEEE","IEEE Conferences"
"Developing a tagset for Pashto part of speech tagging","I. Rabbi; M. A. Khan; R. Ali","Department of Computer Science, N.W.F.P, University of Peshawar, Pakistan; Department of Computer Science, N.W.F.P, University of Peshawar, Pakistan; Department of Computer Science, N.W.F.P, University of Peshawar, Pakistan","2008 Second International Conference on Electrical Engineering","27 Jun 2008","2008","","","1","6","While building a machine translation system, the embedded part-of-speech (POS) tagger deserves special attention. The ever first tagset discussed here is created in accordance with the EAGLES guidelines. These guidelines were written for the languages of European Union. They can also be applied to Pashto language. This paper presents the creation process of Pashto tagset, which helps in the development of a POS tagger.","","978-1-4244-2292-0","10.1109/ICEE.2008.4553909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553909","","Tagging;Natural languages;Guidelines;Speech processing;Computer science;Encyclopedias;Educational institutions;Computational linguistics","language translation;natural languages;speech recognition","machine translation system;speech tagging;EAGLES guideline;Pashto language","","4","","14","IEEE","27 Jun 2008","","","IEEE","IEEE Conferences"
"Sign Language to Speech Translation","A. Sharma; S. Panda; S. Verma","Mukesh Patel School of Technology, Management and Engineering, NMIMS, Mumbai, India; Mukesh Patel School of Technology, Management and Engineering, NMIMS, Mumbai, India; Mukesh Patel School of Technology, Management and Engineering, NMIMS, Mumbai, India","2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","15 Oct 2020","2020","","","1","8","Although a certain fraction of the world suffers from speech and hearing disabilities, sign language is not a widespread language across the world. In today's oral world, a gesture-based language is not particularly popular with the general masses. However, sign language itself is a fully developed language with multiple regional dialects across the globe. Therefore, to aid with a smoother communication between the speaking and non-speaking world, technical developments can be introduced. A considerable amount of work has been done in this direction. The basic need of the hour is for an application that can function in real-time and can facilitate real-time conversations between a person who can sign in sign language and one that cannot. This paper proposes an application that works on this problem statement. In order to construct such an application designed to respond in real-time and aid a live conversation between a speaking and nonspeaking individual, it is necessary to allow for live video inputs to be made to the app which would then be translated to speech. This paper proposes the use of convolutional neural networks (CNN) alongside the use of Text-to-Speech translator. By use of the CNN algorithm, the gestures can be identified by the proposed application and converted to text, which can then be converted into speech.","","978-1-7281-6851-7","10.1109/ICCCNT49239.2020.9225422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225422","Indian Sign Language;real-time;gesture recognition;neural networks;machine learning;text-to-speech algorithms","Assistive technology;Gesture recognition;Training;Real-time systems;Speech recognition;Libraries;Text recognition","convolutional neural nets;gesture recognition;handicapped aids;language translation;natural language processing;sign language recognition;speech processing","oral world;gesture-based language;sign language;fully developed language;nonspeaking world;text-to-speech translator;speech translation;convolutional neural networks;CNN","","3","","25","IEEE","15 Oct 2020","","","IEEE","IEEE Conferences"
"Shefce: A Cantonese-English bilingual speech corpus for pronunciation assessment","R. W. M. Ng; A. C. M. Kwan; T. Lee; T. Hain","Department of Computer Science, University of Sheffield, United Kingdom; Faculty of Education, The University of Hong Kong, Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science, University of Sheffield, United Kingdom","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","5825","5829","This paper introduces the development of ShefCE: a Cantonese-English bilingual speech corpus from L2 English speakers in Hong Kong. Bilingual parallel recording materials were chosen from TED online lectures. Script selection were carried out according to bilingual consistency (evaluated using a machine translation system) and the distribution balance of phonemes. 31 undergraduate to postgraduate students in Hong Kong aged 20-30 were recruited and recorded a 25-hour speech corpus (12 hours in Cantonese and 13 hours in English). Baseline phoneme/syllable recognition systems were trained on background data with and without the ShefCE training data. The final syllable error rate (SER) for Cantonese is 17.3% and final phoneme error rate (PER) for English is 34.5%. The automatic speech recognition performance on English showed a significant mismatch when applying L1 models on L2 data, suggesting the need for explicit accent adaptation. ShefCE and the corresponding baseline models will be made openly available for academic research.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953273","Bilingual parallel speech corpus;Cantonese;English pronunciation assessment","Speech;Speech recognition;Data models;Training;Error analysis;Tongue;Pragmatics","computer aided instruction;further education;natural language processing;speech processing;speech recognition;text analysis","Cantonese-English bilingual speech corpus;pronunciation assessment;L2 English speakers;Hong Kong;bilingual parallel recording materials;TED online lectures;script selection;bilingual consistency;machine translation system;undergraduate students;postgraduate students;baseline phoneme-syllable recognition systems;ShefCE training data;syllable error rate;phoneme error rate;automatic speech recognition performance;L1 models;L2 data;accent adaptation","","3","","20","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Deaf Community Integration: Sociotechnical Systems","M. Vallejos-Villanueva; L. Naranjo-Zeled√≥n; M. Chac√≥n-Rivas","Instituto Tecnol√≥gico de Costa Rica, Cartago, Costa Rica; Instituto Tecnol√≥gico de Costa Rica, Cartago, Costa Rica; Instituto Tecnol√≥gico de Costa Rica, Cartago, Costa Rica","2019 International Conference on Inclusive Technologies and Education (CONTIE)","30 Jan 2020","2019","","","196","1964","Deaf people must constantly face a society that builds or reinforces barriers that make communication difficult. As a result, an initiative emerged in Inclutec, an interest center at the Technological Institute of Costa Rica (TEC), and a support product was developed that seeks to lend a bridge among Spanish and Costa Rican Sign Language (LESCO). The ""LESCO Translator"" project has been designed to facilitate translation in both ways, Spanish - LESCO, and LESCO - Spanish, languages that have different grammar and therefore the achievement of an automatic translation is further complicated. As the first stage of this project, a sign editor and a speech editor were created. The former one is used to create vocabulary and the latter one to form speeches that use this vocabulary, in order to have informational text translations. This article evidences, as a product of internal research of ethnomethodological design, the flow of experience of this process from a sociotechnical systems perspective, having conceived and validated each of the stages of the project with users of the product, in this case, representatives of the deaf community of Costa Rica. It also exposes the validation of some formal aspects, through the application of a test.","","978-1-7281-5436-7","10.1109/CONTIE49246.2019.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971408","Culture and deaf community;Sociotechnical systems;Assisstive technologies;living lab;Editor Sign Language;LESCO","Sociotechnical systems;Tools;Avatars;Proposals;Assistive technology;Gesture recognition;Ethics","handicapped aids;language translation;social aspects of automation;text analysis;vocabulary","Technological Institute of Costa Rica;TEC;Spanish Sign Language;Costa Rican Sign Language;LESCO Translator project;grammar;automatic translation;sign editor;speech editor;vocabulary;speeches;informational text translations;ethnomethodological design;sociotechnical systems perspective;deaf community integration;deaf people;Inclutec","","1","","16","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"A Review of Statistical and Neural Network Based Hybrid Machine Translators","M. Akter; M. Shahidur Rahman; M. Zafar Iqbal; M. Reza Selim","Department of Computer Science & Engineering, Shahjalal University of Science & Technology, Sylhet, Bangladesh; Department of Computer Science & Engineering, Shahjalal University of Science & Technology, Sylhet, Bangladesh; Department of Computer Science & Engineering, Shahjalal University of Science & Technology, Sylhet, Bangladesh; Department of Computer Science & Engineering, Shahjalal University of Science & Technology, Sylhet, Bangladesh","2018 International Conference on Bangla Speech and Language Processing (ICBSLP)","2 Dec 2018","2018","","","1","6","Machine Translation has become one of the top research interests for a few years. Compared to others, statistical and neural network approaches toward machine translation are performing the best. However, both approaches have advantages over one another. In recent years, few researchers have conducted experiments to create hybrid machine translation systems that combine the strength of statistical and neural network approach to produce a better translation. In this paper, we review all of these research methodologies, datasets, evaluation scores and point out their shortcomings.","","978-1-5386-8207-4","10.1109/ICBSLP.2018.8554792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554792","Machine translation;Hybrid Machine Translator;Statistical Machine Translation;Neural Machine Translation","Neural networks;Data models;Decoding;Vocabulary;Training;Interpolation;Probability","language translation;neural nets;statistical analysis","neural network;research methodologies;research interests;hybrid machine translation systems;statistical network","","1","","36","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Access-independent Cloud-based Real-Time Translation Service for Voice Calls in Mobile Networks","M. A. TUNDIK; A. HILT; G. BOTA; L. NAGY; K. LUUKKANEN","Budapest University of Technology and Economics, Hungary; Nokia Mobile Networks, Cloud Core, Budapest, Hungary; Nokia Mobile Networks, Cloud Core, Budapest, Hungary; Nokia Mobile Networks, Cloud Core, Budapest, Hungary; Business Management, Nokia Mobile Networks, Espoo, Finland","2018 11th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP)","27 Sep 2018","2018","","","1","6","Nokia offers a speech translation solution for mobile network operators: Operators can provide real-time machine translation of speech, as a useful extension to their currently available voice service. The concept offers a beneficial difference against Over-the-Top (OTT) service providers. The successfully implemented solution presented in this paper works with any device, that is capable of voice calls, even with simple mobile handsets (feature phones). In the service, the Operator‚Äôs MSS (Mobile Switching Center Server) and/or IMS (IP Multimedia Subsystem) core is involved. The core network is connected to the Microsoft Azure Data Center that provides the speech translation through a Nokia Translation Application Server, running in a Nokia Data Center.","","978-1-5386-1335-1","10.1109/CSNDSP.2018.8471757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471757","speech translation;voice call;mobile operator;core network;IMS;VoLTE;VoWiFi","Real-time systems;Servers;Speech processing;3G mobile communication;Business;Data centers;Automobiles","cloud computing;IP networks;language translation;mobile computing;mobile handsets","OTT service providers;MSS;voice service;IP multimedia subsystem;Microsoft Azure Data Center;real-time machine translation;mobile network operators;speech translation solution;Mobile networks;access-independent cloud-based real-time Translation service;Nokia Data Center;Nokia Translation Application Server;core network;IMS core;Mobile Switching Center Server;simple mobile handsets;voice calls","","1","","30","IEEE","27 Sep 2018","","","IEEE","IEEE Conferences"
"Translation from Simple Marathi sentences to Indian Sign Language Using Phrase-Based Approach","S. R. Bhagwat; R. P. Bhavsar; B. V. Pawar","School of Computer Sciences, KBC NMU, Jalgaon, India; School of Computer Sciences, KBC NMU, Jalgaon, India; School of Computer Sciences, KBC NMU, Jalgaon, India","2021 International Conference on Emerging Smart Computing and Informatics (ESCI)","9 Apr 2021","2021","","","367","373","Machine Translation (MT) has proven to be a useful tool for translating one spoken language to another since the mid-,90s. Ultimately, it has helped to bridge the communication gap between people unaware of each other's languages. A variation of MT, named `Sign Language Machine Translation (SLMT),' enables the translation of spoken text/speech to Sign Language. It helps reduce the `hearing' people's difficulties in connecting with the hearing-impaired community. Thus, it can aid in the inclusion of hard-hearing in the rest of society. We have developed a system to translate a simple Marathi language sentence to the equivalent Indian Sign Language representation. We have studied the morpho-syntactic characteristics of both languages, and contrastive analysis is discussed here. This study gave us a cue to attempt translation at phrase level from the source language side. We have formulated the rules to extract necessary grammatical knowledge and to utilize it for translation. The detailed approach, experimentation, and results are also addressed in this research work. This automated system can be used at public places like banks, post-offices, railway stations for effective short communications. It can be availed as a self-learning tool to learn Indian Sign Language. The system can be used whenever sign language interpreters are not readily available.","","978-1-7281-8519-4","10.1109/ESCI50559.2021.9396900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396900","Phrase-Based Machine Translation;Syntactic Transfer;Indian Sign Language;Computational Linguistics;Marathi","Assistive technology;Gesture recognition;Tools;Animation;Rail transportation;Machine translation;Informatics","handicapped aids;language translation;linguistics;natural language processing;sign language recognition;speech synthesis;text analysis","phrase-based approach;MT;spoken language;Sign Language Machine Translation;hearing people;hearing-impaired community;sign language interpreters;Indian sign language representation;Marathi language sentence;hard-hearing;contrastive analysis","","5","","24","IEEE","9 Apr 2021","","","IEEE","IEEE Conferences"
"A hybrid approach to Chinese-English machine translation","Liu Ying; Liu Qun; Zhang Xiang; Chang Baobao","Institute of Computing Technology, Chinese Academy and Sciences, BeiJing, China; Institute of Computing Technology, Chinese Academy and Sciences, BeiJing, China; Institute of Computing Technology, Chinese Academy and Sciences, BeiJing, China; Institute of'computational Linguistics, Peking University, BeiJing, China","1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335)","6 Aug 2002","1997","2","","1146","1150 vol.2","A hybrid method for Chinese-English machine translation is presented. A rule-based analysis is combined with statistical data. The rule-based lexical analyzer and syntactic analyzer leave some amount of ambiguity that is resolved using a statistical approach. A hidden Markov model (HMM) is used to return a score for each part of speech. An improved probabilistic context free grammar (PCFG) is used for syntactic scoring, and a probabilistic score function is used for the semantic aspect. In addition, parameters are usually estimated poorly when the training data is sparse. Smoothing the parameters is thus important in the estimation process. A back-off procedure is used to smooth the parameters.","","0-7803-4253-4","10.1109/ICIPS.1997.669166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=669166","","Hidden Markov models;Speech;Parameter estimation;Training data;Smoothing methods","language translation;knowledge based systems;statistical analysis;hidden Markov models;context-free grammars;probability;parameter estimation;computational linguistics;natural languages","hybrid approach;Chinese-English machine translation;rule-based analysis;statistical data;rule-based lexical analyzer;syntactic analyzer;statistical approach;hidden Markov model;probabilistic context free grammar;syntactic scoring;probabilistic score function;parameter estimation;training data;parameter smoothing","","","6","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"An English part-of-speech tagger for machine translation in business domain","J. Ma; H. Liu; D. Huang; W. Sheng","School of Computer Science and Technology, and School of Foreign Languages, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, and School of Foreign Languages, Dalian University of Technology, Dalian, China","2011 7th International Conference on Natural Language Processing and Knowledge Engineering","26 Jan 2012","2011","","","183","189","Part-of-speech tagging is a crucial preprocessing step for machine translation. Current studies mainly focus on the methods, linguistic, statistic, machine learning or hybrid. But so far not many serious attempts have been performed to test the reported accuracy of taggers on different, perhaps domain-specific, corpora. Therefore, this paper presents an English POS tagger for English-Chinese machine translation in business domain, demonstrating how a present tagger can be adapted to learn from a small amount of data and handle unknown words for the purpose of machine translation. A small size of 998k English annotated corpus in business domain is built semi-automatically based on a new tagset, the maximum entropy model is adopted and rule-based approach is used in post-processing. Experiments show that our tagger achieves an accuracy of 99.08% in closed test and 98.14% in open test, which is a quite satisfactory result, compared with the reported best open test result of 97.18% of Stanford English tagger.","","978-1-61284-729-0","10.1109/NLPKE.2011.6138191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138191","English POS tagging;maximum entropy;rule-based approach;machine translation;business domain","Indium phosphide;Hidden Markov models","business data processing;knowledge based systems;language translation;maximum entropy methods;natural language processing","English part-of-speech tagger;business domain;English-Chinese machine translation;maximum entropy model;rule-based approach","","3","","20","IEEE","26 Jan 2012","","","IEEE","IEEE Conferences"
"Research on Feature Weights of Liheci Word Sense Disambiguation","Z. Zhang; X. Li; X. Tian","Hebei key laboratory of Machine Learning and Computational Intelligence, Baoding, China; Hebei key laboratory of Machine Learning and Computational Intelligence, Baoding, China; Hebei key laboratory of Machine Learning and Computational Intelligence, Baoding, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","12 May 2016","2015","2","","7","10","Concerning the problems of the comparative translation is not accurate in machine translation and the useful information is unable to match in information retrieval, a liheci word sense disambiguation method is adopted and a classifier model is established using Support Vector Machines(SVM). To improve the accuracy of the liheci word sense disambiguation, it extracts not only local words(LW), local part-of-speeches(POS), local words and part-of-speeches (LWP) but also the middle insert part of the separated form as disambiguation features according to the characteristics of liheci, When the text features are converted to feature vectors for improving on boolean weight method, we can fix feature weights of some type in turn and change the other two types' to verify the disambiguation effect of three kinds of features, respectively. The results show that the effect of LW, LWP is higher than POS. Setting higher feature weights to LW and LWP, the disambiguation accuracy can effectively improve.","","978-1-4673-9587-8","10.1109/ISCID.2015.221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469048","Liheci;SVM;word sense disambiguation;feature weights;calssifier","Feature extraction;Support vector machines;Context;Kernel;Information retrieval;Knowledge based systems","Boolean algebra;information retrieval;language translation;natural language processing;support vector machines;text analysis","Liheci word sense disambiguation;machine translation;information retrieval;classifier model;support vector machines;SVM;LW;POS;LWP;local word-and-part-of-speeches;text features;feature vectors;boolean weight method","","","","14","IEEE","12 May 2016","","","IEEE","IEEE Conferences"
"A machine learning based approach for the detection and recognition of Bangla sign language","M. Hasan; T. H. Sajib; M. Dey","Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, Chittagong, Bangladesh","2016 International Conference on Medical Engineering, Health Informatics and Technology (MediTec)","30 Jan 2017","2016","","","1","5","Speech impaired people are detached from the mainstream society due to the lacking of proper communication aid. Sign language is the primary means of communication for them which normal people do not understand. In order to facilitate the conversation conversion of sign language to audio is very necessary. This paper aims at conversion of sign language to speech so that disabled people have their own voice to communicate with the general people. In this paper, Hand Gesture recognition is performed using HOG (Histogram of Oriented Gradients) for extraction of features from the gesture image and SVM (Support Vector Machine) as classifier. Finally, predict the gesture image with output text. This output text is converted into audible sound using TTS (Text to Speech) converter.","","978-1-5090-5421-3","10.1109/MEDITEC.2016.7835387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835387","SVM;Classification;BdSL;TTS Engine;Feature;HOG;Contouring;Prediction;Recognition rate","Gesture recognition;Support vector machines;Assistive technology;Training;Testing;Feature extraction;Databases","feature extraction;language translation;medical image processing;pattern classification;sign language recognition;speech synthesis;support vector machines","machine learning;Bangla sign language detection;Bangla sign language recognition;speech impaired people;sign language conversation conversion;hand gesture recognition;Histogram of Oriented Gradients;feature extraction;gesture image;support vector machine;SVM classifier;audible sound;text-to-speech converter","","22","","12","IEEE","30 Jan 2017","","","IEEE","IEEE Conferences"
"Real-Time Translation of Sign Language for Speech Impaired","A. D. Shetty; J. Shetty; K. K; Rakshitha; S. S. B","Department of CSE, Nitte (Deemed to be University), NMAM Institute of Technology (NMAMIT), Nitte, India; Department of CSE, Nitte (Deemed to be University), NMAM Institute of Technology (NMAMIT), Nitte, India; Department of CSE, SJEC, Mangalore; Department of AI/ML, Nitte (Deemed to be University), NMAM Institute of Technology (NMAMIT), Nitte, India; Department of CSE, Nitte (Deemed to be University), NMAM Institute of Technology (NMAMIT), Nitte, India","2023 7th International Conference on Computing Methodologies and Communication (ICCMC)","4 Apr 2023","2023","","","570","575","Sign language is a visual language that uses hand gestures, change of hand shape, and tracking information to express meaning, and is the main communication tool for people with hearing and language impairment. Given the barriers faced by speech-impaired individuals, this system introduced a tool that bridges communication gaps and supports better interactions. This work focuses on introducing a tool that should bridge the communication gap among speech impaired community. The work involves the development of a system that enables two-way conversation between people with speech disorders and noisy people. LSTM networks were studied and implemented for the classification of gesture data because of their ability to learn long-term dependencies. In real-time, the sign language gestures of speech-impaired individuals are fed to the system by the device's computer vision capabilities. These gestures are recognized using deep neural networks, while hand recognition is cracked with edge detection algorithms that interpret in both text and speech formats. The model is trained with the dataset that is collected using holistic key points from the video of the person which detects the pose, face and hand landmarks. These will convert speech to text and finally displays the relevant hand gestures. This model can predict with an accuracy of 90%, showing the feasibility of using LSTM-based neural networks for the purpose of sign language translation.","","978-1-6654-6408-6","10.1109/ICCMC56507.2023.10084053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10084053","Sign Language Converter;Long-short Term Memory;Convolutional Neural Network;Natural Language Processing;Machine Learning","Bridges;Visualization;Text recognition;Computational modeling;Neural networks;Gesture recognition;Speech recognition","computer vision;deep learning (artificial intelligence);edge detection;feature extraction;gesture recognition;handicapped aids;language translation;learning (artificial intelligence);neural nets;recurrent neural nets;sign language recognition;visual languages","bridges communication gaps;communication gap;gesture data;hand landmarks;hand recognition;hand shape;hearing;language impairment;LSTM-based neural networks;main communication tool;noisy people;relevant hand gestures;sign language gestures;sign language translation;speech disorders;speech formats;speech impaired community;speech-impaired individuals;time translation;visual language","","","","16","IEEE","4 Apr 2023","","","IEEE","IEEE Conferences"
"Machine translation of different systemic languages using a Apertium platform (with an example of English and Kazakh languages)","S. Assem; S. Aida","Department of Information Systems, Kazakh National University, Almaty, Kazakhstan; Department of Information Systems, Kazakh National University, Almaty, Kazakhstan","2013 International Conference on Computer Applications Technology (ICCAT)","30 May 2013","2013","","","1","4","This paper describes the first steps in the project of building a prototype of a free/open-source rule-based machine translation system that translates from English to Kazakh. The goal of this article is to examine a grammatical and lexical problems, which we often face while translating English texts, and not giving any detailed statement of grammatical or lexical phenomenon. Apertium platform translates many different languages and it uses Hidden Markov Models. This article shows description of machine translation which translates english to kazakh.","","978-1-4673-5285-7","10.1109/ICCAT.2013.6522002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522002","","Speech;Prototypes;Companies;Dictionaries;Educational institutions;Open source software;Reactive power","grammars;knowledge based systems;language translation;natural language processing;public domain software","systemic languages;Apertium platform;free/open-source rule-based machine translation system;English;Kazakh;grammatical problems;lexical problems","","","","4","IEEE","30 May 2013","","","IEEE","IEEE Conferences"
"Knowledge Graph Generation From Text Using Neural Machine Translation Techniques","A. Gupte; S. Sapre; S. Sonawane","Dept. of Computer Engineering, Pune Institute of Computer Technology, Pune, India; Dept. of Computer Engineering, Pune Institute of Computer Technology, Pune, India; Dept. of Computer Engineering, Pune Institute of Computer Technology, Pune, India","2021 International Conference on Communication information and Computing Technology (ICCICT)","12 Aug 2021","2021","","","1","8","As the applications of data science become pervasive in daily life, there arises a dire need to represent data in machine-understandable forms like knowledge graphs. Over the years, there have been numerous developments in extracting entities and their relations for augmenting knowledge graphs, but many of them depend on external dependencies like dependency parsers and part-of-speech taggers. These approaches, while indeed accomplishing this task, induce a certain degree of inflexibility in their implementation. Recent explorations in this domain have attempted to utilize Neural Machine Translation techniques to convert natural language to SPARQL queries, with a focus on information retrieval from pre-established Knowledge Graphs. We explore in detail, the variety of approaches followed for SPARQL machine translation, with a keen focus on insertion of extracted knowledge into the graphs.As part of our research, we curated a dataset- Scientists-100, extracted from Dbpedia, for the task of translation of natural language to SPARQL insertion statements. We also propose two models - an Attention RNN and a Transformer for the same. These models achieve an accuracy of 99.27% and a 98.61% respectively on the dataset. In addition to this, we present a metric for examining the syntactic accuracy of the generated SPARQL statements. Our models exhibit 99.25% and 98.71% syntactic accuracy as calculated on the same.","","978-1-6654-0430-3","10.1109/ICCICT50803.2021.9510164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510164","Semantic Web;Knowledge Graphs;Machine Translation;Neural Networks;SPARQL","Measurement;Semantic Web;Natural languages;Semantics;Syntactics;Information retrieval;Knowledge discovery","grammars;graph theory;information retrieval;language translation;natural language processing;natural languages;ontologies (artificial intelligence);query processing;semantic Web;text analysis","SPARQL queries;pre-established Knowledge Graphs;SPARQL machine translation;extracted knowledge;natural language;SPARQL insertion statements;generated SPARQL statements;Knowledge graph generation;Neural Machine Translation techniques;machine-understandable forms;numerous developments;extracting entities;augmenting knowledge graphs;external dependencies;dependency parsers;part-of-speech taggers;recent explorations","","2","","26","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"Bilingual chunk alignment in statistical machine translation","Yu Zhou; Chengqing Zong; Bo Xu","NLPR, Institute of Automation, Chinese Academy and Sciences, China; NLPR, Institute of Automation, Chinese Academy and Sciences, China; NLPR, Institute of Automation, Chinese Academy and Sciences, China","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)","7 Mar 2005","2004","2","","1401","1406 vol.2","In this paper a new algorithm called multilayer filtering (MLF) is proposed for extracting bilingual alignment chunks automatically from a Chinese-English parallel corpus. Multiple layers are used to extract bilingual chunks according to different features of chunks in the bilingual corpus. And the alignment chunks are one-to-one corresponding with each other. The chunking and alignment algorithm doesn't rely on the information from tagging, parsing, syntax analyzing or segmenting for Chinese corpus as most conventional algorithms do. Preliminary experimental results show that the algorithm achieves a good performance in chunking and alignment. Moreover, the translations generated by this algorithm are much better than the results generated by the baseline (word-based statistical machine translation).","1062-922X","0-7803-8566-7","10.1109/ICSMC.2004.1399826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1399826","","Filtering algorithms;Automation;Surface-mount technology;Statistical analysis;Data mining;Tagging;Performance analysis;Speech;Robustness;Training data","language translation;statistical analysis;natural languages","bilingual chunk alignment;word-based statistical machine translation;multilayer filtering;Chinese-English parallel corpus;syntax analyzing","","","","13","IEEE","7 Mar 2005","","","IEEE","IEEE Conferences"
"Future-Aware Knowledge Distillation for Neural Machine Translation","B. Zhang; D. Xiong; J. Su; J. Luo","Software School, Xiamen University, Xiamen, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Software School, Xiamen University, Xiamen, China; Department of Computer Science, University of Rochester, Rochester, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","26 Nov 2019","2019","27","12","2278","2287","Although future context is widely regarded useful for word prediction in machine translation, it is quite difficult in practice to incorporate it into neural machine translation. In this paper, we propose a future-aware knowledge distillation framework (FKD) to address this issue. In the FKD framework, we learn to distill future knowledge from a backward neural language model (teacher) to future-aware vectors (student) during the training phase. The future-aware vector for each word position is computed in a bridge network and optimized towards the corresponding hidden state in the backward neural language model via a knowledge distillation mechanism. We further propose an algorithm to jointly train the neural machine translation model, neural language model and knowledge distillation module end-to-end. The learned future-aware vectors are incorporated into the attention layer of the decoder to provide full-range context information during the decoding phase. Experiments on the NIST Chinese-English and WMT English-German translation tasks show that the proposed method significantly improves translation quality and word alignment.","2329-9304","","10.1109/TASLP.2019.2946480","Baidu Scholarship; National Natural Science Foundation of China(grant numbers:61672440,61622209,61861130364); Fundamental Research Funds for the Central Universities(grant numbers:ZK1024); National Key R&D Program of China(grant numbers:2019QY1802/03); Scientific Research Project of National Language Committee of China(grant numbers:YB135-49); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8863411","Future context;knowledge distillation;natural language processing;neural machine translation","Decoding;History;Context modeling;Predictive models;Computational modeling;Training;Semantics","language translation;learning (artificial intelligence);natural language processing;word processing","backward neural language model;neural machine translation model;WMT English-German translation tasks;word prediction;future-aware knowledge distillation framework;FKD framework;future-aware vectors learning;NIST Chinese-English","","12","","37","IEEE","9 Oct 2019","","","IEEE","IEEE Journals"
"An Approach to Word Sense Disambiguation in English-Vietnamese-English Statistical Machine Translation","Q. Nguyen; A. Nguyen; D. Dinh","Faculty of Computer Sciences, University of Information Technology, Vietnam National University, Ho Chi Minh, Vietnam; Linguistic Research and Development Department, Kim Tu Dien Multilingual Data Center, Ho Chi Minh, Vietnam; Faculty of Information Technology, University of Natural Sciences, Vietnam National University, Ho Chi Minh, Vietnam","2012 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future","15 Mar 2012","2012","","","1","4","NA","","978-1-4673-0309-5","10.1109/rivf.2012.6169839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6169839","","Buildings;Testing;Vectors;Training;Tagging;Semantics;Support vector machines","language translation;natural language processing;text analysis","word sense disambiguation;English-Vietnamese-English statistical machine translation;polysemous words;text context;text topic;part-of-speech;morphology;BLEU scores","","2","","11","IEEE","15 Mar 2012","","","IEEE","IEEE Conferences"
"Fusion of multi-features for Mongolian part of speech","Congjiao Xie; H. Hou; Jing Wu","College of Computer Science, Inner Mongolia University, Hohhot, P. R. China; College of Computer Science, Inner Mongolia University, Hohhot, P. R. China; College of Computer Science, Inner Mongolia University, Hohhot, P. R. China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","14 Jan 2016","2015","","","1291","1295","Mongolian is a kind of typical agglutinative language. The configuration of Mongolian expresses different grammatical meanings by connecting different affixes. The additional ingredients of agglutinative languages usually have only one meaning, and the connections have rules to follow. As a kind of agglutinative language, Mongolian can infer the part of speech and the semantic information of the stem from the additional components. Mongolian can also decide the collocation information matched with the stem. According to the characteristics of Mongolian word formation, and taking morphemes as partition granularity, this paper puts forward a method of fusing multi-features for Mongolian part of speech by using conditional random field model. Experiments show that this method obtains a satisfactory result with the part of speech tagging accuracy of 98.8%.","","978-1-4673-7682-2","10.1109/FSKD.2015.7382129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382129","Multi feature;Mongolian;Part of Speech Tagging;CRF","Tagging;Hidden Markov models;Speech;Context;Computational modeling;Computer science;Semantics","language translation;learning (artificial intelligence);natural language processing","Mongolian part of speech;agglutinative language;Mongolian word formation;statistical machine learning;machine translation;natural language processing system","","","","32","IEEE","14 Jan 2016","","","IEEE","IEEE Conferences"
"Translating language with technology's help","M. Paulik; S. Stuker; C. Fugen; T. Schultz; A. Waibel","InterACT Res. Labs.; NA; NA; Carnegie Mellon University; School of Computer Science, Carnegie Mellon University, Carnegie","IEEE Potentials","21 May 2007","2007","26","3","30","35","In this article, we introduced an iterative system for improving speech recognition in the context of human mediated translation scenarios. In contrast to related work conducted in this field, we included scenarios in which only spoken language representations are available. One key feature of our iterative system is that all involved system components, ASR as well as MT, are improved. Particularly in the context of a spoken source language representation, not only is the target language ASR automatically improved but so is the source language ASR. Using Spanish as the source language and English as the target language, we were able to reduce the WER of the English ASR by 35.8% when given a written-source language representation. Given a spoken-source language representation, we achieved a relative WER reduction of 29.9% for English and 20.9% for Spanish","1558-1772","","10.1109/MP.2007.361642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4205212","","Automatic speech recognition;Natural languages;Speech recognition;Speech enhancement;Speech processing;Humans;Decoding;Equations;History;Sociotechnical systems","iterative methods;language translation;speech recognition","iterative system;automatic speech recognition;ASR;human mediated translation scenario;spoken language representation;machine translation;Spanish;source language;English;target language;language translation","","","","9","IEEE","21 May 2007","","","IEEE","IEEE Magazines"
"Evaluation of different English translations of Holy Koran in scope of verb process type","M. E. Shenassa; M. J. Khalvandi","Computer Department Science and Researches Campus, Islamic Azad University, Tehran, Iran; Computer Department South Tehran Campus, Islamic Azad University, Tehran, Iran","2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications","23 May 2008","2008","","","1","4","Access to a text which is the best translation from one language to another is one of the most important requirements of information societies. In other word, there may be many translations of a source text in which the most convenient one should be selected. The Holy Koran is one of the well-known books in the world which has been descended in Arabic and most of peoples would like to understand more about this book. As a result, there are more than 100 translations of Koran from Arabic to English. In this paper a system has been designed to evaluate different English translations of Koran using tools and concepts such as pos-tagging, natural language processing, computational linguistic and machine learning. To do such an evaluation, each verb process type in translated text should be compared with it's of Koran text. The system uses Halliday Grammar, which is a useful theory for analyzing a formal text, to do this. It has been assumed that each verb in the text of Koran has been tagged manually. At the other hand, to detect each verb process type in translated English texts, a tagger is used to detect and tag each verb based on Halliday grammar. The best translation is one in which the number of similar verb process type in the source and translated text is maximum. This evaluation's been performed on 5 English translations of Holy Koran (Qaraei, Pickthal, Arberry, Progressive Muslims, Yusuf Ali) in scope of Bashir and Nadhir Ayahs of Baqarah and Alelmran Surahs and the results show that Yusuf Ali translation is the best. In this paper, it has been tried to create not only a new way to analyze the quality of translating a text but also take a step to discover concepts and meanings of the Koran. Although it is seemed that some of Koran words are so meaningful that it is nearly impossible to put them in one class of verb process type.","","978-1-4244-1751-3","10.1109/ICTTA.2008.4530013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530013","","Natural language processing;Computational linguistics;Books;Machine learning;Speech analysis;Sensor phenomena and characterization;Java;Performance evaluation;Statistics;Natural languages","language translation;natural language processing","English translations;holy Koran;verb process type;Arabic;computational linguistic;machine learning;Halliday Grammar","","2","","11","IEEE","23 May 2008","","","IEEE","IEEE Conferences"
"BaNeP: An End-to-End Neural Network Based Model for Bangla Parts-of-Speech Tagging","J. A. Ovi; M. A. Islam; M. R. Karim","Department of Computer Science and Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Dhaka, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Dhaka, Dhaka, Bangladesh","IEEE Access","4 Oct 2022","2022","10","","102753","102769","In Natural Language Processing, Parts-of-Speech tagging is a vital component that significantly impacts applications like machine translation, spell-checker, information retrieval, and speech processing. In languages such as English and Dutch, POS tagging is considered a solved problem (accuracy: 97%). However, for low-resource languages like Bangla, challenges are still there. In this article, we have proposed a novel RNN-based network named BaNeP to determine parts of speech for Bangla words. The proposed network extracts structural features through a bidirectional LSTM-based sub-network, and intricate contextual relations among words of a sentence are identified through an elaborate weighted context extraction procedure. These features are then combinedly utilized to generate the final Parts-of-Speech prediction. Training the model requires only an annotated dataset vanishing the need for any hand-crafted features. Experimental results on the LDC2010T16 dataset show significant accuracy improvement compared to existing Bangla POS taggers.","2169-3536","","10.1109/ACCESS.2022.3208269","Centennial Research Grant (CRG), University of Dhaka; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896860","Bangla;POS tagging;RNN;sequence labeling","Tagging;Sequential analysis;Feature extraction;Labeling;Hidden Markov models;Machine translation;Natural language processing;Speech processing;Neural networks;Information retrieval;Recurrent neural networks;Text recognition","information retrieval;language translation;natural language processing;recurrent neural nets;speech recognition;text analysis;word processing","weighted context extraction procedure;Bangla POS taggers;Bangla parts-of-speech tagging;hand-crafted features;bidirectional LSTM-based sub-network;structural features;Bangla words;RNN-based network;low-resource languages;speech processing;information retrieval;machine translation;natural language processing;end-to-end neural network based model;BaNeP","","1","","46","CCBY","21 Sep 2022","","","IEEE","IEEE Journals"
"Multimodal Word Discovery and Retrieval With Spoken Descriptions and Visual Concepts","L. Wang; M. Hasegawa-Johnson","Electrical and Computer Engineering, University of Illinois System, Urbana, USA; Electrical and Computer Engineering, University of Illinois System, Urbana, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Jun 2020","2020","28","","1560","1573","In the absence of dictionaries, translators, or grammars, it is still possible to learn some of the words of a new language by listening to spoken descriptions of images. If several images, each containing a particular visually salient object, each co-occur with a particular sequence of speech sounds, we can infer that those speech sounds are a word whose definition is the visible object. A multimodal word discovery system accepts, as input, a database of spoken descriptions of images (or a set of corresponding phone transcriptions) and learns a mapping from waveform segments (or phone strings) to their associated image concepts. In this article, four multimodal word discovery systems are demonstrated: three models based on statistical machine translation (SMT) and one based on neural machine translation (NMT). The systems are trained with phonetic transcriptions, MFCC and multilingual bottleneck features (MBN). On the phone-level, the SMT outperforms the NMT model, achieving a 61.6% F1 score in the phone-level word discovery task on Flickr30k. On the audio-level, we compared our models with the existing ES-KMeans algorithm for word discovery and present some of the challenges in multimodal spoken word discovery.","2329-9304","","10.1109/TASLP.2020.2996082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097433","Unsupervised word discovery;language acquisition;machine translation;multimodal learning","Hidden Markov models;Acoustics;Speech processing;Task analysis;Image retrieval;Random variables;Adaptation models","feature extraction;language translation;learning (artificial intelligence);natural language processing;neural nets;speech processing;statistical analysis","phone transcriptions;multimodal spoken word discovery;phone-level word discovery task;neural machine translation;statistical machine translation;associated image concepts;phone strings;multimodal word discovery system;visible object;speech sounds;particular visually salient object;visual concepts;spoken descriptions","","1","","41","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"Delphi-Based Evaluation of Mobile Online Translation Tools","M. Fanqi; B. Songbin","School of Information Engineering Northeast Dianli University Jilin, China; School of Foreign Languages Northeast Dianli University Jilin, China","2015 Fifth International Conference on Communication Systems and Network Technologies","1 Oct 2015","2015","","","679","683","Mobile online translation (MOT) is a new form of application of machine translation in the age of mobile learning. Nowadays, there are lots of MOT apps in Android app store. Since the techniques used in MOT apps and their functions are different, their features are also different. In order to objectively evaluate MOT apps for selecting suitable translation tools, we have studied a Delphi-based evaluation method. In this paper, firstly, we describe the basic functions and realization mechanisms of MOT tools, and then we analyze 10 factors which should be considered in the evaluation of MOT tools, on the basis of the analyses, we propose the Delphi-based evaluation method with a relatively complete indicator system, and we also determine the weights of every indicator by using Delphi. The experiment results on 6 MOT apps show that the evaluation method is practical and objective for selecting suitable translation tools.","","978-1-4799-1797-6","10.1109/CSNT.2015.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280004","software testing;machine translation;computer aided translation;system evaluation","Mobile communication;Software;Computers;Speech recognition;Computational linguistics;Mobile computing;Internet","Android (operating system);Internet;learning (artificial intelligence);mobile computing;natural language processing;smart phones","Delphi-based evaluation method;Android app store;MOT apps;mobile learning;machine translation;mobile online translation tools","","","","10","IEEE","1 Oct 2015","","","IEEE","IEEE Conferences"
"Machine translation : an Indian perspective","R. M. K. Sinha","Indian Institute of Technology, Kanpur, India","Language Engineering Conference, 2002. Proceedings","28 Feb 2003","2002","","","181","182","I present a review of machine translation strategies and examine the infrastructure that needs to be created to facilitate R&D efforts in an Indian context. In particular, I present work on ANGABHARTI and ANUBHARTI approaches and their hybridization.","","0-7695-1885-0","10.1109/LEC.2002.1182306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1182306","","Natural languages;Humans;Speech;Planets;Business;Vehicles;Knowledge engineering;Artificial intelligence;Computer science;Informatics","language translation","machine translation;Indian context;R&D;ANGABHARTI approach;ANUBHARTI approach","","","","23","IEEE","28 Feb 2003","","","IEEE","IEEE Conferences"
"An Online Relevant Set Algorithm for Statistical Machine Translation","C. Tillmann; T. Zhang","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Statistics Department, Rutgers University, Piscataway, NJ, USA","IEEE Transactions on Audio, Speech, and Language Processing","15 Aug 2008","2008","16","7","1274","1286"," This paper presents a novel online relevant set algorithm for a linearly scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a statistical machine translation (SMT) decoder. This training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The novel algorithm is evaluated using different feature types: 1) commonly used probabilistic features, such as translation, language, or distortion model probabilities, and 2) binary features. In particular, encouraging results on a standard Arabic‚ÄìEnglish translation task are presented for a translation system that uses only binary feature functions. To further demonstrate the effectiveness of the novel training algorithm, a detailed comparison with the widely used minimum-error-rate (MER) training algorithm is presented using the same decoder and feature set. The online algorithm is simplified by introducing so-called ‚Äúseed‚Äù block sequences which enable the training to be carried out without a gold standard block translation. While the online training algorithm is extremely fast, it also improves translation scores over the MER algorithm in some experiments. ","1558-7924","","10.1109/TASL.2008.921760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599396","Discriminative learning;online algorithm;statistical machine translation","Decoding;Surface-mount technology;Probability;Gold;Machine learning;Concatenated codes;Australia;Statistics;Natural languages;Tagging","language translation;natural languages;probability;statistical analysis","online relevant set algorithm;linearly scored block sequence translation model;global scoring function;statistical machine translation decoder;probabilistic features;binary features;Arabic-English translation;minimum-error-rate training algorithm","","2","","30","IEEE","15 Aug 2008","","","IEEE","IEEE Journals"
"Design and development of a frame based MT system for English-to-ISL","K. Anuja; S. Suryapriya; S. M. Idicula","Department of Computer Science, Cochin university of Science and Technology, Cochin, Kerala, India; Department of Computer Science, Cochin university of Science and Technology, Cochin, Kerala, India; Department of Computer Science, Cochin university of Science and Technology, Cochin, Kerala, India","2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)","22 Jan 2010","2009","","","1382","1387","This paper presents the design and development of a frame based approach for speech to sign language machine translation system in the domain of railways and banking. This work aims to utilize the capability of Artificial intelligence for the improvement of physically challenged, deaf-mute people. Our work concentrates on the sign language used by the deaf community of Indian subcontinent which is called Indian Sign Language (ISL). Input to the system is the clerk's speech and the output of this system is a 3D virtual human character playing the signs for the uttered phrases. The system builds up 3D animation from pre-recorded motion capture data. Our work proposes to build a Malayalam to ISL.","","978-1-4244-5053-4","10.1109/NABIC.2009.5393721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393721","machine translation;Indian sign language;language processing;motioncapture;3D virtual human","Handicapped aids;Deafness;Speech;Morphology;Auditory system;Rail transportation;Banking;Humans;Natural languages;Shape","artificial intelligence;language translation;natural language processing;speech processing","machine translation system;artificial intelligence;deaf-mute people;Indian sign language;virtual human character;uttered phrases;3D animation;motion capture data;language processing","","17","","17","IEEE","22 Jan 2010","","","IEEE","IEEE Conferences"
"Semi-Supervised Neural Machine Translation via Marginal Distribution Estimation","Y. Wang; Y. Xia; L. Zhao; J. Bian; T. Qin; E. Chen; T. -Y. Liu","Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Jul 2019","2019","27","10","1564","1576","Neural machine translation (NMT) heavily relies on parallel bilingual corpora for training. Since large-scale, high-quality parallel corpora are usually costly to collect, it is appealing to exploit monolingual corpora to improve NMT. Inspired by the law of total probability, which connects the probability of a given target-side monolingual sentence to the conditional probability of translating from a source sentence to the target one, we propose to explicitly exploit this connection and help the training procedure of NMT models using monolingual data. The key technical challenge of this approach is that there are exponentially many source sentences for a target monolingual sentence while computing the sum of the conditional probability given each possible source sentence. We address this challenge by leveraging the reverse translation model (target-to-source translation model) to sample several mostly likely source-side sentences and avoid enumerating all possible candidate source sentences. Then we propose two different methods to leverage the law of total probability, including marginal distribution regularization and likelihood maximization of monolingual corpora. Experiment results on English-French and German-English tasks demonstrate that our methods achieve significant improvement over several strong baselines.","2329-9304","","10.1109/TASLP.2019.2921423","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000904); National Natural Science Foundation of China(grant numbers:61727809,U1605251); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732422","Neural machine translation;semi-supervised learning;natural language processing","Training;Data models;Decoding;Neural networks;Mathematical model;Computational modeling;Task analysis","language translation;natural language processing;neural nets;probability","marginal distribution regularization;likelihood maximization;monolingual corpora;semisupervised neural machine translation;marginal distribution estimation;high-quality parallel corpora;total probability;conditional probability;NMT models;monolingual data;target monolingual sentence;reverse translation model;target-to-source translation model;source-side sentences;source sentences;target-side monolingual sentence;parallel bilingual corpora;English-French tasks;German-English tasks","","7","","51","IEEE","6 Jun 2019","","","IEEE","IEEE Journals"
"A Novel Sentence-Level Agreement Architecture for Neural Machine Translation","M. Yang; R. Wang; K. Chen; X. Wang; T. Zhao; M. Zhang","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; National Institute of Information and Communications Technology, Kyoto-shi, Japan; National Institute of Information and Communications Technology, Kyoto-shi, Japan; Tencent AI Lab, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Soochow University, Suzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Sep 2020","2020","28","","2585","2597","In neural machine translation (NMT), there is a natural correspondence between source and target sentences. The traditional NMT method does not explicitly model the translation agreement on sentence-level. In this article, we propose a comprehensive and novel sentence-level agreement architecture to alleviate this problem. It directly minimizes the difference between the representations of the source-side and target-side sentence on sentence-level. First, we compare a variety of sentence representation strategies and propose a ‚ÄúGated Sum‚Äù sentence representation to achieve better sentence semantic information. Then, rather than a single-layer sentence-level agreement architecture, we further propose a multi-layer sentence agreement architecture to make the source and target semantic spaces closer layer by layer. The proposed agreement module can be integrated into NMT as an additional training objective function, and can also be used to enhance the representation of the source-side sentences. Experiments on the NIST Chinese-to-English and the WMT English-to-German translation tasks show that the proposed agreement architecture achieves significant improvements over state-of-the-art baselines, demonstrating the effectiveness and necessity of exploiting sentence-level agreement for NMT.","2329-9304","","10.1109/TASLP.2020.3021347","National Natural Science Foundation of China(grant numbers:61525205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185042","Neural machine translation (NMT);sentence-level agreement","Semantics;Computer architecture;Task analysis;Decoding;Logic gates;Training","language translation;natural language processing;text analysis","NMT method;gated sum sentence representation;semantic spaces;WMT English-to-German translation tasks;source-side sentences;agreement module;multilayer sentence agreement architecture;single-layer sentence-level agreement architecture;sentence semantic information;target-side sentence;translation agreement;neural machine translation","","5","","62","IEEE","2 Sep 2020","","","IEEE","IEEE Journals"
"Multi-scale-audio indexing for translingual spoken document retrieval","Hsin-Min Wang; H. Meng; P. Schone; B. Chen; Wai-Kit Lo","Inst. of Inf. Sci., Acad. Sinica, Taipei, Taiwan; NA; NA; NA; NA","2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)","7 Aug 2002","2001","1","","605","608 vol.1","MEI (Mandarin-English Information) is an English-Chinese crosslingual spoken document retrieval (CL-SDR) system developed during the Johns Hopkins University Summer Workshop 2000. We integrate speech recognition, machine translation, and information retrieval technologies to perform CL-SDR. MEI advocates a multi-scale paradigm, where both Chinese words and subwords (characters and syllables) are used in retrieval. The use of subword units can complement the word unit in handling the problems of Chinese word tokenization ambiguity, Chinese homophone ambiguity, and out-of-vocabulary words in audio indexing. This paper focuses on multi-scale audio indexing in MEI. Experiments are based on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3), where we indexed Voice of America Mandarin news broadcasts by speech recognition on both the word and subword scales. We discuss the development of the MEI syllable recognizer, the representations of spoken documents using overlapping subword n-grams and lattice structures. Results show that augmenting words with subwords is beneficial to CL-SDR performance.","1520-6149","0-7803-7041-4","10.1109/ICASSP.2001.940904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=940904","","Indexing;Information retrieval;Speech recognition;Error analysis;Vocabulary;Natural languages;Radio broadcasting;Gold;Information science;Systems engineering and theory","audio signal processing;information retrieval;grammars;speech recognition;language translation;signal representation;database indexing","multi-scale audio indexing;Mandarin-English Information system;Johns Hopkins University;speech recognition;machine translation;information retrieval;multi-scale paradigm;English-Chinese crosslingual spoken document retrieval;Chinese words;Chinese subwords;Chinese characters;Chinese word token ambiguity;Chinese homophone ambiguity;out-of-vocabulary words;Topic Detection and Tracking Corpora;TDT-2;TDT-3;Voice of America;Mandarin news broadcasts;syllable recognizer;spoken document representation;subword n-grams;lattice structures;Chinese syllables","","1","","11","IEEE","7 Aug 2002","","","IEEE","IEEE Conferences"
"A Generalized Subspace Distribution Adaptation Framework for Cross-Corpus Speech Emotion Recognition","S. Li; P. Song; L. Ji; Y. Jin; W. Zheng","School of Computer and Control Engineering, Yantai University, China; School of Computer and Control Engineering, Yantai University, China; School of Computer and Control Engineering, Yantai University, China; School of Physics and Electronic Engineering, Jiangsu Normal University, China; Key Laboratory of Child Development and Learning Science (Southeast University), Ministry of Education, Research Center for Learning Science, Southeast University, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","In this paper, we propose a novel transfer learning framework, named generalized subspace distribution adaptation (GSDA), to tackle the challenging cross-corpus speech emotion recognition problem. First, we learn a common low-dimensional feature subspace by utilizing a generalized subspace learning method. Second, we develop a novel distance metric to reduce the divergence between the source and target corpora, which can efficiently explore the similarity and dissimilarity information in the process of knowledge transfer. Third, to demonstrate the effectiveness of our framework, we apply GSDA to the traditional subspace learning algorithms. Finally, we conduct extensive experiments by using the low-level features and deep features on three popular emotional databases, i.e., Berlin, IEMOCAP, and CVE. The results demonstrate that the proposed framework can achieve better performance than several state-of-the-art transfer learning approaches.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10097258","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097258","Transfer Learning;Cross-corpus Speech Emotion Recognition;Distribution Adaptation","Measurement;Learning systems;Emotion recognition;Databases;Transfer learning;Signal processing algorithms;Speech recognition","","","","","","29","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Towards integrated machine translation using structural alignment from syntax-augmented synchronous parsing","B. Xiang; B. Zhou; M. ƒåmejrek","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","513","518","In current statistical machine translation, IBM model based word alignment is widely used as a starting point to build phrase-based machine translation systems. However, such alignment model is separated from the rest of machine translation pipeline and optimized independently. Furthermore, structural information is not taken into account in the alignment model, which sometimes leads to incorrect alignments. In this paper, we present a novel method to connect a re-alignment model with a translation model in an integrated framework. We conduct bilingual chart parsing based on syntax-augmented synchronous context-free grammar. A Viterbi derivation tree is generated for each sentence pair with multiple features employed in a log-linear model. A new word alignment is created under the structural constraint from the Viterbi tree. Extensive experiments are conducted in a Farsi-to-English translation task in conversational speech domain and also a German-to-English translation task in text domain. Systems trained on the new alignment provide significant higher BLEU scores compared to a state-of-the-art baseline.","","978-1-4244-5478-5","10.1109/ASRU.2009.5372892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5372892","","Viterbi algorithm;Hidden Markov models;Context modeling;Pipelines;Surface-mount technology;Natural languages;Solids;Joining processes;Speech analysis","context-free grammars;language translation;natural languages;trees (mathematics)","machine translation;structural alignment;syntax-augmented synchronous parsing;bilingual chart parsing;syntax-augmented synchronous context-free grammar;Viterbi derivation tree;log-linear model;word alignment;Farsi-to-English translation task;German-to-English translation task","","","1","26","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Efficient One-Pass Decoding with NNLM for Speech Recognition","Y. Shi; W. -Q. Zhang; M. Cai; J. Liu","Department of Electronic and Engineering, Tsinghua University, Beijing, China; Department of Electronic and Engineering, Tsinghua University, Beijing, China; Department of Electronic and Engineering, Tsinghua University, Beijing, China; Department of Electronic and Engineering, Tsinghua University, Beijing, China","IEEE Signal Processing Letters","5 Feb 2014","2014","21","4","377","381","Neural network language model (NNLM) has achieved very good results in the field of speech recognition, machine translation, etc. Direct decoding with NNLM is challenging for the overwhelmingly heavy burden in complexity. Most of the previous work focused on rescoring the N-best list and lattice with NNLM in the second pass. In this work, several techniques are explored to directly incorporate the NNLM into the decoder of speech recognition. A novel training algorithm based on variance regularization is proposed to approximate the softmax-normalizing factor as a constant for fast evaluation. Also, the evaluation of NNLM is further speeded up via our advanced storage. Moreover, a simple cache-based strategy is explored to avoid redundant computations during the decoding process. To the authors' knowledge, it is the first time to directly incorporate NNLM into decoding. We evaluate our proposed methods on an English-Switchboard phone-call speech-to-text task. Experimental results show that incorporating the NNLM into the decoder significantly reduces the word error rate (WER) by 1.5% and 1.4% absolutely on the Hub5'00-SWB and RT03S-FSH sets, respectively. Also, the decoding with NNLM is twice as fast as the baseline at the same word error rate.","1558-2361","","10.1109/LSP.2014.2303136","National Natural Science Foundation of China(grant numbers:61370034,61273268,61005019,90920302); Beijing Natural Science Foundation Program(grant numbers:KZ201110005005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727395","Neural network language model;one-pass decoding;speech recognition","Decoding;Artificial neural networks;Speech recognition;Complexity theory;Approximation methods;Lattices","decoding;neural nets;speech recognition","one pass decoding;NNLM;speech recognition;neural network language model;machine translation;direct decoding;variance regularization;softmax normalizing factor;cache based strategy;word error rate","","18","","11","IEEE","28 Jan 2014","","","IEEE","IEEE Journals"
"A graph-based cross-lingual projection approach for spoken language understanding portability to a new language","S. Kim","Human Language Technology Department, Institute for Infocomm Research, Singapore","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8332","8336","The portability of spoken language understanding to a new language can be improved by the results of automatic translation. However, the translation errors can cause the falling-off in the quality of the target language system. This paper proposes a graph-based projection approach to improve the robustness against the translation errors in cross-lingual spoken language understanding. The experimental results show that our proposed approach can significantly improve the performances of the task in a new language.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639290","Spoken Dialogue Systems;Spoken Language Understanding;Language Portability;Statistical Machine Translation","Training;Speech;Vectors;Semantics;Speech processing;Noise reduction;Noise measurement","electronic data interchange;graph theory;language translation","graph-based cross-lingual projection;spoken language portability;automatic translation;translation errors;target language system","","","","16","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Attention With Sparsity Regularization for Neural Machine Translation and Summarization","J. Zhang; Y. Zhao; H. Li; C. Zong","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","17 Dec 2018","2019","27","3","507","518","The attention mechanism has become the de facto standard component in neural sequence to sequence tasks, such as machine translation and abstractive summarization. It dynamically determines which parts in the input sentence should be focused on when generating each word in the output sequence. Ideally, only few relevant input words should be attended to at each decoding time step and the attention weight distribution should be sparse and sharp. However, previous methods have no good mechanism to control this attention weight distribution. In this paper, we propose a sparse attention model in which a sparsity regularization term is designed to augment the objective function. We explore two kinds of regularizations: $L_{\infty }$-norm regularization and minimum entropy regularization, both of which aim to sharpen the attention weight distribution. Extensive experiments on both neural machine translation and abstractive summarization demonstrate that our proposed sparse attention model can substantially outperform the strong baselines. And the detailed analyses reveal that the final attention distribution indeed becomes sparse and sharp.","2329-9304","","10.1109/TASLP.2018.2883740","National Natural Science Foundation of China(grant numbers:61673380); Beijing Advanced Innovation for Language Resources of Beijing Language and Culture University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550728","Sequence to sequence learning;attention mechanism;sparsity regularization;machine translation;summarization","Decoding;Speech processing;Linear programming;Entropy;Task analysis;Standards;Training","entropy;language translation;neural nets;visual perception","attention weight distribution;sparse attention model;sparsity regularization term;minimum entropy regularization;neural machine translation;abstractive summarization;final attention distribution;attention mechanism;neural sequence;sequence tasks;input sentence;output sequence;relevant input words;de facto standard component","","15","","45","IEEE","28 Nov 2018","","","IEEE","IEEE Journals"
"Cross-lingual latent semantic analysis for language modeling","Woosung Kim; S. Khudanpur","Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","1","","I","257","Statistical language model estimation requires large amounts of domain-specific text, which is difficult to obtain in many languages. We propose techniques which exploit domain-specific text in a resource-rich language to adapt a language model in a resource-deficient language. A primary advantage of our technique is that in the process of cross-lingual language model adaptation, we do not rely on the availability of any machine translation capability. Instead, we assume that only a modest-sized collection of story-aligned document-pairs in the two languages is available. We use ideas from cross-lingual latent semantic analysis to develop a single low-dimensional representation shared by words and documents in both languages, which enables us to (i) find documents in the resource-rich language pertaining to a specific story in the resource-deficient language, and (ii) extract statistics from the pertinent documents to adapt a language model to the story of interest. We demonstrate significant reductions in perplexity and error rates in a Mandarin speech recognition task using this technique.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1325971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1325971","","Natural languages;Automatic speech recognition;Speech processing;Natural language processing;Information retrieval;Adaptation model;Availability;Statistical analysis;Error analysis;Speech recognition","semantic networks;speech processing;natural languages;computational linguistics;statistical analysis;error statistics;speech recognition","speech processing;natural language processing;cross-lingual latent semantic analysis;language modeling;statistical language model estimation;domain-specific text;resource-rich language;resource-deficient language;cross-lingual language model adaptation;story-aligned document-pairs;low-dimensional representation;document statistics extraction;perplexity reduction;error rate reduction;speech recognition","","1","","9","IEEE","30 Aug 2004","","","IEEE","IEEE Conferences"
"A Deep Learning Approach for Identifying and Discriminating Spoken Arabic Among Other Languages","A. A. Alashban; Y. A. Alotaibi","Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Access","9 Feb 2023","2023","11","","11613","11628","Spoken Language Identification (SLID) is an important step in speech-to-speech translation systems and multi-lingual automatic speech recognition. In recent research, deep learning mechanisms have been the prevailing approaches for spoken language identification. This paper aims to study, detect, and analyze spoken languages similar to Arabic in pronouncing certain words and then proposes a deep learning-based architecture, specifically the Bidirectional Long Short Term Memory (BLSTM), for spoken Arabic language identification and discrimination between these similar languages, namely, German, Spanish, French, and Russian, all of which are taken from Mozilla speech corpus languages. Additionally, our work involves a linguistic study of these considered languages. A total of ten thousand speakers are chosen for all five languages, and the BLSTM architecture is designed and implemented using acoustic signal features and applied to five experiments in this paper. The results show a precision of 98.97%, 98.73%, 98.47%, and 99.75% for identifying the spoken Arabic language separately along with German, Spanish, French, and Russian, respectively. Additionally, we achieved an average accuracy of 95.15% for discriminating between all these considered five languages in terms of the pronunciation of words. Our findings confirm that a BLSTM architecture is able to distinguish between observable similar pronunciations of words in considered languages.","2169-3536","","10.1109/ACCESS.2023.3241855","King Saud University, Riyadh, Saudi Arabia, through the Researchers Supporting Project(grant numbers:RSP-2021/322); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035385","Arabic;BLSTM architecture;deep learning;language identification;Mozilla speech corpus;multi-lingual;SLID;speech and language processing","Deep learning;Speech recognition;Convolutional neural networks;Support vector machines;Speech processing;Random forests;Mel frequency cepstral coefficient;Natural language processing","deep learning (artificial intelligence);feature extraction;language translation;linguistics;natural language processing;recurrent neural nets;speech recognition","Bidirectional Long Short Term Memory;BLSTM architecture;considered languages;deep learning approach;deep learning mechanisms;deep learning-based architecture;discriminating spoken Arabic;Mozilla speech corpus languages;multilingual automatic speech recognition;prevailing approaches;similar languages;speech-to-speech translation systems;spoken Arabic language;spoken language identification","","","","55","CCBY","2 Feb 2023","","","IEEE","IEEE Journals"
"Linguistic Knowledge-Aware Neural Machine Translation","Q. Li; D. F. Wong; L. S. Chao; M. Zhu; T. Xiao; J. Zhu; M. Zhang","University of Macau, Taipa, Macau, MO; University of Macau, Taipa, Macau, MO; University of Macau, Taipa, Macau, MO; Alibaba Inc, Hangzhou, China; Northeastern University, Shenyang, Liaoning, CN; Northeastern University, Shenyang, Liaoning, CN; Soochow University, Suzhou, Jiangsu, CN","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Aug 2018","2018","26","12","2341","2354","Recently, researchers have shown an increasing interest in incorporating linguistic knowledge into neural machine translation (NMT). To this end, previous works choose either to alter the architecture of NMT encoder to incorporate syntactic information into the translation model, or to generalize the embedding layer of the encoder to encode additional linguistic features. The former approach mainly focuses on injecting the syntactic structure of the source sentence into the encoding process, leading to a complicated model that lacks the flexibility to incorporate other types of knowledge. The latter extends word embeddings by considering additional linguistic knowledge as features to enrich the word representation. It thus does not explicitly balance the contribution from word embeddings and the contribution from additional linguistic knowledge. To address these limitations, this paper proposes a knowledge-aware NMT approach that models additional linguistic features in parallel to the word feature. The core idea is that we propose modeling a series of linguistic features at the word level (knowledge block) using a recurrent neural network (RNN). And in sentence level, those word-corresponding feature blocks are further encoded using a RNN encoder. In decoding, we propose a knowledge gate and an attention gate to dynamically control the proportions of information contributing to the generation of target words from different sources. Extensive experiments show that our approach is capable of better accounting for importance of additional linguistic, and we observe significant improvements from 1.0 to 2.3 BLEU points on Chinese$\leftrightarrow$ English and English$\rightarrow$German translation tasks.","2329-9304","","10.1109/TASLP.2018.2864648","National Natural Science Foundation of China(grant numbers:6176116608,61672555,61432013,61732005); Fundamental Research Funds for the Central Universities; Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research; Joint Project of Macao Science and Technology Development Fund and National Natural Science Foundation of China(grant numbers:045/2017/AFJ); Multiyear Research Grant from the University of Macau(grant numbers:MYRG2017-00087-FST,MYRG2015-00175-FST,MYRG2015-00188-FST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430540","Attention gate;knowledge block;knowledge gate;neural machine translation (NMT)","Linguistics;Syntactics;Decoding;Speech processing;Recurrent neural networks;Encoding","encoding;language translation;linguistics;natural language processing;recurrent neural nets","linguistic knowledge-aware neural machine translation;NMT encoder;syntactic information;word embeddings;word representation;knowledge-aware NMT approach;recurrent neural network;RNN encoder;knowledge gate","","18","","54","IEEE","9 Aug 2018","","","IEEE","IEEE Journals"
"HMM Word and Phrase Alignment for Statistical Machine Translation","Y. Deng; W. Byrne","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Engineering Department, University of Cambridge, Cambridge, UK","IEEE Transactions on Audio, Speech, and Language Processing","12 Feb 2008","2008","16","3","494","507","Estimation and alignment procedures for word and phrase alignment hidden Markov models (HMMs) are developed for the alignment of parallel text. The development of these models is motivated by an analysis of the desirable features of IBM Model 4, one of the original and most effective models for word alignment. These models are formulated to capture the desirable aspects of Model 4 in an HMM alignment formalism. Alignment behavior is analyzed and compared to human-generated reference alignments, and the ability of these models to capture different types of alignment phenomena is evaluated. In analyzing alignment performance, Chinese-English word alignments are shown to be comparable to those of IBM Model 4 even when models are trained over large parallel texts. In translation performance, phrase-based statistical machine translation systems based on these HMM alignments can equal and exceed systems based on Model 4 alignments, and this is shown in Arabic-English and Chinese-English translation. These alignment models can also be used to generate posterior statistics over collections of parallel text, and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality.","1558-7924","","10.1109/TASL.2008.916056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4443885","Hidden Markov model;phrase alignment;statistical machine translation;word alignment","Hidden Markov models;Surface-mount technology;Viterbi algorithm;Statistics;Performance analysis;Research initiatives;Parameter estimation;Power generation","hidden Markov models;language translation;natural language processing;text analysis;word processing","hidden Markov models;word alignment;phrase alignment;statistical machine translation;parallel text;Chinese-English word alignments;Model 4 alignments;Arabic-English translation","","17","","34","IEEE","12 Feb 2008","","","IEEE","IEEE Journals"
"Efficient computation of confidence intervals forword error rates","J. M. Vilar","Departamento de Lenguajes y Sistemas Inform√°ticos, Universitat Jaume I, Castellon, Spain","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","5101","5104","Word error rate is a standard measure of quality for different tasks such as speech recognition, OCR or machine translation. As such, it is important to compute it together with confidence intervals. Previous works in the literature employ Monte Carlo methods in order to compute those intervals. We show how to compute them without simulations. We also adapt a method that compares two systems over the same test data so that it can be used without simulations.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518806","error analysis;word error rate","Error analysis;Computational modeling;Sampling methods;Random variables;Speech recognition;Optical character recognition software;Distributed computing;Measurement standards;System testing;Length measurement","error statistics;language translation;optical character recognition;speech recognition","confidence intervals;word error rates;speech recognition;OCR;machine translation;Monte Carlo methods","","3","","6","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"Disambiguating Discourse Connectives for Statistical Machine Translation","T. Meyer; N. Hajlaoui; A. Popescu-Belis","Google, Inc., Switzerland; European Parliament, Luxembourg; Idiap Research Institute, Switzerland","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","7","1184","1197","This paper shows that the automatic labeling of discourse connectives with the relations they signal, prior to machine translation (MT), can be used by phrase-based statistical MT systems to improve their translations. This improvement is demonstrated here when translating from English to four target languages-French, German, Italian and Arabic-using several test sets from recent MT evaluation campaigns. Using automatically labeled data for training, tuning and testing MT systems is beneficial on condition that labels are sufficiently accurate, typically above 70%. To reach such an accuracy, a large array of features for discourse connective labeling (morpho-syntactic, semantic and discursive) are extracted using state-of-the-art tools and exploited in factored MT models. The translation of connectives is improved significantly, between 0.7% and 10% as measured with the dedicated ACT metric. The improvements depend mainly on the level of ambiguity of the connectives in the test sets.","2329-9304","","10.1109/TASLP.2015.2422576","Schweizerische Nationalfonds zur F√∂rderung der Wissenschaftlichen Forschung(grant numbers:CRSI22 127510,CRSII2 147653); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7084603","Discourse connectives;machine translation (MT)","Testing;Training;Feature extraction;Tuning;Labeling;IEEE transactions;Speech","language translation;natural language processing;statistical analysis","statistical machine translation;discourse connective disambiguation;phrase-based statistical MT system;French language;German language;Italian language;Arabic language","","3","","67","IEEE","13 Apr 2015","","","IEEE","IEEE Journals"
"Out-of-domain detection based on confidence measures from multiple topic classification","L. R. Lane; T. Kawahara; T. Matsui; S. Nakamura","ATR Spoken Language Translation Laboratories, Kyoto, Japan; ATR Spoken Language Translation Laboratories, Kyoto, Japan; School of Informatics, Kyoto University, Kyoto, Japan; ATR Spoken Language Translation Laboratories, Kyoto, Japan","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","1","","I","757","One significant problem for spoken language systems is how to cope with users' OOD (out-of-domain) utterances which cannot be handled by the back-end system. In this paper, we propose a novel OOD detection framework, which makes use of classification confidence scores of multiple topics and trains a linear discriminant in-domain verifier using gradient probabilistic descent (GPD). Training is based on deleted interpolation of the in-domain data, and thus does not require actual OOD data, providing high portability. Three topic classification schemes of word N-gram models, latent semantic analysis (LSA), and support vector machines (SVM) are evaluated, and SVM is shown to have the greatest discriminative ability. In an OOD detection task, the proposed approach achieves an absolute reduction in equal error rate (EER) of 6.5% compared to a baseline method based on a simple combination of multiple-topic classifications. Furthermore, comparison with a system trained using OOD data demonstrates that the proposed training scheme realizes comparable performance while requiring no knowledge of the OOD data set.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1326096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326096","","Natural languages;Support vector machines;Support vector machine classification;Routing;Informatics;Laboratories;Mathematics;Interpolation;User interfaces;Predictive models","speech recognition;error statistics;pattern classification;support vector machines;gradient methods;interpolation","out-of-domain detection;multiple topic classification;linear discriminant;in-domain verifier;GPD;deleted interpolation;portability;word N-gram models;LSA;SVM;EER reduction;equal error rate;performance;gradient probabilistic descent;latent semantic analysis;support vector machines;spoken language systems;classification confidence scores","","3","2","8","IEEE","30 Aug 2004","","","IEEE","IEEE Conferences"
"Report on workshop on high performance computing and communications for grand challenge applications: computer vision, speech and natural language processing, and artificial intelligence","B. W. Wah; T. S. Huang; A. K. Joshi; D. Moldovan; J. Aloimonos; R. K. Bajcsy; D. Ballard; D. DeGroot; K. DeJong; C. R. Dyer; S. E. Fahlman; R. Grishman; L. Hirschman; R. E. Korf; S. E. Levinson; D. P. Miranker; N. H. Morgan; S. Nirenburg; T. Poggio; E. M. Riseman; C. Stanfill; S. J. Stolfo; S. L. Tanimoto; C. Weems","Coordinated Science Laboratory, University of Illinois, Urbana, IL, USA; Coordinated Science Laboratory, University of Illinois, Urbana, IL, USA; Computer and Information Science Department, Moore School, University of Pennsylvania, Philadelphia, PA, USA; Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA, USA; Center of Automation Research, University of Maryland, College Park, MD, USA; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA; Department of Computer Science, University of Rochester, Rochester, NY, USA; Texas Instruments, Inc., Plano, TX, USA; Computer Science Department, George Mason University, Fairfax, VA, USA; Department of Computer Science, University of Wisconsin, Madison, WI, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Computer Science Department, New York University, New York, NY, USA; Spoken Language Group, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science Department, University of California, Los Angeles, CA, USA; Speech Research Department, AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA; Department of Computer Sciences, University of Technology, Austin, TX, USA; International Computer Science Institute, Berkeley, CA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer and Information Science Department, University of Massachusetts, Amherst, MA, USA; Advanced Information Systems, Thinking Machines Corporation, Cambridge, MA, USA; Computer Science Department, Columbia University, New York, NY, USA; Department of Computer Science, University of Washington, Seattle, WA, USA; Computer and Information Science Department, University of Massachusetts, Amherst, MA, USA","IEEE Transactions on Knowledge and Data Engineering","6 Aug 2002","1993","5","1","138","154","The findings of a workshop, the goals of which were to identify applications, research problems, and designs of high performance computing and communications (HPCC) systems for supporting applications are discussed. In computer vision, the main scientific issues are machine learning, surface reconstruction, inverse optics and integration, model acquisition, and perception and action. In speech and natural language processing (SNLP), issues were identified statistical analysis in corpus-based speech and language understanding, search strategies for language analysis, auditory and vocal-tract modeling, integration of multiple levels of speech and language analyses, and connectionist systems. In AI, important issues that need immediate attention include the development of efficient machine learning and heuristic search methods that can adapt to different architectural configurations, and the design and construction of scalable and verifiable knowledge bases, active memories, and artificial neural networks.<>","1558-2191","","10.1109/69.204098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=204098","","Machine vision;Learning systems;Natural languages;Speech processing","computer vision;learning (artificial intelligence);natural languages;speech analysis and processing","speech processing;high performance computing and communications;computer vision;natural language processing;artificial intelligence;machine learning;surface reconstruction;inverse optics;model acquisition;statistical analysis;search strategies;language analysis;auditory;vocal-tract modeling;connectionist systems;heuristic search methods;verifiable knowledge bases;active memories;artificial neural networks","","23","21","","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Implementation of Audio Signals Denoising for Perfect Speech-to-Speech Translation Using Principal Component Analysis","O. Julius; I. C. Obagbuwa; A. A. Adebiyi; E. B. Michael","Department of Computer Science, College of Pure and Applied Sciences, Landmark University, Omu-Aran, Kwara-State, Nigeria; Department of Computer Science & Information Technology, School of Natural and Applied Sciences, Sol Plaatje University, Kimberley, South Africa; Department of Computer Science, College of Pure and Applied Sciences, Landmark University, Omu-Aran, Kwara-State, Nigeria; Department of Computer Science, North-West University, Mafikeng, South Africa","2023 International Conference on Science, Engineering and Business for Sustainable Development Goals (SEB-SDG)","22 May 2023","2023","1","","1","6","The accuracy of any speech translation system essentially depends on the quality of the audio signal inputted into it. Many researchers have worked on different approaches in an attempt to reduce the level of noise in audio signals. Such approaches, among others, include Wavelet, Fourier Transform (FT), and deep learning. These algorithms worked well on noisy speech to a certain degree, but their degree of accuracy is not sufficient enough for speech-to-speech (S2S) translation because the presence of just a little noise in the signal can alter the semantic representation of the underlying language. Since it is nearly impossible for any of this single algorithm to produce a perfect (noiseless) signal, this paper presents a layered approach for total noise removal by stacking Principal Component Analysis (PCA) on Short Time Fourier Transform (STFT). In this approach, a band-pass channel is created using STFT, which reduces the signal noise level to the barest minimum while the residual noise is completely removed by performing PCA on the refined signals. Experimental results clearly showed that this approach almost doubles the signal-to-noise ratio (SNR) of the output signal for all the 10 audio samples being tested, thus making it relatively than the aforementioned approaches in terms of quality of outputs, and suitable for accuracy-sensitive domains such as speech-to-speech translation system development.","","979-8-3503-2478-5","10.1109/SEB-SDG57117.2023.10124385","Landmark University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124385","Deep Learning;Fourier Transform;Machine Translation;Principal Component Analysis;Wavelet","Band-pass filters;Wavelet transforms;Fourier transforms;Noise reduction;Semantics;Wavelet analysis;Signal denoising","audio signal processing;deep learning (artificial intelligence);Fourier transforms;image denoising;principal component analysis;signal denoising;wavelet transforms","10 audio samples;aforementioned approaches;audio signal;layered approach;noisy speech;output signal;perfect signal;Principal Component Analysis;refined signals;residual noise;Short Time Fourier Transform;signal noise level;signal-to-noise ratio;speech-to-speech translation system development;total noise removal","","","","26","IEEE","22 May 2023","","","IEEE","IEEE Conferences"
"Toward HMM based machine translation for ASL","M. Boulares; M. Jemni","Research Laboratory of Technologies of Information and Communication & Electrical Ingineering (LaTICE), Ecole Sup√©rieure des Sciences et Techniques de Tunis, Tunis, TUNISIA; Research Laboratory of Technologies of Information and Communication & Electrical Ingineering (LaTICE), Ecole Sup√©rieure des Sciences et Techniques de Tunis, Tunis, TUNISIA","Fourth International Conference on Information and Communication Technology and Accessibility (ICTA)","15 May 2014","2013","","","1","4","HMM-based models are widely used in many fields such as pattern recognition, speech recognition or Part-of-speech tagging. However, A HMM can be considered as a simplest dynamic Bayesian network. This network allows us to design a probabilistic graphical model that can be used in machine translation field especially for sign language machine translation. In this paper, we present a Bayesian Learning based method to train the alignment between a simple GLOSS form and a more complicated GLOSS form using sign language specificities such as space locative and classifier predicates.","2379-4402","978-1-4799-2725-8","10.1109/ICTA.2013.6815295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815295","American Sign Language GLOSS;Dynamic Bayesian Network;HMM;Locative Space;Classifier Predicates","Assistive technology;Gesture recognition;Hidden Markov models;Bayes methods;Fingers;Manuals","Bayes methods;hidden Markov models;language translation;learning (artificial intelligence);pattern classification;sign language recognition","HMM-based machine translation model;ASL;dynamic Bayesian network;probabilistic graphical model;sign language machine translation;Bayesian learning-based method;alignment training;simple-GLOSS form;complicated-GLOSS form;sign language specificities;space locative;classifier predicates;American Sign Language","","1","","17","IEEE","15 May 2014","","","IEEE","IEEE Conferences"
"Time-varying noise compensation by sequential Monte Carlo method","K. Yao; S. Nakamura","Spoken Language Translation Research Laboratories, ATR, Soraku-gun, Kyoto, Japan; Spoken Language Translation Research Laboratories, ATR, Soraku-gun, Kyoto, Japan","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","163","166","We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. At each frame, the method generates a set of samples, approximating the posterior distribution of speech and noise parameters for given observation sequences to the current frame. An explicit model representing noise effects on speech features is used, so that an extended Kalman filter is constructed for each sample, generating an updated continuous state as the estimation of the noise parameter, and prediction likelihood as the weight of each sample for minimum mean square error inference of the time-varying noise parameter over these samples. A selection step and a smoothing step are used to improve efficiency. Through experiments, we observed significant performance improvement over that achieved by noise compensation with a stationary noise assumption. It also performed better than the sequential EM algorithm in machine-gun noise.","","0-7803-7343-X","10.1109/ASRU.2001.1034613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034613","","Noise generators;Additive noise;Speech enhancement;Noise robustness;Speech recognition;Predictive models;State estimation;Mean square error methods;Smoothing methods;Inference algorithms","acoustic noise;interference suppression;Monte Carlo methods;speech recognition;least mean squares methods;parameter estimation;Kalman filters;prediction theory;inference mechanisms","time-varying noise compensation;sequential Monte Carlo method;robust speech recognition;extended Kalman filter;noise parameter estimation;prediction likelihood;minimum mean square error inference;MMSE inference;machine-gun noise","","7","","10","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Automatic Speech Translation at ATR","S. Nirenburg; H. L. Somers; Y. A. Wilks",NA; NA; NA,"Readings in Machine Translation","","2003","","","363","369","This chapter contains sections titled: Introduction, Current Status of Basic Technologies for Speech Translation, ATR Speech Translation System, International Joint Experiment, Further Enhancement and Extension, Problems and Approaches, Conclusion, References","","9780262280679","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6283744.pdf&bkn=6267420&pdfType=chapter","","","","","","","","","","24 Sep 2012","","","MIT Press","MIT Press eBook Chapters"
"Neural Machine Translation with Acoustic Embedding","T. Kano; S. Sakti; S. Nakamura","Nara Institute of Science and Technology, Japan; RIKEN, Center for Advanced Intelligence Project AIP, Japan; RIKEN, Center for Advanced Intelligence Project AIP, Japan","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","578","584","Neural machine translation (NMT) has successfully redefined the state of the art in machine translation on several language pairs. One popular framework models the translation process end-to-end using attentional encoder-decoder architecture and treats each word in the vectors of intermediate representation. These embedding vectors are sensitive to the meaning of words and allow semantically similar words to be near each other in the vector spaces and share their statistical power. Unfortunately, the model often maps such similar words too closely, which complicates distinguishing them. Consequently, NMT systems often mistranslate words that seem natural in the context but do not reflect the content of the source sentence. Incorporating auxiliary information usually enhances the discriminability. In this research, we integrate acoustic information within NMT by multi-task learning. Here, our model learns how to embed and translate word sequences based on their acoustic and semantic differences by helping it choose the correct output word based on its meaning and pronunciation. Our experiment results show that our proposed approach provides more significant improvement than the standard text-based transformer NMT model in BLEU score evaluation.","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003802","Neural machine translation;acoustic and semantic embedding representation","Decoding;Acoustics;Training;Standards;Computer architecture;Dairy products;Semantics","decoding;language translation;learning (artificial intelligence);natural language processing;sequences;vectors","neural machine translation;acoustic embedding;translation process end-to-end;attentional encoder-decoder architecture;embedding vectors;vector spaces;NMT systems;acoustic information;word sequences;acoustic differences;semantic differences;correct output word;standard text-based transformer NMT model","","1","","36","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Toward machine translation with statistics and syntax and semantics","D. Wu","Department of Computer Science & Engineering, Human Language Technology Center, Hong Kong University of Science and Technology, Hong Kong, China","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","12","21","In this paper, we survey some central issues in the historical, current, and future landscape of statistical machine translation (SMT) research, taking as a starting point an extended three-dimensional MT model space. We posit a socio-geographical conceptual disparity hypothesis, that aims to explain why language pairs like Chinese-English have presented MT with so much more difficulty than others. The evolution from simple token-based to segment-based to tree-based syntactic SMT is sketched. For tree-based SMT, we consider language bias rationales for selecting the degree of compositional power within the hierarchy of expressiveness for transduction grammars (or synchronous grammars). This leads us to inversion transductions and the ITG model prevalent in current state-of-the-art SMT, along with the underlying ITG hypothesis, which posits a language universal. Against this backdrop, we enumerate a set of key open questions for syntactic SMT. We then consider the more recent area of semantic SMT. We list principles for successful application of sense disambiguation models to semantic SMT, and describe early directions in the use of semantic role labeling for semantic SMT.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373509","","Statistics;Surface-mount technology;Space technology;Machine learning;Humans;Computer science;Labeling;Hardware;Speech recognition;Pattern recognition","computational linguistics;language translation;programming language semantics;statistics","toward machine translation;statistics;syntax;semantics;statistical machine translation;three dimensional MT model space;socio geographical conceptual;token based evolution;tree based syntactic SMT;hierarchy transduction grammars;inversion transductions;set key open;sense disambiguation models;SMT semantic","","3","","54","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Asynchronous translations with recurrent neural nets","R. P. Neco; M. L. Forcada","Department de Llenguatges i Sistemes Inform√†tics, Universitat d''Alacant, Alacant, Spain; Department de Llenguatges i Sistemes Inform√†tics, Universitat d''Alacant, Alacant, Spain","Proceedings of International Conference on Neural Networks (ICNN'97)","6 Aug 2002","1997","4","","2535","2540 vol.4","Many researchers have explored the relation between discrete-time recurrent neural networks (DTRNN) and finite-state machines (FSMs) either by showing their computational equivalence or by training them to perform as finite-state recognizers from examples. Most of this work has focused on the simplest class of deterministic state machines, that is deterministic finite automata and Mealy (or Moore) machines. The class of translations these machines can perform is very limited, mainly because these machines output symbols at the same rate as they input symbols, and therefore, the input and the translation have the same length; one may call these translations synchronous. Real-life translations are more complex: word reorderings, deletions, and insertions are common in natural-language translations; or, in speech-to-phoneme conversion, the number of frames corresponding to each phoneme is different and depends on the particular speaker or word. There are, however, simple deterministic, finite-state machines (extensions of Mealy machines) that may perform these classes of ""asynchronous"" or ""time-warped"" translations. A simple DTRNN model with input and output control lines inspired on this class of machines is presented and successfully applied to simple asynchronous translation tasks with interesting results regarding generalization. Training of these nets from input-output pairs is complicated by the fact that the time alignment between the target output sequence and the input sequence is unknown and has to be learned: we propose a new error function to tackle this problem. This approach to the induction of asynchronous translators is discussed in connection with other approaches.","","0-7803-4122-8","10.1109/ICNN.1997.614693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=614693","","Recurrent neural networks;Automata;Informatics;Computer networks;Hafnium;Inference algorithms","recurrent neural nets;finite state machines;multilayer perceptrons;learning (artificial intelligence);generalisation (artificial intelligence)","asynchronous translations;recurrent neural nets;discrete-time recurrent neural networks;finite-state machines;computational equivalence;deterministic finite automata;Mealy machines;Moore machines;word reorderings;natural-language translations;speech-to-phoneme conversion;deterministic finite-state machines;time-warped translations","","12","3","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A Context-Aware Recurrent Encoder for Neural Machine Translation","B. Zhang; D. Xiong; J. Su; H. Duan","Software School, Xiamen University, Xiamen, China; Provincial Key Laboratory for Computer Information Processing Technology, Soochow University, Suzhou, China; Software School, Xiamen University, Xiamen, China; Software School, Xiamen University, Xiamen, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Nov 2017","2017","25","12","2424","2432","Neural machine translation (NMT) heavily relies on its encoder to capture the underlying meaning of a source sentence so as to generate a faithful translation. However, most NMT encoders are built upon either unidirectional or bidirectional recurrent neural networks, which either do not deal with future context or simply concatenate the history and future context to form context-dependent word representations, implicitly assuming the independence of the two types of contextual information. In this paper, we propose a novel context-aware recurrent encoder (CAEncoder), as an alternative to the widely-used bidirectional encoder, such that the future and history contexts can be fully incorporated into the learned source representations. Our CAEncoder involves a two-level hierarchy: The bottom level summarizes the history information, whereas the upper level assembles the summarized history and future context into source representations. Additionally, CAEncoder is as efficient as the bidirectional RNN encoder in terms of both training and decoding. Experiments on both Chinese-English and English-German translation tasks show that CAEncoder achieves significant improvements over the bidirectional RNN encoder on a widely-used NMT system.","2329-9304","","10.1109/TASLP.2017.2751420","National Natural Science Foundation of China(grant numbers:61672440,61622209,61403269); Scientific Research Project of National Language Committee of China(grant numbers:YB135-49); Natural Science Foundation of Fujian Province(grant numbers:2016J05161); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031316","Context-aware encoder;neural machine translation (NMT);natural language processing;recurrent encoder","Context awareness;Semantics;Training;Computer architecture;Logic gates;Decoding;Computational modeling;Natural language processing;Recurrent neural networks","language translation;learning (artificial intelligence);natural language processing;recurrent neural nets","NMT encoders;context-dependent word representations;context-aware recurrent encoder;CAEncoder;bidirectional RNN encoder;English-German translation tasks;neural machine translation;recurrent neural networks;source representations learning","","42","","25","IEEE","11 Sep 2017","","","IEEE","IEEE Journals"
"Comparative Study on Spoken Language Identification Based on Deep Learning","P. Heracleous; K. Takai; K. Yasuda; Y. Mohammad; A. Yoneyama","KDDI Research, Inc., Japan; KDDI Research, Inc., Japan; Nara Institute of Science and Technology (NAIST), Japan; AIST, Artificial Intelligence Research Center, Japan; KDDI Research, Inc., Japan","2018 26th European Signal Processing Conference (EUSIPCO)","2 Dec 2018","2018","","","2265","2269","Spoken language identification is the process by which the language in a spoken utterance is recognized automatically. Spoken language identification is commonly used in speech translation systems, in multi-lingual speech recognition, and in speaker diarization. In the current paper, spoken language identification based on deep learning (DL) and the i-vector paradigm is presented. Specifically, a comparative study is reported, consisting of experiments on language identification using deep neural networks (DNN) and convolutional neural networks (CNN). Also, the integration of the two methods into a complete system is investigated. Previous studies demonstrated the effectiveness of using DNN in spoken language identification. However, to date, the integration of CNN and i-vectors in language identification has not been investigated. The main advantage of using CNN is that fewer parameters are required compared to DNN. As a result, CNN is cheaper in terms of memory and the computational power needed. The proposed methods are evaluated on the NIST 2015 i-vector Machine Learning Challenge task for the recognition of 50 in-set languages. Using DNN, a 3.55% equal error rate (EER) was achieved. The EER when using CNN was 3.48%. When DNN and CNN systems were fused, an EER of 3.3% was obtained. The results are very promising, and they also show the effectiveness of using CNN and i-vectors in spoken language identification. The proposed methods are compared to a baseline method based on support vector machines (SVM) and they demonstrated significantly superior performance.","2076-1465","978-9-0827-9701-5","10.23919/EUSIPCO.2018.8553347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553347","","Support vector machines;NIST;Training data;Training;Task analysis;Speech recognition;Machine learning","convolution;feedforward neural nets;learning (artificial intelligence);natural language processing;speaker recognition;speech processing;support vector machines","SVM;spoken utterance;support vector machines;equal error rate;machine learning;computational power;DNN;convolutional neural networks;deep neural networks;multilingual speech recognition;speaker diarization;speech translation systems;deep learning;CNN;spoken language identification","","19","","22","","2 Dec 2018","","","IEEE","IEEE Conferences"
"Abstractive Cross-Language Summarization via Translation Model Enhanced Predicate Argument Structure Fusing","J. Zhang; Y. Zhou; C. Zong","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Wuxi, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Beijing","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","10","1842","1853","Cross-language multidocument summarization is the task to generate a summary in a target language (e.g., Chinese) from a collection of documents in a different source language (e.g., English). Previous methods such as the extractive and compressive algorithms focus only on single sentence selection and compression, which cannot make full use of the similar sentences containing complementary information. Furthermore, the translation model knowledge is not fully explored in previous approaches. To address these two problems, we propose in this paper an abstractive cross-language summarization framework. First, the source language documents are translated into target language with a machine translation system. Then, the method constructs a pool of bilingual concepts and facts represented by the bilingual elements of the source-side predicate-argument structures (PAS) and their target-side counterparts. Finally, new summary sentences are produced by fusing bilingual PAS elements with the integer linear programming algorithm to maximize both of the salience and translation quality of the PAS elements. The experimental results on English-to-Chinese cross-language summarization demonstrate that our proposed method outperforms the state-of-the-art extractive systems in both automatic and manual evaluations.","2329-9304","","10.1109/TASLP.2016.2586608","Natural Science Foundation of China(grant numbers:61333018,61303181); Strategic Priority Research Program; CAS(grant numbers:XDB02070007); Open Project Program; State Key Laboratory of Mathematical Engineering and Advanced Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502066","Abstractive cross-language summarization;predicate-argument structure;translation model;integer linear programming.","Feature extraction;IEEE transactions;Speech;Hurricanes;Speech enhancement;Integer linear programming","document handling;integer programming;language translation;linear programming;linguistics;natural language processing","abstractive cross-language multidocument summarization;enhanced predicate argument structure fusing;single sentence selection;source language documents;machine translation system;bilingual concepts;source-side predicate-argument structures;source-side PAS;bilingual PAS elements;integer linear programming;salience quality;translation quality;English-to-Chinese cross-language summarization","","28","","36","IEEE","30 Jun 2016","","","IEEE","IEEE Journals"
"Combined Approach to Problem of Part-of-Speech Homonymy Resolution in Russian Texts","T. Batura; E. Bruches","Department of Information Technologies, A.P. Ershov Institute of Informatics Systems SB RAS, Novosibirsk, Russia; Institute for the Humanities, A.P. Ershov Institute of Informatics Systems SB RAS, Novosibirsk, Russia","2018 International Russian Automation Conference (RusAutoCon)","21 Oct 2018","2018","","","1","6","The Russian language has an inflective structure and does not have a strict word order. This causes processing difficulties, such as part-of-speech homonymy. This article is devoted to the mentioned issue. The existing approaches to resolving the morphological homonymy problem can be divided into the following groups: rule-based approaches, statistical approaches, machine learning approaches, and combined methods. In the paper, we showed that each approach has its advantages and disadvantages; however, combining several approaches can significantly increase the precision of the algorithm. Moreover, the article provides the analysis of the influence of certain features on the morphological homonymy resolution. The precision of the proposed algorithm is sufficient for its use in the tasks of intellectual text processing texts, for example, in machine translation and summarization systems. The proposed method is successfully used in the geographic location system. The main problem is the distinction between function words (conjunctions, particles, prepositions, interjections). Solving this problem is one of the priorities for the further work. We also plan to implement a system without a dictionary, in order to determine better morphological features for unknown words.","","978-1-5386-4938-1","10.1109/RUSAUTOCON.2018.8501718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501718","text processing;part-of-speech homonymy;combined approach;machine learning;homonymy resolution","Dictionaries;Hidden Markov models;Machine learning;Machine learning algorithms;Semantics;Neural networks;Text processing","language translation;learning (artificial intelligence);natural languages;speech synthesis;text analysis;word processing","morphological homonymy resolution;intellectual text processing texts;machine translation;summarization systems;geographic location system;function words;morphological features;unknown words;part-of-speech homonymy resolution;Russian texts;Russian language;inflective structure;strict word order;morphological homonymy problem;machine learning approaches","","","","20","IEEE","21 Oct 2018","","","IEEE","IEEE Conferences"
"A Translation Framework for Visually Grounded Spoken Unit Discovery","L. Wang; M. Hasegawa-Johnson","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign","2021 55th Asilomar Conference on Signals, Systems, and Computers","4 Mar 2022","2021","","","1419","1425","Multimodal acoustic unit discovery (MAUD) is a key task in self-supervised spoken language learning and low-resource speech recognition. In this paper, we proposed two models for MAUD inspired by machine translation models where we treat speech and image as source and target languages. Our word discovery model outperforms previous state-of-the-art approach by 5.3% alignment F1 on SpeechCOCO dataset and our phoneme discovery model outperforms previous state-of-the-art approach by 7% normalized mutual information on TIMIT dataset.","2576-2303","978-1-6654-5828-3","10.1109/IEEECONF53345.2021.9723367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723367","","Computers;Computational modeling;Speech recognition;Benchmark testing;Acoustics;Machine translation;Task analysis","acoustic signal processing;language translation;natural language processing;speech recognition;supervised learning","multimodal acoustic unit discovery;MAUD;self-supervised spoken language learning;low-resource speech recognition;machine translation;target languages;word discovery;phoneme discovery;visually grounded spoken unit discovery;source languages;SpeechCOCO dataset;TIMIT dataset","","","","32","IEEE","4 Mar 2022","","","IEEE","IEEE Conferences"
"Statistical multimodal integration for audio-visual speech processing","S. Nakamura","ATR Translational Research Laboratories, Kyoto, Japan","IEEE Transactions on Neural Networks","7 Nov 2002","2002","13","4","854","866","Sensory information is indispensable for living things. It is also important for living things to integrate multiple types of senses to understand their surroundings. In human communications, human beings must further integrate the multimodal senses of audition and vision to understand intention. In this paper, we describe speech related modalities since speech is the most important media to transmit human intention. To date, there have been a lot of studies concerning technologies in speech communications, but performance levels still have room for improvement. For instance, although speech recognition has achieved remarkable progress, the speech recognition performance still seriously degrades in acoustically adverse environments. On the other hand, perceptual research has proved the existence of the complementary integration of audio speech and visual face movements in human perception mechanisms. Such research has stimulated attempts to apply visual face information to speech recognition and synthesis. This paper introduces works on audio-visual speech recognition, speech to lip movement mapping for audio-visual speech synthesis, and audio-visual speech translation.","1941-0093","","10.1109/TNN.2002.1021886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021886","","Speech processing;Speech synthesis;Speech recognition;Humans;Hidden Markov models;Keyboards;Mice;Man machine systems;Communications technology;Oral communication","speech processing;speech recognition;statistical analysis;face recognition","statistical multimodal integration;audio-visual speech processing;sensory information;living things;audition;vision;intention;speech related modalities;speech communications;speech recognition;visual face movements;human perception mechanisms;speech synthesis","","23","1","101","IEEE","7 Nov 2002","","","IEEE","IEEE Journals"
"Multilingual person to person communication at IRST","B. Angelini; M. Cettolo; A. Corazza; D. Falavigna; G. Lazzari","IRST-Istituto per la Ricerca Scientifica e Technologica, Trento, Italy; IRST-Istituto per la Ricerca Scientifica e Technologica, Trento, Italy; IRST-Istituto per la Ricerca Scientifica e Technologica, Trento, Italy; IRST-Istituto per la Ricerca Scientifica e Technologica, Trento, Italy; IRST-Istituto per la Ricerca Scientifica e Technologica, Trento, Italy","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","1","","91","94 vol.1","This paper refers to a machine-mediated person-to-person multilingual communication system. Stress is put on robustness, that is the ability of the system to preserve communication even in presence of the variability and errors typical of spoken language systems. The statistical approach is adopted not only at the acoustic level, but also for the linguistic processing. Therefore, while an overview of the global architecture is briefly introduced, the focus is put on the acoustic recognizer and the understanding module. Experimental evaluations complete the presentation.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.599555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599555","","Natural languages;Speech synthesis;Speech analysis;Robustness;Signal synthesis;Stress;Context;History;Signal analysis;Speech recognition","speech recognition;speech synthesis;language translation;natural language interfaces;errors;statistical analysis;computational linguistics","multilingual person to person communication;IRST;machine-mediated communication;robustness;variability;errors;spoken language systems;statistical approach;acoustic level;speech understanding;acoustic recognizer","","1","1","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Syntax-Based Translation With Bilingually Lexicalized Synchronous Tree Substitution Grammars","J. Zhang; F. Zhai; C. Zong","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Audio, Speech, and Language Processing","25 Apr 2013","2013","21","8","1586","1597","Syntax-based models can significantly improve the translation performance due to their grammatical modeling on one or both language side(s). However, the translation rules such as the non-lexical rule ‚Äú VP‚Üí(x0x1,VP:x1PP:x0)‚Äù in string-to-tree models do not consider any lexicalized information on the source or target side. The rule is so generalized that any subtree rooted at VP can substitute for the nonterminal VP:x1. Because rules containing nonterminals are frequently used when generating the target-side tree structures, there is a risk that rules of this type will potentially be severely misused in decoding due to a lack of lexicalization guidance. In this article, inspired by lexicalized PCFG, which is widely used in monolingual parsing, we propose to upgrade the STSG (synchronous tree substitution grammars)-based syntax translation model with bilingually lexicalized STSG. Using the string-to-tree translation model as a case study, we present generative and discriminative models to integrate lexicalized STSG into the translation model. Both small- and large-scale experiments on Chinese-to-English translation demonstrate that the proposed lexicalized STSG can provide superior rule selection in decoding and substantially improve the translation quality.","1558-7924","","10.1109/TASL.2013.2255283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490019","Bilingually lexicalized synchronous tree substitution grammars;discriminative model;generative model;syntax-based statistical machine translation","Grammar;Decoding;Vegetation;Adaptation models;Reliability;Training;Syntactics","computational linguistics;decoding;grammars;language translation;natural language processing;tree data structures","syntax-based translation;bilingually lexicalized synchronous tree substitution grammars;syntax-based models;translation performance improvement;grammatical modeling;translation rules;nonlexical rule;subtree;nonterminal VP;target-side tree structure generation;decoding;lexicalization guidance;lexicalized PCFG;monolingual parsing;syntax translation model;bilingually lexicalized STSG;string-to-tree translation model;generative models;discriminative models;large-scale experiments;small-scale experiments;Chinese-to-English translation;rule selection;translation quality improvement","","4","1","39","IEEE","28 Mar 2013","","","IEEE","IEEE Journals"
"Towards Unified Multi-domain Machine Translation with Mixture of Domain Experts","J. Lu; J. Zhang","Institute of Automation Chinese Academy of Sciences, Beijing, China; Institute of Automation Chinese Academy of Sciences, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2023","PP","99","1","12","Multi-domain machine translation (MDMT) aims to construct models with mixed-domain training corpora to switch translation between different domains. Previous studies either assume that the domain information is given and leverage the domain knowledge to guide the translation process, or suppose that the domain information is unknown and utilize the model to automatically recognize it. However, the cases are mixed in practical scenarios, which means that some sentences are labeled with domain information while others are unlabeled, which is beyond the capacity of the previous methods. In this paper, we propose a unified MDMT model with a mixture of sub-networks (experts) to address the cases with or without domain labels. The mixture of sub-networks in our MDMT model includes a shared expert and multiple domain-specific experts. For the inputs with domain labels, our MDMT model goes through the shared and the corresponding domain-specific experts. For the unlabeled inputs, our MDMT model activates all the experts, each of which makes a dynamic contribution. Experimental results on multiple diverse domains in De $\rightarrow$ En, Fr $\rightarrow$ En, and En $\rightarrow$ Ro demonstrate that our method can outperform the strong baselines in both scenarios with or without domain labels. Further analyses show that our model has good generalization ability when transferring into new domains.","2329-9304","","10.1109/TASLP.2023.3316451","National Key R&D Program of China(grant numbers:2022ZD0160602); National Natural Science Foundation of China(grant numbers:62122088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255669","Machine Translation;Multi-domain;Mixture-of-expert","Training;Adaptation models;Transformers;Task analysis;Speech processing;Machine translation;Switches","","","","","","","IEEE","19 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Pragmatic analysis of malayalam sentences","T. A. Shaharban; R. P. Haroon","Dept. of CSE, Ilahia College of Engineering & Technology, Muvattupuzha, India; Dept. of CSE, Ilahia College of Engineering & Technology, Muvattupuzha, India","2016 International Conference on Inventive Computation Technologies (ICICT)","26 Jan 2017","2016","3","","1","5","Natural language processing is one of the major field in computer science. NLP is the ability of the system to process different sentences in natural language. Parts of speech tagging, pragmatic analysis, machine translation, discourse analysis etc are the different fields in NLP. Malayalam is the one of the important language in Dravidian family, where the difficult grammar structure will be the main problem. Pragmatics analysis of Malayalam sentences mainly represent how sentences are used in different situation or different context. In proposed system we can apply pragmatics for Malayalam sentence to understand the user intention about a particular sentence.","","978-1-5090-1285-5","10.1109/INVENTIVE.2016.7830067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830067","Natural language processing(NLP);Parts of Speech(PoS)","Pragmatics;Speech;Syntactics;Natural language processing;Tagging;Semantics;Silicon","computational linguistics;computer science;language translation;natural language processing;speech processing","pragmatic analysis;Malayalam sentences;natural language processing;computer science;speech tagging;machine translation;discourse analysis","","","","9","IEEE","26 Jan 2017","","","IEEE","IEEE Conferences"
"STD: An Automatic Evaluation Metric for Machine Translation Based on Word Embeddings","P. Li; C. Chen; W. Zheng; Y. Deng; F. Ye; Z. Zheng","National Engineering Research Center of Digital Life, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; National Engineering Research Center of Digital Life, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Technical Architecture Department, Wechat Group, Tencent, Guangzhou, China; Technical Architecture Department, Wechat Group, Tencent, Guangzhou, China; National Engineering Research Center of Digital Life, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; National Engineering Research Center of Digital Life, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Jun 2019","2019","27","10","1497","1506","Lexical-based metrics such as BLEU, NIST, and WER have been widely used in machine translation (MT) evaluation. However, these metrics badly represent semantic relationships and impose strict identity matching, leading to moderate correlation with human judgments. In this paper, we propose a novel MT automatic evaluation metric Semantic Travel Distance (STD) based on word embeddings. STD incorporates both semantic and lexical features (word embeddings and n-gram and word order) into one metric. It measures the semantic distance between the hypothesis and reference by calculating the minimum cumulative cost that the embedded n-grams of the hypothesis need to ‚Äútravel‚Äù to reach the embedded n-grams of the reference. Experiment results show that STD has a better and more robust performance than a range of state-of-the-art metrics for both the segment-level and system-level evaluation.","2329-9304","","10.1109/TASLP.2019.2922845","Technical Architecture Department at WeChat Group of Tencent; National Natural Science Foundation of China(grant numbers:11801595,61802450); Natural Science Foundation of Guangdong(grant numbers:2018A030310076); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); CCF Opening Project of Information System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736840","Machine translation evaluation;metric;semantic;word embeddings;earth mover's distance;n-gram;word order","Measurement;Semantics;Syntactics;NIST;Speech processing;Earth;Linguistics","language translation;natural language processing","word embeddings;STD;semantic features;embedded n-grams;lexical-based metrics;semantic relationships;machine translation;MT automatic evaluation;semantic travel distance","","8","","61","IEEE","14 Jun 2019","","","IEEE","IEEE Journals"
"Deep learning approach for Translating Arabic Holy Quran into Italian language","H. Hamed; A. M. Helmy; A. Mohammed","Department of Computer Science FGSSR, Cairo Uinversity; Department of Information Technology FGSSR, Cairo University; Department of Computer Science FGSSR, Cairo Uinversity","2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)","9 Jun 2021","2021","","","193","199","Machine translation has become an urgent necessity due to the difference between the languages used within the world's societies. Automatic machine translation uses a computer to translate text or speech from one language to another. Arabic Language is one of the seven united nations officially spoken languages. Few efforts tried to translate officially Arabic language to other languages. The inaccurate Translation of Arabic language might lead to several problem especially when we translate the holy text of Quran into other language. The aim of this paper is to use neural machine translation to enhance the translation quality of Arabic to Italian. The main contribution of this paper is to propose two deep learning models, namely LSTM and GRU, with attention mechanism to translate the Arabic Holy Quran to Italian. The proposed models are evaluated based on the BLEU and ROUGE score. The experimental results indicate that the BLEU and ROUGE score of LSTM have achieved Average score 0.18 and 0.17 on BLEU and ROUGE score respectively. also, the result of GRU have achieved Average score 0.17 and 0.16 on BLEU score and ROUGE score respectively.","","978-1-6654-1243-8","10.1109/MIUCC52538.2021.9447650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447650","Machine Translation;Deep Recurrent Network;LSTM;GRU.","Deep learning;Computational modeling;Training data;Ubiquitous computing;Data models;Machine translation","language translation;learning (artificial intelligence);natural language processing;text analysis","deep learning models;Average score 0;deep learning approach;translating Arabic Holy Quran;Italian language;automatic machine translation;seven united nations;officially Arabic language;inaccurate Translation;neural machine translation;translation quality","","18","","41","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Making Machines Understand Us in Reverberant Rooms: Robustness Against Reverberation for Automatic Speech Recognition","T. Yoshioka; A. Sehr; M. Delcroix; K. Kinoshita; R. Maas; T. Nakatani; W. Kellermann","NTT Communication Science Laboratories, NTT Corporation, Kyoto, Japan; University of Erlangen-Nuremberg, Germany; NTT Communication Science Laboratories, Kyoto, Japan; NTT Communication Science Laboratories, Japan; Audio Research Laboratory, Chair of Multimedia Communications and Signal Processing, University of Erlangen-Nuremberg, Germany; NTT Communication Science Laboratories, NTT Corporation, Japan; University of Erlangen-Nuremberg, Germany","IEEE Signal Processing Magazine","18 Oct 2012","2012","29","6","114","126","Speech recognition technology has left the research laboratory and is increasingly coming into practical use, enabling a wide spectrum of innovative and exciting voice-driven applications that are radically changing our way of accessing digital services and information. Most of today's applications still require a microphone located near the talker. However, almost all of these applications would benefit from distant-talking speech capturing, where talkers are able to speak at some distance from the microphones without the encumbrance of handheld or body-worn equipment [1]. For example, applications such as meeting speech recognition, automatic annotation of consumer-generated videos, speech-to-speech translation in teleconferencing, and hands-free interfaces for controlling consumer-products, like interactive TV, will greatly benefit from distant-talking operation. Furthermore, for a number of unexplored but important applications, distant microphones are a prerequisite. This means that distant talking speech recognition technology is essential for extending the availability of speech recognizers as well as enhancing the convenience of existing speech recognition applications.","1558-0792","","10.1109/MSP.2012.2205029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6296524","","Automatic speech recognition;Speech recognition;Hidden Markov models;Reverberation;Robustness","microphones;reverberation;speech recognition;teleconferencing;television","reverberant rooms;reverberation;automatic speech recognition;speech recognition technology;distant-talking speech capturing;handheld equipment;body-worn equipment;meeting speech recognition;automatic annotation;consumer-generated videos;speech-to-speech translation;teleconferencing;hands-free interfaces;consumer-products;interactive TV;distant-talking operation;distant microphones;speech recognizers","","166","12","48","IEEE","18 Oct 2012","","","IEEE","IEEE Magazines"
"Scaling recurrent neural network language models","W. Williams; N. Prasad; D. Mrva; T. Ash; T. Robinson","Cantab Research, Cambridge, UK; Cantab Research, Cambridge, UK; Cantab Research, Cambridge, UK; Cantab Research, Cambridge, UK; Cantab Research, Cambridge, UK","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5391","5395","This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7179001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179001","recurrent neural network;language modelling;GPU;speech recognition;RNNLM","Training;Computational modeling;Recurrent neural networks;Graphics processing units;Benchmark testing;Entropy","graphics processing units;language translation;prediction theory;recurrent neural nets;speech recognition;word processing","scaling recurrent neural network language model;RNNLM scaling properties;GPU;RNN;n-gram model;word error rate;ASR;machine translation;word prediction;speech recognition","","22","1","22","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Learning How Long to Wait: Adaptively-Constrained Monotonic Multihead Attention for Streaming ASR","J. Song; H. Shim; E. Yang","KAIST, South Korea; KAIST, South Korea; KAIST, South Korea","2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","3 Feb 2022","2021","","","441","448","Monotonic Multihead Attention, which allows multiple heads to learn their own alignments per head, shows great performance on simultaneous machine translation and streaming speech recognition. However, it causes high latency waiting for the slowest head. Some recent advances such as Head-Synchronous Beam Search Decoding and its learnable version Mutually-Constrained Monotonic Multihead Attention, try to address this issue by restricting the difference in times of chosen frames among multi-heads to a fixed waiting time threshold. In this paper, we hypothesis that the optimal threshold for high performance with low latency depends on the input sequence, and propose an adaptive algorithm that learns how long to wait depending on input tokens by introducing a threshold prediction module. We evaluate our approach on two benchmark datasets for online Automatic Speech Recognition task and demonstrate that our method reduces the latency together with even improving the recognition accuracy.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9688138","National Research Foundation of Korea (NRF)(grant numbers:2018R1A5A1059921); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688138","online speech recognition;Transformer;decoding algorithm;Monotonic Multihead Attention;Head-Synchronous Beam Search Decoding","Degradation;Conferences;Benchmark testing;Prediction algorithms;Magnetic heads;Decoding;Machine translation","language translation;speech recognition","monotonic multihead attention;streaming ASR;machine translation;streaming speech recognition;head-synchronous beam search decoding;online automatic speech recognition","","","","44","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Preliminary work on speech unit selection using syntax-phonology interface","S. Tiun; Tang Enya Kong","Universiti Sains Islam Malaysia, Penang, Malaysia; Universiti Sains Islam Malaysia, Penang, Malaysia","2008 International Conference on Computer and Communication Engineering","29 Jul 2008","2008","","","95","98","This paper proposes an approach which uses a syntax-phonology interface to select the most appropriate speech units for a target sentence. The selection of the speech units is done by constructing the syntax-phonology tree structure of the target sentence. The construction of the syntax-phonology tree is adapted from the example-based parsing of UTMK machine translation. In the process of constructing the syntax-phonology tree, we first identify the related trees from the speech corpus. Then, the generated subtrees based on the related trees are combined. The concatenation of the combined subtrees nodes is the synthetic utterance of the target sentence.","","978-1-4244-1691-2","10.1109/ICCCE.2008.4580575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580575","","Tree data structures;Speech synthesis;Speech processing;Labeling;Computer interfaces;Vegetation mapping;Information retrieval;Merging;Databases","grammars;speech processing","speech unit selection;syntax-phonology interface;target sentence;syntax-phonology tree;parsing;UTMK machine translation;speech corpus","","1","","10","IEEE","29 Jul 2008","","","IEEE","IEEE Conferences"
"Logical operative processes of semantic grammar for machine interpretation","S. Ramakrishnan; P. Isawasan; V. Mohanan","School of Materials & Mineral Resources Engineering, Universiti Sains Malaysia, Pinang, Malaysia; School of Computer Science, Universiti Sains Malaysia, Pinang, Malaysia; School of Computer Science, Universiti Sains Malaysia, Pinang, Malaysia","2014 International Conference on Asian Language Processing (IALP)","4 Dec 2014","2014","","","95","98","The purpose of this paper is to identify and reveal the significance of primary logical operative processes of semantic grammar of any languages for the establishment of machine interpretation. This neo generative mechanism for logical semantic representation for machine interpretation has been systematically analyzed by logical linguistic and mathematical postulations. These logical operative processes structurally provide a way in which grammatical properties of language can be treated within a framework of speech acts to accommodate and to ease the machine interpretation for ontological representation and cognitive act. This treatment also allows the sentences to be semantically interpreted and hermeneutically analyzed within the temporal movement of speech act for machine interpretation. The logical postulation of operative processes of grammar enables to provide an explanation of the grammatical intuitions of a native speaker of a language in terms of both a variety of cognitive operations and knowledge of distinct object categories to be applied in the machine interpretation.","","978-1-4799-5330-1","10.1109/IALP.2014.6973487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973487","Linguistic Competence;Cognitive Acts;Grammar;Meta Operative Processes;Speech Acts;Logical Semantic Grammar;Machine Intepretation Introduction","Grammar;Speech;Pragmatics;Semantics;Speech processing;Educational institutions;Concrete","grammars;language translation;natural language processing","primary logical operative processes;semantic grammar;machine interpretation;neo generative mechanism;logical linguistic;mathematical postulations;language grammatical properties;ontological representation;cognitive act","","","","6","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"MT on and for the Web","C. Boitet; H. Blanchon; M. Seligman; V. Bellynck","LIG (UJF, UPMF), GETALP, Grenoble, France; LIG (UJF, UPMF), GETALP, Grenoble, France; Spoken Translation, Inc., Berkeley, CA, USA; LIG, GETALP, Grenoble, France","Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)","30 Sep 2010","2010","","","1","10","A Systran MT server became available on the minitel network in 1984, and on Internet in 1994. Since then we have come to a better understanding of the nature of MT systems by separately analyzing their linguistic, computational, and operational architectures. Also, thanks to the CxAxQ metatheorem, the systems' inherent limits have been clarified, and design choices can now be made in an informed manner according to the translation situations. MT evaluation has also matured: tools based on reference translations are useful for measuring progress; those based on subjective judgments for estimating future usage quality; and task-related objective measures (such as post-editing distances) for measuring operational quality. Moreover, the same technological advances that have led to ‚ÄúWeb 2.0‚Äù have brought several futuristic predictions to fruition. Free Web MT services have democratized assimilation MT beyond belief. Speech translation research has given rise to usable systems for restricted tasks running on PDAs or on mobile phones connected to servers. New man-machine interface techniques have made interactive disambiguation usable in large-coverage multimodal MT. Increases in computing power have made statistical methods workable, and have led to the possibility of building low-linguistic-quality but still useful MT systems by machine learning from aligned bilingual corpora (SMT, EBMT). In parallel, progress has been made in developing interlingua-based MT systems, using hybrid methods. Unfortunately, many misconceptions about MT have spread among the public, and even among MT researchers, because of ignorance of the past and present of MT R&D. A compensating factor is the willingness of end users to freely contribute to building essential parts of the linguistic knowledge needed to construct MT systems, whether corpus-related or lexical. Finally, some developments we anticipated fifteen years ago have not yet materialized, such as online writing tools equipped with interactive disambiguation, and as a corollary the possibility of transforming source documents into self-explaining documents (SEDs) and of producing corresponding SEDs fully automatically in several target languages. These visions should now be realized, thanks to the evolution of Web programming and multilingual NLP techniques, leading towards a true Semantic Web, ‚ÄúWeb 3.0‚Äù, which will support ubilingual (ubiquitous multilingual) computing.","","978-1-4244-6899-7","10.1109/NLPKE.2010.5587865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587865","MT;linguistic architecture;computational architecture;operational architecture;task-related evaluation;speech MT;interactive disambiguation;self-explaining documents;Semantic Web MT","Computer architecture;Pragmatics;Speech recognition;Dictionaries;Humans;Speech;Internet","language translation;learning (artificial intelligence);natural language processing;semantic Web","Systran MT server;CxAxQ metatheorem;speech translation research;man-machine interface techniques;machine learning;self-explaining documents;multilingual NLP techniques;semantic Web;machine translation;Web 2.0","","2","","91","IEEE","30 Sep 2010","","","IEEE","IEEE Conferences"
"Survey Paper: Study of Natural Language Processing and its Recent Applications","B. D. Shivahare; A. K. Singh; N. Uppal; A. Rizwan; V. S. Vaathsav; S. Suman","Dept. of Computer Science, Amity School of Engineering and Technology, Greater Noida, India; Dept. of Mathematics and Computer Science, PNG University of Technology, Lae, Papua New Guinea; Dept. of Computer Science, Amity School of Engineering and Technology, Greater Noida, India; Dept. of Computer Science, Amity School of Engineering and Technology, Greater Noida, India; Dept. of Computer Science, Amity School of Engineering and Technology, Greater Noida, India; Dept. of Computer Science, Amity School of Engineering and Technology, Greater Noida, India","2022 2nd International Conference on Innovative Sustainable Computational Technologies (CISCT)","22 Feb 2023","2022","","","1","5","Natural Language Processing (NLP) is a field of research related to developing computer systems that can process native language intelligently. Considering natural language such as English has always been one of the key issues of the practical feasibility of research, both because of the key language that works with human ingenuity and because of the richness of potential applications. The main functions of natural language processing are machine translation, speech analysis, automatic summarization, conference resolution, speech recognition, etc. NLP is used to make the virtual computer interface easier for humans. This paper gives you an overview of NLP from scratch and discusses some of the key applications.","","978-1-6654-7416-0","10.1109/CISCT55310.2022.10046440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046440","Natural Language Processing;AI;Speech Recognition;Machine Translation;NLU;NLG","Computer interfaces;Speech analysis;Natural language processing;Machine translation","natural language processing;user interfaces","human ingenuity;native language;natural language processing;NLP;virtual computer interface","","","","15","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Experts Versus All-Rounders: Target Language Extraction for Multiple Target Languages","M. Borsdorf; K. Scheck; H. Li; T. Schultz","Machine Listening Lab (MLL), University of Bremen, Germany; Cognitive Systems Lab (CSL), University of Bremen, Germany; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Cognitive Systems Lab (CSL), University of Bremen, Germany","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","846","850","Target language extraction (TLE) is a novel task in the field of selective auditory attention, which seeks to extract all speech signals that are spoken in a target language from other sources in a multilingual cocktail party. In our prior studies, a TLE model was trained to extract a predefined, single target language, referred to as Single-TLE. In this paper, we extend the Single-TLE framework to Multi-TLE. Multi-TLE models can also extract all speech signals of one specific target language, but they are optimized on a set of multiple target languages during training. As such, they learn the characteristics of several target languages and can replace multiple Single-TLE models without retraining. We perform experiments on the GlobalPhoneMCP database and incorporate a dynamic language mixing scheme for training. The Multi-TLE model does not only outperform Single-TLE models, but when given a language ID as additional input, it is also able to extract the speech of a specific target language from a mixture which contains multiple learned target languages.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746130","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746130","Target language extraction;selective auditory attention;multilingual;GlobalPhone;cocktail party problem","Training;Databases;Conferences;Signal processing;Acoustics;Task analysis;Speech processing","grammars;hearing;language translation;learning (artificial intelligence);natural language processing;natural languages;speech recognition","target language extraction;multiple target languages;speech signals;TLE model;predefined target language;single target language;Single-TLE framework;MultiTLE model;specific target language;multiple Single-TLE models;dynamic language mixing scheme;outperform Single-TLE models;language ID;multiple learned target languages","","","","34","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Improving Thai-English word alignment for interrogative sentences in SMT by grammatical knowledge","K. Phodong; R. Kongkachandra","Department of Computer Science, Thammasat University, Pathumthanee, Thailand; Department of Computer Science, Thammasat University, Pathumthanee, Thailand","2017 9th International Conference on Knowledge and Smart Technology (KST)","27 Mar 2017","2017","","","226","231","This paper presents a method to improve Thai-English word alignment in statistical machine translation (SMT) for interrogative sentences in a parallel corpus. We utilize the Thai and English grammatical knowledge i.e. tense, part of speech (POS), and question inversion pattern. The proposed method handles the difference of Thai and English interrogative sentences using sentence transformation, interrogative grammatical attribute extraction, and interrogative grammatical attribute annotation. This method works as a pre-processing of GIZA, a standard word co-occurrence alignment tool in SMT. We hypothesize that using grammatical knowledge as a pre-processing of GIZA can provide higher accuracy. We experiment by using 43,500 interrogative sentences to compare alignment result between interrogative sentences attached an interrogative grammatical label and interrogative sentences unattached interrogative grammatical label. The experimental results yield 95% of accuracy with significant improvement than the conventional one. With the increasing accuracy of word alignment, the translation accuracy is consequently improved.","","978-1-4673-9077-4","10.1109/KST.2017.7886115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886115","Thai-English interrogative grammatical attribute;Thai-English interrogative sentence;linguistic knowledge;word alignment;statistical machine translation","Standards;Pragmatics;Tagging;Data mining;Speech;Transforms;Dictionaries","language translation;natural language processing;statistical analysis;word processing","Thai-English word alignment;interrogative sentences;statistical machine translation;SMT;parallel corpus;Thai grammatical knowledge;English grammatical knowledge;word tense;part of speech;POS;question inversion pattern;sentence transformation;interrogative grammatical attribute extraction;interrogative grammatical attribute annotation;GIZA preprocessing;standard word cooccurrence alignment tool","","1","","15","IEEE","27 Mar 2017","","","IEEE","IEEE Conferences"
"A hybrid Parts Of Speech tagger for Malayalam language","Anisha Aziz T; Sunitha C","Department of Computer Science and Engg. VAST, Thalakkottukara Thrissur, Kerala; Department of Computer Science and Engg. VAST, Thalakkottukara Thrissur, Kerala","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","28 Sep 2015","2015","","","1502","1507","Parts of speech tagging is an important research topic in Natural Language Processing research are. Since it is one among the first steps of any natural language processing (NLP) techniques such as machine translation, if any error happens for tagging the same will repeat in the whole NLP process. So far works had been done on POS tagging based on SVM, MBLP, HMM, Ngram. All of these methods were not fixing the problem of ambiguity. So for fixing ambiguity, we put forward a new Hybrid tagger for Malayalam. The combination of traditional rules and n-gram may produce better result compared to other methodologies. And also the ambiguity will be reduced by enriching the bigram dictionary. A bigram dictionary of co-occurring words are built with their tags. About 100000 more words are there in bigram dictionary. A corpus for Malayalam must be built which may be supposed to access by the model. It contains about 100000 words which are Malayalam words as well as the words originated from English. Since it's a hybrid tagger, we can take advantage of both traditional rules as well as bigrams. Also the heart of the research is the rule set, which contains 267 manually created rules. Rules can be applied with help of a morph analyzer. Rules are also used for tagging if bigram and corpus can't be referred for tagging. The proposed method when tested on 150 words, only 11 words were not identified, and obtained 90.5% accuracy. For the unidentified words, it can be caused by either the root word may not be in corpus or bigram, or the absence of rule. So adding the word, bigram or rule, we can improve the result and enhance the work. Addition is simple task. The size of bigram dictionary, corpus, and rule set and accuracy of morph analyzer influences the performance of the system.","","978-1-4799-8792-4","10.1109/ICACCI.2015.7275825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275825","N-gram;Rule based tagger;Natural Language Processing;Parts Of Speech tagging;Bigram dictionary;Corpus","Tagging;Dictionaries;Speech;Training;Natural language processing;Accuracy;Hidden Markov models","natural language processing;support vector machines","hybrid parts of speech tagger;Malayalam language;natural language processing;NLP techniques;machine translation;POS tagging;SVM;MBLP;HMM;N-gram;bigram dictionary;morph analyzer;rule based tagger;memory based language processing","","5","","10","IEEE","28 Sep 2015","","","IEEE","IEEE Conferences"
"The IXM2 parallel associative processor for AI","T. Higuchi; K. Handa; N. Takahashi; T. Furuya; H. Iida; E. Sumita; O. Oi; H. Kitano","Electro Technical Laboratory, Japan; Electro Technical Laboratory, Japan; Electro Technical Laboratory, Japan; NA; ATR Interpreting Telecommunications Research Laboratories, Japan; ATR Interpreting Telecommunications Research Laboratories, Japan; ATR Interpreting Telecommunications Research Laboratories, Japan; Sony Computer Science Laboratories, Inc., France","Computer","6 Aug 2002","1994","27","11","53","63","Describes the IXM2 associative processor and its main application in speech-to-speech translation. The IXM2 is a semantic memory system machine that began as a faithful implementation of the NETL semantic network machine and grew into a massively parallel SIMD machine that has demonstrated the power of large associative memories. Such processors can support robust performance in speech applications. In fact, the IXM2 with 73 transputers has outperformed a Cray in some language-translation tasks. We selected speech-to-speech translation as our main application because it is one of the grand challenges of massively parallel artificial intelligence. The social implications of successful automatic translation are enormous-e.g. people who speak different languages could communicate in real time by using interpreting telephony.<>","1558-0814","","10.1109/2.330048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=330048","","Artificial intelligence;Associative memory;Natural languages;Terminology;Laboratories;Speech processing;Writing;Dictionaries;Computer science;Robustness","parallel machines;associative processing;content-addressable storage;speech processing;language translation;transputer systems;natural languages","IXM2 parallel associative processor;artifical intelligence;speech-to-speech translation;semantic memory system machine;NETL semantic network machine;massively parallel SIMD machine;associative memories;robust performance;transputers;language-translation tasks;social implications;real-time communication;interpreting telephony","","17","9","13","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"Design and implementation of a Luganda text normalization module for a speech synthesis software program","R. Kizito; W. S. Okello; S. Kagumire","Department of Electrical and Computer Engineering, Makerere University, P.O. Box 7062, Kampala, Uganda; netLabs!UG, a Research Centre of Excellence in the Department of Electrical and Computer Engineering, Makerere University, P.O. Box 7062, Kampala, Uganda; netLabs!UG, a Research Centre of Excellence in the Department of Electrical and Computer Engineering, Makerere University, P.O. Box 7062, Kampala, Uganda","SAIEE Africa Research Journal","10 Sep 2020","2020","111","4","149","154","This paper describes a Luganda text normalization module, a crucial component needed for a Luganda Text to Speech system. We describe the use of a rule-based approach for detection, classification and verbalization of Luganda text. At the core of this module are the Luganda grammar rules that were hand-built to normalize Non-Standard Words (NSWs) from different semiotic and noun classes. Input text is first analyzed, matched against handcrafted patterns developed using regular expressions to detect any NSWs. Upon detection, NSWs are tokenized and classified into one of the semiotic classes and then if necessary, into one of the Luganda noun classes. These are subsequently verbalized, each according to its semiotic as well as noun class, and a new text file is produced. We tested the module with 7 datasets and achieved average detection and normalization rates of 82% and 77.7% respectively.","1991-1696","","10.23919/SAIEE.2020.9194384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194384","Automatic Speech Recognition;Detection-conversion;Luganda;Machine Translation;NLP;Number system;Speech Synthesis;Text Normalization;Text-to-speech;TTS","Semiotics;Standards;Currencies;Natural language processing;Speech synthesis;Grammar;Measurement units","","","","2","","23","","10 Sep 2020","","","SAIEE","SAIEE Journals"
"Deep learning & convolutional networks","Y. LeCun","Facebook AI Research & Center for Data Science, NYU","2015 IEEE Hot Chips 27 Symposium (HCS)","7 Jul 2016","2015","","","1","95","Presents a collection of slides covering the following topics: deep learning; convolutional neural network; image recognition; speech recognition; language translation; reasoning; unsupervised learning; and intelligent machine.","","978-1-4673-8885-6","10.1109/HOTCHIPS.2015.7477328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477328","","Pose estimation;Supervised learning;Feature extraction;Natural language processing;Pattern recognition;Machine learning;Face detection;Object recognition;Speech recognition","convolution;image recognition;inference mechanisms;language translation;neural nets;speech recognition;unsupervised learning","intelligent machine;unsupervised learning;reasoning;language translation;speech recognition;image recognition;convolutional neural network;deep learning","","41","","","IEEE","7 Jul 2016","","","IEEE","IEEE Conferences"
"Entity Highlight Generation as Statistical and Neural Machine Translation","J. Huang; Y. Sun; W. Zhang; H. Wang; T. Liu","Baidu, Inc., Beijing, China; Baidu, Inc., Beijing, China; Baidu, Inc., Beijing, China; Baidu, Inc., Beijing, China; Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Jun 2018","2018","26","10","1860","1872","Entity highlight refers to a short, concise, and characteristic description for an entity, which can be applied to various applications. In this article, we study the problem of automatically generating entity highlights from the descriptive sentences of entities. Specifically, we develop two computational approaches, one is inspired by the statistical machine translation (SMT) and another is a sequence-to-sequence learning (Seq2Seq) approach, which has been successfully applied in neural machine translation and neural summarization. In the Seq2Seq approach, we use attention mechanism, copy mechanism, and coverage mechanism. To generate entity-specific highlights, we also incorporate entity name into the Seq2Seq model to guide the decoding process. We automatically collect large-scale instances as training data without any manual annotation, and ask annotators to create a test set. We compare with several strong baseline methods, and evaluate the approaches with both automatic evaluation and manual evaluation. Experimental results show that the entity enhanced Seq2Seq model with attention, copy, and coverage mechanisms significantly outperforms all other approaches in terms of multiple evaluation metrics.1","2329-9304","","10.1109/TASLP.2018.2845111","National Basic Research Program of China(grant numbers:2014CB340505); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374886","Entity highlight generation;Seq2Seq model;attention mechanism;copy mechanism;coverage mechanism","Task analysis;Computational modeling;Decoding;Speech processing;Web search;Training data;Manuals","language translation;learning (artificial intelligence);natural language processing;neural nets;statistical analysis","neural summarization;Seq2Seq approach;attention mechanism;copy mechanism;coverage mechanism;entity-specific highlights;entity name;Seq2Seq model;entity highlight generation;neural machine translation;statistical machine translation;sequence-to-sequence learning approach;decoding process","","8","","56","IEEE","7 Jun 2018","","","IEEE","IEEE Journals"
"Sentence Similarity-Based Source Context Modelling in PBSMT","R. Haque; S. K. Naskar; A. Way; M. R. Costa-jussa; R. E. Banchs","CNGL, School of Computing, University of Dublin, Dublin, Ireland; CNGL, School of Computing, University of Dublin, Dublin, Ireland; CNGL, School of Computing, University of Dublin, Dublin, Ireland; Speech and language, Barcelona Media Research Center, Barcelona, Spain; Institute of Infocomm Research, Human Language Technology A-STAR, Singapore","2010 International Conference on Asian Language Processing","6 Jan 2011","2010","","","257","260","Target phrase selection, a crucial component of the state-of-the-art phrase-based statistical machine translation(PBSMT) model, plays a key role in generating accurate translation hypotheses. Inspired by context-rich word-sense disambiguation techniques, machine translation (MT) researchers have successfully integrated various types of source language context into the PBSMT model to improve target phrase selection. Among the various types of lexical and syntactic features, lexical syntactic descriptions in the form of super tags that preserve long-range word-to-word dependencies in a sentence have proven to be effective. These rich contextual features are able to disambiguate a source phrase, on the basis of the local syntactic behaviour of that phrase. In addition to local contextual information, global contextual information such as the grammatical structure of a sentence, sentence length and n-gram word sequences could provide additional important information to enhance this phrase-sense disambiguation. In this work, we explore various sentence similarity features by measuring similarity between a source sentence to be translated with the source-side of the bilingual training sentences and integrate them directly into the PBSMT model. We performed experiments on an English-to-Chinese translation task by applying sentence-similarity features both individually, and collaboratively with super tag-based features. We evaluate the performance of our approach and report a statistically significant relative improvement of 5.25% BLEU score when adding a sentence-similarity feature together with a super tag-based feature.","","978-1-4244-9063-9","10.1109/IALP.2010.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681568","sentence similarity;source context information;statistical machine translation","Training;Context;Context modeling;Syntactics;Grammar;Feature extraction;Computational modeling","language translation;natural language processing;statistical analysis","sentence similarity-based source context modelling;PBSMT;phrase-based statistical machine translation model;translation hypotheses;context-rich word-sense disambiguation techniques;target phrase selection;lexical syntactic descriptions;long-range word-to-word dependencies;grammatical structure;sentence length;n-gram word sequences;bilingual training sentences;English-to-Chinese translation task;sentence-similarity features;super tag-based features","","2","","13","IEEE","6 Jan 2011","","","IEEE","IEEE Conferences"
"Probabilistic finite-state machines - part II","E. Vidal; F. Thollard; C. de la Higuera; F. Casacuberta; R. C. Carrasco","Departamento de Sistemas Inform√°ticos y Computaci√≥n and Instituto Tecnol√≥gico de Inform√°tica, Universidad Polit√©cnica de Valencia, Valencia, Spain; Facult√© des Sciences et Techniques, EURISE, Saint-Etienne, France; Facult√© des Sciences et Techniques, EURISE, Saint-Etienne, France; Departamento de Sistemas Inform√°ticos y Computaci√≥n and Instituto Tecnol√≥gico de Inform√°tica, Universidad Polit√©cnica de Valencia, Valencia, Spain; Departamento de Lenguajes y Sistemas Inform√°ticos, Universidad de Alicante, Alicante, Spain","IEEE Transactions on Pattern Analysis and Machine Intelligence","23 May 2005","2005","27","7","1026","1039","Probabilistic finite-state machines are used today in a variety of areas in pattern recognition or in fields to which pattern recognition is linked. In part I of this paper, we surveyed these objects and studied their properties. In this part, we study the relations between probabilistic finite-state automata and other well-known devices that generate strings like hidden Markov models and n-grams and provide theorems, algorithms, and properties that represent a current state of the art of these objects.","1939-3539","","10.1109/TPAMI.2005.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432737","Index Terms- Automata;classes defined by grammars or automata;machine learning;language acquisition;language models;language parsing and understanding;machine translation;speech recognition and synthesis;structural pattern recognition;syntactic pattern recognition.","Pattern recognition;Learning automata;Probability distribution;Hidden Markov models;Machine learning;Natural languages;Computer Society;Machine learning algorithms;Stochastic processes;Speech recognition","finite state machines;probabilistic automata;pattern recognition;hidden Markov models","probabilistic finite-state machines;pattern recognition;probabilistic finite-state automata;hidden Markov models;n-grams","Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Signal Processing, Computer-Assisted","45","","106","IEEE","23 May 2005","","","IEEE","IEEE Journals"
"Enhancing global and synchronous distance learning and teaching by using instant transcript and translation","I. Ho; H. Kiyohara; A. Sugimoto; K. Yana","Hosei University Research Institute, California, Burlingame, CA, USA; Hosei University Research Institute, California, Burlingame, CA, USA; Hosei University Research Institute, California, Burlingame, CA, USA; Hosei University Research Institute, California, Burlingame, CA, USA","2005 International Conference on Cyberworlds (CW'05)","6 Feb 2006","2005","","","5 pp.","377","The dramatic increases in computing power and bandwidth over the past decade and more have enabled significant improvements in the global learning environment via distance education. Yet, while distance education has dramatically reduced the barriers between different geographic regions, it has not yet had a similar impact on reducing the barriers between different languages. This paper discusses a system that helps non-native English students to better understand lectures that are delivered in English via the Internet in distance learning environments. The system integrates speech recognition, a language parser, and machine translation to instantly generate transcripts of lectures and display them simultaneously with audio and video streams. We present the method, design, functionality, and implementation issues, which provide a synchronous and interactive distance learning environment in which non-native English students can learn materials in English more easily.","","0-7695-2378-1","10.1109/CW.2005.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1587561","","Computer aided instruction;Education;Distance learning;Natural languages;Bandwidth;Internet;Speech recognition;Auditory displays;Streaming media;Design methodology","distance learning;Internet;language translation;speech recognition;teaching","distance learning;instant transcript;instant translation;global learning environment;distance education;nonnative English students;Internet;transcript generation","","2","","10","IEEE","6 Feb 2006","","","IEEE","IEEE Conferences"
"Attention-Based Convolution Skip Bidirectional Long Short-Term Memory Network for Speech Emotion Recognition","H. Zhang; H. Huang; H. Han","School of Computer Science, Qinghai Normal University, Xining, China; School of Computer Science, Qinghai Normal University, Xining, China; School of Computer Science, Qinghai Normal University, Xining, China","IEEE Access","11 Jan 2021","2021","9","","5332","5342","Speech emotion recognition is a challenging task in natural language processing. It relies heavily on the effectiveness of speech features and acoustic models. However, existing acoustic models may not handle speech emotion recognition efficiently for their built-in limitations. In this work, a novel deep-learning acoustic model called attention-based skip convolution bi-directional long short-term memory, abbreviated as SCBAMM, is proposed to recognize speech emotion. It has eight hidden layers, namely, two dense layers, convolutional layer, skip layer, mask layer, Bi-LSTM layer, attention layer, and pooling layer. SCBAMM makes better use of spatiotemporal information and captures emotion-related features more effectively. In addition, it solves the problems of gradient exploding and gradient vanishing in deep learning to some extent. On the databases EMO-DB and CASIA, the proposed model SCBAMM achieves an accuracy rate of 94.58% and 72.50%, respectively. As far as we know, compared with peer models, this is the best accuracy rate.","2169-3536","","10.1109/ACCESS.2020.3047395","National Science Foundation of China (NSFC)(grant numbers:62066039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308936","Emotion recognition;attention mechanism;weighted pooling;skip connection","Hidden Markov models;Convolution;Speech recognition;Acoustics;Emotion recognition;Deep learning;Logic gates","convolutional neural nets;emotion recognition;learning (artificial intelligence);natural language processing;speech recognition","speech emotion recognition;speech features;acoustic model;hidden layers;dense layers;convolutional layer;skip layer;mask layer;attention layer;pooling layer;spatiotemporal information;emotion-related features;convolution skip bidirectional long short-term memory network;biLSTM layer;SCBAMM;attention-based skip convolution bidirectional long short-term memory","","11","","77","CCBY","25 Dec 2020","","","IEEE","IEEE Journals"
"Automatic standardization of spelling variations of Hindi text","V. Goyal; G. S. Lehal","Department of Computer Science, Punjabi University, Patiala, India; Department of Computer Science, Punjabi University, Patiala, India","2010 International Conference on Computer and Communication Technology (ICCCT)","18 Nov 2010","2010","","","764","767","The phonetic nature of Indian languages and multiple dialects, transliteration of proper names, words borrowed from foreign languages has resulted in spelling variations of the same word. Such variations sometimes can be treated as errors in writing. While developing machine translation system, the task of standardizing the spellings for further processing the text is considered to play vital role in improving the accuracy of translation. In this paper, the rule based approach for standardizing spelling variations in the Hindi text while developing Hindi to Punjabi Machine Translation System has been explained. It was analyzed that only 7.45% text was standardized using this approach and thus had increased the accuracy of the machine translation system.","","978-1-4244-9034-9","10.1109/ICCCT.2010.5640441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640441","Text Normalization;Standardizing spelling variations;Preprocessing Module;Machine Translation;Natural Language Processing","Databases;Accuracy;Standardization;Speech recognition;Dictionaries;Speech;Knowledge based systems","language translation;natural language processing;text analysis","spelling variation standardization;Hindi text;Indian languages;Hindi-Punjabi machine translation system;rule based approach","","2","","9","IEEE","18 Nov 2010","","","IEEE","IEEE Conferences"
"An English Part of Speech Tagging Method Based on Maximum Entropy","T. Jianchao","Wuhan University of Science and Technology, College of Foreign Languages, Wuhan, Hubei","2015 International Conference on Intelligent Transportation, Big Data and Smart City","21 Jan 2016","2015","","","76","80","As part of speech is a fundamental step in syntactic parsing and machine translation, this paper proposes an English part of speech tagging method based on maximum entropy, and this issue is very crucial for natural language processing. The proposed part of speech tagging system is made up of two steps, that is, (a) Training process and (b) Tagging process. Maximum Entropy estimation is able to compute Probability Density Function of the random variables, and in this paper, we solve the problem of tagging part of speech for English by tackling an optimization problem using maximum entropy. Using the ISO-639-1 code: EN dataset, a series of experiments are conducted. Finally, experimental results prove that the proposed method is very effective to tag part of speech for English.","","978-1-5090-0464-5","10.1109/ICITBS.2015.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383971","Part of speech tagging;Maximum entropy;Mapping matrix;Tagged corpus","Transportation;Big data;Smart cities","entropy;language translation;natural language processing;probability","English part of speech tagging method;syntactic parsing;machine translation;natural language processing;training process;tagging process;maximum entropy estimation;probability density function","","2","","17","IEEE","21 Jan 2016","","","IEEE","IEEE Conferences"
"The Conversational Short-phrase Speaker Diarization (CSSD) Task: Dataset, Evaluation Metric and Baselines","G. Cheng; Y. Chen; R. Yang; Q. Li; Z. Yang; L. Ye; P. Zhang; Q. Zhang; L. Xie; Y. Qian; K. A. Lee; Y. Yan","Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Tsinghua University, Beijing, China; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China; Magic Data Technology Co., Ltd., China; Northwestern Polytechnical University, China; Shanghai Jiao Tong University, Shanghai, China; Institute for Infocomm Research, A *Star, Singapore; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences, China","2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP)","8 Feb 2023","2022","","","488","492","The conversation scenario is one of the most important and most challenging scenarios for speech processing technologies because people in conversation respond to each other in a casual style. Detecting the speech activities of each person in a conversation is vital to downstream tasks, like natural language processing, machine translation, etc. People refer to the detection technology of ‚Äùwho speak when‚Äù as speaker diarization (SD). Traditionally, diarization error rate (DER) has been used as the standard evaluation metric of SD systems for a long time. However, DER fails to give enough importance to short conversational phrases, which are short but important on the semantic level. Also, a carefully and accurately manually-annotated testing dataset suitable for evaluating the conversational SD technologies is still unavailable in the speech community. In this paper, we design and describe the Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of training and testing datasets, evaluation metric and baselines. In the dataset aspect, despite the previously open-sourced 180-hour conversational MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech test dataset with carefully and artificially verified speakers timestamps annotations for the CSSD task. In the metric aspect, we design the new conversational DER (CDER) evaluation metric, which calculates the SD accuracy at the utterance level. In the baseline aspect, we adopt a commonly used method: Variational Bayes HMM x-vector system, as the baseline of the CSSD task. Our evaluation metric is publicly available at https://github.com/SpeechClub/CDER_Metric.","","979-8-3503-9796-3","10.1109/ISCSLP57327.2022.10038258","Youth Innovation Promotion Association; Chinese Academy of Sciences; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038258","speaker diarization;short-phrase;conversational speech","Measurement;Training;Annotations;Semantics;Hidden Markov models;Oral communication;Manuals","Bayes methods;hidden Markov models;language translation;natural language processing;speaker recognition;speech processing","180-hour conversational MagicData-RAMC dataset;20-hour conversational speech test dataset;artificially verified speakers timestamps annotations;carefully verified speakers timestamps annotations;conversation respond;conversation scenario;conversational DER evaluation metric;conversational SD technologies;Conversational Short-phrase Speaker Diarization task;Conversational Short-phrases Speaker Diarization task;CSSD task;diarization error rate;important scenarios;manually-annotated testing dataset;most challenging scenarios;short conversational phrases;speech processing technologies;standard evaluation metric;testing datasets","","1","","34","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Software Architecture for Better Text-Based Infromation Accessibility","V. Topac; V. Stoicu-Tivadar","Department of Computer Science and Information Technology, University of Politehnica Timi≈üoara, Timisoara, Romania; Department of Automation and Applied Informatics, University of Politehnica Timi≈üoara, Timisoara, Romania","2009 Fifth Advanced International Conference on Telecommunications","12 Jun 2009","2009","","","198","202","The paper suggests a software architecture to improve the accessibility to information from texts. This software combines techniques like: image processing, optical character recognition, machine translation, text analyze and text to speech. The application uses a scanner or a Web cam as an image input device, recognizes the text by using OCR, enables text translation by using Googlepsilas machine translation implementation, interpret the text as a future development and reads the text by using TTS technology. In this way the user can put the text information source into a scanner or under a Web cam and can hear the text translated and interpreted, if required. A functional prototype is presented and conclusions are issued.","","978-1-4244-3840-2","10.1109/AICT.2009.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072431","Text-based information;text analysis;machine translation;OCR;TTS","Software architecture;Optical character recognition software;Speech synthesis;Image processing;Character recognition;Image analysis;Speech analysis;Application software;Image recognition;Text recognition","handicapped aids;language translation;optical character recognition;software architecture;text analysis","software architecture;text based information accessibility;image processing;optical character recognition;machine translation;TTS technology;text analysis","","1","3","13","IEEE","12 Jun 2009","","","IEEE","IEEE Conferences"
"Speech analysis and synthesis systems for the tatar language","A. Khusainov; A. Khusainova","Institute of Applied Semiotics of the Tatarstan Academy of Sciences, Kazan, Russia; Kazan (Volga region) Federal University, Kazan, Russia","2016 IEEE Artificial Intelligence and Natural Language Conference (AINL)","6 Apr 2017","2016","","","1","6","In this paper we describe our recent work of creation speech human-machine interface for the Tatar language. Our work consists of three main elements: speech recognition system, speech synthesizer and language identification system. These systems will be used in mobile and desktop applications, for instance, machine translation system, smart assistant.","","978-952-68397-8-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891858","","","natural language processing;speech recognition;speech synthesis","speech analysis system;speech synthesis system;Tatar language;speech human-machine interface;speech recognition system;speech synthesizer;language identification system;mobile applications;desktop applications;machine translation system","","","","15","","6 Apr 2017","","","IEEE","IEEE Conferences"
"An improved multisection vector quantization model with application to Chinese digits recognition","Zhang Rong; Chen Zhaoxiong; Huang Heyan","Institute of Computing Technology, Machine Translation Center, Chinese Academy and Sciences, Beijing, China; Institute of Computing Technology, Machine Translation Center, Chinese Academy and Sciences, Beijing, China; Institute of Computing Technology, Machine Translation Center, Chinese Academy and Sciences, Beijing, China","Proceedings of Third International Conference on Signal Processing (ICSP'96)","6 Aug 2002","1996","1","","749","752 vol.1","In this paper, an improved model using multisection vector quantization (MVQ) is proposed and detailed experimental results are discussed. The major weakness of conventional MVQ is that it can not make segments accurately. Thus, a dynamic segmenting algorithm based on the Viterbi algorithm is presented. Moreover explicit state duration density and dynamic features are integrated into the model. Thus more acoustic information is depicted and a better performance is acquired.","","0-7803-2912-0","10.1109/ICSIGP.1996.567371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=567371","","Vector quantization;Speech recognition;Hidden Markov models;Viterbi algorithm;Heuristic algorithms;Encoding;Acoustic distortion;Computers;Gaussian processes;Cepstral analysis","vector quantisation;speech recognition;maximum likelihood estimation","improved multisection vector quantization model;Chinese digits recognition;MVQ;dynamic segmenting algorithm;Viterbi algorithm;explicit state duration density features;dynamic features;acoustic information;performance","","","","12","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Improving Chinese to English SMT with Multiple CWS Results","Y. Ma; T. Zhao","MOE-Microsoft Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China; MOE-Microsoft Key Laboratory of Natural Language Processing and Speech, Harbin Institute of Technology, Harbin, China","2009 International Conference on Asian Language Processing","15 Jan 2010","2009","","","135","140","In Chinese to English statistical machine translation (SMT), Chinese texts always need a pre-processing high segments sentences into words and this standard approach is Chinese word segmentation (CWS). However, CWS is not developed for SMT, its results are not necessarily optimal for SMT. In recent years, many investigations have been performed concerning making CWS suitable for SMT, but we explore it from another direction. In this paper, our basic idea is to use multiple CWS results as additional language knowledge sources and we present a simple and effective approach to use multiple CWS results for SMT. We also give experiment results over range of strategy settings, and obtain substantial improvements in performance for translation from Chinese to English. The best result shows we gain 1.89 BLEU percentage points over a state of the art HPBT baseline system without using multiple CWS results.","","978-0-7695-3904-1","10.1109/IALP.2009.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380785","Chinese word segmentation;SMT;feature blending;feature interpolation","Surface-mount technology;Natural languages;Interpolation;Support vector machines;Laboratories;Natural language processing;Speech processing;White spaces;Dictionaries;Hidden Markov models","language translation;word processing","statistical machine translation;Chinese word segmentation;Chinese to English language translation","","","","22","IEEE","15 Jan 2010","","","IEEE","IEEE Conferences"
"Opportunities for advanced speech processing in military computer-based systems","C. J. Weinstein","Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, MA, USA","Proceedings of the IEEE","6 Aug 2002","1991","79","11","1626","1641","The author presents a study of military applications of advanced speech processing technology which includes three major elements: (1) review and assessment of current efforts in military applications of speech technology. (2) identification of opportunities for future military applications of advanced speech technology, and (3) identification of problem areas where research in speech processing is needed to meet application requirements, and of current research thrusts which appear promising. Opportunities for advanced applications are identified by means of descriptions of several generic systems which would be possible with advances in speech technology and in system integration. These generic systems include an integrated multirate voice/data communications terminal, an interactive speech enhancement system, a voice-controlled pilot's associate system, advanced air traffic control training systems, a battle management command and control support system with spoken natural language interface, and a spoken language translation system. In the applications discussed, various spectral analysis and digitization techniques are teamed up with artificial intelligence schemes to allow a machine to recognize continuous-speech sentences with vocabularies running to thousands of words.<>","1558-2256","","10.1109/5.118986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=118986","","Speech processing;Management training;Natural languages;Data communication;Speech enhancement;Air traffic control;Command and control systems;Spectral analysis;Artificial intelligence;Vocabulary","air-traffic control;artificial intelligence;command and control systems;language translation;military systems;natural languages;speech analysis and processing;voice communication","advanced speech processing;military computer-based systems;speech technology;generic systems;system integration;integrated multirate voice/data communications terminal;speech enhancement system;voice-controlled pilot's associate system;air traffic control training systems;battle management command and control support system;spoken natural language interface;spoken language translation system;spectral analysis;digitization techniques;artificial intelligence schemes;continuous-speech sentences","","13","3","168","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Automatic Generation with Manual Post-Editing of Shallow-Transfer Rule for English-to-Indonesian","E. Yulianti; M. Adriani; H. M. Manurung; I. Budi; A. N. Hidayanto","Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia","2012 International Conference on Asian Language Processing","7 Mar 2013","2012","","","173","176","Machine translation from English to Indonesian is required in line with the flood of document written in English. In order to develop shallow-transfer machine translation for English-to-Indonesian, we need shallow-transfer rule to change the structure of English into Indonesian. However, it cost high expense to build it manually from the scratch as it needs much of linguistic knowledge to cover most of the translation rules. In this research, we combine the automatic and manual approach to generate the shallow-transfer rule for English-to-Indonesian. Automatic generated rule is manually post-edited to improve its quality. We compare the performance of system when applying no rule (word-for-word translation), automatic rule and post-edited rule. The result shows that the automatic rule can significantly improve word-for-word translation and post-edited rule also fairly improve the quality of automatic rule. Overall, post-edited rule can significantly improve word-for-word translation by 40.45%.","","978-0-7695-4886-9","10.1109/IALP.2012.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473724","machine translation;automatic;shallow-transfer rule;Indonesian;English","Dictionaries;Manuals;Morphology;Speech;Grammar;Pattern matching;Open source software","language translation;natural language processing","automatic generation;machine translation;manual post-editing;shallow-transfer rule;English-to-Indonesian language;linguistic knowledge;automatic rule;post-edited rule;word-for-word translation","","1","","15","IEEE","7 Mar 2013","","","IEEE","IEEE Conferences"
"Applying PSO to natural language processing tasks: Optimizing the identification of syntactic phrases","G. Tambouratzis","Dept. of Machine Translation, Institute for Language and Speech Processing / Athena Research Centre, Greece","2016 IEEE Congress on Evolutionary Computation (CEC)","21 Nov 2016","2016","","","1831","1838","The present article discusses the use of Particle Swarm Optimisation (PSO) in a natural language processing task, namely the creation of a phrasing model which splits any sentence into linguistically-motivated phrases. This involves taking a limited-size training dataset, where sentences are split into syntactically motivated sentences, and learning how to best segment arbitrary sentences into their corresponding phrases. The extrapolation of phrases is needed in numerous applications involving the generation of texts in natural language. One such application is machine translation, namely the automatic translation of unconstrained text from a source language to a target language. The phrasing model to be learnt comprises a number of parameters which need to be optimized based on the training data. To that end, a machine learning approach has been developed, which is based on the concept of attractive and repulsive forces. Instead of previous efforts using manual tuning of the parameters, here PSO is used to achieve the optimization of the model parameters. Experimental results indicate that the proposed PSO approach is promising, giving the most accurate phrasing in this specific application, with statistically significant improvements over earlier results.","","978-1-5090-0623-6","10.1109/CEC.2016.7744011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744011","parsing of natural language;machine translation;syntactically-derived phrasing;particle swarm optimization;parameter optimization","Training;Force;Optimization;Natural language processing;Syntactics;Particle swarm optimization","computational linguistics;extrapolation;language translation;learning (artificial intelligence);natural language processing;particle swarm optimisation","PSO;natural language processing;syntactic phrases identification;particle swarm optimisation;phrasing model;linguistically-motivated phrases;syntactically motivated sentences;phrases extrapolation;machine translation;unconstrained text automatic translation;machine learning approach","","6","","28","IEEE","21 Nov 2016","","","IEEE","IEEE Conferences"
"Widening the NLP pipeline for spoken language processing","S. Bangalore",AT&T Labs - Research,"2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","15","15","Summary form only given. A typical text-based natural language application (eg. machine translation, summarization, information extraction) consists of a pipeline of preprocessing steps such as tokenization, stemming, part-of-speech tagging, named entity detection, chunking, parsing. Information flows downstream through the preprocessing steps along a narrow pipe: each step transforms a single input string into a single best solution string. However, this narrow pipe is limiting for two reasons: First, since each of the preprocessing steps are erroneous, producing a single best solution could magnify the error propogation down the pipeline. Second, the preprocessing steps are forced to resolve genuine ambiguity prematurely. While the widening of the pipeline can potentially benefit text-based language applications, it is imperative for spoken language processing where the output from the speech recognizer is typically a word lattice/graph. In this talk, we review how such a goal has been accomplished in tasks such as spoken language understanding, speech translation and multimodal language processing. We will also sketch methods that encode the preprocessing steps as finite-state transductions in order to exploit composition of finite-state transducers as a general constraint propogation method.","","1-4244-0872-5","10.1109/SLT.2006.326787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123350","","Pipelines;Natural languages;Speech processing;Data mining;Tagging;Data preprocessing;Speech recognition;Lattices;Transducers","language translation;natural language processing;pipeline processing;speech processing","NLP pipeline;spoken language processing;text-based natural language application;error propogation;speech translation;multimodal language processing;finite-state transducers","","","","","IEEE","19 Mar 2007","","","IEEE","IEEE Conferences"
"Analysing The Sentiments Of Marathi-English Code-Mixed Social Media Data Using Machine Learning Techniques","V. Patwardhan; G. Takawane; N. Kelkar; O. Gaikwad; R. Saraf; S. Sonawane","Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India; Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India; Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India; Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India; Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India; Computational Linguistics Research Lab, Pune Institute of Computer Technology, Pune, India","2023 International Conference on Emerging Smart Computing and Informatics (ESCI)","19 Apr 2023","2023","","","1","5","A vast amount of data is generated every day through social media platforms. Various techniques and methodologies are used to bring different forms of data to use. One such form of data is textual data generated from social media platforms in the form of chats, comments, and tweets. The term ‚Äúcode-mixed data‚Äù describes data that combines components of different languages or linguistic subgroups such as text written in several different languages or speech that shifts between languages. Due to increased social media use and worldwide communication, many individuals are using multiple languages in their daily communication, making this type of data even more crucial. Machine translation, speech recognition, and text categorization are just a few examples of natural language processing activities that can be performed on code-mixed data. Research on code-mixed data can also aid in the understanding of multilingual communication. In this paper, we present an empirical study on the problem of word-level language identification and text normalisation for Marathi-English code-mixed text. We have created a new dataset of 1009 sentences that exhibit code-mixing of Marathi (Romanised) and English textual data. This data was collected from Whatsapp chats and Youtube comments.","","978-1-6654-7524-2","10.1109/ESCI56872.2023.10100304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100304","NLP;SVM;Naive Bayes;Code-mixed Marathi English Romanised;sentiment analysis;language annotations","Video on demand;Social networking (online);Speech coding;Text categorization;Speech recognition;Machine learning;Linguistics","language translation;learning (artificial intelligence);natural language processing;sentiment analysis;social networking (online);speech recognition;text analysis","code-mixed data;code-mixed social media data;exhibit code-mixing;increased social media;Marathi-English code-mixed text;natural language processing activities;social media platforms;textual data","","","","21","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"Updated MINDS report on speech recognition and understanding, Part 2 [DSP Education]","J. M. Baker; L. Deng; S. Khudanpur; C. -H. Lee; J. R. Glass; N. Morgan; D. O'Shaughnessy","Dragon Systems, Saras Institute, USA; Microsoft Research in Redmond, University of Washington, WA, USA; GWC Whiting School of Engi neering, Johns Hopkins University, Baltimore, MD, USA; School of ECE, Georgia Institute of Technology, USA; Health Sciences and Technology, Computer Science and Artificial Intelligence Laboratory, MIT, USA; Nonprofit Research Laboratory, University of California Berkeley, USA; IRS EMT, University of Quebec, Canada","IEEE Signal Processing Magazine","24 Jul 2009","2009","26","4","78","85","This article is the second part of an updated version of the ""MINDS 2006-2007 Report of the Speech Understanding Working Group,"" one of five reports emanating from two workshops entitled ""Meeting of the MINDS: Future Directions for Human Language Technology,"" sponsored by the U.S. Disruptive Technology Office (DTO). (MINDS is an acronym for ""machine translation, information retrieval, natural-language processing, data resources, and speech understanding"").","1558-0792","","10.1109/MSP.2009.932707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5174501","","Speech recognition;Digital signal processing;Humans;Hidden Markov models;Speech processing;Automatic speech recognition;Natural languages;Psychology;Stochastic processes;Information retrieval","information retrieval;language translation;natural language processing;speech recognition","MINDS;speech recognition;speech understanding;human language technology;machine translation;information retrieval;natural-language processing;data resources","","40","3","68","IEEE","24 Jul 2009","","","IEEE","IEEE Magazines"
"Learning Edit Machines for Robust Multimodal Understanding","M. Johnston; S. Bangalore","AT and T Research Laboratories, Florham Park, NJ, USA; AT and T Research Laboratories, Florham Park, NJ, USA","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","1","","I","I","Multimodal grammars provide an expressive formalism for multimodal integration and understanding. However, hand-crafted multimodal grammars can be brittle with respect to unexpected, erroneous, or disfluent inputs. In previous work, we have shown how the robustness of stochastic language models can be combined with the expressiveness of multimodal grammars by adding a finite-state edit machine to the multimodal language processing cascade. In this paper, we present an approach where the edits are trained from data using a noisy channel model paradigm. We evaluate this model and compare its performance against hand-crafted edit machines from our previous work in the context of a multimodal conversational system (MATCH)","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1660096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1660096","","Machine learning;Robustness;Speech processing;Cities and towns;Stochastic processes;Context modeling;Graphics;Natural languages;Transducers;Lattices","language translation;natural languages","learning edit machines;robust multimodal understanding;multimodal grammars;stochastic language models;finite-state edit machine;noisy channel model paradigm","","1","","18","IEEE","24 Jul 2006","","","IEEE","IEEE Conferences"
"A Bilingual Adversarial Autoencoder for Unsupervised Bilingual Lexicon Induction","X. Bai; H. Cao; K. Chen; T. Zhao","Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","22 Jul 2019","2019","27","10","1639","1648","Unsupervised bilingual lexicon induction aims to generate bilingual lexicons without any cross-lingual signals. Successfully solving this problem would benefit many downstream tasks, such as unsupervised machine translation and transfer learning. In this work, we propose an unsupervised framework, named bilingual adversarial autoencoder, which automatically generates bilingual lexicon for a pair of languages from their monolingual word embeddings. In contrast to existing frameworks which learn a direct cross-lingual mapping of word embeddings from the source language to the target language, we train two autoencoders jointly to transform the source and the target monolingual word embeddings into a shared embedding space, where a word and its translation are close to each other. In this way, we capture the cross-lingual features of word embeddings from different languages and use them to induce bilingual lexicons. By conducting extensive experiments across eight language pairs, we demonstrate that the proposed method significantly outperforms the existing adversarial methods and even achieves best-published results across most language pairs.","2329-9304","","10.1109/TASLP.2019.2925973","National Natural Science Foundation of China(grant numbers:61572154); National Key Research and Development Program of China(grant numbers:2017YFB1002102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754809","Word embeddings;unsupervised word mapping;bilingual lexicon induction","Training;Transforms;Decoding;Speech processing;Computational modeling;Dictionaries;Correlation","learning (artificial intelligence);natural language processing","direct cross-lingual mapping;source language;cross-lingual features;monolingual word embeddings;bilingual adversarial autoencoder;unsupervised framework;cross-lingual signals;unsupervised bilingual lexicon induction","","8","","45","IEEE","3 Jul 2019","","","IEEE","IEEE Journals"
"A Review Study for Arabic Machine Learning and Deep Learning Methods","R. Al‚ÄìShalabi; G. Kanaan; T. Kanan; M. ElBes","Applied Science University-Bahrain, Bahrain; International Center for Scientific Research and Studies, Jordan; Al Zaytoonah University of Jordan-Jordan, Jordan; Al Zaytoonah University of Jordan-Jordan, Jordan","2022 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)","26 Sep 2022","2022","","","225","232","The fields of Natural Language Processing (NLP), Computer vision and speech processing witnessed great breakthroughs caused by the continuous advances in Deep Learning (DL). The implementation of DL techniques in Online Social Networking and Sentiment Analysis have proved to provide a state-of-the-art result in these areas. In this research, we survey the papers that claims to use the DL methods in NLP. We focus on the research that is related to the Arabic language due to the scares resources in Arabic NLP. We concluded that most of the early research in Arabic NLP focusses of OCR-Digitization and most recently is focusing on applying DL methods in Sentiment Analysis, diacrization and machine translation. We present this survey to provide the growing community of researchers in ANLP in the hope of bridging the gap between the ANLP and the English NLP.","","978-1-6654-6919-7","10.1109/ICETSIS55481.2022.9888948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9888948","Artificial Intellegence;Machine Learning;deep Learning","Deep learning;Sentiment analysis;Social networking (online);Focusing;Learning (artificial intelligence);Natural language processing;Machine translation","language translation;learning (artificial intelligence);natural language processing;optical character recognition;social networking (online);speech processing;text analysis","applying DL methods;Arabic language;arabic machine learning;Arabic NLP focusses;continuous advances;deep learning methods;diacrization;DL techniques;early research;English NLP;great breakthroughs;machine translation;Natural Language Processing;Online Social Networking;scares resources;Sentiment Analysis;speech processing","","","","34","IEEE","26 Sep 2022","","","IEEE","IEEE Conferences"
"Using ASR Methods for OCR","A. Arora; C. C. Chang; B. Rekabdar; B. BabaAli; D. Povey; D. Etter; D. Raj; H. Hadian; J. Trmal; P. Garcia; S. Watanabe; V. Manohar; Y. Shao; S. Khudanpur","Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; School of Mathematics, Statistics and Computer Sciences, University of Tehran, Iran; School of Mathematics, Statistics and Computer Sciences, University of Tehran, Iran; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, USA","2019 International Conference on Document Analysis and Recognition (ICDAR)","3 Feb 2020","2019","","","663","668","Hybrid deep neural network hidden Markov models (DNN-HMM) have achieved impressive results on large vocabulary continuous speech recognition (LVCSR) tasks. However, the recent approaches using DNN-HMM models are not explored much for text recognition. Inspired by the current work in automatic speech recognition (ASR) and machine translation, we present an open vocabulary sub-word text recognition system. The sub-word lexicon and sub-word language model (LM) helps in overcoming the challenge of recognizing out of vocabulary (OOV) words, and a time delay neural network (TDNN) and convolution neural network (CNN) based DNN-HMM optical model (OM) efficiently models the sequence dependency in the line image. We present results on 12 datasets with training data varying from 6k lines to 600k lines. The system is built for 8 languages, i.e., English, French, Arabic, Chinese, Farsi, Tamil, Russian, and Korean. We report competitive results on several commonly used handwritten and printed text datasets.","2379-2140","978-1-7281-3014-9","10.1109/ICDAR.2019.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978150","OCR;ASR;LF MMI;Open Vocabulary;BPE","Hidden Markov models;Text recognition;Vocabulary;Training;Optical imaging;Training data;Feature extraction","convolutional neural nets;handwritten character recognition;hidden Markov models;optical character recognition;speech recognition;text detection;vocabulary","large vocabulary continuous speech recognition tasks;machine translation;subword language model;out of vocabulary words;ASR methods;hybrid deep neural network;hidden Markov models;subword lexicon;open vocabulary subword text recognition system;automatic speech recognition;OOV words;time delay neural network;TDNN;convolution neural network;CNN;DNN-HMM optical models;printed text datasets;handwritten text datasets","","12","","30","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"An intelligent bilingual system for spoken language","F. -L. Huang; M. -S. Yu; J. -H. Lin","Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan; Department of Computer Science and Information Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan","2010 International Conference on Machine Learning and Cybernetics","20 Sep 2010","2010","5","","2549","2554","In the paper, we studied and implemented the system for CALL systems based on the NLP technologies-bilingual system for spoken Chinese and Hakka with four-county accent. The online system provides mainly several tasks of learning; bilingual lexical information, transliteration of words and examples, specially the function for practice of oral speeches. The bilingual corpora, dictionary and speech database are generated. In our speech databases, two kinds of unit, character and word speeches units for Hakka are recorded. There are 1408 monosyllabic speech units for Chinese, 2500 HAKKA's characters and 2200 word speeches for synthesis unit, respectively. The bilingual lexicon is bidirectional and allows the translation to and from both languages. The synthesized examples of speeches were evaluated for the quality of intelligibility and the results of satisfaction testing for two kinds of units achieve 5.31 and 7.82, respectively. It is obvious that our bilingual system provides real speech with natural quality and helpful lexical information on web, and achieve the goals for language learning.","2160-1348","978-1-4244-6527-9","10.1109/ICMLC.2010.5580859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5580859","Computer-Assisted Language Learning (CALL);Prosody Module;Bilingual Lexicon;Energy Normalization","Speech;Databases;Feature extraction;Electronic learning;Text analysis;Speech synthesis","computer aided instruction;language translation;linguistics;natural language processing;speech synthesis","intelligent bilingual system;spoken language;CALL systems;NLP technologies-bilingual system;bilingual lexical information;words transliteration;bilingual corpora;dictionary;speech database;computer assisted language learning","","2","","16","IEEE","20 Sep 2010","","","IEEE","IEEE Conferences"
"Deep Learning Techniques for Artistic Image Transformations: A Survey","R. C. Aralikatti; S. S. V; B. R. Chandavarkar","Dept. of Computer Science and Engg, NIT Karnataka, Surathkal, Mangalore, India; Dept. of Computer Science and Engg, NIT Karnataka, Surathkal, Mangalore, India; Dept. of Computer Science and Engg, NIT Karnataka, Surathkal, Mangalore, India","2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)","3 Nov 2021","2021","","","1","6","Deep learning has greatly revolutionized the ways in which computers tackle problems in vision, speech recognition, machine translation, etc., and has produced results which are almost inconceivable to conventional algorithms. Creative tasks such as fine arts and music composition, which were initially thought to be impossible to computers, are now possible. In this paper, we look at a particular class of problems called image-to-image translation problems and see how it can be leveraged to perform artistic image transformations. Generative Adversarial Networks (GANs) and related neural networks are particularly useful for this task. We explore some of the artistic image transformation tasks that deep learning can be used for and discuss the different machine learning architectures used, the results produced and the advancements made in literature towards tackling such tasks.","","978-1-7281-8595-8","10.1109/ICCCNT51525.2021.9580159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580159","Deep Learning;Image Processing;Neural Style Transfer;Generative Adversarial Networks (GANs);CycleGAN","Deep learning;Computers;Art;Image color analysis;Neural networks;Computer architecture;Speech recognition","art;language translation;learning (artificial intelligence);music;neural nets;speech recognition","image-to-image translation problems;artistic image transformations;related neural networks;artistic image transformation tasks;different machine learning architectures;deep learning techniques;speech recognition;machine translation;conventional algorithms;creative tasks;fine arts;music composition","","","","17","IEEE","3 Nov 2021","","","IEEE","IEEE Conferences"
"Improving transformer model translation for low resource South African languages using BERT","P. Chiguvare; C. W. Cleghorn","Department of Computer Science, University of Pretoria, Pretoria, South Africa; School of Computer Science and Applied Mathematics, University of Witwatersrand, Johannesburg, South Africa","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","1","8","Language pre-training methods such as BERT have led to significant performance gains for a wide range of natural language processing tasks. This paper investigates the improvement in transformer model translation for low resource South African languages when pre-trained models are used in language translation. A South African low resource language multilingual BERT, dubbed SALR-mBERT is pre-trained from scratch using all nine of the low resource South African languages. SALR-mBERT performs better than mBERT and the transformer models for language corpora with approximately less than 600000 sentences. SALR-mBERT obtained the best results for Sesotho, Ndebele, Swati and Venda. SALR-mBERT outperforms multilingual BERT (mBERT) on language corpora with less than 600000 sentences by about 29% and also outperforms the transformer model by 50%. The transformer model translation obtained the best results compared to mBERT and the SALR-mBERT models for language corpora with more than 600000 sentences. The transformer obtained the best results followed by SALR-mBERT for Northern Sotho, Tsonga, Xhosa, Tswana and Zulu to English translations. The transformer model outperforms SALR-mBERT by 15% for Zulu which has the largest corpus size compared to other low resource South African languages. The main finding of this work is that the BERT type module is beneficial for machine translation if the corpus size is small and has less than approximately 600000 sentences, and further improvement can be gained when the BERT model is trained using languages of a similar nature like in the case of SALR-mBERT. However, transformer model translations outperform BERT model translations if the corpus size is greater than 600000 sentences.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9659923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659923","pre-training;transformer model;multilingual BERT","Computational modeling;Bit error rate;Performance gain;Transformers;Machine translation;Task analysis;Computational intelligence","language translation;learning (artificial intelligence);natural language processing;speech recognition","improving transformer model translation;low resource South African languages;natural language processing tasks;language translation;South African low resource language multilingual BERT;dubbed SALR-mBERT;language corpora;SALR-mBERT outperforms multilingual BERT;SALR-mBERT models;BERT model translations","","","","20","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"An Integrated Model for Text to Text, Image to Text and Audio to Text Linguistic Conversion using Machine Learning Approach","A. R. Singh; D. Bhardwaj; M. Dixit; L. Kumar","Department of CEA, GLA University, Mathura, India; Department of CEA, GLA University, Mathura, India; Department of CEA, GLA University, Mathura, India; Amity University, Noida, India","2023 6th International Conference on Information Systems and Computer Networks (ISCON)","4 May 2023","2023","","","1","7","This paper presents an integrated model that uses machine learning techniques to perform text-to-text, image-to-text, and audio-to-text conversions, with particularly focus on Indian languages. The proposed model which can translate text, image, and voice has been tested on large datasets of various Indian languages and utilizes state-of-the-art techniques such as machine learning, computer vision, and speech recognition to accurately transcribe and translate the input data. The results obtained from the experiments demonstrate the effectiveness of the model by accurately converting text, images, and audio to text, and the potential applications of our proposed model range from language learning, accessibility for non-verbal or non-hearing individuals to cross-language communication. The proposed model is intended to bridge the language gap and facilitate communication among people from different linguistic backgrounds.","2832-143X","979-8-3503-4696-1","10.1109/ISCON57294.2023.10112123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112123","image-to-text voice-to-text;voice-to-voice;computer vision; cross language communication)","Bridges;Computer vision;Computational modeling;Machine learning;Speech recognition;Linguistics;Data models","computer vision;language translation;learning (artificial intelligence);linguistics;natural language processing;speech recognition;text analysis","audio-to-text conversions;cross-language communication;image-to-text;Indian languages;integrated model;language gap;language learning;machine learning approach;text linguistic conversion;text-to-text;utilizes state-of-the-art techniques","","","","6","IEEE","4 May 2023","","","IEEE","IEEE Conferences"
"An autoencoder with bilingual sparse features for improved statistical machine translation","B. Zhao; Y. -C. Tam; J. Zheng","SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","7103","7107","Though sparse features have produced significant gains over traditional dense features in statistical machine translation, careful feature selection and feature engineering are necessary to avoid over-fitting in optimizations. However, many sparse features are highly overlapping with each other; that is, they cover the same or similar information of translational equivalence from slightly different points of view, and eventually overfit easily with only very feature training samples in given bilingual stochastic context-free grammar (SCFG) rules. We propose a natural autoencoder that maps all the discrete and overlapping sparse features for each SCFG rule into a continuous vector, so that the information encoded in sparse feature vectors becomes a dense vector that may enjoy more samples during training and avoid overfitting. Our experiments showed that for a 33-million bilingual SCFG rules statistical machine translation system, the autoencoder generalizes much better than sparse features alone using the same optimization framework.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854978","machine translation;sparse features;SCFG grammar induction;optimization;autoencoder;PRO","Training;Optimization;Principal component analysis;Neural networks;Computational linguistics;Vectors;Tuning","context-free grammars;encoding;feature selection;language translation;natural language processing;optimisation;statistical analysis","autoencoder;bilingual sparse features;improved statistical machine translation;feature selection;feature engineering;optimization;translational equivalence;feature training sample;bilingual stochastic context-free grammar;SCFG rules;sparse feature vector;overfitting avoidance","","6","","17","IEEE","14 Jul 2014","","","IEEE","IEEE Conferences"
"Word-Based Bantu Language Identification using Na√Øve Bayes","B. Okgetheng; E. A. W. Budu","Department of Computer Science, University of Botswana, Gaborone, Botswana; Department of Computer Science, University of Botswana, Gaborone, Botswana","2022 IST-Africa Conference (IST-Africa)","18 Aug 2022","2022","","","1","7","Language identification of text has become increasingly important as large quantities of text are processed or filtered automatically. It is one of the preprocessing steps in Natural Language Processing (NLP) tasks such as information retrieval and machine translation. Few studies have worked on Bantu Languages in automatic language identification. Language identification is a challenge in Bantu languages because of lack of data and in addition to that, languages which are written similarly like Setswana and Sesotho are also challenging. In this paper, we present a word-based Na√Øve Bayes classifier to identify words of Sesotho and Setswana language. The classifier was trained with words from both Setswana and Sesotho in a supervised manner. Adjectives, pronouns, adverbs and enumeratives are also included. The classifier shows that the two languages can be individually identified as it gives an accuracy of 71.4%. Despite that when we increase the data to double the number of words, the model increased performance to 78%. We also report that the classifier fails with homographs. The performance could be improved by using more data. Additionally, the syllable identification and sentence identification could be implemented along with word-based classifier.","2576-8581","978-1-905824-69-4","10.23919/IST-Africa56635.2022.9845618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845618","Language Identification;NLP;Na√Øve Bayes;Setswana;Sesotho","Information filters;Data models;Machine translation;Task analysis","Bayes methods;information retrieval;language translation;natural language processing;pattern classification;speech processing;text analysis","sentence identification;word-based classifier;word-based Bantu Language identification;Natural Language Processing tasks;information retrieval;machine translation;Bantu Languages;automatic language identification;Bantu languages;word-based Na√Øve Bayes classifier;syllable identification","","","","19","","18 Aug 2022","","","IEEE","IEEE Conferences"
"TT-Net: Dual-Path Transformer Based Sound Field Translation in the Spherical Harmonic Domain","Y. Wang; Z. Lan; X. Wu; T. Qu","Key Laboratory on Machine Perception (Ministry of Education) School of Intelligence Science and Technology, Peking University, Beijing, China; Key Laboratory on Machine Perception (Ministry of Education) School of Intelligence Science and Technology, Peking University, Beijing, China; Key Laboratory on Machine Perception (Ministry of Education) School of Intelligence Science and Technology, Peking University, Beijing, China; Key Laboratory on Machine Perception (Ministry of Education) School of Intelligence Science and Technology, Peking University, Beijing, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","In the current method for the sound field translation tasks based on spherical harmonic (SH) analysis, the solution based on the additive theorem usually faces the problem of singular values caused by large matrix condition numbers. The influence of different distances and frequencies of the spherical radial function on the stability of the translation matrix will affect the accuracy of the SH coefficients at the selected point. Due to the problems mentioned above, we propose a neural network scheme based on the dual-path transformer. More specifically, the dual-path network is constructed by the selfattention module along the two dimensions of the frequency and order axes. The transform-average-concatenate layer and upscaling layer are introduced in the network, which provides solutions for multiple sampling points and upscaling. Numerical simulation results indicate that both the working frequency range and the distance range of the translation are extended. More accurate higher-order SH coefficients are obtained with the proposed dual-path network.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094775","Research and Development; National Natural Science Foundation of China; Peking University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094775","Spherical harmonic analysis;dual-path transformer;translation matrix;sound field reproduction","Simulation;Neural networks;Scattering;Transformers;Harmonic analysis;Stability analysis;Recording","","","","","","23","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Verification of multi-class recognition decision using classification approach","T. Matsui; F. K. Soong; Biing-Hwang Juang","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; Bell Laboratories, Lucent Technologies, Inc., Murray Hill, NJ, USA; Bell Laboratories, Lucent Technologies, Inc., Murray Hill, NJ, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","123","126","We investigate various strategies to improve the utterance verification performance using a 2-class pattern classifier. They include utilizing N-best candidate scores, modifying segmentation boundaries, applying background and out-of-vocabulary filler models, incorporating contexts, and minimizing verification errors via discriminative training. A connected-digit database containing utterances recorded in a noisy, moving car with a hands-free microphone mounted on a sun-visor is used to evaluate the verification performance. The equal error rate (EER) of word verification is employed as the performance measure in our evaluations. All factors considered in our study and their effects on the verification performance are presented in detail. The EER is reduced from 29%, using the standard likelihood ratio test, down to 21.4%, when all enhancements are integrated together.","","0-7803-7343-X","10.1109/ASRU.2001.1034603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034603","","Testing;Automatic speech recognition;Natural languages;Context modeling;Databases;Microphones;Performance evaluation;Man machine systems;Degradation;Working environment noise","speech recognition;decision theory;pattern classification;vocabulary;microphones;error statistics;speech enhancement","multi-class recognition decision;classification;utterance verification performance;2-class pattern classifier;N-best candidate scores;segmentation boundaries;background filler model;out-of-vocabulary filler model;contexts;discriminative training;connected-digit database;noisy moving car;hands-free microphone;equal error rate;word verification;speech enhancement","","3","","11","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"The statistical approach to spoken language translation","H. Ney","Lehrstuhl f√ºr Informatik VI, Computer Science Department, RWTH-Aachen-University of Technology, Aachen, Germany","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","367","374","This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the VERBMOBIL project. The goal of the VERBMOBIL project is the translation of spoken dialogues in the domains of appointment scheduling and travel planning. Starting with the Bayes decision rule as in speech recognition; we show how the required probability distributions can be structured into three parts: the language model, the alignment model and the lexicon model. We describe the components of the system and report results on the VERBMOBIL task. The experience obtained in the VERBMOBIL project, in particular a largescale end-to-end evaluation, showed that the statistical approach resulted in significantly lower error rates than three competing translation approaches: the sentence error rate was 29% in comparison with 52% to 62% for the other translation approaches. Finally, we discuss the integrated approach to speech translation as opposed to the serial approach that is widely used nowadays.","","0-7803-7343-X","10.1109/ASRU.2001.1034663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034663","","Natural languages;Hidden Markov models;Probability distribution;Search problems;Performance loss","speech recognition;language translation;statistical analysis;natural language interfaces;Bayes methods;decision theory;knowledge based systems;probability;error statistics","spoken language translation;statistical machine translation;spoken dialogue;VERBMOBIL project;appointment scheduling;travel planning;Bayes decision rule;speech recognition;language model;alignment model;lexicon model;sentence error rate;recognition errors","","","","29","IEEE","7 Nov 2002","","","IEEE","IEEE Conferences"
"Entering Tone Recognition in a Support Vector Machine Approach","X. Wang; Y. Liu; L. Cai","Department of Chinese Language and Literature, Tsinghua University, China; Department of Chinese Language and Literature, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China","2008 Fourth International Conference on Natural Computation","7 Nov 2008","2008","2","","61","65","This paper introduces support vector machine classifiers into entering tone recognition. Not every syllable needs recognition in a statistical way. The recognition accuracy of syllables which need recognition in the support vector machine approach is about 90%, which makes it possible to analyze poems' rhymes and translate Mandarin into many Chinese dialects. The experiments also check the influence of context window attributes on the entering tone recognition.","2157-9563","978-0-7695-3304-9","10.1109/ICNC.2008.757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666957","Support Vector Machine;entering tone;pinyin","Support vector machines;Support vector machine classification;Kernel;Natural languages;Computer science;Statistical learning;Handwriting recognition;Tagging;Asia;Polynomials","learning (artificial intelligence);natural languages;signal classification;speech recognition;statistical analysis;support vector machines","entering tone recognition;support vector machine classifier approach;statistical learning;syllable recognition;poem rhyme analysis;Mandarin translation;Chinese dialect;context window attribute","","2","","12","IEEE","7 Nov 2008","","","IEEE","IEEE Conferences"
"Adversarial Mask Transformer for Sequential Learning","H. Lio; S. -E. Li; J. -T. Chien","Dept of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Dept of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan; Dept of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Taiwan","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","4178","4182","Mask language model has been successfully developed to build a transformer for robust language understanding. The transformer-based language model has achieved excellent results in various downstream applications. However, typical mask language model is trained by predicting the randomly masked words and is used to transfer the knowledge from rich-resource pre-training task to low-resource downstream tasks. This study incorporates a rich contextual embedding from pre-trained model and strengthens the attention layers for sequence-to-sequence learning. In particular, an adversarial mask mechanism is presented to deal with the shortcoming of random mask and accordingly enhance the robustness in word prediction for language understanding. The adversarial mask language model is trained in accordance with a minimax optimization over the word prediction loss. The worst-case mask is estimated to build an optimal and robust language model. The experiments on two machine translation tasks show the merits of the adversarial mask transformer.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746154","Adversarial learning;transformer;mask language model;sequential learning","Adaptation models;Predictive models;Signal processing;Transformers;Robustness;Machine translation;Task analysis","language translation;learning (artificial intelligence);minimax techniques;natural language processing;speech recognition","adversarial mask transformer;sequential learning;robust language understanding;transformer-based language model;downstream applications;typical mask language model;rich-resource pre-training task;low-resource downstream tasks;rich contextual embedding;strengthens;sequence-to-sequence learning;adversarial mask mechanism;random mask;adversarial mask language model;word prediction loss;worst-case mask;optimal language model;robust language model","","5","","28","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Evaluating the translation of speech to virtually-performed sign language on AR glasses","L. T. Nguyen; F. Schicktanz; A. Stankowski; E. Avramidis","Technische Universit√§t, Berlin, Germany; Technische Universit√§t, Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Berlin, Germany","2021 13th International Conference on Quality of Multimedia Experience (QoMEX)","30 Jun 2021","2021","","","141","144","This paper describes the proof-of-concept evaluation for a system that provides translation of speech to virtually performed sign language on augmented reality (AR) glasses. The discovery phase via interviews confirmed the idea for a signing avatar displayed within the users field of vision through AR glasses. In the evaluation of the first prototype through a wizard-of-Oz-experiment, the presented AR solution received a high acceptance rate among deaf and hard-of-hearing persons. However, the machine learning based method used to generate sign language from video still lacks the required accuracy for fully preserving comprehensibility. Signed sentences with large recognisable arm movements were understood better than sentences relying mainly on finger movements, where only a small interaction space is visible.","2472-7814","978-1-6654-3589-5","10.1109/QoMEX51781.2021.9465430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465430","sign language translation;real-time translation;augmented reality;AR glasses;avatar;inclusion","Avatars;Assistive technology;Fingers;Prototypes;Glass;Gesture recognition;Machine learning","augmented reality;avatars;computer animation;gesture recognition;handicapped aids;learning (artificial intelligence);sign language recognition;wearable computers","sign language;AR glasses;proof-of-concept evaluation;augmented reality glasses;signing avatar;signed sentences;finger movements;small interaction space;hard-of-hearing persons","","3","","12","IEEE","30 Jun 2021","","","IEEE","IEEE Conferences"
"Statistical language approach translates into success","S. J. Vaughan-Nichols","Arden, NC","Computer","10 Nov 2003","2003","36","11","14","16","Machine translation currently translates text from one language into another. However, work is under way on speech-to-speech translation. There are two kinds of machine translation: knowledge-based and statistical. Knowledge-based systems translate documents by converting words and grammar directly from one language into another. Rather than using the knowledge-based system's direct word-by-word translation techniques, statistical approaches translate documents by statistically analyzing entire phrases and, over time, ""learning"" how various languages work. The article examines the pros and cons of both systems, and predicts that statistical methods will become more popular, however the future will involve combining statistical and knowledge-based methods to create better systems.","1558-0814","","10.1109/MC.2003.1244527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244527","","Business;Knowledge based systems;Programming profession;Market research;Companies;Internet;Mission critical systems;Turning;Machine intelligence;Vocabulary","language translation;statistical analysis;knowledge based systems","statistical language approach;machine translation;speech-to-speech translation;knowledge-based approach;expert systems;direct word-by-word translation techniques;phrase analysis;statistical translation","","","5","","IEEE","10 Nov 2003","","","IEEE","IEEE Magazines"
"Development of Analysis Rules for Bangla Part of Speech for Universal Networking Language","M. Banik; M. R. Rasel; A. K. Saha; F. Hassan; M. F. Mridha; M. N. Huda","Department of computer science and engineering, Ahsanullah university of science and technology, Dhaka, Bangladesh; Department of computer science and engineering, United international university, Dhaka, Bangladesh; Department of computer science and engineering, University of asia pacific, Dhaka, Bangladesh; Department of computer science and engineering, Stamford University, Dhaka, Bangladesh; Department of computer science and engineering, University of asia pacific, Dhaka, Bangladesh; Department of computer science and engineering, United international university, Dhaka, Bangladesh","2011 Eighth International Conference on Information Technology: New Generations","11 Jul 2011","2011","","","797","802","The Universal Networking Language (UNL) is a worldwide generalizes form human interactive in machine independent digital platform for defining, recapitulating, amending, storing and dissipating knowledge or information among people of different affiliations. The theoretical and practical research associated with these interdisciplinary endeavor facilities in a number of practical applications in most domains of human activities such as creating globalization trends of market or geopolitical independence among nations. In our research work we have tried to develop analysis rules for Bangla part of speech which will help to create a doorway for converting the Bangla language to UNL and vice versa and overcome the barrier between Bangla to other Languages.","","978-1-61284-427-5","10.1109/ITNG.2011.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945338","universal networking language;morphology;bangla part of speech;morphological rules","Semantics;Speech;Computers;Electronic mail;Mood;Syntactics;Humans","globalisation;knowledge based systems;language translation;natural language processing","analysis rules;Bangla part of speech;universal networking language;UNL;human interactive;machine independent digital platform;interdisciplinary endeavor facility;human activity;globalization trends;market independence;geopolitical independence;Bangla language","","1","","16","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Learning the Multilingual Translation Representations for Question Retrieval in Community Question Answering via Non-Negative Matrix Factorization","G. Zhou; Z. Xie; T. He; J. Zhao; X. T. Hu","School of Computer Science, Central China Normal University, Wuhan, China; School of Computer Science, Central China Normal University, Wuhan, China; School of Computer Science, Central China Normal University, Wuhan, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Computing and Informatics, Drexel University, Philadelphia, PA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","7","1305","1314","Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.","2329-9304","","10.1109/TASLP.2016.2544661","National Natural Science Foundation of China(grant numbers:61303180,61573163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437414","Natural Language Processing;Information Retrieval;Community Question Answering;Question Retrieval;Text Mining;Natural Language Processing;Information Retrieval;Community Question Answering;Question Retrieval;Text Mining","Semantics;Context modeling;Speech;Speech processing;Knowledge discovery;IEEE transactions;Computers","language translation;learning (artificial intelligence);matrix decomposition;question answering (information retrieval);statistical analysis","multilingual translation representation learning;question retrieval problem;community question answering;nonnegative matrix factorization;CQA;word ambiguity;word mismatch problems;monolingual translation models;parallel monolingual corpora quality;statistical machine translation;question representation;CQA data sets","","24","","39","IEEE","21 Mar 2016","","","IEEE","IEEE Journals"
"Exploiting Morphology and Local Word Reordering in English-to-Turkish Phrase-Based Statistical Machine Translation","I. D. El-Kahlout; K. Oflazer","LIMSI-CNRS, Orsay, France; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey","IEEE Transactions on Audio, Speech, and Language Processing","16 Aug 2010","2010","18","6","1313","1322","In this paper, we present the results of our work on the development of a phrase-based statistical machine translation prototype from English to Turkish‚Äîan agglutinative language with very productive inflectional and derivational morphology. We experiment with different morpheme-level representations for English‚ÄìTurkish parallel texts. Additionally, to help with word alignment, we experiment with local word reordering on the English side, to bring the word order of specific English prepositional phrases and auxiliary verb complexes, in line with the morpheme order of the corresponding case-marked nouns and complex verbs, on the Turkish side. To alleviate the dearth of the parallel data available, we also augment the training data with sentences just with content word roots obtained from the original training data to bias root word alignment, and with highly reliable phrase-pairs from an earlier corpus alignment. We use a morpheme-based language model in decoding and a word-based language model in re-ranking the  $n$-best lists generated by the decoder. Lastly, we present a scheme for repairing the decoder output by correcting words which have incorrect morphological structure or which are out-of-vocabulary with respect to the training data and language model, to further improve the translations. We improve from 15.53 BLEU points for our word-based baseline model to 25.17 BLEU points for an improvement of 9.64 points or about 62% relative.","1558-7924","","10.1109/TASL.2009.2033321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5272404","Complex morphology;English;statistical machine translation (SMT);Turkish;word reordering","Morphology;Decoding;Natural languages;Training data;Context modeling;Surface-mount technology;Prototypes;Natural language processing;Concurrent computing;Information resources","language translation","morphology;local word reordering;English-to-Turkish phrase;statistical machine translation;auxiliary verb complexes;English prepositional phrases;BLEU points","","11","","43","IEEE","29 Sep 2009","","","IEEE","IEEE Journals"
"Comparative Study of Different Models for Language Translation","S. J. Kalyanshetti; M. S. Jagtap; A. U. Kale; P. Waghmare","Department of Electronics and Telecommunication, MKSSS Cummins College of Engineering for Women, Pune, India; Department of Electronics and Telecommunication, MKSSS Cummins College of Engineering for Women, Pune, India; Department of Electronics and Telecommunication, MKSSS Cummins College of Engineering for Women, Pune, India; Department of Electronics and Telecommunication, MKSSS Cummins College of Engineering for Women, Pune, India","2022 6th International Conference On Computing, Communication, Control And Automation (ICCUBEA","16 Jan 2023","2022","","","1","4","As India is country with diverse culture, there are 22 official languages and many other regional languages. Because of this, it is difficult for people to exchange the ideas and communicate with each other. In Neural Machine Translation (NMT), one language is translated to other which helps to overcome the barrier of language difference. In this paper, we have studied NMT Models to translate English sentences to Marathi. The neural machine translation models on the basis of Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU) are implemented respectively considering the differences in the structure of their networks. Transformer is also implemented after we discover the limitations of the above three basic models which are Long Term Dependency of Words, Exploding Gradient and Vanishing Gradient problem. Common steps followed in all models are Preprocessing, encoding and decoding. After studying all the models, a comparative analysis of models is done based on the Bilingual Evaluation Understudy (BLEU) score where we calculated Unigram(1-gram) and Bigram(2-gram) scores using reference and translated sentences.","2771-1358","978-1-6654-8451-0","10.1109/ICCUBEA54992.2022.10011127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011127","Seq2Seq;RNN;LSTM;GRU;BLEU Score;NMT;NLP","Measurement;Analytical models;Recurrent neural networks;Computational modeling;Logic gates;Transformers;Encoding","electronic calculators;handicapped aids;language translation;natural language processing;recurrent neural nets;speech synthesis;text analysis","22 official languages;comparative analysis;diverse culture;English sentences;Gated Recurrent Units;language difference;language Translation;Long Short-Term Memory;neural machine translation models;NMT Models;Recurrent Neural Network;regional languages;translated sentences;Vanishing Gradient problem","","","","13","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Variational Bayesian approach for automatic generation of HMM topologies","T. Jitsuhiro; S. Nakamura","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","77","82","We propose a new method of automatically creating non-uniform, context-dependent HMM topologies by using the variational Bayesian (VB) approach. The maximum likelihood (ML) criterion is generally used to create HMM topologies. However, it has an overfitting problem. Information criteria have been used to overcome this problem, but, theoretically, they cannot be applied to complicated models like HMMs. Recently, to avoid these problems, a VB approach has been developed in the machine-learning field. The successive state splitting (SSS) algorithm is a method of creating contextual and temporal variations for HMMs. We introduce the VB approach to the SSS algorithm, and define the prior and posterior probability densities and free energy as split and stop criteria. Experimental results show that the proposed method can automatically create the proper model and obtain better performance, especially for vowels, than the original method.","","0-7803-7980-2","10.1109/ASRU.2003.1318407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318407","","Bayesian methods;Hidden Markov models;Topology;Maximum likelihood estimation;Clustering algorithms;Decision trees;Speech recognition;Training data;Natural languages;Laboratories","hidden Markov models;speech recognition;Bayes methods;topology;variational techniques;learning (artificial intelligence)","variational Bayesian approach;HMM topologies;maximum likelihood criterion;overfitting problem;machine-learning;successive state splitting algorithm;contextual variations;temporal variations;probability densities;free energy;split criteria;stop criteria;vowels;speech recognition","","2","","11","IEEE","2 Aug 2004","","","IEEE","IEEE Conferences"
"Streaming Attention-Based Models with Augmented Memory for End-To-End Speech Recognition","C. -F. Yeh; Y. Wang; Y. Shi; C. Wu; F. Zhang; J. Chan; M. L. Seltzer","Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA; Facebook AI, USA","2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","8","14","Attention-based models have been gaining popularity recently for their strong performance demonstrated in fields such as machine translation [1] and automatic speech recognition [2]. One major challenge of attention-based models is the need of access to the full sequence and the quadratically growing computational cost concerning the sequence length. These characteristics pose challenges, especially for low-latency scenarios, where the system is often required to be streaming. In this paper, we build a compact and streaming speech recognition system on top of the end-to-end neural transducer architecture [3] with attention-based modules augmented with convolution [2]. The proposed system equips the end-to-end models with the streaming capability and reduces the large footprint from the streaming attention-based model using augmented memory [4], [5]. On the LibriSpeech [6] dataset, our proposed system achieves word error rates 2.7% on test-clean and 5.8% on test-other, to our best knowledge the lowest among streaming approaches reported so far.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383504","transformer;transducer;end-to-end;self-attention;speech recognition","Transducers;Error analysis;Convolution;Computational modeling;Conferences;Machine translation;Context modeling","convolutional neural nets;natural language processing;speech recognition","attention-based models;augmented memory;end-to-end speech recognition;machine translation;automatic speech recognition;streaming speech recognition system;end-to-end neural transducer architecture;attention-based modules;end-to-end models;streaming attention-based model","","2","","25","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"Towards a Bidirectional Machine Translator Generator for Multilingual Communication","R. Lane; A. Bansal","Arizona State University, School of Computing, Informatics, and Decision Systems Engineering, Mesa, AZ; Arizona State University, School of Computing, Informatics, and Decision Systems Engineering, Mesa, AZ","2019 IEEE International Conference on Conversational Data & Knowledge Engineering (CDKE)","6 Jan 2020","2019","","","25","32","Machine Translation (MT) systems are typically quite complex, especially those used in production environments where high-quality conversational text or speech translation from one language to another is important. As a result, the vast majority of MT systems support translation between a single language pair, often uni-directionally. This research study extends previous work to assess the efficacy of developing a bidirectional translator generator in Prolog programming language using Lexical Functional Grammars. The main research objective is building a machine translator generator for multilingual communication, i.e. developing a system whose inputs are linguistic descriptions of a desired source and target language and whose output is a program that translates between the two natural languages. The implementation of a bidirectional machine translator between English and Hungarian, developed as a proof-of-concept case study, is discussed and assessed in terms of four general classes of translation. The benefits and drawbacks of this approach as generalized to MT systems are also discussed, along with possible areas of future work.","","978-1-7281-6088-7","10.1109/CDKE46621.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949379","lexical functional grammars, conversational text translation, logic programming, machine translation, natural language processing","Syntactics;Grammar;Linguistics;Natural language processing;Buildings","grammars;language translation;natural language processing;PROLOG;text analysis","bidirectional machine translator generator;multilingual communication;production environments;high-quality conversational text;MT systems support translation;single language pair;bidirectional translator generator;Prolog programming language;target language;natural languages;lexical functional grammars;machine translation systems","","","","13","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Sign Language Translation using a Chrome extension for Google Meet","M. Chattopadhyay; M. Parulekar; V. Bhat; B. Raisinghani; S. Arya","Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, India; Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, India; Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, India; Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, India; Department of Electronics and Telecommunication, Vivekanand Education Society's Institute of Technology, Mumbai, India","2022 IEEE Region 10 Symposium (TENSYMP)","29 Aug 2022","2022","","","1","5","Sign language is a source of communication used primarily by the community of speech and hearing-impaired people. The history of sign language goes as far behind as the Seventeenth century wherein the basic tool of communication for the aforementioned community was not universal but geography-specific. In this paper, we discuss our findings on the implementation of a Google Chrome extension for the Google meet (Videoconferencing) software which translates sign language into captions with the use of Machine Learning. Additionally, there is also a provision for text to speech for the translated texts. By making this extension, we lay the foundation for a bridge between users of ASL signs and the people who cannot understand them without translation for seamless communication.","2642-6102","978-1-6654-6658-5","10.1109/TENSYMP54529.2022.9864398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864398","Sign language;communication;videoconferencing;Google chrome extension;Google meet;Machine Learning","Teleconferencing;Machine learning algorithms;Gesture recognition;Assistive technologies;Software;Internet;Classification algorithms","handicapped aids;language translation;learning (artificial intelligence);natural language processing;online front-ends;sign language recognition;teleconferencing","sign language translation;Google meet;hearing-impaired people;aforementioned community;Google Chrome extension;translated texts;ASL signs","","","","15","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Part-Of-Speech Tagger in Malayalam Using Bi-directional LSTM","R. Rajan; A. J. Joseph; E. K. Robin; N. T. K. Fathima","College of Engineering, Trivandrum, Kerala, India; College of Engineering, Trivandrum, Kerala, India; College of Engineering, Trivandrum, Kerala, India; College of Engineering, Trivandrum, Kerala, India","2020 23rd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","28 Dec 2020","2020","","","22","27","The majority of activities performed by humans are done through language, whether communicated directly or reported using natural language. As technology is increasingly making the methods and platforms on which we communicate ever more accessible, there is a great need to understand the languages we use to communicate. By combining the power of artificial intelligence, computational linguistics and computer science, natural language processing (NLP) helps machines ‚Äúread‚Äù text by simulating the human ability to understand language. Part-of-speech tagging (POS Tagging) is done as a pre-requisite to simplify a lot of different NLP applications like question answering, speech recognition, machine translation, and so on. Here, we attempt a comparison between part-of-speech taggers in Malayalam using decision tree algorithm and bi-directional long short term memory (BLSTM). The experiments presented in this paper use two corpora, one of 29076 sentences and the other of 500 sentences for performance evaluation. The experiments demonstrate the potential of architectural choice of BLSTM-based tagger over conventional decision tree-based tagging in Malayalam.","2472-7695","978-1-7281-9896-5","10.1109/O-COCOSDA50338.2020.9295018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9295018","POS tagging;Malayalam;NLP;Decision tree;BLSTM;Stochastic process","Logic gates;Tagging;Bidirectional control;Task analysis;Deep learning;Computational modeling;Classification algorithms","computational linguistics;decision trees;language translation;natural language processing;recurrent neural nets;speech recognition;text analysis","BLSTM-based tagger;Malayalam;part-of-speech tagger;bidirectional LSTM;artificial intelligence;computational linguistics;computer science;natural language processing;human ability;part-of-speech tagging;POS tagging;NLP applications;speech recognition;machine translation;part-of-speech taggers;decision tree algorithm;performance evaluation","","2","","12","IEEE","28 Dec 2020","","","IEEE","IEEE Conferences"
"Optimizing kernel alignment by data translation in feature space","J. -B. Pothin; C. Richard","Institut Charles Delaunay (ICD-M2S, FRE CNRS2848), Universit√© de Technologie de Troyes, Troyes, France; Institut Charles Delaunay (ICD-M2S, FRE CNRS2848), Universit√© de Technologie de Troyes, Troyes, France","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","3345","3348","Kernel-target alignment is commonly used to predict the behavior of any given reproducing kernel in a classification context, without training any kernel machine. However, a poor position of the data in feature space can drastically reduce the value of the alignment. This implies that, in a kernel selection setting, the best kernel in a given collection may be associated with a low value of alignment. In this paper, we present a new algorithm for maximizing the alignment by data translation in feature space. The aim is to reduce the biais introduced by the translation non-invariance of this criterion. Experimental results on multi-dimensional benchmarks show the effectiveness of our approach.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518367","kernel alignment;data translation;SVM","Kernel;Support vector machines;Space technology","optimisation;pattern classification","kernel alignment optimization;data translation;feature space;kernel-target alignment;kernel selection","","2","","9","IEEE","12 May 2008","","","IEEE","IEEE Conferences"
"ASR domain adaptation methods for low-resourced languages: Application to Romanian language","H. Cucu; L. Besacier; C. Burileanu; A. Buzo","University Politehnica of Bucharest, Romania; LIG, Joseph Fourier University, Grenoble, France; University Politehnica of Bucharest, Romania; University Politehnica of Bucharest, Romania","2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)","18 Oct 2012","2012","","","1648","1652","This study investigates the possibility of using statistical machine translation to create domain-specific language resources. We propose a methodology that aims to create a domain-specific automatic speech recognition system for a low-resourced language when in-domain text corpora are available only in a high-resourced language. We evaluate a new semi-supervised method and compare it with previously developed semi-supervised and unsupervised approaches. Moreover, in the effort of creating an out-of-domain language model for Romanian, we introduce and experiment an effective diacritics restoration algorithm.","2076-1465","978-1-4673-1068-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6333947","ASR domain adaptation;SMT;language modeling;diacritics restoration","Adaptation models;Hidden Markov models;Probabilistic logic;Speech;Acoustics;Context;Automatic speech recognition","language translation;learning (artificial intelligence);natural language processing;speech recognition;statistical analysis","ASR domain adaptation methods;low-resourced languages;Romanian language;statistical machine translation;domain specific language resource;speech recognition system;text corpora;semisupervised method","","1","","14","","18 Oct 2012","","","IEEE","IEEE Conferences"
"Punctuated transcription of multi-genre broadcasts using acoustic and lexical approaches","O. Klejch; P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, Edinburgh, UK","2016 IEEE Spoken Language Technology Workshop (SLT)","9 Feb 2017","2016","","","433","440","In this paper we investigate the punctuated transcription of multi-genre broadcast media. We examine four systems, three of which are based on lexical features, the fourth of which uses acoustic features by integrating punctuation into the speech recognition acoustic models. We also explore the combination of these component systems using voting and log-linear interpolation. We performed experiments on the English language MGB Challenge data, which comprises about 1,600h of BBC television recordings. Our results indicate that a lexical system, based on a neural machine translation approach is significantly better than other systems achieving an F-Measure of 62.6% on reference text, with a relative degradation of 19% on ASR output. Our analysis of the results in terms of specific punctuation indicated that using longer context improves the prediction of question marks and acoustic information improves prediction of exclamation marks. Finally, we show that even though the systems are complementary, their straightforward combination does not yield better F-measures than a single system using neural machine translation.","","978-1-5090-4903-5","10.1109/SLT.2016.7846300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846300","punctuation;speech recognition;neural machine translation;rich transcription","Acoustics;Context;Predictive models;Hidden Markov models;Labeling;Interpolation;Context modeling","acoustic signal processing;interpolation;neural nets;speech recognition;text analysis","exclamation mark prediction;acoustic information;question mark prediction;specific punctuation;ASR output;relative degradation;reference text;F-measure;neural machine translation approach;BBC television recordings;English language MGB Challenge data;log-linear interpolation;voting;speech recognition acoustic model;acoustic features;lexical features;multigenre broadcast media;acoustic approach;punctuated transcription","","10","","34","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Automatic Stopword Detection Using Term Ranking between Written and Machine Speech Recognition Transcribed Reviews","J. J. Hind; M. Mahyoub; D. Woods; C. Wong; A. Hussain; D. Al-Jumeily","LivingLens, UK; LivingLens, UK; LivingLens, UK; LivingLens, UK; Liverpool John Moores University, UK; Liverpool John Moores University, UK","2019 12th International Conference on Developments in eSystems Engineering (DeSE)","23 Apr 2020","2019","","","301","308","Video feedback and machine speech recognition are fast-becoming a popular choice for companies to gain insight into their products. In conjunction with this, text analytics can be used to extract insight from these video translations. Currently, there is little work in the area to analyse and compare techniques for natural language processing, information retrieval and information extraction. A commonly practiced technique in text analytics is the extraction of stop words; words whose presence do not contribute context or information to a document. In this paper, we explore statistical techniques for the automated extraction of stop words, comparing 4 datasets from written and translated reviews. Using statistical variations of the successful technique `term ranking', we evaluate their performance using a common list of stop words. Results suggest that variation, TFnormIDFnorm, was the most successful with a best performing precision rate of 46.7% and a recall rate of 86.6%. The best results were seen in the largest dataset using written reviews, however comparison of the remaining 3 datasets revealed that spoken text performed 0.4% better in precision than the next best dataset and 2.6% better in recall. Initial results show marginally better performance in machine speech recognition transcribed texts from videos in comparison to comparably size datasets of written reviews.","2161-1351","978-1-7281-3021-7","10.1109/DeSE.2019.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9073508","Term Ranking;TFIDF;Machine Speech Recognition (MSR);stop words;marketing;reviews.","Ranking (statistics);Speech recognition;Measurement;Text mining;Dictionaries","information retrieval;natural language processing;speech recognition;statistical analysis;text analysis","written translated reviews;statistical variations;spoken text;automatic Stopword detection;transcribed reviews;video feedback;text analytics;video translations;natural language processing;information retrieval;information extraction;statistical techniques;term ranking;machine speech recognition transcribed reviews","","","","20","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"Enhancing the Quality of Phrase-Table in Statistical Machine Translation for Less-Common and Low-Resource Languages","M. -T. Nguyen; V. TanBui; H. -H. Vu; P. -T. Nguven; C. -M. Luong","Department of Computer Science, VNU Hanoi; Department of Information Technology, University of Economic and Technical Industries; University of Engineering and Technology, VNU Hanoi; Department of Computer Science, VNU Hanoi; Department of Language and Speech Processing, Institute of Information Technology","2018 International Conference on Asian Language Processing (IALP)","31 Jan 2019","2018","","","165","170","The phrase-table plays an important role in traditional phrase-based statistical machine translation (SMT) system. During translation, a phrase-based SMT system relies heavily on phrase-table to generate outputs. In this paper, we propose two methods for enhancing the quality of phrase-table. The first method is to recompute phrase-table weights by using vector representations similarity. The remaining method is to enrich the phrase-table by integrating new phrase-pairs from an extended dictionary and projections of word vector presentations on the target-language space. Our methods produce an attainment of up to 0.21 and 0.44 BLEU scores on in-domain and cross-domain (Asian Language Treebank - ALT) English - Vietnamese datasets respectively.","","978-1-7281-1175-9","10.1109/IALP.2018.8629188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629188","statistical machine translation;phrase-table;vector representation similarity;extend dictionary","Dictionaries;Matrix converters;Viterbi algorithm;Interpolation;Data models;Information technology;Semantics","language translation;natural language processing;vectors;word processing","word vector presentations;cross-domain ALT English - Vietnamese datasets;asian language treebank;low-resource languages;less-common languages;phrase-table quality enhancement;phrase-based statistical machine translation system;phrase-pairs;phrase-table weights;phrase-based SMT system","","","","17","IEEE","31 Jan 2019","","","IEEE","IEEE Conferences"
"A Statistical Approach to Expandable Spoken Dialog Systems using WFSTs","C. Hori; K. Ohtake; T. Misu; H. Kashioka; S. Nakamura","National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan","2008 Second International Symposium on Universal Communication","22 Dec 2008","2008","","","24","27","We have proposed an efficient approach to manage a dialog system using a weighted finite-state transducer (WFST) in which users¬ø concept and system¬øs action tags are input and output of the transducer, respectively. A WFST for dialog management was automatically created using a corpus annotated with inter-change format (IF) consisting of dialog acts and argument which is an interlingua for machine translation. A word-to-concept WFST for spoken language understanding (SLU) was created using the same corpus. The scenario and SLU WFSTs acquired from the corpus were composed together and then optimized. We have confirmed the WFST automatically trained using the annotated corpus can manage dialog reasonably.","","978-0-7695-3433-6","10.1109/ISUC.2008.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724437","WFST-based dialog system;statistical dialog management","Cities and towns;Transducers;Natural languages;Weather forecasting;Communications technology;Technology management;Decoding;Speech processing;Automata;Automatic speech recognition","finite state machines;interactive systems;language translation;speech-based user interfaces;statistical analysis","statistical approach;expandable spoken dialog system;weighted finite-state transducer;dialog management;interchange format;dialog acts;dialog argument;interlingua;machine translation;word-to-concept WFST;spoken language understanding","","","","7","IEEE","22 Dec 2008","","","IEEE","IEEE Conferences"
"Adequacy‚ÄìFluency Metrics: Evaluating MT in the Continuous Space Model Framework","R. E. Banchs; L. F. D‚ÄôHaro; H. Li","Human Language Technology Department, Singapore; Human Language Technology Department of ${\hbox {I}}^{2}{\hbox {R}}$ , Singapore; Human Language Technology Department, Singapore","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","3","472","482","This work extends and evaluates a two-dimensional automatic evaluation metric for machine translation, which is designed to operate at the sentence level. The metric is based on the concepts of adequacy and fluency, aiming at decoupling both semantic and syntactic components of the translation process to provide a more balanced view on translation quality. These two elements are independently evaluated by using continuous space and $n$ -gram language modeling frameworks, respectively. Two different implementations are evaluated: a monolingual version that fully operates on the target language side, and a cross-language version that has the main advantage of not requiring reference translations. Both implementations are evaluated by comparing their performance with state-of-the-art automatic metrics over a dataset involving five different European languages.","2329-9304","","10.1109/TASLP.2015.2405751","Human Language Technology Department of the Institute for Infocomm Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045551","Machine translation;natural language processing;system evaluation","Semantics;IEEE transactions;Speech;Speech processing;Syntactics;Extraterrestrial measurements","language translation;natural language processing","adequacy-fluency metrics;MT;continuous space model framework;machine translation process;two-dimensional automatic evaluation metric;sentence level;syntactic components;semantic components;translation quality;n-gram language modeling frameworks;monolingual version;cross-language version;European languages;natural language processing","","22","","40","IEEE","19 Feb 2015","","","IEEE","IEEE Journals"
"Smart Sight: a tourist assistant system","Jie Yang; Weiyi Yang; M. Denecke; A. Waibel","Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA","Digest of Papers. Third International Symposium on Wearable Computers","6 Aug 2002","1999","","","73","78","In this paper, we present our efforts towards developing an intelligent tourist system. The system is equipped with a unique combination of sensors and software. The hardware includes two computers, a GPS receiver, a lapel microphone plus an earphone, a video camera and a head-mounted display. This combination includes a multimodal interface to take advantage of speech and gesture input to provide assistance for a tourist. The software supports natural language processing, speech recognition, machine translation, handwriting recognition and multimodal fusion. A vision module is trained to locate and read written language, is able to adapt to to new environments, and is able to interpret intentions offered by the user such as a spoken clarification or pointing gesture. We illustrate the applications of the system using two examples.","","0-7695-0428-0","10.1109/ISWC.1999.806662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=806662","","Intelligent sensors;Computer displays;Intelligent systems;Sensor systems;Hardware;Global Positioning System;Microphones;Headphones;Cameras;Speech","travel industry;natural language interfaces;speech recognition;language translation;handwriting recognition;portable computers;head-up displays","tourist assistant system;Smart Sight;intelligent tourist system;sensors;software;computers;GPS receiver;lapel microphone;earphone;video camera;head-mounted display;multimodal interface;speech input;gesture input;natural language processing;speech recognition;machine translation;handwriting recognition;multimodal fusion;vision module;written language reading;written language location;new environment adaptation;user intention interpretation;spoken clarification;pointing gesture","","28","307","17","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A system of text reading and translation to voice for blind persons","L. Leija; S. Santiago; C. Alvarado","Secci√≥n Bioelectr√≥nica, Departmento de Ing. El√©ctrica, CINVESTAV, Mexico, Mexico; Secci√≥n Bioelectr√≥nica, Departmento de Ing. El√©ctrica, CINVESTAV, Mexico, Mexico; Departamento I.C.E., LP. N-E. S. I. M. E, Mexico, Mexico","Proceedings of 18th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","6 Aug 2002","1996","1","","405","406 vol.1","The developed system falls into the category of text to speech reading machines and can give to the blind autonomous text reading capacity. The read characters are obtained through an optic lector and a computer program based on pattern recognition by means of an artificial neural network. The system uses a central processing unit from a personal computer containing only a hard disk, a scanner unit, and a sound blaster audio card.","","0-7803-3811-1","10.1109/IEMBS.1996.657015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=657015","","Optical computing;Speech synthesis;Optical network units;Optical fiber networks;Computer networks;Pattern recognition;Artificial neural networks;Central Processing Unit;Microcomputers;Hard disks","handicapped aids;speech synthesis;character recognition equipment;backpropagation;character recognition;image scanners;audio-visual systems;neural nets","blind persons aid;text reading;translation to voice;text to speech reading machine;read characters;optic lector;computer program;pattern recognition;artificial neural network;central processing unit;personal computer;hard disk;scanner unit;sound blaster audio card;word recognition;printed characters recognition;backpropagation","","","1","3","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Application of Computer Intelligent Proofreading System in English Phrase Translation","J. Li","College of Humanities and Social Sciences, Heilongjiang Bayi Agricultural University, Daqing, Heilongjiang Province, China","2023 International Conference on Data Science and Network Security (ICDSNS)","22 Sep 2023","2023","","","1","5","The application of computer intelligent proofreading system in English phrase translation is of great significance. With the advancement of globalization and the increase of cross-border communication, the accurate translation of English phrases has become particularly crucial. However, due to the complexity and polysemy of language, traditional Machine translation systems still have some difficulties in dealing with English phrases. This article introduces the application of computer intelligent proofreading system in English phrase translation. Firstly, the traditional Machine translation system is summarized, and the possible problems in English phrase translation, such as semantic ambiguity, vocabulary selection and Syntax error, are pointed out. Next, the working principle and application methods of the computer intelligent proofreading system were introduced. Computer intelligent proofreading system can automatically detect and correct errors in Machine translation results by combining Natural language processing and machine learning technology. This system can be trained based on a large-scale corpus to learn translation rules and accuracy in various contextual contexts. Finally, through experimental simulation, it can be seen that the algorithm proposed in this article achieves both accuracy and accuracy of over 95%.","","979-8-3503-0159-5","10.1109/ICDSNS58469.2023.10245870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245870","computer;Intelligence;Proofreading system;English phrase translation","Vocabulary;Machine learning algorithms;Semantics;Globalization;Machine learning;Speech recognition;Manuals","","","","","","10","IEEE","22 Sep 2023","","","IEEE","IEEE Conferences"
"Multi-label Classification of Indonesian Hate Speech on Twitter Using Support Vector Machines","K. M. Hana; Adiwijaya; S. A. Faraby; A. Bramantoro","School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia","2020 International Conference on Data Science and Its Applications (ICoDSA)","6 Oct 2020","2020","","","1","7","Hate speech has become a hot issue as it spreads massively on today's social media with specific targets, categories, and levels. In addition, hate speech can cause social conflict and even genocide. This research proposes a system that classifies hate speech written in Indonesian language on Twitter. It also handles the noisiness of twitter data, such as mixed languages and non-standard text. We not only use Support Vector Machines (SVM) as a classifier, but also compare it with other methods, such as deep learning, CNN and DistilBERT. Apart from standard text preprocessing, we propose to accommodate the effect of translating in handling the multilingual content. The data transformation methods used in the SVM model are Label Power-set (LP) and Classifier Chains (CC). The experiment result shows that the classification using the SVM and CC without stemming, stopword removal, and translation provides the best accuracy of 74.88%. The best SVM hyperparameter on multilabel classification is the sigmoid kernel, the regularization parameter value of 10, and the gamma value of 0.1. Stemming, stopword removal, and translation preprocessing are less effective in this research. Moreover, CNN has a flaw in predicting labels for the training data with a low occurrence rate.","","978-1-7281-8235-3","10.1109/ICoDSA50139.2020.9212992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212992","classification;hate speech;social media;support vector machine","Support vector machines;Twitter;Feature extraction;Machine learning;Dictionaries;Forestry","computational linguistics;natural language processing;pattern classification;social networking (online);support vector machines;text analysis","data transformation;classifier chains;multilabel classification;support vector machines;Indonesian hate speech;social media;social conflict;Indonesian language;Twitter;nonstandard text;text preprocessing;label power-set;multilingual content;SVM hyperparameter;sigmoid kernel;regularization parameter","","8","","17","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"Integrated Language Translation IoT Devices : A Systematic Literature Review","A. Santo; A. Santos; H. S. Mamede","Universidade Aberta, Lisboa, Portugal; Universidade Aberta, Lisboa, Portugal; Universidade Aberta, Lisboa, Portugal","2022 17th Iberian Conference on Information Systems and Technologies (CISTI)","14 Jul 2022","2022","","","1","7","IoT devices that perform translation are often closed proprietary systems which offer no, or very limited, possibilities of being freely integrated with other IoT devices or systems/platforms and cannot be easily updated. The objective of this paper is to offer a Systematic Literature Review (SLR) approach to find and synthesise articles on IoT applications in machine translation and potential integration and interoperability functionalities, along with challenges that arise from it. This research identified IoT based projects/devices that implemented machine translation solutions, including ones that resorted to low-cost devices such as the raspberry pi, as well as challenges faced by IoT devices when it comes to integration/interoperability. The findings in this paper provide a way to become more familiar with IoT based machine translation devices/projects and IoT integration/interoperability challenges, as well as proposing new paths for research.","2166-0727","978-9-8933-3436-2","10.23919/CISTI54924.2022.9820059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820059","IoT;machine translation;integration;raspberry pi","Performance evaluation;Patents;Systematics;Social networking (online);Speech recognition;Production;Real-time systems","Internet of Things;language translation;open systems","low-cost devices;integrated language translation IoT devices;closed proprietary systems;systematic literature review;interoperability functionalities;IoT based machine translation","","","","25","","14 Jul 2022","","","IEEE","IEEE Conferences"
"AnHitz, Development and Integration of Language, Speech and Visual Technologies for Basque","K. Arrieta; I. Leturia; U. Iturraspe; A. D. de Ilarraza; K. Sarasola; I. Hern√°ez; E. Navas","VICOM Technology; Elhuyar Foundation; Robotiker, Spain; IXA Group, Basque Country University; IXA Group, Basque Country University, Spain; Aholab Group Basque Country University, Spain; Aholab Group Basque Country University, Spain","2008 Second International Symposium on Universal Communication","22 Dec 2008","2008","","","338","343","AnHitz is a project promoted by the Basque Government to develop language technologies for the Basque language. The participants in AnHitz are research groups with very different backgrounds: text processing, speech processing and multimedia. The project aims to further develop existing language, speech and visual technologies for Basque: up to now its fruit is a set of 7 different language resources, 9 NLP tools, and 5 applications.. But also, in the last year of this project we are integrating, for the first time, such resources and tools (both existing and generated in the project) into a content management application for Basque with a natural language communication interface.This application consists of a Question Answering and a Cross Lingual Information Retrieval system on the area of Science and Technology. The interaction between the system and the user will be in Basque (the results of the CLIR module that are not in Basque will be translated through Machine Translation) using Speech Synthesis, Automatic Speech Recognition and a Visual Interface.The various resources, technologies and tools that we are developing are already in a very advanced stage, and the implementation of the content management application to integrate them all is in work and is due to be completed by October 2008.","","978-0-7695-3433-6","10.1109/ISUC.2008.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724482","CLIR;QA;MT;ASR;TTS","Natural languages;Standardization;Morphology;Speech synthesis;Robots;Government;Speech processing;Content management;Technology planning;Dictionaries","content management;information retrieval;natural language processing;speech recognition;speech synthesis;visual languages","AnHitz;language technologies;Basque language;Basque Government;speech processing;text processing;multimedia;visual technologies;NLP tools;content management;natural language communication interface;question answering;cross lingual information retrieval system;speech synthesis;automatic speech recognition;visual interface","","2","","19","IEEE","22 Dec 2008","","","IEEE","IEEE Conferences"
"Paraphrase detection on SMS messages in automobiles","W. Wu; Y. -C. Ju; X. Li; Y. -Y. Wang","University of Washington, WA, USA; Microsoft Research Limited, WA, USA; Microsoft Research Limited, WA, USA; Microsoft Research Limited, WA, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","5326","5329","Voice search technology has been successfully applied to help drivers reply SMS messages in automobiles, in which a predefined SMS message template set is searched with ASR hypotheses to form the reply candidate list. In order to efficiently organize the SMS message template set and improve the quality of the reply candidate list, we proposed to apply n-gram translation model and logistic regression to detect paraphrase SMS messages. Both of the proposed algorithms outperform the edit distance based paraphrase detection baseline, brining 40.9% and 50.5% EER reduction (relative), respectively.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5494959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5494959","SMS message;paraphrase detection;n-gram;translation model;logistic regression","Automobiles;Automatic speech recognition;Logistics;Redundancy;Data mining;Acoustic signal detection;Degradation;Support vector machines;Design methodology;Feature extraction","automobiles;electronic messaging;natural language processing;speech recognition","paraphrase detection;SMS message;automobiles;voice search technology;n gram translation model;logistic regression","","1","","10","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Optimization Algorithms and Applications for Speech and Language Processing","S. J. Wright; D. Kanevsky; L. Deng; X. He; G. Heigold; H. Li","Department of Computer Sciences, University of Wisconsin, Madison, Madison, WI, USA; IBM Research, NY, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Google Research, Mountain View, CA, USA; Institute for Infocomm Research, Singapore","IEEE Transactions on Audio, Speech, and Language Processing","17 Oct 2013","2013","21","11","2231","2243","Optimization techniques have been used for many years in the formulation and solution of computational problems arising in speech and language processing. Such techniques are found in the Baum-Welch, extended Baum-Welch (EBW), Rprop, and GIS algorithms, for example. Additionally, the use of regularization terms has been seen in other applications of sparse optimization. This paper outlines a range of problems in which optimization formulations and algorithms play a role, giving some additional details on certain application problems in machine translation, speaker/language recognition, and automatic speech recognition. Several approaches developed in the speech and language processing communities are described in a way that makes them more recognizable as optimization procedures. Our survey is not exhaustive and is complemented by other papers in this volume.","1558-7924","","10.1109/TASL.2013.2283777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636122","Natural language processing;optimization methods;speech processing","Optimization;Pattern recognition;Speech processing;Large scale systems;Computational modeling;Mathematical model","optimisation;speech processing","optimization algorithms;language processing;speech processing;optimization techniques;extended Baum-Welch;EBW;GIS algorithms;Rprop;sparse optimization;machine translation;speaker-language recognition;automatic speech recognition","","16","","97","IEEE","17 Oct 2013","","","IEEE","IEEE Journals"
"MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation","P. Xie; Z. Li; Z. Zhao; J. Liu; X. Hu","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; China Academy of Launch Vehicle Technology, Beijing, China; China Academy of Launch Vehicle Technology, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of SciencesorgName, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2022","PP","99","1","10","Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by predicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model. Specifically, MvSR consists of two parts: (1) shared mask consistency: we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) model consistency, we maintain an exponential moving average of the model weights, and enforce the predictions to be consistent between the average model and the online model. Without changing the CMLM-based architecture, our approach achieves remarkable performance on three public benchmarks with 0.7-1.15 BLEU gains over previous NAT models. And, we reduce the gap to the stronger Transformer baseline.","2329-9304","","10.1109/TASLP.2022.3221043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944912","non-autoregressive Machine Translation;shared mask consistency;model consistency","Predictive models;Decoding;Training;Machine translation;Data models;Symbols;Robustness","","","","2","","","IEEE","10 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Hybrid sub-word segmentation for handling long tail in morphologically rich low resource languages","S. Manghat; S. Manghat; T. Schultz","IEEE; IEEE; Cognitive Systems Lab, University Bremen, Germany","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6122","6126","Dealing with Out Of Vocabulary (OOV) words or unseen words is one of the main issues of Machine Translation (MT) as well as automatic speech recognition (ASR) systems. For morphologically rich languages having high type token ratio, the OOV percentage is also quite high. Sub-word segmentation has been found to be one of the major approaches in dealing with OOVs. In this paper we present a hybrid sub-word segmentation algorithm to deal with OOVs. A sub-word segmentation evaluation methodology is also presented. We also present results of our segmentation approach in comparison to some of the popular sub-word segmentation algorithms. Malayalam is a morphological rich low resource Indic language with very high type token ratio. All the experiments are done for conversational code-switched Malayalam-English corpus.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746652","OOV;low resource languages;Malayalam;code-switching;language modelling;sub-word segmentation","Vocabulary;Conferences;Signal processing algorithms;Morphology;Tail;Signal processing;Phonetics","image segmentation;language translation;natural language processing;speech recognition;vocabulary","long tail;morphologically rich low resource languages;Vocabulary words;unseen words;Machine Translation;automatic speech recognition systems;morphologically rich languages;high type token ratio;OOV percentage;hybrid sub-word segmentation algorithm;sub-word segmentation evaluation methodology;segmentation approach;popular sub-word segmentation algorithms;morphological rich low resource Indic language","","1","","26","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Capturing Multi-Resolution Context by Dilated Self-Attention","N. Moritz; T. Hori; J. Le Roux","Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","5869","5873","Self-attention has become an important and widely used neural network component that helped to establish new state-of-the-art results for various applications, such as machine translation and automatic speech recognition (ASR). However, the computational complexity of self-attention grows quadratically with the input sequence length. This can be particularly problematic for applications such as ASR, where an input sequence generated from an utterance can be relatively long. In this work, we propose a combination of restricted self-attention and a dilation mechanism, which we refer to as dilated self-attention. The restricted self-attention allows attention to neighboring frames of the query at a high resolution, and the dilation mechanism summarizes distant information to allow attending to it with a lower resolution. Different methods for summarizing distant frames are studied, such as subsampling, mean-pooling, and attention-based pooling. ASR results demonstrate substantial improvements compared to restricted self-attention alone, achieving similar results compared to full-sequence based self-attention with a fraction of the computational costs.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9415001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415001","dilated self-attention;transformer;automatic speech recognition;computational complexity","Conferences;Neural networks;Signal processing;Acoustics;Computational efficiency;Machine translation;Speech processing","computational complexity;language translation;neural nets;speech recognition","dilated self-attention;restricted self-attention;dilation mechanism;attention-based pooling;ASR results;full-sequence based self-attention;multiresolution context;important used neural network component;widely used neural network component;automatic speech recognition;input sequence length","","4","","28","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A Discriminative Hierarchical PLDA-Based Model for Spoken Language Recognition","L. Ferrer; D. Castan; M. McLaren; A. Lawson","Instituto de Investigaci√≥n en Ciencias de la Computaci√≥n, CONICET-UBA, Buenos Aires, Argentina; SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA; SRI International, Menlo Park, CA, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Jul 2022","2022","30","","2396","2410","Spoken language recognition (SLR) refers to the automatic process used to determine the language present in a speech sample. SLR is an important task in its own right, for example, as a tool to analyze or categorize large amounts of multi-lingual data. Further, it is also an essential tool for selecting downstream applications in a work flow, for example, to chose appropriate speech recognition or machine translation models. SLR systems are usually composed of two stages, one where an embedding representing the audio sample is extracted and a second one which computes the final scores for each language. In this work, we approach the SLR task as a detection problem and implement the second stage as a probabilistic linear discriminant analysis (PLDA) model. We show that discriminative training of the PLDA parameters gives large gains with respect to the usual generative training. Further, we propose a novel hierarchical approach where two PLDA models are trained, one to generate scores for clusters of highly-related languages and a second one to generate scores conditional to each cluster. The final language detection scores are computed as a combination of these two sets of scores. The complete model is trained discriminatively to optimize a cross-entropy objective. We show that this hierarchical approach consistently outperforms the non-hierarchical one for detection of highly related languages, in many cases by large margins. We train our systems on a collection of datasets including over 100 languages, and test them both on matched and mismatched conditions, showing that the gains are robust to condition mismatch.","2329-9304","","10.1109/TASLP.2022.3190736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844653","Spoken language recognition;probabilistic linear discriminant analysis;discriminative training","Training;Computational modeling;Speech recognition;Detectors;Probabilistic logic;Linear discriminant analysis;Machine translation","entropy;feature extraction;language translation;natural language processing;speaker recognition;speech recognition;statistical analysis","discriminative hierarchical PLDA-based model;spoken language recognition;automatic process;speech sample;multilingual data;work flow;appropriate speech recognition;SLR systems;audio sample;final scores;SLR task;probabilistic linear discriminant analysis model;discriminative training;PLDA parameters;usual generative training;hierarchical approach;PLDA models;highly-related languages;final language detection scores;highly related languages","","1","","67","IEEE","28 Jul 2022","","","IEEE","IEEE Journals"
"OCR Based Image Text to Speech Conversion using K-Nearest Neighbors and Comparing with Fuzzy K-Means Clustering Algorithm","M. P. Babu; A. G","Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, Tamilnadu, India; Department of Electronics and Communication Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, Tamilnadu, India","2023 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)","4 Aug 2023","2023","","","1","5","Using a variety of various techniques, this research is to transform OCR-based visual text to spoken language. The image dataset was used for novel K-Nearest Neighbours (KNN) and fuzzy k-means clustering (FKM). The novel k-nearest neighbours (KNN) (N = 10) and the fuzzy k-means clustering (FKM) (N = 10) groups are compared to one another in this study. according to the results of a calculation made with the g power software, the total sample size was determined to be 80% powerful, with a confidence interval of 95%, an enrolment ratio of 0.1, and an alpha value of 0.05. The SPSS software as well as an independent sample t-test were used to conduct the comparison of the accuracy and specificity rates. The results of a MATLAB simulation indicate that fuzzy k-means clustering, also known as FKM, obtains an accuracy of 93 percent when applied to text to vocal translation. The statistical analysis done with SPSS managed to reach a significant level of accuracy (0.02). The newly proposed revolutionary k-nearest neighbours method performs better than the FKM method for voice converting using the provided pictures.","","979-8-3503-1590-5","10.1109/ACCAI58221.2023.10200864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200864","Image Text to Speech;Machine Learning;Optical Character Recognition;Novel K-Nearest Neighbours;Fuzzy K-Means Clustering;Accuracy","Image quality;Visualization;Image recognition;Text recognition;Statistical analysis;Optical character recognition;Speech recognition","nearest neighbour methods;optical character recognition;pattern clustering;speech synthesis;statistical analysis","FKM method;fuzzy K-means clustering algorithm;g power software;image dataset;image text to speech conversion;independent sample t-test;k-nearest neighbours method;KNN;Matlab simulation;OCR;SPSS software;statistical analysis;visual text to spoken language","","","","27","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"Automatic generation of non-uniform HMM structures based on variational Bayesian approach","T. Jitsuhiro; S. Nakamura","ATR Spoken Language Translation Research Laboratories, Kyoto, Japan; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","1","","I","805","We propose using the variational Bayesian (VB) approach for automatically creating nonuniform, context-dependent HMM topologies in speech recognition. The maximum likelihood (ML) criterion is generally used to create HMM topologies. However, it has an over-fitting problem. Information criteria have been used to overcome this problem, but theoretically they cannot be applied to complicated models like HMM. Recently, to avoid these problems, the VB approach has been developed in the machine-learning field. We introduce the VB approach to the successive state splitting (SSS) algorithm, which can create both contextual and temporal variations for HMM. We define the prior and posterior probability densities and free energy with latent variables as split and stop criteria. Experimental results show that the proposed method can automatically create a more efficient model and obtain better performance, especially for vowels, than the original method.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1326108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326108","","Hidden Markov models;Bayesian methods;Clustering algorithms;Maximum likelihood estimation;Topology;Speech recognition;Decision trees;Training data;Natural languages;Laboratories","hidden Markov models;Bayes methods;speech recognition;topology;variational techniques","automatic generation;nonuniform HMM structures;variational Bayesian approach;context-dependent HMM topologies;successive state splitting algorithm;SSS algorithm;vowel performance;speech recognition","","4","","9","IEEE","30 Aug 2004","","","IEEE","IEEE Conferences"
"Detection of Voice Pathologies and Evaluation of Pronunciation Based on Prosodic Features: Case of Arabic Discourse","N. Terbeh; M. Zrigui","Tunis 1008-Tunisia Faculty of sciences of Monastir, LaTICE Laboratory, Monastir, Tunisia; Tunis 1008-Tunisia Faculty of sciences of Monastir, LaTICE Laboratory, Monastir, Tunisia","2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)","13 Aug 2018","2018","","","335","339","Automatic speech processing is a growing area of research, taking advantage of the development and popularity of human-machine interaction applications. Automatic speech recognition is a famous application that allows translating spoken language into text. However, these systems often work in a less efficient way when the produced speech presents degradation. In this article, we address the issue of detecting vocal pathologies contained in Arabic discourse, based on prosodic parameters [12]; we use metrics related to duration describing the rhythm of pronunciation proper to concerned speakers. The obtained results are satisfactory. Indeed, the proposed detector of voice pathologies has attained a performance of 88.63%. Consequently, researchers in similar areas can benefit from our contribution to the development of their systems.","2161-377X","978-1-5386-6049-2","10.1109/ICALT.2018.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8433531","Arabic healthy speech;Arabic pathological speech;elocution evaluation;elocution speed","Pathology;Feature extraction;Measurement;Hidden Markov models;Rhythm;Sociology","language translation;natural language processing;speech recognition","Arabic discourse;voice pathologies;human-machine interaction applications;automatic speech recognition;pronunciation;spoken language translation","","","","16","IEEE","13 Aug 2018","","","IEEE","IEEE Conferences"
"Speech Recognition to Build Context: A Survey","S. Raju; V. Jagtap; P. Kulkarni; M. Ravikanth; M. Rafeeq","Department of Computer Science Engineering, CMR Technical Campus, Hyderabad, Telangana, India; Department of Computer Engineering and Information Technology, College of Engineering, Pune, Pune, Maharashtra, India; iKnowlation Research Labs Pvt. Ltd., Pune, Maharashtra, India; Department of Computer Science Engineering, CMR Technical Campus, Hyderabad, Telangana, India; Department of Computer Science Engineering, CMR Technical Campus, Hyderabad, Telangana, India","2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)","3 Jul 2020","2020","","","1","7","In era Computer evolution many problems can be solved using computer vision and signal processing. These domains are typically Digitized in binary files like Images, Audio, and Videos. The translation, recognition and synthesis are required while understating the meaning of the binary content. The recognition process is also having many problems in case of audio processing. The missing context is the major reason in pattern-based matching. This is due to unclear or low-quality input, as well as training model on different frequencies but by using context some of the accuracy may improve. Context finding from binary files is a challenge as it works in temporal and space domain. Binary data like images contain special information, while audio files contain temporal information. Video files have both time and space domains. Updating context in the temporal domain, to find proper context from the audio corpus, speech recognition is applied. Over the time period, there are different models adapted like Hidden Markov Model (HMM), Rule Based models with fuzzy support, pattern-based models including machine learning techniques K-nearest neighbor, Support Vector Machine, also latest techniques like Artificial Neural Network (ANN). These technologies are typically included in Automatic Speech Recognition (ASR). ASR uses Language resources with any one of the above models. Here, an in-depth survey on ASR and available APIs. Technologies used to build APIs also discussed.","","978-1-7281-5830-3","10.1109/ICCSEA49143.2020.9132848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132848","Automatic Speech Recognition (ASR);Language Resource;Machine Learning;Hidden Markov Model (HMM);Artificial Neural Network (ANN);Sequence Modeling;Context determination","Training;Support vector machines;Adaptation models;Computational modeling;Time series analysis;Hidden Markov models;Artificial neural networks","application program interfaces;computer vision;hidden Markov models;nearest neighbour methods;neural nets;speech recognition;support vector machines;video signal processing","artificial neural network;K-nearest neighbor;pattern-based matching models;automatic speech recognition;support vector machine;machine learning techniques;rule based models;hidden Markov model;audio corpus speech recognition;temporal domain;space domains;video files;temporal information;audio files;binary data;temporal space domain;low-quality input;audio processing;binary content;binary files;signal processing;computer vision","","","","27","IEEE","3 Jul 2020","","","IEEE","IEEE Conferences"
"Improved Spoken Uyghur Segmentation for Neural Machine Translation","C. Mi; Y. Yang; X. Zhou; L. Wang; T. Jiang","Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences, Urumqi, China; Xinjiang Technical Institute of Physics and Chemistry of Chinese Academy of Sciences, Urumqi, China","2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)","16 Dec 2018","2018","","","47","51","To increase vocabulary overlap in spoken Uyghur neural machine translation (NMT), we propose a novel method to enhance the common used subword units based segmentation method. In particular, we apply a log-linear model as the main framework and integrate several features such as subword, morphological information, bilingual word alignment and monolingual language model into it. Experimental results show that spoken Uyghur segmentation with our proposed method improves the performance of the spoken Uyghur-Chinese NMT significantly (yield up to 1.52 BLEU improvements).","2375-0197","978-1-5386-7449-9","10.1109/ICTAI.2018.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576016","spoken Uyghur segmentation;neural machine translation;BPE;morphologically-rich;low-resource language","Feature extraction;Vocabulary;Adaptation models;Task analysis;Training;Analytical models;Linguistics","language translation;natural language processing;neural nets;speech processing","spoken Uyghur segmentation;spoken Uyghur neural machine translation;log-linear model;monolingual language model;spoken Uyghur-Chinese NMT;subword units based segmentation method;bilingual word alignment","","","","22","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Quasi Character-Level Transformers to Improve Neural Machine Translation on Small Datasets","S. Carri√≥n; F. Casacuberta","PRHLT Research Center, Universitat Polit√®cnica de Val√®ncia; PRHLT Research Center, Universitat Polit√®cnica de Val√®ncia","2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)","16 Mar 2022","2021","","","1","6","In the Neural Machine Translation community, it is a common practice to use some form of subword segmentation to encode words as a sequence of subword units. This allows practitioners to represent their entire dataset using the least amount of tokens, thus avoiding memory and performance-related problems derived from the full wordor purely character-level representations. Even though there is strong evidence that each dataset has an optimal vocabulary size, in practice it is common to use as many ‚Äúwords‚Äù as possible. In this work, we show how this standard approach might be counter-productive for small datasets or low-resource environments, where models trained with quasi character-level vocabularies seem to con-sistently outperform models with large subword vocabularies. Nonetheless, these improvements come at the expense of requiring a neural architecture capable of dealing with long sequences and long-term dependencies.","","978-1-6654-9495-3","10.1109/SNAMS53716.2021.9732120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732120","Neural Machine Translation;Deep Learning;Transformer architecture;Tokenization;Byte-Pair Encoding;Character-level","Vocabulary;Analytical models;Social networking (online);Memory management;Transformers;Data models;Probability distribution","embedded systems;language translation;natural language processing;neural nets;speech recognition;text analysis;vocabulary;word processing","long sequences;low-resource environments;neural architecture;neural machine translation community;optimal vocabulary size;performance-related problems;quasicharacter-level transformers;quasicharacter-level vocabularies;subword segmentation;subword units;subword vocabularies;wordor purely character-level representations","","","","45","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"The AESOP continuous speech understanding system","J. Mariani","LIMSI-CNRS, Orsay, France","ICASSP '82. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1982","7","","1637","1640","The continuous speech understanding system AESOP that we are presently developing at LIMSI, has gone through several phases since the first preliminary version, AESOP0 in 1978. This version allowed for man-machine speech communication in real time, with unrestricted text-to-speech synthesis. About 80% of the sentences were correctly recognised, for a language having an average branching factor of 10. In the AESOP 1 system, sentence recognition is 90 %,(95% on words) for the same language. Apart from this improvement, the system has been designed to be used easily for artificial languages, by adding graceful interaction to define semantic grammar (including automatic grapheme-to-phoneme translation of the vocabulary) and talker adaptation. Simultaneously, we developed a method for diphone recognition of continuous speech, using a 1089-diphone dictionary. Although the results are presently mediocre on pure phoneme recognition, its use as the acoustic level of the SUS gave good results. All those systems use top-down, left-to-right strategy, with beam-search. A bottom-up approach has also been tested for future use, having a tree-structure vocabulary; this still yields the same results.","","","10.1109/ICASSP.1982.1171487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1171487","","Speech recognition;Vocabulary;Natural languages;Dictionaries;Decoding;Prototypes;Man machine systems;Oral communication;Speech synthesis;Automatic speech recognition","","","","","","28","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Performance analysis of SS based speech enhancement algorithms for ASR with Non-stationary Noisy Database-NOIZEUS","U. G. Patil; S. D. Shirbahadurkar","Electronics and Telecommunication Department, Rajarshi Shahu College of Engineering, Pune, India; Electronics and Telecommunication Dept., Zeal College of Engineering, Narhe, Pune, India","2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on","28 Feb 2019","2018","","","636","641","In Human computer interface the correct translation by computer and exact perception of human depends on the quality of speech input to the machine. Hence speech enhancement technique is very essential in any HCI technique. In this paper we present an algorithm for speech enhancement on the non-stationary noisy database (NOIZEUES). The speech enhancement technique, Spectral Subtraction technique falls under the category of methods based on short time spectral amplitude (or power) estimate. This algorithm is found to be very simple and computationally efficient. Various modifications are made in the spectral subtraction technique and therefore spectral over subtraction, improved spectral over subtraction, iterative spectral over subtraction and multiband spectral subtraction techniques are derived. In this paper basic performance of spectral subtraction, spectral over subtraction and improved spectral over subtraction technique and SS with cross spectral terms with non-stationary noisy database is discussed. It is observed that algorithm works effectively to reduce additive noise present in the noisy speech signal. Limitations of spectral subtraction such as dependence on VAD accuracy and musical noise are described in the proposed paper. All listed algorithms are executed on available database experimentation results of speech and enhanced speech are provided in result section of this paper. Also numerical results are displayed graphically.","","978-1-5386-1442-6","10.1109/I-SMAC.2018.8653675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653675","VAD;ASR;SOS;FFT;IFFT;HCI","Speech enhancement;Noise measurement;Music;Time-domain analysis;Subtraction techniques;Airports;Conferences","human computer interaction;signal denoising;speech enhancement","HCI technique;nonstationary noisy database-NOIZEUS;human computer interface;speech input;SS based speech enhancement algorithms;enhanced speech;noisy speech signal;cross spectral terms;iterative spectral;short time spectral amplitude;spectral subtraction technique;speech enhancement technique","","","","19","IEEE","28 Feb 2019","","","IEEE","IEEE Conferences"
"An Arabic WordNet enrichment approach using machine translation and external linguistic resources","C. Lachichi; C. Bendiaf; L. Berkani; A. Guessoum","Dep. of Computer Sc., USTHB University, Algiers, Algeria; Dep. of Computer Sc., USTHB University, Algiers, Algeria; Dep. of Computer Sc., USTHB University, Algiers, Algeria; Dep. of Computer Sc., USTHB University, Algiers, Algeria","2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)","7 Jun 2018","2018","","","1","6","The development of Arabic WordNet (AWN) has provided the Arabic Natural Language Processing (NLP) community with a lexical resource. However, due to the considerable lack of AWN Synsets compared to other WordNets, the use of this resource in Arabic NLP applications cannot be considered as an option yet. In this paper, we propose an automatic approach for AWN enrichment, based on three features: (1) the translation of Princeton WordNet (PWN) words and their direct Hypernyms; (2) an automatic validation of the hypernymy relation that is suggested as existing between words using Wikipedia articles; and (3) the integration of validated Synsets in AWN including their definitions, examples and synonyms that get extracted using dictionaries and other resources. The proposed approach has been implemented and a preliminary evaluation shows that the obtained results are promising.","","978-1-5386-4543-7","10.1109/ICNLSP.2018.8374385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374385","Arabic WordNet;princeton WordNet;AWN anrichment;WordNet concepts alignment;machine translation;wikipedia;ranking metric;dictionaries","Encyclopedias;Electronic publishing;Internet;Dictionaries;Semantics;Natural language processing","computational linguistics;language translation;natural language processing;text analysis","Arabic WordNet enrichment approach;machine translation;external linguistic resources;lexical resource;AWN Synsets;Arabic NLP applications;automatic approach;AWN enrichment;Princeton WordNet words;automatic validation;validated Synsets;Arabic natural language processing community;PWN words;hypernymy relation;Wikipedia articles","","","","10","IEEE","7 Jun 2018","","","IEEE","IEEE Conferences"
"An Automatic Post Editing With Efficient and Simple Data Generation Method","H. Moon; C. Park; J. Seo; S. Eo; H. Lim","Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Korea University, Seoul, Republic of Korea","IEEE Access","1 Mar 2022","2022","10","","21032","21040","Automatic post-editing (APE) research considers methods for correcting translation results inferred by machine translation systems. The training of APE models, generally require triplets including a source sentence ( $src$ ), machine translation sentence ( $mt$ ), and post-edited sentence ( $pe$ ). As considerable expert-level human labor is required in creating  $pe$ , APE researches have encountered difficulty in constructing suitable dataset for most of language pairs. This has led to the absence of APE data for most of language pairs, such as Korean-English, and imposed limitation to the sustainable researches of APE. Motivated by this problem, we propose a method that can generate APE triplets using only a parallel corpus without human labor. Our proposal comprises three noise generation techniques, including random, part of speech tagging (POS) based, and semantic level noises, and the effectiveness of these methods are verified by the results of quantitative and qualitative experiments on Korean-English APE tasks. As a result of our experiments, we find that POS based noise encourages the best APE performance. The proposed method is influential in that it can obviate expert human labor which was generally required in APE data construction, and enable the sustainable APE researches for the most language pairs where human-edited APE triplets are unavailable.","2169-3536","","10.1109/ACCESS.2022.3152001","Ministry of Science and ICT (MSIT), South Korea, under the Information Technology Research Center (ITRC) Support Program IITP-2018-0-01405 supervised by the Institute of Information and Communications Technology Planning and Evaluation (IITP); Korean Government (MSIT) under IITP Grant 2020-0-00368 (A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques); MSIT under the ICT Creative Consilience Program IITP-2021-2020-0-01819 supervised by the IITP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714400","Automatic post editing;neural machine translation;data generation;machine translation;post editing","Mathematical models;Machine translation;Semantics;Training;Task analysis;Tagging;Planning","natural language processing","language pairs;noise generation techniques;semantic level noises;Korean-English APE tasks;APE performance;APE data construction;sustainable APE researches;human-edited APE triplets;data generation method;machine translation sentence;postedited sentence;automatic post editing;random noise;part of speech tagging based noise;POS based noise;semantic level noise","","3","","35","CCBY","15 Feb 2022","","","IEEE","IEEE Journals"
"Efficient Classification for Neural Machines Interpretations based on Mathematical models","D. K. Sharma; B. Singh; R. Regin; R. Steffi; M. K. Chakravarthi","Department of Mathematics, Jaypee University of Engineering and Technology, MP, India; Amity University, Dubai; Department of information Technology, Adhiyamaan college of Engineering, Tamil Nadu, India; Department of Electronics and Communication Engineering, Vins Christian Women's College of Engineering, Tamil Nadu; School of Electronics Engineering, VIT-AP University, Amaravathi, India","2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)","3 Jun 2021","2021","1","","2015","2020","Developing successful embedded vision applications necessitates a detailed review of various algorithmic optimization trade-offs and a wide variety of hardware design options. This makes it difficult for developers to navigate the solution space and find design points with the best performance trade-offs. In neural machine translation, large Transformer frameworks have produced state-of-the-art results and have become the industry standard. We will clarify the mathematical mechanisms behind the paper's efficient interpretation for neural machine translations and look for the best combination of known techniques to improve interpretation speed without compromising recognition accuracy in this paper. We perform an empirical study that compares various approaches and shows that using a hybrid of decoder self-attention replacement with simplified recurrent units, a deep encoder and shallow decoder architecture, and multiple head attention reseeding can achieve up to higher accuracy. By replacing heavy functions with lighter ones and enhancing the autoencoder's layers structure, excellent results can be achieved in a harmonious mix of time series, network architecture, and probabilistic solutions.","2575-7288","978-1-6654-0521-8","10.1109/ICACCS51430.2021.9441718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9441718","Deep learning;Encoder;Natural language processing;autoregressive;Attention mechanism;Activation function;Cell weights;Neural Machine translation;Decoder","Technological innovation;Time series analysis;Computer architecture;Network architecture;Probabilistic logic;Decoding;Machine translation","computational complexity;computer vision;decoding;embedded systems;neural nets;object recognition;optimisation;program interpreters;recurrent neural nets;speech recognition","neural machines interpretations;mathematical models;successful embedded vision applications;algorithmic optimization trade-offs;hardware design options;solution space;design points;performance trade-offs;neural machine translation;Transformer frameworks;industry standard;mathematical mechanisms;efficient interpretation;interpretation speed;decoder self-attention replacement;multiple head attention reseeding","","19","","22","IEEE","3 Jun 2021","","","IEEE","IEEE Conferences"
"A Japanese preprocessor for syntactic and semantic parsing","T. Kitani; T. Mitamura","Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA","Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications","6 Aug 2002","1993","","","86","92","The authors describe a Japanese preprocessor which includes a morphological analyzer called MAJESTY, and a proper noun identification program. The original morphological analyzer was modified to disambiguate its output when multiple possibilities for segmentations and parts of speech are found. Ambiguous segments are packed locally in the output enabling a syntactic and semantic parser to perform efficiently. Then the proper noun identification program groups several segments constructing a proper noun to present a meaningful set of segments to the parser. Tested on financial news articles, the preprocessor successfully segmented text and tagged parts of speech with greater than 98% accuracy. Over 80% of company names and 90% of personal and place names have been identified.<>","","0-8186-3840-0","10.1109/CAIA.1993.366657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=366657","","Speech analysis;Natural languages;Morphology;Testing;Artificial intelligence;Natural language processing;Finance;Data analysis;Data mining;Performance analysis","natural languages;speech processing;grammars","syntactic parsing;ambiguous segmentation;semantic parsing;Japanese preprocessor;morphological analyzer;MAJESTY;proper noun identification program;financial news articles;tagged parts of speech;company names;place names","","1","1","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Efficient rule scoring for improved grapheme-based lexicons","W. Hartmann; L. Lamel; J. -L. Gauvain","Spoken Language Processing Group, LIMSI-CNRS, Orsay, France; Spoken Language Processing Group, LIMSI-CNRS, Orsay, France; Spoken Language Processing Group, LIMSI-CNRS, Orsay, France","2014 22nd European Signal Processing Conference (EUSIPCO)","13 Nov 2014","2014","","","1477","1481","For many languages, an expert-defined phonetic lexicon may not exist. One popular alternative is the use of a grapheme-based lexicon. However, there may be a significant difference between the orthography and the pronunciation of the language. In our previous work, we proposed a statistical machine translation based approach to improving grapheme-based pronunciations. Without knowledge of true target pronunciations, a phrase table was created where each individual rule improved the likelihood of the training data when applied. The approach improved recognition accuracy, but required significant computational cost. In this work, we propose an improvement that increases the speed of the process by more than 80 times without decreasing recognition accuracy.","2076-1465","978-0-9928-6261-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952535","automatic speech recognition;grapheme-based speech recognition;pronunciation learning","Hidden Markov models;Abstracts;Acoustics","language translation;speech recognition;statistical analysis","rule scoring;improved grapheme-based lexicons;expert-defined phonetic lexicon;orthography;statistical machine translation based approach;language pronunciation;grapheme based pronunciations;automatic speech recognition","","","","17","","13 Nov 2014","","","IEEE","IEEE Conferences"
"Active and unsupervised learning for spoken word acquisition through a multimodal interface","N. Iwahashi","ATR Spoken Language Translation Laboratories, Soraku-gun, Kyoto, Japan","RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)","10 Jan 2005","2004","","","437","442","This work presents a new interactive learning method for spoken word acquisition through human-machine multimodal interfaces. During the course of learning, a machine makes a decision about whether an orally input word is a word in the lexicon the machine has learned using both speech and visual cues. Learning is carried out on-line, incrementally, based on a combination of active and unsupervised learning principles. If the machine judges with a high degree of confidence that its decision is correct, it learns the statistical models of the word and a corresponding image class as its meaning in an unsupervised way. Otherwise, it asks the user a question in an active way. The function used to estimate the degree of confidence is also learned adaptively on-line. Experimental results show that the method enables a machine and a user to adapt to each other, which makes the learning process more efficient.","","0-7803-8570-5","10.1109/ROMAN.2004.1374800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1374800","","Unsupervised learning;Natural languages;Machine learning;Speech;Learning systems;Man machine systems;Ubiquitous computing;Humans;Robot sensing systems;Intelligent robots","speech-based user interfaces;speech processing;unsupervised learning;decision making;statistical analysis","unsupervised learning;spoken word acquisition;human-machine multimodal interfaces;interactive learning;decision making;adaptive online learning;statistical word models","","12","","14","IEEE","10 Jan 2005","","","IEEE","IEEE Conferences"
"Spoken Language Identification Based on I-vectors and Conditional Random Fields","P. Heracleous; Y. Mohammad; K. Takai; K. Yasuda; A. Yoneyama","KDDI Research, Inc., 2-1-15 Ohara, Fujimino-shi, Saitama, Japan; Artificial Intelligence Research Center, AIST; KDDI Research, Inc., 2-1-15 Ohara, Fujimino-shi, Saitama, Japan; Artificial Intelligence Research Center, AIST; Artificial Intelligence Research Center, AIST","2018 14th International Wireless Communications & Mobile Computing Conference (IWCMC)","30 Aug 2018","2018","","","1443","1447","The task of an automatic language identification (LID) system is to automatically identify the language in a spoken utterance. Language identification can be applied as front-end to speech-to-speech translation systems, in speaker diarization, and at call centers to automatically route incoming calls to appropriate native speaker operators. In the current study, a method for automatic language identification based on i-vector paradigm and conditional random fields (CRF) is presented. CRF belong to discriminative classifiers and use an exponential distribution to model a sequence given the observation sequence. This allows non-independent observations, and allows also non-local dependencies between state and observation. When the proposed method is evaluated on the NIST 2015 i-vector Machine Learning Challenge task for the recognition of 50 in-set languages, a 3.7% equal error rate (EER) (i.e., miss probability equal to false alarms) is achieved. Using support vector machines (SVM) a 5.2% EER, using probabilistic linear discriminant analysis (PLDA) a 6.7% EER, and when using convolutional neural networks (CNN) a 4.2% EER are achieved.","2376-6506","978-1-5386-2070-0","10.1109/IWCMC.2018.8450327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8450327","","Support vector machines;Hidden Markov models;Training;Task analysis;Machine learning;NIST;Training data","feedforward neural nets;language translation;learning (artificial intelligence);natural language processing;probability;speaker recognition;speech recognition;support vector machines","equal error rate;EER;NIST 2015 i-vector Machine Learning Challenge task;SVM;probabilistic linear discriminant analysis;PLDA;convolutional neural networks;CNN;support vector machines;in-set languages;nonlocal dependencies;nonindependent observations;observation sequence;CRF;i-vector paradigm;appropriate native speaker operators;call centers;speaker diarization;speech-to-speech translation systems;spoken utterance;automatic language identification system;conditional random fields;spoken language identification","","4","","27","IEEE","30 Aug 2018","","","IEEE","IEEE Conferences"
"Japanese named entity recognition for question answering system","Y. Liu; F. Ren","Graduate School of Advanced Technology and Science, University of Tokushima, Tokushima, Japan; AIM, Hefei University of technology, China","2011 IEEE International Conference on Cloud Computing and Intelligence Systems","13 Oct 2011","2011","","","402","406","Current question answering (QA) systems usually contain named entity recognizer (NER) as a core component. NER is an important and difficult task in computational linguistics. It plays an important role in natural language processing application such as Question Answering, Machine Translation, and Information Retrieval etc. NER includes the identification and classification of certain proper nouns (like location, organization, person, data, money and others) in a text. The purpose of our study is to recognize and extract the exact Japanese sight seeing domain named entities. It is a basic step for the following processing: question analysis and keyword extraction information retrieval. As well as, through doing the named entity recognition, we consider that it can mine exact information from text document to respond to user. This paper describes how to do the Japanese sightseeing named entity recognition due to we are constructing a Japanese sightseeing question answering system. We adopt the hybrid method which combined with machine learning and rule-base method. In the experiment of Japanese sightseeing domain named entity recognition we have got excellent precision and recalling rates. It shows that our method is effective and can be used in a practical question answering system.","2376-595X","978-1-61284-204-2","10.1109/CCIS.2011.6045098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045098","question answering system;named entity recognition;machine learning;rule-base","Organizations;Educational institutions;Training;Text recognition;Machine learning;Support vector machines;Data mining","information retrieval;language translation;natural language processing;speech recognition","Japanese named entity recognition;question answering system;QA;NER;computational linguistics;natural language processing;machine translation;keyword extraction information retrieval;text document","","3","","15","IEEE","13 Oct 2011","","","IEEE","IEEE Conferences"
"A syntactic language model based on incremental CCG parsing","H. Hassan; K. Sima'an; A. Way","School of Computing, University of Dublin, Dublin, Ireland; Language and Computations Universiteit van Amsterdam, Amsterdam, Netherlands; School of Computing, University of Dublin, Dublin, Ireland","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","205","208","Syntactically-enriched language models (parsers) constitute a promising component in applications such as machine translation and speech-recognition. To maintain a useful level of accuracy, existing parsers are non-incremental and must span a combinatorially growing space of possible structures as every input word is processed. This prohibits their incorporation into standard linear-time decoders. In this paper, we present an incremental, linear-time dependency parser based on Combinatory Categorial Grammar (CCG) and classification techniques. We devise a deterministic transform of CCG-bank canonical derivations into incremental ones, and train our parser on this data. We discover that a cascaded, incremental version provides an appealing balance between efficiency and accuracy.","","978-1-4244-3471-8","10.1109/SLT.2008.4777876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777876","Natural languages;Language Modeling;Grammar","Natural languages;Context modeling;Decoding;Speech recognition;Assembly;Probability;Large-scale systems;Delay;Usability;Tin","language translation;speech recognition","syntactic language model;incremental CCG parsing;machine translation;speech-recognition;parsers;standard linear-time decoders;Combinatory Categorial Grammar;CCG-bank canonical derivations","","3","","10","IEEE","6 Feb 2009","","","IEEE","IEEE Conferences"
"Improving Minimal Gated Unit for Sequential Data","K. Takamura; S. Yamane","Kanazawa University, Kakuma-machi, Kanazawa, Japan; Kanazawa University, Kakuma-machi, Kanazawa, Japan","2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)","27 Feb 2020","2019","","","696","698","In order to obtain a model which can process sequential data related to machine translation and speech recognition faster and more accurately, we propose adopting Chrono Initializer as the initialization method of Minimal Gated Unit. We evaluated the method with two tasks: adding task and copy task. As a result of the experiment, the effectiveness of the proposed method was confirmed.","2378-8143","978-1-7281-3575-5","10.1109/GCCE46687.2019.9015461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9015461","","Task analysis;Logic gates;Recurrent neural networks;Mathematical model;Data models;Speech recognition;Entropy","data handling;language translation;speech recognition","Minimal Gated Unit;sequential data;machine translation;speech recognition;Chrono Initializer;initialization method;adding task;copy task","","","","5","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Self-Attention With Restricted Time Context And Resolution In Dnn Speech Enhancement","M. Strake; A. Behlke; T. Fingscheidt","Institute for Communications Technology, Technische Universit√§t Braunschweig, Braunschweig, Germany; Institute for Communications Technology, Technische Universit√§t Braunschweig, Braunschweig, Germany; Institute for Communications Technology, Technische Universit√§t Braunschweig, Braunschweig, Germany","2022 International Workshop on Acoustic Signal Enhancement (IWAENC)","17 Oct 2022","2022","","","1","5","The multi-head attention mechanism, which has been successfully applied in, e.g., machine translation and ASR, was also found to be a promising approach for temporal modeling in speech enhancement DNNs. Since speech enhancement can be expected to take less profit from long-term temporal context than machine translation or ASR, we propose to employ self-attention with modified context access. We first show that a restriction of the employed temporal context in the self-attention layers of a CNN-based network architecture is crucial for good speech enhancement performance. Furthermore, we propose to combine restricted attention with a subsampled attention variant that considers long-term context with a lower temporal resolution, which helps to effectively consider both long- and short-term context. We show that our proposed attention-based network outperforms similar networks using RNNs for temporal modeling as well as a strong reference method using unrestricted attention.","","978-1-6654-6867-1","10.1109/IWAENC53105.2022.9914702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914702","Speech enhancement;attention;fully convolutional networks;temporal modeling","Convolution;Conferences;Speech enhancement;Network architecture;Data models;Acoustics;Machine translation","convolutional neural nets;deep learning (artificial intelligence);signal resolution;speech enhancement","speech enhancement;multihead attention mechanism;DNNs;long-term temporal context;self-attention layers;CNN-based network architecture;subsampled attention variant;attention-based network;temporal modeling;temporal resolution","","1","","30","IEEE","17 Oct 2022","","","IEEE","IEEE Conferences"
"A multilingual chat system with image presentation for detecting mistranslation","E. Hosogai; T. Mukai; S. Jung; Y. Kowase; A. Bossard; Y. Xu; M. Ishikawa; K. Kaneko","Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Business Administration, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan; Department Computer and Information Sciences, Tokyo University of AgricultureÏä†andÏä†Technology, Koganei, Tokyo, Japan","Proceedings of the ITI 2011, 33rd International Conference on Information Technology Interfaces","4 Aug 2011","2011","","","287","292","We have designed and developed a multilingual chat system, MCHI (Multilingual Chat with Hint Images), which is based on machine translation and equipped with a presentation function of images related to the contents of the messages by utterers so that listeners are able to notice the mistranslation. MCHI accepts English, French, Chinese, Japanese, and Korean languages. It uses Google API to retrieve related images from the image posting site Flickr. As a result of evaluation experiment, we have observed that participants detected the mismatch of a translated message with its related image. According to the answers of participants for a question in a questionnaire, it turned out that the usability of the MCHI system is good enough though the related images are not satisfactory.","1330-1012","978-1-61284-897-6","10.2498/cit.1002028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974037","machine translation;image retrieval;keywords;morphological analysis","Google;Image retrieval;Collaborative work;Time factors;Servers;Speech;Agriculture","application program interfaces;image retrieval;language translation;natural languages","image presentation;mistranslation detection;multilingual chat with hint images;MCHI;machine translation;English language;French language;Chinese language;Japanese language;Korean language;Google API;Flickr","","1","","8","","4 Aug 2011","","","IEEE","IEEE Conferences"
"General labelled data generator framework for network machine learning","K. Kim; Y. -G. Hong; Y. -H. Han","ETRI, Daejeon, Korea; ETRI, Daejeon, Korea; KOREATECH, Cheonan-si, Chungcheongnam-do, Korea","2018 20th International Conference on Advanced Communication Technology (ICACT)","26 Mar 2018","2018","","","127","131","Artificial Intelligence (AI) technology has made remarkable achievements in various fields. Especially, deep learning technology that is the representative technology of AI, showed high accuracy in speech recognition, image recognition, pattern recognition, natural language processing and translation. In addition, there are many interesting research results such as art, literature and music that cannot be distinguished whether it was made by human or AI. In the field of networks, attempts to solve problems that have not been able to be solved or complex problems using AI have started to become a global trend. However, there is a lack of data sets to apply machine learning to the network and it is difficult to know network problem to solve. So far, there have been a lot of efforts to study network machine learning, but there are few studies to make a necessary dataset. In this paper, we introduce basic network machine learning technology and propose a method to easily generate data for network machine learning. Based on the data generation framework proposed in this paper, the results of automatic generation of labelled data and the results of learning and inferencing from the corresponding dataset are also provided.","","979-11-88428-01-4","10.23919/ICACT.2018.8323670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323670","machine learning;data generator;deep learning;network machine learning;supervised learning","Machine learning algorithms;Payloads;Databases;Protocols;Generators;Supervised learning","data handling;learning (artificial intelligence)","complex problems;music;literature;translation;natural language processing;pattern recognition;image recognition;speech recognition;deep learning technology;AI;Artificial Intelligence technology;general labelled data;data generation framework;basic network machine;network machine learning;data sets","","1","","11","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Acoustic unit discovery and pronunciation generation from a grapheme-based lexicon","W. Hartmann; A. Roy; L. Lamel; J. -L. Gauvain","Spoken Language Processing Group, CNRS, Orsay, France; Spoken Language Processing Group, CNRS, Orsay, France; Spoken Language Processing Group, CNRS, Orsay, France; Spoken Language Processing Group, CNRS, Orsay, France","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","9 Jan 2014","2013","","","380","385","We present a framework for discovering acoustic units and generating an associated pronunciation lexicon from an initial grapheme-based recognition system. Our approach consists of two distinct contributions. First, context-dependent grapheme models are clustered using a spectral clustering approach to create a set of phone-like acoustic units. Next, we transform the pronunciation lexicon using a statistical machine translation-based approach. Pronunciation hypotheses generated from a decoding of the training set are used to create a phrase-based translation table. We propose a novel method for scoring the phrase-based rules that significantly improves the output of the transformation process. Results on an English language dataset demonstrate the combined methods provide a 13% relative reduction in word error rate compared to a baseline grapheme-based system. Our approach could potentially be applied to low-resource languages without existing lexicons, such as in the Babel project.","","978-1-4799-2756-2","10.1109/ASRU.2013.6707760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707760","acoustic unit discovery;automatic speech recognition;grapheme-based speech recognition;pronunciation learning","Hidden Markov models;Acoustics;Context modeling;Dictionaries;Training;Computational modeling;Training data","acoustic signal processing;language translation;natural language processing;pattern clustering;spectral analysis;speech recognition;statistical analysis","phone-like acoustic unit discovery;pronunciation lexicon generation;grapheme-based lexicon;grapheme-based speech recognition system;context-dependent grapheme models;spectral clustering approach;statistical machine translation-based approach;pronunciation hypothesis generation;training set decoding;phrase-based translation table;phrase-based rule scoring;transformation process output improvement;English language dataset;relative reduction;word error rate;low-resource languages;Babel project","","6","","18","IEEE","9 Jan 2014","","","IEEE","IEEE Conferences"
"Syntactic neural networks for text-phonetics translation","S. Lucas; B. Damper","Department of Electronics and Computer Science, University of Southampton, Southampton, UK; Department of Electronics and Computer Science, University of Southampton, Southampton, UK","[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1991","","","509","512 vol.1","It is shown how syntactic neural networks can be applied to the problem of translating orthographic strings to phonetics strings, and vice versa, due to the symmetry of the model. This is unusual in text-phonetics translation systems; most systems have to be trained to operate in a single direction. Another novel feature is the lack of supervision required during training. The only requirement is that one have whole-word orthographic/phonetic symbol-string pairs. To test the system the authors have formed a lexicon of (6000, single-syllable) such pairs in English by extracting the relevant information from the machine-readable Oxford Advanced Learner's Dictionary. Results are presented for cases where the training set varies between 10 and 2000 words. In each case, the trained nets are tested on the training set and an equal-size (disjoint) test set. Present results are poor compared to conventional translation systems, but most of the errors may be due to the system's immaturity.<>","1520-6149","0-7803-0003-3","10.1109/ICASSP.1991.150388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=150388","","Neural networks;Probability;Statistics;Shock absorbers;Damping;Computer science;System testing;Data mining;Writing;Multilayer perceptrons","neural nets;speech synthesis","machine readable dictionary;system testing;text-phonetics translation;syntactic neural networks;orthographic strings;phonetics strings;lexicon;English;Oxford Advanced Learner's Dictionary;training set","","","2","9","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"An approach to automatic acquisition of translation templates based on phrase structure extraction and alignment","Rile Hu; Chengqing Zong; Bo Xu","Nokia China Research Center, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy and Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy and Sciences, Beijing, China","IEEE Transactions on Audio, Speech, and Language Processing","21 Aug 2006","2006","14","5","1656","1663","In this paper, we propose a new approach for automatically acquiring translation templates from unannotated bilingual spoken language corpora. Two basic algorithms are adopted: a grammar induction algorithm, and an alignment algorithm using bracketing transduction grammar. The approach is unsupervised, statistical, and data-driven, and employs no parsing procedure. The acquisition procedure consists of two steps. First, semantic groups and phrase structure groups are extracted from both the source language and the target language. Second, an alignment algorithm based on bracketing transduction grammar aligns the phrase structure groups. The aligned phrase structure groups are post-processed, yielding translation templates. Preliminary experimental results show that the algorithm is effective.","1558-7924","","10.1109/TSA.2005.858521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1677985","Bilingual grammar induction;machine translation;structure alignment;translation template acquisition","Laboratories;Pattern recognition;Automation;Associate members;Natural languages;Knowledge acquisition;Natural language processing;Veins;Tagging;Statistical analysis","linguistics;grammars;natural languages;language translation","translation templates acquisition;automatic acquisition;phrase structure extraction;phrase structure alignment;unannotated bilingual spoken language corpora;grammar induction algorithm;bracketing transduction grammar;semantic groups;phrase structure groups;source language;target language","","6","","25","IEEE","21 Aug 2006","","","IEEE","IEEE Journals"
"When (Shared) Space and Time Don‚Äôt Matter. Remote Video-Mediated (Synchronous and Asynchronous) Communication in Flemish Sign Language.","L. Soetemans; M. Vermeerbergen","Faculty of Arts, KU Leuven, Belgium; Faculty of Arts, KU Leuven, Belgium","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","4","This paper addresses the potential impact of video-mediated and video-recorded communication on the sign language production of Flemish signers. We present preliminary results of a comparison of face-to-face communication in Flemish Sign Language with real-time online communication and with video-recorded messages intended for later viewing. We pay particular attention to the use of the signing space, simultaneity and referent tracking mechanisms. A better understanding of this effect is needed for the design and development of sign language recognition, which is fundamental for machine translation of these languages and the development of avatar technology.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193316","Flemish sign language;video-mediated communication;remote communication;sign language machine translation","Space technology;Conferences;Gesture recognition;Speech recognition;Production;Assistive technologies;Signal processing","avatars;handicapped aids;language translation;sign language recognition","avatar technology;face-to-face communication;Flemish sign language;Flemish signers;real-time online communication;remote video-mediated;sign language production;sign language recognition;signing space;video-recorded communication;video-recorded messages","","","","16","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Comparing Stochastic Approaches to Spoken Language Understanding in Multiple Languages","S. Hahn; M. Dinarelli; C. Raymond; F. Lefevre; P. Lehnen; R. De Mori; A. Moschitti; H. Ney; G. Riccardi","Computer Science Department, RWTH Aachen University, Aachen, Germany; LIMSI-CNRS, Orsay, France; LIA-University of Avignon, Avignon, France; LIA-University of Avignon, Avignon, France; Computer Science Department, RWTH Aachen University, Aachen, Germany; Department of Computer Science, McGill University, Montreal, QUE, Canada; Department of Computer Science, University of Trento, Povo di Trento, Italy; Computer Science Department, RWTH Aachen University, Aachen, Germany; Department of Computer Science, University of Trento, Povo di Trento, Italy","IEEE Transactions on Audio, Speech, and Language Processing","23 May 2011","2011","19","6","1569","1583","One of the first steps in building a spoken language understanding (SLU) module for dialogue systems is the extraction of flat concepts out of a given word sequence, usually provided by an automatic speech recognition (ASR) system. In this paper, six different modeling approaches are investigated to tackle the task of concept tagging. These methods include classical, well-known generative and discriminative methods like Finite State Transducers (FSTs), Statistical Machine Translation (SMT), Maximum Entropy Markov Models (MEMMs), or Support Vector Machines (SVMs) as well as techniques recently applied to natural language processing such as Conditional Random Fields (CRFs) or Dynamic Bayesian Networks (DBNs). Following a detailed description of the models, experimental and comparative results are presented on three corpora in different languages and with different complexity. The French MEDIA corpus has already been exploited during an evaluation campaign and so a direct comparison with existing benchmarks is possible. Recently collected Italian and Polish corpora are used to test the robustness and portability of the modeling approaches. For all tasks, manual transcriptions as well as ASR inputs are considered. Additionally to single systems, methods for system combination are investigated. The best performing model on all tasks is based on conditional random fields. On the MEDIA evaluation corpus, a concept error rate of 12.6% could be achieved. Here, additionally to attribute names, attribute values have been extracted using a combination of a rule-based and a statistical approach. Applying system combination using weighted ROVER with all six systems, the concept error rate (CER) drops to 12.0%.","1558-7924","","10.1109/TASL.2010.2093520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5639034","Generative and discriminative models;spoken dialogue systems;system combination","Hidden Markov models;Semantics;Media;Speech recognition;Speech processing;Speech;Support vector machines","interactive systems;Markov processes;maximum entropy methods;natural language processing;speech recognition","stochastic approach;multiple language;spoken language understanding module;dialogue system;word sequence;automatic speech recognition system;concept tagging;generative method;discriminative method;finite state transducer;statistical machine translation;maximum entropy Markov model;support vector machine;natural language processing;conditional random field;dynamic Bayesian network;French MEDIA evaluation corpus;portability;concept error rate","","34","4","49","IEEE","18 Nov 2010","","","IEEE","IEEE Journals"
"A Hybrid Approach for Part-of-Speech Tagging of Burmese Texts","C. Myint","University of Computer Studies, Mandalay, Myanmar","2011 International Conference on Computer and Management (CAMAN)","27 May 2011","2011","","","1","4","In Myanmar to English language translation system, in order to provide meaningful sentence from one language to another is non-trivial task. POS tagging is used as an early stage of linguistic text analysis in many applications. POS tagging is a process of assigning correct syntactic categories to each word. Tagsets and word disambiguation rules are fundamental parts of any POS tagger. This paper presents a new approach for POS tagging of Myanmar Language. Firstly, Users input a simple Myanmar sentence and then this sentence is segmented into words by using segmentation rules. These words are assigned to appropriate syntactic categories of Myanmar language by using rule based and probabilistic approach. This system applied CRF method for tagging POS ambiguities on words. CRF is a framework for building discriminative probabilistic models for segmenting and labeling sequential data. The tagsets for Myanmar POS, segmentation rule, tagging algorithm and CRF method are designed. The proposed approach is used UCSM Lexicon. So, this hybrid approach for POS tagging can give the optimal accuracy and robustness of machine translation system.","","978-1-4244-9283-1","10.1109/CAMAN.2011.5778890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5778890","","Tagging;Speech;Accuracy;Probabilistic logic;Syntactics;Grammar;Training data","knowledge based systems;language translation;probability;text analysis","part-of-speech tagging;Burmese texts;English;language translation system;linguistic text analysis;syntactic categories;Myanmar language;Myanmar sentence;segmentation rules;rule based approach;probabilistic approach;CRF method;UCSM Lexicon","","4","","8","IEEE","27 May 2011","","","IEEE","IEEE Conferences"
"Constructing online audio dictionaries for bilingual Mandarin-Taiwan dialects based on Web 2.0 concept","N. -H. Pan; F. -L. Huang; C. -H. Ho; X. -W. Lin; S. -H. Shiu","Department of Information Management, Chienkuo Technology University, Changhua, Taiwan; Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan; Department of Computer Science and Engineering, National Chung Hsing University, Taichung, Taiwan; Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan; Department of Computer Science and Engineering, National Chung Hsing University, Taichung, Taiwan","2010 7th International Symposium on Chinese Spoken Language Processing","10 Jan 2011","2010","","","446","450","In recent years, due to the popularity of Internet, more and more people use online dictionaries instead of the traditional dictionaries. However, the functions of the existing online Mandarin-Taiwan dialects dictionaries are not complete enough. We developed Mandarin-Taiwan dialects (MinNan and Hakka) online audio dictionaries with speech synthesis technology. The Mandarin-MinNan online dictionary is constructed based on the concept of Web 2.0. And it has 8 major functions. They are phonetic symbols providing, online pronunciation, semantic interpretation, example sentence providing, machine translation, unknown word processing, expanding mechanism providing and test. The purpose of expanding mechanism providing is to increase the number of words in the Mandarin- Taiwan dialects dictionary of our system. Currently the number of words in the bilingual dictionary is still not enough. We hope to solve this problem based on the concept of web 2.0.","","978-1-4244-6246-9","10.1109/ISCSLP.2010.5684484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5684484","Online audio dictionary;Speech synthesis;Web 2.0;e-learnin;Taiwan dialect","Dictionaries;Text processing;Speech synthesis;Speech;Internet;Semantics;Web pages","computational linguistics;dictionaries;Internet;speech synthesis","online audio dictionaries construction;bilingual Mandarin-Taiwan dialects;Web 2.0 concept;Internet;traditional dictionaries;speech synthesis technology;semantic interpretation;online pronunciation;example sentence providing;machine translation;unknown word processing;expanding mechanism","","","","9","IEEE","10 Jan 2011","","","IEEE","IEEE Conferences"
"Robot supporting for deaf and less hearing people","N. T. Thinh; T. P. Tho; T. T. T. Nga","Department of Mechatronics, HCMC University of Technology and Education, Ho Chi Minh City, Viet Nam; Department of Mechatronics, HCMC University of Technology and Education, Ho Chi Minh City, Viet Nam; Thu Dau Mot University, Binh Duong province, Viet Nam","2017 17th International Conference on Control, Automation and Systems (ICCAS)","14 Dec 2017","2017","","","889","892","This paper discusses the development of a service robot for translating spoken language text into signed languages and vice versa. The motivation for our study is the improvement of accessibility to public information announcements for deaf and less hearing people. The robot can translate Vietnamese sign language into speech and recognize Vietnamese/English speech to suitable gesture/sign language. The paper describes the use of service robot in a sign language machine translation system. Several sign language visualization methods were evaluated on the robot. In order to perform this study a machine translation service robot that uses display screen on robot as service-delivery device was developed as well as a 3D avatar. It was concluded that service robot are suitable service-delivery platforms for sign language machine translation systems.","","978-89-93215-14-4","10.23919/ICCAS.2017.8204351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8204351","Service robot;assistance robot;hand gesture;deaf;less-hearing people","Gesture recognition;Assistive technology;Auditory system;Speech;Mobile robots;Tools","avatars;gesture recognition;handicapped aids;language translation;natural language processing","machine translation service robot;sign language visualization methods;suitable gesture/sign language;Vietnamese/English speech;Vietnamese sign language;public information announcements;signed languages;spoken language text;deaf hearing people;sign language machine translation system;service-delivery device","","1","","4","","14 Dec 2017","","","IEEE","IEEE Conferences"
"Systematic Literature Review of Dialectal Arabic: Identification and Detection","A. Elnagar; S. M. Yagi; A. B. Nassif; I. Shahin; S. A. Salloum","Department of Computer Science, University of Sharjah, Sharjah, United Arab Emirates; Department of Foreign Language, University of Sharjah, Sharjah, United Arab Emirates; Department of Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates; Department of Electrical Engineering, University of Sharjah, Sharjah, United Arab Emirates; Machine Learning and NLP Research Group, University of Sharjah, Sharjah, United Arab Emirates","IEEE Access","24 Feb 2021","2021","9","","31010","31042","It is becoming increasingly difficult to know who is working on what and how in computational studies of Dialectal Arabic. This study comes to chart the field by conducting a systematic literature review that is intended to give insight into the most and least popular research areas, dialects, machine learning approaches, neural network input features, data types, datasets, system evaluation criteria, publication venues, and publication trends. It is a review that is guided by the norms of systematic reviews. It has taken account of all the research that adopted a computational approach to dialectal Arabic identification and detection and that was published between 2000 and 2020. It collected, analyzed, and collated this research, discovered its trends, and identified research gaps. It revealed, inter alia, that our research effort has not been directed evenly between speech and text or between the vernaculars; there is some bias favoring text over speech, regional varieties over individual vernaculars, and Egyptian over all other vernaculars. Furthermore, there is a clear preference for shallow machine learning approaches, for the use of n-grams, TF-IDF, and MFCC as neural network features, and for accuracy as a statistical measure of validation of results. This paper also pointed to some glaring gaps in the research: (1) total neglect of Mauritanian and Bahraini in the continuous Arabic language area and of such enclave varieties as Anatolian Arabic, Khuzistan Arabic, Khurasan Arabic, Uzbekistan Arabic, the Subsaharan Arabic of Nigeria and Chad, Djibouti Arabic, Cypriot Arabic and Maltese; (2) scarcity of city dialect resources; (3) rarity of linguistic investigations that would complement our research; (4) and paucity of deep machine learning experimentation.","2169-3536","","10.1109/ACCESS.2021.3059504","University of Sharjah, Sharjah, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354635","Arabic dialects;Arabic natural language processing;dialect identification;modern standard Arabic;systematic review","Standards;Systematics;Speech recognition;Machine learning;Machine translation;Tools;Social networking (online)","learning (artificial intelligence);natural language processing;neural nets","bias favoring text;shallow machine learning approaches;neural network features;continuous Arabic language area;Anatolian Arabic;Khuzistan Arabic;Khurasan Arabic;Uzbekistan Arabic;Subsaharan Arabic;Djibouti Arabic;Cypriot Arabic;city dialect resources;deep machine learning experimentation;systematic literature review;neural network input features;data types;system evaluation criteria;publication venues;publication trends;systematic reviews;computational approach;dialectal Arabic identification","","10","","220","CCBY","15 Feb 2021","","","IEEE","IEEE Journals"
"LETR: A Lightweight and Efficient Transformer for Keyword Spotting","K. Ding; M. Zong; J. Li; B. Li",SenseTime Research; SenseTime Research; SenseTime Research; SenseTime Research,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","7987","7991","Transformer recently has achieved impressive success in a number of domains, including machine translation, image recognition, and speech recognition. Most of the previous work on Keyword Spotting (KWS) is built upon convolutional or recurrent neural networks. In this paper, we explore a family of Transformer architectures for keyword spotting, optimizing the trade-off between accuracy and efficiency in a high-speed regime. We also studied the effectiveness and summarized the principles of applying key components in vision Transformers to KWS, including patch embedding, position encoding, attention mechanism, and class token. On top of the findings, we propose the LeTR: a lightweight and highly efficient Transformer for KWS. We consider different efficiency measures on different edge devices so as to reflect a wide range of application scenarios best. Experimental results on two common benchmarks demonstrate that LeTR has achieved state-of-the-art results over competing methods with respect to the speed/accuracy trade-off.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747295","Transformer;keyword spotting;latency","Recurrent neural networks;Image recognition;Computational modeling;Computer architecture;Speech recognition;Benchmark testing;Transformers","convolutional neural nets;deep learning (artificial intelligence);recurrent neural nets;speech processing","LETR;keyword spotting;machine translation;image recognition;speech recognition;KWS;convolutional neural network;recurrent neural network;transformer architectures;LeTR;lightweight and efficiency transformer;patch embedding;position encoding;attention mechanism;class token","","5","","26","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Perception changes with and without a video channel: A study from a speech-to-speech, machine translation mediated map task","H. Akira; C. Vogel; N. Campbell; S. Luz","Trinity College Dublin, School of Computer Science and Statistics, Dublin, Ireland; Trinity College Dublin, School of Computer Science and Statistics, Dublin, Ireland; Trinity College Dublin, School of Computer Science and Statistics, Dublin, Ireland; Usher Institute of Population Health Sciences & Informatics, University of Edinburgh, Edinburgh, UK","2017 8th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","25 Jan 2018","2017","","","000401","000406","This study looks into the effect of the video channel, that provides realtime visual information of the subject's inter-locutor in computer mediated multi-lingual map task dialogues. The addition of a video channel in long distance audio communication has been commercially available since 1964, pioneered by AT&T's Picturephone. However the complexity of adding an image channel to a task oriented dialogue has not penetrated the user audience enough to change the user expectation from a like-to-like alternative of Face-to-Face communication, to a new different communication style. This study reports the increase in visual cognitive state occurrences when communicating with a video channel and the different perception that this setting provided to the subjects of the ILMT-s2s corpus.","","978-1-5386-1264-4","10.1109/CogInfoCom.2017.8268279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268279","","Task analysis;Visualization;Speech;Telephone sets;Videophone systems;Conferences;Terrorism","telecommunication channels;video communication","face-to-face communication;task oriented dialogue;image channel;long distance audio communication;computer mediated multilingual map task dialogues;realtime visual information;speech-to-speech;video channel","","2","","26","IEEE","25 Jan 2018","","","IEEE","IEEE Conferences"
"EasyTalk: A Translator for Sri Lankan Sign Language using Machine Learning and Artificial Intelligence","D. Manoj Kumar; K. Bavanraj; S. Thavananthan; G. M. A. S. Bastiansz; S. M. B. Harshanath; J. Alosious","Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka; Department of Software Engineering Sri Lanka Institute of Information Technology, Colombo, Sri Lanka","2020 2nd International Conference on Advancements in Computing (ICAC)","26 Feb 2021","2020","1","","506","511","Sign language is used by the hearing-impaired and inarticulate community to communicate with each other. But not all Sri Lankans are aware of the sign language or verbal languages and a translation is required. The Sri Lankan Sign Language is tightly bound to the hearing-impaired and inarticulate. The paper presents EasyTalk, a sign language translator which can translate Sri Lankan Sign Language into text and audio formats as well as translate verbal language into Sri Lankan Sign Language which would benefit them to express their ideas. This is handled in four separate components. The first component, Hand Gesture Detector captures hand signs using pre-trained models. Image Classifier component classifies and translates the detected hand signs. The Text and Voice Generator component produces a text or an audio formatted output for identified hand signs. Finally, Text to Sign Converter works on converting an entered English text back into the sign language based animated images. By using these techniques, EasyTalk can detect, translate and produce relevant outputs with superior accuracy. This can result in effective and efficient communication between the community with differently-abled people and the community with normal people.","","978-1-7281-8412-8","10.1109/ICAC51239.2020.9357154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357154","machine learning;image processing;low-resolution image recognition;convolutional neural networks;natural language processing;real-time translation;semantic analysis;text to speech conversion","Image recognition;Assistive technology;Gesture recognition;Speech recognition;Machine learning;Generators;Speech processing","computer animation;gesture recognition;handicapped aids;language translation;learning (artificial intelligence)","Sri Lankan Sign Language;verbal language;sign language translator;detected hand signs;identified hand signs","","8","","13","IEEE","26 Feb 2021","","","IEEE","IEEE Conferences"
"Analysis of Detected Silent Segments in Call Center Recordings","≈û. Ozan; L. O. Iheme","R&D Department, AdresGezgini Inc., ƒ∞zmir, T√ºrkiye; R&D Department, AdresGezgini Inc., ƒ∞zmir, T√ºrkiye","2019 International Artificial Intelligence and Data Processing Symposium (IDAP)","21 Oct 2019","2019","","","1","4","Interpreting speech signals by making a speech to text translation is an active research area especially in current machine learning/deep learning literature. The speech to text translation of call center recordings is an important and specialized application for speech to text translation. Detecting silence in audio recordings can be a pre-processing step in order to optimize processing speed by not-considering audio parts not having significant information. In this work, such a preprocessing framework for detecting silence parts in an audio signal is considered. It is shown that further statistical analysis on the silence distributions results in detecting interesting audio features which can help in finding audio recordings which do not have actual speech sound but a fax machine tone sequence. This foundation can be directly implemented in a call center management software and makes it possible to discriminate between a normal conversation recording and a fax sound recording.","","978-1-7281-2932-7","10.1109/IDAP.2019.8875904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8875904","pre-processing audio signal;statistical audio analysis;variance;entropy","Entropy;Measurement;Mathematical model;Audio recording;Machine learning;Training;Telephone sets","audio recording;learning (artificial intelligence);speech processing;speech recognition;statistical analysis","specialized application;audio recordings;audio parts;silence parts;audio signal;statistical analysis;silence distributions results;audio features;fax machine tone sequence;call center management software;normal conversation recording;fax sound recording;detected silent segments;call center recordings;speech signals;current machine learning;speech to text translation","","","","6","IEEE","21 Oct 2019","","","IEEE","IEEE Conferences"
"An investigation into language model data augmentation for low-resourced STT and KWS","G. Huang; T. F. da Silva; L. Lamel; J. -L. Gauvain; A. Gorin; A. Laurent; R. Lileikyte; A. Messouadi","LIMSI, Universit√© Paris-Saclay, Orsay Cedex, France; LIMSI, Universit√© Paris-Saclay, Orsay Cedex, France; Vocapia Research, Orsay Cedex, France; Vocapia Research, Orsay Cedex, France; LIMSI, Universit√© Paris-Saclay, Orsay Cedex, France; LIMSI, Universit√© Paris-Saclay, Orsay Cedex, France; LIMSI, Universit√© Paris-Saclay, Orsay Cedex, France; Vocapia Research, Orsay Cedex, France","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","5790","5794","This paper reports on investigations using two techniques for language model text data augmentation for low-resourced automatic speech recognition and keyword search. Lowresourced languages are characterized by limited training materials, which typically results in high out-of-vocabulary (OOV) rates and poor language model estimates. One technique makes use of recurrent neural networks (RNNs) using word or subword units. Word-based RNNs keep the same system vocabulary, so they cannot reduce the OOV, whereas subword units can reduce the OOV but generate many false combinations. A complementary technique is based on automatic machine translation, which requires parallel texts and is able to add words to the vocabulary. These methods were assessed on 10 languages in the context of the Babel program and NIST OpenKWS evaluation. Although improvements vary across languages with both methods, small gains were generally observed in terms of word error rate reduction and improved keyword search performance.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953266","multilingual;low resourced languages;speech recognition;keyword search","Training;Vocabulary;Data models;Keyword search;Recurrent neural networks;NIST;Error analysis","natural language processing;recurrent neural nets;speech recognition","language model data augmentation;low-resourced STT;KWS;low-resourced automatic speech recognition;keyword search;out-of-vocabulary rates;language model estimates;recurrent neural networks;word-based RNN;complementary technique;automatic machine translation;Babel program;NIST OpenKWS evaluation;word error rate reduction","","3","","24","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Syntax directed translator for English to Hindi language","P. Kumar; S. Srivastava; M. Joshi","SRMGPC, Lucknow; SRMGPC, Lucknow; SRMGPC, Lucknow","2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)","17 Mar 2016","2015","","","455","459","Hindi Machine Translation is the conversion of text from one language to other by a computer without human involvement. MT is a part of Natural Language Processing which aims to change text or speech of one natural language to another with the help of software. This conversion helps us to overcome the language barriers. Also with the invent of MT technological and cultural barriers can be conquered easily thereby allowing frequent exchange of knowledge. The present work attempts to build an automatic translation system for conversion of text from English to Hindi. Several works related to Machine Translation have also been presented here. Firstly we take a text as input in English, store it in a file, extract words and punctuations from it and store them in an array, then we do grouping of words, find their meanings in context to the sentence and convert it to the target language i.e. Hindi. During this translation we also deal with several issues like word order, word sense, ambiguity and idioms. Finally when we get the text in Hindi we check it for correctness of grammatical rules of the language.","","978-1-4673-6735-6","10.1109/ICRCICN.2015.7434282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434282","Machine Translation;Natural Languages;Text Processing","Syntactics;Dictionaries;Automation;Encyclopedias;Synthesizers;Natural language processing","computational linguistics;grammars;language translation;natural language processing;text analysis","syntax directed translator;English to Hindi language;Hindi machine translation;MT;text conversion;natural language processing;knowledge exchange;automatic translation system;grammatical rule correctness","","5","","14","IEEE","17 Mar 2016","","","IEEE","IEEE Conferences"
"Inflected Words Translation on Japanese-to-Korean Machine Translation","J. -i. Kim; K. -h. Lee","Department of Computer Engineering, Tongmyong University, South Korea; Department of Game Engineering, Tongmyong University, South Korea","2006 International Conference on Hybrid Information Technology","12 Dec 2011","2006","2","","568","575","As well-known, there are many syntactic similarities between Japanese and Korean language. These similarities allow us to build Japanese-Korean translation systems without depending too much on sophisticated syntactic analysis and semantic analysis. To further improve translation accuracy, we have been developing a Japanese-Korean translation system using these similarities for several years. However, there still remain some problems with regard to translation of inflected words, processing of multitranslatable words and so on. In this paper, we propose a new method for Japanese-Korean machine translation by using the relationships of two neighboring words. To solve the problems, we investigate the connection rules of auxiliary verb priority. And we design the translation table, which consists of entry tables and connection form tables. For unambiguous words, we can translate a Japanese word to the corresponding Korean word in terms of direct-matching method by consulting the only entry table. Otherwise, we have to evaluate the connection value computed from connection form tables and then we can select the most appropriate target word.","","0-7695-2674-8","10.1109/ICHIT.2006.253663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021268","","Natural languages;Dictionaries;Information analysis;Data mining;Pattern matching;Information technology;Speech;Humans","","","","1","","15","IEEE","12 Dec 2011","","","IEEE","IEEE Conferences"
"The Function of Fixed Word Combination in Chinese Chunk Parsing","L. Wang; S. Yokoyama","Graduate School of Science and Engineering, Yamagata University, Japan; Graduate School of Science and Engineering, Yamagata University, Japan","2010 International Conference on Asian Language Processing","6 Jan 2011","2010","","","73","76","In this paper, we described an approach about chunk parsing using fixed word combination. It is different from the previous researches. We presented a pattern extraction and matching method of Chinese sentence with fixed word combination. After that we tested the pattern, and got a correct rate more than 96%. From the result of our experiment, we can identify that the analysis of syntax has been improved. At last we used the results of chunk phrase to test through translation system on Internet. Since this approach improves the syntactic analysis of Chinese, the Percentage of accuracy of Chinese-Japanese translations of complex sentence has been improved. And we hope it will be helpful to the study of machine translation.","","978-1-4244-9063-9","10.1109/IALP.2010.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681547","Machine translation;chunk parsing;fixed word combination","Syntactics;Training data;Presses;Bellows;Speech;Information processing;Time frequency analysis","grammars;Internet;language translation;text analysis;word processing","fixed word combination;pattern extraction;pattern matching method;chunk parsing;syntax analysis;Internet;Chinese Japanese translation","","1","","11","IEEE","6 Jan 2011","","","IEEE","IEEE Conferences"
"A morpheme-based lexical chunking system for Chinese","Guo-Hong Fu; Chun-Yu Kit; J. J. Webster","School of Computer Science and Technology, Heilongjiang University, Harbin, China; Department of Chinese, Translation and Linguistics, City University of Hong Kong, Kowloon, Hong Kong, China; Department of Chinese, Translation and Linguistics, City University of Hong Kong, Kowloon, Hong Kong, China","2008 International Conference on Machine Learning and Cybernetics","5 Sep 2008","2008","5","","2455","2460","Chinese lexical analysis consists of word segmentation and part-of-speech tagging. Most previous studies consider them as two separate tasks. In this paper we formalize the two processes as a unique chunking task on a sequence of morphemes and present an integrated lexical analysis system for Chinese based on lexicalized hidden Markov models. In this way, both contextual lexical information and word-internal morphological features can be statistically explored and further combined for disambiguation and unknown word resolution. Experimental results show that the proposed system outperforms several baselines, illustrating the benefits of the unified lexical chunking method with morphemes as the basic units.","2160-1348","978-1-4244-2095-7","10.1109/ICMLC.2008.4620820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620820","Chinese lexical analysis;Lexical chunking;Word segmentation;Part-of-speech tagging","Hidden Markov models;Tagging;Dictionaries;Training data;Machine learning;Cybernetics;Data mining","hidden Markov models;natural language processing","Chinese lexical analysis;morpheme-based lexical chunking system;word segmentation;part-of-speech tagging;hidden Markov model;word-internal morphological feature;statistical analysis","","","","12","IEEE","5 Sep 2008","","","IEEE","IEEE Conferences"
"Part-of-speech tagging based on dictionary and statistical machine learning","Z. Ye; Z. Jia; J. Huang; H. Yin","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; DOCOMO Innovations Incorporation, Palo Alto, USA","2016 35th Chinese Control Conference (CCC)","29 Aug 2016","2016","","","6993","6998","Part-of-speech tagging is the basis of Natural Language Processing, and is widely used in information retrieval, text processing and machine translation fields. The traditional statistical machine learning methods of POS tagging rely on the high quality training data, but obtaining the training data is very time-consuming. The methods of POS tagging based on dictionaries ignore the context information, which lead to lower performance. This paper proposed a POS tagging approach which combines methods based on dictionaries and traditional statistical machine learning. The experimental results show that the approach not only can solve the problem that the training data are insufficient in statistical methods, but also can improve the performance of the methods based on dictionaries. The People's Daily corpus in January 1998 is used as testing data, and the accurate rate of POS tagging achieves 95.80%. For the ambiguity word POS tagging, the accuracy achieves 88%.","1934-1768","978-9-8815-6391-0","10.1109/ChiCC.2016.7554459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7554459","part-of-speech tagging;ambiguity word;word segmentation dictionary;maximum entropy;big data","Decision support systems","learning (artificial intelligence);natural language processing;text analysis","part-of-speech tagging;dictionary learning;statistical machine learning;natural language processing;context information;performance improvement;ambiguity word POS tagging;peoples daily corpus","","7","","20","","29 Aug 2016","","","IEEE","IEEE Conferences"
"Comparison and Combination of Lightly Supervised Approaches for Language Portability of a Spoken Language Understanding System","B. Jabaian; L. Besacier; F. Lefevre","LIG, University Joseph Fourier, Grenoble, France; LIG, University Joseph Fourier, Grenoble, France; LIA, University of Avignon, Avignon, France","IEEE Transactions on Audio, Speech, and Language Processing","31 Dec 2012","2013","21","3","636","648","Portability of a spoken dialogue system (SDS) to a new domain or a new language is a hot topic as it may imply gains in time and cost for building new SDSs. In particular in this paper we investigate several fast and efficient approaches for language portability of the spoken language understanding (SLU) module of a dialogue system. We show that the use of statistical machine translation (SMT) can reduce the time and the cost of porting a system from a source to a target language. For conceptual decoding, a state-of-the-art module based on conditional random fields (CRF) is used and a new approach based on phrase-based statistical machine translation (PB-SMT) is also evaluated. The experimental results show the efficiency of the proposed methods for a fast and low cost SLU language portability. In addition, we propose two methods to increase SLU robustness to translation errors. Overall, it is shown that the combination of all these approaches can further reduce the concept error rate. While most of the experiments in this paper deal with portability from French to Italian (given the availability of the Media French corpus and its subset manually translated into Italian), a validation of our methodology is eventually proposed in Arabic.","1558-7924","","10.1109/TASL.2012.2229983","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362182","Language portability;spoken dialogue systems;spoken language understanding;statistical machine translation","Humans;Adaptation models;Buildings;Availability;Speech;Training;Training data","language translation;natural language processing;random processes","lightly supervised approach;language portability;spoken language understanding system;spoken dialogue system;SDS;SLU module;time reduce;cost reduce;conceptual decoding;conditional random fields;CRF;phrase-based statistical machine translation;PB-SMT;translation errors;concept error rate reduction;Media French corpus;Italian Language;Arabic Language","","9","","43","IEEE","27 Nov 2012","","","IEEE","IEEE Journals"
"Empirical Use of Information Retrieval to Build Synthetic Data for SMT Domain Adaptation","S. Abdul-Rauf; H. Schwenk; P. Lambert; M. Nawaz","Fatima Jinnah Women University, Rawalpindi, Pakistan; LIUM, LUNAM University, University of Le Mans, Le Mans, France; Pompeu Fabra University, Barcelona, Spain; Balochistan University Of Information Technology, Engineering, and Managment Sciences, Quetta, Pakistan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2016","24","4","745","754","In this paper, we present information retrieval as a powerful tool for addressing an imperative problem in the field of statistical machine translation, i.e., improving translation quality when not enough parallel corpora are available. We devise a framework, which uses information retrieval to create a synthetic corpus from the easily available monolingual corpora. We propose an improved unsupervised training approach with a data selection mechanism, which selects only the most appropriate sentences, thus reducing the amount of data, which is less related to the domain in the additional bitext. We also introduce a new method to choose sentences based on their relative similarity/difference from the query sentence. Using the synthetic corpus created by our method, we are able to improve state-of-the-art statistical machine translation systems.","2329-9304","","10.1109/TASLP.2016.2517318","Higher Education Commission Pakistan; French Government(grant numbers:ANR JCJC06 143038,ANR 2009 CORD 004 01); European project FP7 Euromatrix Plus; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378935","Domain adaptation;information retrieval;monolingual corpus;statistical machine translation","Information retrieval;Speech;Training;Electronic mail;Mathematical model;Adaptation models;IEEE transactions","language translation;learning (artificial intelligence);query processing;text analysis","information retrieval empirical use;SMT domain adaptation;imperative problem;translation quality improvement;synthetic corpus;monolingual corpora;unsupervised training approach improvement;synthetic data selection mechanism;query sentence;state-of-the-art statistical machine translation system improvement","","3","","43","IEEE","12 Jan 2016","","","IEEE","IEEE Journals"
"Hybrid part of speech tagger for Malayalam","M. Francis; K. N. R. Nair","Faculty of Information Technology, Ministry of Higher Education, OMAN; NA","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","1 Dec 2014","2014","","","1744","1750","The process of assigning part of speech for every word in a given sentence according to the context is called as part of speech tagging. Part of speech tagging (POS tagging) plays an important role in the area of natural language processing (NLP) including applications such as speech recognition, speech synthesis, natural language parsing, information retrieval, multi words term extraction, word sense disambiguation and machine translation. This paper proposes an efficient and accurate POS tagging technique for Malayalam language using hybrid approach. We propose a Conditional Random Fields(CRF) based method integrated with Rule-Based method. We use SVM based method to compare the accuracy. The corpus both tagged and untagged used for training and testing the system is in the unicode format. The tagset developed by IIIT Hyderabad for Indian Languages is used. The system is tested for selected books of Bible and perform with an accuracy of 94%.","","978-1-4799-3080-7","10.1109/ICACCI.2014.6968565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968565","","Speech recognition;Hidden Markov models;Speech;Speech processing;Probabilistic logic;Accuracy;Compounds","grammars;knowledge based systems;natural language processing;statistical distributions;support vector machines","hybrid part of speech tagging;POS tagging;Malayalam language;natural language processing;NLP;natural language parsing;information retrieval;word sense disambiguation;conditional random fields;CRF;rule-based method;SVM","","","","14","IEEE","1 Dec 2014","","","IEEE","IEEE Conferences"
"Neural Speech Decoding During Audition, Imagination and Production","R. A. Sharon; S. S. Narayanan; M. Sur; A. H. Murthy","Department of Computer Science and Engineering, IIT Madras, Chennai, India; Viterbi School of Engineering, University of Southern California, Los Angeles, USA; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, USA; Department of Computer Science and Engineering, IIT Madras, Chennai, India","IEEE Access","20 Aug 2020","2020","8","","149714","149729","Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively.","2169-3536","","10.1109/ACCESS.2020.3016756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167421","Assistive technology;brain computer interface;EEG;imagined speech;speech-EEG correlation;unit classification","Electroencephalography;Production;Protocols;Image segmentation;Brain modeling;Correlation;Image reconstruction","brain-computer interfaces;electroencephalography;hearing;learning (artificial intelligence);medical signal processing;signal classification;speech coding","translation protocols;selection-by-exclusion based temporal modeling algorithms;signal signatures;single trial multiunit EEG classification;brain signals;neural speech decoding;neural signals;communication mediums;speech perception;imagination;human communication;brain computer interfaces;noninvasive techniques;signal analysis;audition;speech production;electroencephalogram patterns;machine learning approach;subject independence tests","","13","","60","CCBY","14 Aug 2020","","","IEEE","IEEE Journals"
"MaRePhoR ‚Äî An open access machine-readable phonetic dictionary for Romanian","≈û. -A. Toma; A. Stan; M. -L. Pura; T. B√¢rsan","Computer Science Department, Military Technical Academy, Bucharest, Romania; Communications Department, Technical University of Cluj-Napoca, Romania; Computer Science Department, Military Technical Academy, Bucharest, Romania; Computer Science Department, Military Technical Academy, Bucharest, Romania","2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)","27 Jul 2017","2017","","","1","6","This paper introduces a novel open access resource, the machine-readable phonetic dictionary for Romanian - MaRePhoR. It contains over 70,000 word entries, and their manually performed phonetic transcription. The paper describes the dictionary format and statistics, as well as an initial use of the phonetic transcription entries by building a grapheme to phoneme converter based on decision trees. Various training strategies were tested enabling the correct selection of a final setup for our predictor. The best results showed that using the dictionary as training data, an accuracy of over 99% can be achieved.","","978-1-5090-6497-7","10.1109/SPED.2017.7990435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990435","phonetic dictionary;phoneme;grapheme;phonetic transcription;open access","Dictionaries;Decision trees;Open Access;Stress;Classification algorithms;Prediction algorithms;Training","decision trees;language translation;natural language processing;public domain software;speech recognition","MaRePhoR;open access machine-readable phonetic dictionary;open access resource;Romanian language;manually performed phonetic transcription;dictionary format;statistics;phonetic transcription entries;grapheme to phoneme converter;decision trees;training strategies","","5","","21","IEEE","27 Jul 2017","","","IEEE","IEEE Conferences"
"Syntactically Lexicalized Phrase-Based SMT","H. Hassan; K. Sima'an; A. Way","School of Computing, University of Dublin, Dublin, Ireland; Institute for Logic, Language and Computation, University of Amsterdam, Amsterdam, SM, The Netherlands; School of Computing, University of Dublin, Dublin, Ireland","IEEE Transactions on Audio, Speech, and Language Processing","15 Aug 2008","2008","16","7","1260","1273","Until quite recently, extending phrase-based statistical machine translation (PBSMT) with syntactic knowledge caused system performance to deteriorate. The most recent successful enrichments of PBSMT with hierarchical structure either employ nonlinguistically motivated syntax for capturing hierarchical reordering phenomena, or extend the phrase translation table with redundantly ambiguous syntactic structures over phrase pairs. In this paper, we present an extended, harmonized account of our previous work which showed that incorporating linguistically motivated lexical syntactic descriptions, called supertags, can yield significantly better PBSMT systems at insignificant extra computational cost. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from lexicalized tree-adjoining grammar and combinatory categorial grammar. Despite the differences between the two sets of supertags, they give similar improvements. In addition to integrating the Markov supertagging approach in PBSMT, we explore the utility of a new surface grammaticality measure based on combinatory operators. We perform various experiments on the Arabic-to-English NIST 2005 test set addressing the issues of sparseness, scalability, and the utility of system subcomponents. We show that even when the parallel training data grows very large, the supertagged system retains a relatively stable absolute performance advantage over the unadorned PBSMT system. Arguably, this hints at a performance gap that cannot be bridged by acquiring more phrase pairs. Our best result shows a relative improvement of 6.1% over a state-of-the-art PBSMT model, which compares favorably with the leading systems on the NIST 2005 task. We also demonstrate that the advantages of a supertag-based system carry over to German-English, where improvements of up to 8.9% relative to the baseline system are observed.","1558-7924","","10.1109/TASL.2008.925870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599248","Combinatory categorical grammar (CCG);lexical syntax;lexicalized tree adjoining grammar (LTAG);statistical machine translation (SMT);syntax-based machine translation","Surface-mount technology;NIST;System testing;Scalability;Benchmark testing;System performance;Computational efficiency;Performance evaluation;Training data;Logic","grammars;language translation;natural language processing;trees (mathematics)","phrase-based statistical machine translation;syntactic knowledge;nonlinguistically motivated syntax;supertags;target language model;lexicalized tree-adjoining grammar;combinatory categorial grammar;Markov supertagging approach","","7","","52","IEEE","15 Aug 2008","","","IEEE","IEEE Journals"
"Deep neural based name entity recognizer and classifier for English language","S. P. Singh; A. Kumar; H. Darbari","AAI, Center for development of Advanced Computing, Pune, India; AAI, Center for development of Advanced Computing, Pune, India; AAI, Center for development of Advanced Computing, Pune, India","2017 International Conference on Circuits, Controls, and Communications (CCUBE)","25 Jun 2018","2017","","","242","246","Named entity recognition (NER) is an important and very effective for the Machine Translation, Retrieval (IR), Information Extraction (IE) from huge corpus, Question Answering (QA) system, text Mining and text clustering and etc. NER help us to classify or identify the Noun and its types such place /location, people, department, Ministry, organization, times and etc. The huge data available on social Media, websites, news channels and many more sources can be classified so that it can be used in research for NLP processes such as in Machine Translation, Speech Technology, Information Extraction and etc. To process this huge data or corpus we propose recent techniques of Machine Learning and Deep Neural Network. The Deep Neural Network approach will help to identify the Named entity (NE) from huge corpus or text by training the corpus using Word2vec approach. On the basis of fetched tokens and tag. We categorize these tokens into different Grammar categories based of cosine similarity concept of Deep Neural Network. Cosine similarity help to find the tag of unknown token or phases by finding its neared Vectors which are not trained earlier in Word2vec database. We have used the supervised learning (SL) techniques to train the network.","","978-1-5386-0615-5","10.1109/CCUBE.2017.8394152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8394152","Neural Network (NN);POS (Parts of speech) Tagging;word2vec;Natural Language Processing (NLP);Named Entity Recognition (NER);Hidden Markov Model (HMM);Supervised Learning (SL)","Supervised learning;Tagging;Hidden Markov models;Neural networks;Databases;Organizations;Speech recognition","data mining;grammars;language translation;learning (artificial intelligence);natural language processing;neural nets;pattern clustering;question answering (information retrieval);text analysis","cosine similarity;Deep neural based name entity recognition;information retrieval;text mining;NLP process;English language;classifier;Word2vec approach;Deep Neural Network approach;Machine Learning;Information Extraction;NER;text clustering;Question Answering system;Machine Translation","","2","","11","IEEE","25 Jun 2018","","","IEEE","IEEE Conferences"
"Sparte: A text-to-speech machine using synthesis by diphones","J. . -L. Courbon; F. Emerard","Centre National d''Etudes Des Telecommunications, Lannion, France; Centre National d''Etudes Des Telecommunications, Lannion, France","ICASSP '82. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1982","7","","1597","1600","An operational synthesis system for the French language has been developed at the CNET. Vocal output of any message in the language may be obtained from input typed on a keyboard. This speech synthesis equipment is built in an autonomous cabinet of fairly small dimensions; it can also be used as a single board with a V24-RS232 connection. The device consists of the following items: a keyboard, a display and a built-in loudspeaker, a linear prediction synthesizer, 1200 diphones. The software implemented on a microprocessor includes algorithms for orthographic-phonetic translation and systematic prosody processing; automatic syntactic analysis has been simulated and will soon be implemented. The vocal message is obtained in real time; the synthesized speech is intelligible and fairly natural.","","","10.1109/ICASSP.1982.1171432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1171432","","Speech synthesis;Natural languages;Keyboards;Displays;Loudspeakers;Synthesizers;Microprocessors;Software algorithms;Algorithm design and analysis;Analytical models","","","","4","2","5","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"An Investigation on Speech to Sign Language Translator for Hearing Impaired (SSLT) using Machine Learning Techniques","P. K; D. S; D. K. J. R; G. M; D. K; D. M","ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; Bannari Amman Institute of Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India","2022 Smart Technologies, Communication and Robotics (STCR)","13 Jan 2023","2022","","","1","6","The language of the hearing impaired people is a visual language. It has to transmit sound patterns in the form of signs. A person‚Äôs thoughts are expressed by hand signals and facial expressions. Deaf people normally get struggle by making conversation with normal people; The languages will be different for the various group of deaf people all over the world as well. Our project is to guide the hearing deaf or speech defected persons made communicating with normal persons. It automatically translates the speech in English into Indian sign language. It is the sign-language translating system. For the communication between the normal person and the impaired persons, it could be used as a translator for their natural way of speaking. It‚Äôs helpful for people who didn't understand sign language, the sign language gestures are emojis. Our proposed model brings the accuracy improvement by 10% compare to existing models of CNN based classification and SVM_HMM models. FPV and PPV improved by 23.52 % and 37.34 %.","","978-1-6654-6047-7","10.1109/STCR55312.2022.10009341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009341","Hand signals;hearing aids;Auto translates;Sign Language;Natural signal processing;False Predictive values;Support vector machines","Support vector machines;Bridges;Visualization;Gesture recognition;Auditory system;Oral communication;Machine learning","gesture recognition;handicapped aids;hearing;hidden Markov models;language translation;learning (artificial intelligence);sign language recognition;support vector machines;visual languages","deaf people;facial expressions;hand signals;hearing deaf;impaired persons;Indian sign language;machine learning techniques;normal person;sign language gestures;sign language translator;sign-language translating system;sound patterns;visual language","","","","31","IEEE","13 Jan 2023","","","IEEE","IEEE Conferences"
"An investigation of heuristic, manual and statistical pronunciation derivation for Pashto","U. V. Chaudhari; X. Cui; B. Zhou; R. Zhang","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2011 IEEE Workshop on Automatic Speech Recognition & Understanding","5 Mar 2012","2011","","","249","253","In this paper, we study the issue of generating pronunciations for training and decoding with an ASR system for Pashto in the context of a Speech to Speech Translation system developed for TRANSTAC. As with other low resourced languages, a limited amount of acoustic training data was available with a corresponding set of manually produced vowelized pronunciations. We augment this data with other sources, but lack pronunciations for unseen words in the new audio and associated text. Four methods are investigated for generating these pronunciations, or baseforms: an heuristic grapheme to phoneme map, manual annotation, and two methods based on statistical models. The first of these uses a joint Maximum Entropy N-gram model while the other is based on a log-linear Statistical Machine Translation model. We report results on a state of the art, discriminatively trained, ASR system and show that the manual and statistical methods provide an improvement over the grapheme to phoneme map. Moreover, we demonstrate that the automatic statistical methods can perform as well or better than manual generation by native speakers, even in the case where we have a significant number of high quality, manually generated pronunciations beyond those provided by the TRANSTAC program.","","978-1-4673-0367-5","10.1109/ASRU.2011.6163939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163939","","Manuals;Context;Training;Humans;Acoustics;Data models;Statistical analysis","entropy;speech recognition;statistical analysis","statistical pronunciation derivation;Pashto;ASR system;decoding;speech to speech translation system;TRANSTAC;maximum entropy N-gram model;log-in linear statistical machine translation model;phoneme;grapheme;automatic statistical methods;automatic speech recognition","","","","16","IEEE","5 Mar 2012","","","IEEE","IEEE Conferences"
"A multi-agent system for natural language understanding","M. M. Aref","Department of Computer Science and Engineering, University of Bridgeport, Bridgeport, CT, USA","IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502)","17 Nov 2003","2003","","","36","40","Natural language processing (NLP) has many applications such as database user interfaces, machine translation, knowledge acquisition and report abstraction. Several approaches have been used in dealing with NLP. We describe an ongoing research project about understanding natural language input using multiagent system techniques. The system combines between a lexical structural approach and a cognitive structural approach. The lexical structural contains the English vocabulary as agents with all their linguistic information. The cognitive structural consists of 6 modules: speech-to-text, text-to-speech, morphological, semantic, discourse, and query analyzers. These agents are communicating with each other to construct agent subsocieties representing the user input. The query analyzer agent constructs an agent subsociety that communications with the input agent subsocieties to answer the user questions. We start with a brief introduction to multiagent systems and natural language processing. The descriptions of the project, its different modules, and an example of text understanding are presented.","","0-7803-7958-6","10.1109/KIMAS.2003.1245018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245018","","Multiagent systems;Natural languages;Speech analysis;Natural language processing;Dictionaries;Application software;Databases;Speech synthesis;Robustness;Computer science","natural languages;multi-agent systems;user interfaces;language translation;knowledge acquisition","natural language processing;NLP;database user interfaces;machine translation;knowledge acquisition;report abstraction;multiagent system technique;lexical structural approach;cognitive structural approach;linguistic information;speech-to-text module;text-to-speech module;morphological module;semantic module;discourse module;query analyzer module","","5","1","9","IEEE","17 Nov 2003","","","IEEE","IEEE Conferences"
"Phrase Composing Tool using Natural Language Processing","M. Ahirrao; Y. Joshi; A. Gandhe; S. Kotgire; R. G. Deshmukh","Computer Department, Sinhgad Academy of Engineering, Kondhwa, Pune; Computer Department, Sinhgad Academy of Engineering, Kondhwa, Pune; Computer Department, Sinhgad Academy of Engineering, Kondhwa, Pune; Computer Department, Sinhgad Academy of Engineering, Kondhwa, Pune; Sinhgad Academy of Engineering, Kondhwa, Pune","2021 International Conference on Intelligent Technologies (CONIT)","4 Aug 2021","2021","","","1","4","In this fast-running world, machine communication plays a vital role. To compete with this world, human-machine interaction is a necessary thing. To enhance this, Natural Language Processing technique is used widely. Using this technique, we can reduce the interaction gap between the machine and human. Till now, many such applications are developed which are using this technique.This tool deals with the various methods which are used for development of grammar error correction. These methods include rule-based method, classifier-based method and machine translation-based method. Also, models regarding the Natural Language Processing (NLP) pipeline are trained and implemented in this project accordingly. Additionally, the tool can also perform speech to text operation.","","978-1-7281-8583-5","10.1109/CONIT51480.2021.9498546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498546","Speech-to-Text;Recurrent Neural Network;Computational Linguistics;Natural Language Processing;Grammatical Error Correction;Transformer Model","Law;Pipelines;Machine learning;Documentation;Tools;Natural language processing;Error correction","grammars;language translation;natural language processing;text analysis","necessary thing;Natural Language Processing technique;interaction gap;grammar error correction;rule-based method;classifier-based method;machine translation-based method;Natural Language Processing pipeline;phrase composing tool;machine communication;human-machine interaction","","2","","10","IEEE","4 Aug 2021","","","IEEE","IEEE Conferences"
"Telugu Dialect Speech Dataset Creation and Recognition using Deep Learning Techniques","R. S. A. Podila; G. S. S. Kommula; R. K; S. Vekkot; D. Gupta","Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Electronics & Communication Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India; Department of Computer Science and Engineering, Amrita School of Computing, Bengaluru, Amrita Vishwa Vidyapeetham, India","2022 IEEE 19th India Council International Conference (INDICON)","16 Feb 2023","2022","","","1","6","According to India‚Äôs 2011 demography, there seem to be approximately 8 crore Telugu communicators. Apart from that, the Telugu language has many dialects spread across the states Telangana and Andhra Pradesh. Telangana, Rayalaseema, and Coastal accents are the most common. The main concern is to understand the language irrespective of the dialects to have good communication near border areas of these states. Availability of data for analysis of Telugu speech dialects is of high scope for recognition. So, the creation of data is done for Telugu dialects with a total of 9 speakers, 3 speakers for each dialect. Once the data is created, analysis and recognition can help direct our needs. Classifying dialects cannot only solve this problem but also can act as a subset for solving bigger problems like machine translation, sentiment analysis, etc. We have used four RNN models viz. LSTM, GRU, BiLSTM & BiLSTM with attention layer for classification using speech data as input. Maximum test accuracy of 99.11% was obtained using the BiLSTM model with attention layer.","2325-9418","978-1-6654-7350-7","10.1109/INDICON56171.2022.10040194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040194","Speech samples;Telugu dialect;RNN;LSTM;GRU;BiLSTM;BiLSTM with attention layer;recognition","Deep learning;Sentiment analysis;Analytical models;Demography;Sea measurements;Speech recognition;Machine translation","deep learning (artificial intelligence);demography;feature extraction;learning (artificial intelligence);natural language processing;pattern classification;recurrent neural nets;sentiment analysis","Andhra Pradesh;approximately 8 crore Telugu communicators;classifying dialects;Coastal accents;deep learning techniques;good communication;India's 2011 demography;language irrespective;sentiment analysis;speech data;states Telangana;Telugu dialect speech dataset creation;Telugu dialects;Telugu language;Telugu speech dialects","","1","","24","IEEE","16 Feb 2023","","","IEEE","IEEE Conferences"
"Technology Assisted Device for Bilingual Learning Environment in Higher Education in Taiwan","P. -H. Tsai; B. -Y. Chen; S. -Y. Chou; L. -C. Chang","Computer System and Information Engineering, National Formosa University, Yulin, Taiwan; Computer System and Information Engineering, National Formosa University, Yulin, Taiwan; Computer System and Information Engineering, National Formosa University, Yulin, Taiwan; Education Futures, University of South Australia, Adelaide, Australia","2022 IEEE 5th Eurasian Conference on Educational Innovation (ECEI)","20 Jul 2022","2022","","","274","277","Internationalization is the primary goal of the development of education in Taiwan today. There are foreign students in almost every school in Taiwan. Although they would go to a language school for Chinese classes before coming to Taiwan, each teacher's speaking speed and accent are different, and many technical terms are not taught in a language school. This causes the efficiency of learning to be greatly reduced. However, in order to allow foreign students to understand the content of the class, the switch to teaching in English has caused local students with poor English proficiency to be unable to understand the content of the class. Similarly, in Taiwan, not every subject taken by foreign students is taught in English. The official language, Chinese, is still the main communication tool in Taiwan. In order to improve the learning effectiveness of foreign students and local students in a bilingual environment and to deal with the learning problems caused by insufficient language skills, we have proposed solutions. This solution uses voice recognition and text translation technology, through the teacher‚Äôs microphone for the real-time simultaneous translation and recording of class content. The method solves the above-mentioned problems. Both foreign and local students can choose the source language and destination language at the same time for their better understanding and learning efficiency.","","978-1-6654-3318-1","10.1109/ECEI53102.2022.9829503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829503","Human Language Technology;Artificial Intelligence;Speech-to-text Recognition;Machine Translation;Natural Language Processing","Technological innovation;Conferences;Education;Switches;Speech recognition;Real-time systems;Mobile applications","computer aided instruction;educational institutions;handicapped aids;language translation;learning (artificial intelligence);linguistics;natural languages;teaching","Taiwan today;foreign students;language school;Chinese classes;local students;poor English proficiency;official language;learning effectiveness;bilingual environment;learning problems;insufficient language skills;class content;source language;destination language;learning efficiency;technology assisted device;bilingual learning environment;higher education","","","","16","IEEE","20 Jul 2022","","","IEEE","IEEE Conferences"
"An End-To-End Model from Speech to Clean Transcript for Parliamentary Meetings","M. Mimura; S. Sakai; T. Kawahara","Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan","2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","3 Feb 2022","2021","","","465","470","This paper presents an end-to-end approach for generating readable and clean text directly from speech signal. While conventional automatic speech recognition (ASR) systems are designed to faithfully reproduce utterances word-by-word, we propose a model that emulates the way a human transcriber/editor creates a clean transcript from speech by skipping fillers, substituting colloquial expressions with more formal ones, inserting punctuation, and performing other types of corrections. An evaluation using 700-hour Japanese Parliamentary speech demonstrates the effectiveness of the proposed approach in generating clean texts suitable for human consumption. We also show that forward-backward decoding and multitask learning leveraging approximate faithful transcripts significantly improve the performance of the direct mapping.","2640-0103","978-988-14768-9-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689457","","Costs;Error analysis;Information processing;Decoding;Machine translation;Task analysis;Standards","decoding;learning (artificial intelligence);natural language processing;speech coding;speech recognition","colloquial expressions;700-hour Japanese Parliamentary speech;human consumption;End-To-End Model;clean transcript;Parliamentary meetings;end-to-end approach;readable text;clean text;speech signal;conventional automatic speech recognition systems;utterances word-by-word;skipping fillers;approximate faithful transcripts","","","","31","","3 Feb 2022","","","IEEE","IEEE Conferences"
"CNN Based Speech and Text Translation Using Sign Language","M. Gupta; G. Singh; A. Yadav","Department of CSE, Chandigarh University, Punjab; Department of CSE, Chandigarh University, Punjab; Department of CSE, Chandigarh University, Punjab","2021 3rd International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)","9 Mar 2022","2021","","","433","437","There are around 5,00,000 deaf and hard of hearing persons in the globe. Around 60% of peoples are born as a deaf and deafeningly. These kinds of peoples are not able to understand anything like as normal peoples. The sign language (i.e., hand movements) are used for the dumb and deaf to pass their messages. These people‚Äôs faces lots of challenges on daily basis due to the relative absence of widespread sign language use in society. The helplessness to talk seems like true impairment. People with this defacement communicate in entirely several means. There are a variety of choices present for communication where sign language is one of the ways. Applications existence for sign language is necessary for dumb people so that they become able to talk easily with them who does not know sign language motions. The objective here is to take the essential acts in sustaining the language gap between conventional people and verbally challenged people. Communication becomes extremely difficult for them, the chance that they can talk is through language of sign. Though a dumb or deaf person might know language of sign, the people with whom she wishes to speak might not. Our aim to overcome this difference by using machine learning and neural networks to analyze sign language and translate it to text and voice.","","978-1-6654-3811-7","10.1109/ICAC3N53548.2021.9725601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725601","Sign Language;Image Processing with CNN;Convolution Neural Networks;Recognition;Gesture Recognition;OpenCV;Video Processing using CNN","Uncertainty;Sociology;Neural networks;Process control;Gesture recognition;Auditory system;Assistive technologies","convolutional neural nets;handicapped aids;sign language recognition","CNN based speech-text translation;deaf person;dumb person;machine learning;normal peoples;sign language motions","","","","12","IEEE","9 Mar 2022","","","IEEE","IEEE Conferences"
"Metrics for evaluating phonetics machine translation in Natural Language Processing through modified Edit Distance algorithm-A na√Øve approach","M. Hanumanthappa; Rashmi S; M. V. Reddy","Department of Computer science and Applications, Bangalore University, Bangalore, India; Department of Computer science and Applications, Bangalore University, Bangalore, India; Department of Computer Science, Rani Channamma University, Belgaum, India","2015 International Conference on Computer Communication and Informatics (ICCCI)","24 Aug 2015","2015","","","1","7","Uninhabited mistakes while writing happens are unstoppable. There are certain common errors that occur during writing such as missing letters, extra letters, disordered letters, and misspelled letters. These kind of common spelling errors are called phonetics spelling errors. These are of a major concern while dealing with phonetics. Out of various problems that the phoneticians are trying to solve, major portion of it concentrates on varieties of spelling errors. Phonetic structures are greatly emphasized based on the effectiveness, appropriateness and accuracy. In order to keep abreast with the changing and challenging trends of Natural Language Processing (NLP), it is of great importance that one should resolve the problems of spelling errors. To achieve the goal, numerous realistic and practical approaches have to be adopted that make use of spelling correction algorithms such as Edit distance, Habit distance, Soundex and Asoundex. Through the analysis of these algorithms, a new interface is put forward that calculates the Edit distance, thereby showing the overall comparative study of phonetic algorithms with the proposed modified Edit Distance algorithm. The interface computes the Edit distance between two strings in appropriate and intuitive way, contemplating with the comparisons shown in the distance table. The Results show that an average of 0.937 recall and 0.947 precision have been achieved with the F-measure 0.9417. Through these results, it is evident that the recall and F-measures are improved in the proposed Edit-Distance algorithm. The revised version of the edit distance algorithm consistently attains finer quality results in comparison with the traditional edit distance algorithm.","","978-1-4799-6805-3","10.1109/ICCCI.2015.7218113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7218113","Ambiguity;Edit distance;Natural Language Processing (NLP);Phonetics","Algorithm design and analysis;Computers;Informatics;Natural language processing;Writing;Context;Accuracy","natural language processing;speech processing","phonetics machine translation metrics;natural language processing;modified edit distance algorithm;phonetics spelling errors;spelling correction algorithms;habit distance;Soundex;Asoundex;F-measures","","3","1","8","IEEE","24 Aug 2015","","","IEEE","IEEE Conferences"
"Enhancing Ontology Translation Through Cross-Lingual Agreement","M. Tian; F. Giunchiglia; R. Song; X. Chen; H. Xu","School of Artificial Intelligence, Jilin University, Changchun, China; Department of Information Engineering and Computer Science, University of Trento, Italy; School of Artificial Intelligence, Jilin University, Changchun, China; School of Artificial Intelligence, Jilin University, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Ontology serves as the foundation for the underlying representation of knowledge. In order to achieve the sharing of knowledge across languages, ontologies that are typically only represented in English must be translated into different languages. Building a domain-specific translation system is necessary for ontology due to the extremely specialized vocabulary and absence of contextual information. In this paper, we introduce cross-lingual agreement to alleviate the aforementioned issues. We propose a method for representing ontology labels that constructs agreement constraint objects by minimizing the distances between source and target sides. We also integrate with adversarial learning to reduce the difference between the ontology label and the hypothesis generated by the translation model during the fine-tuning. Finally, the agreement modeling strategy is incorporated into the decoding phase to guide the generation of translation candidates. Experiments on the four domains English-to-German ontology show that the proposed method achieves significant improvements over the baselines.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094574","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094574","ontology translation;cross-lingual agreement;adversarial learning;fusion decoding;cross-lingual alignment","Training;Vocabulary;Buildings;Ontologies;Signal processing;Feature extraction;Adversarial machine learning","","","","","","23","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Joint Morphological-Lexical Language Modeling for Processing Morphologically Rich Languages With Application to Dialectal Arabic","R. Sarikaya; M. Afify; Y. Deng; H. Erdogan; Y. Gao","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Ministry of Communications and Information Technology, ITIDA, Cairo, Egypt; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","IEEE Transactions on Audio, Speech, and Language Processing","15 Aug 2008","2008","16","7","1330","1339","Language modeling for an inflected language such as Arabic poses new challenges for speech recognition and machine translation due to its rich morphology. Rich morphology results in large increases in out-of-vocabulary (OOV) rate and poor language model parameter estimation in the absence of large quantities of data. In this study, we present a joint morphological-lexical language model (JMLLM) that takes advantage of Arabic morphology. JMLLM combines morphological segments with the underlying lexical items and additional available information sources with regards to morphological segments and lexical items in a single joint model. Joint representation and modeling of morphological and lexical items reduces the OOV rate and provides smooth probability estimates while keeping the predictive power of whole words. Speech recognition and machine translation experiments in dialectal-Arabic show improvements over word and morpheme based trigram language models. We also show that as the tightness of integration between different information sources increases, both speech recognition and machine translation performances improve.","1558-7924","","10.1109/TASL.2008.924591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599398","Joint modeling;language modeling;maximum entropy modeling;morphological analysis","Natural languages;Speech recognition;Morphology;Parameter estimation;Vocabulary;Robustness;Predictive models;Entropy;Natural language processing;Information technology","language translation;speech recognition;statistical analysis","joint morphological-lexical language modeling;morphologically rich languages;dialectal Arabic;speech recognition;machine translation;rich morphology;out-of-vocabulary rate;language model parameter estimation;Arabic morphology;morphological segments;smooth probability estimation;trigram language models","","13","1","35","IEEE","15 Aug 2008","","","IEEE","IEEE Journals"
"(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding","S. Upadhyay; M. Faruqui; G. T√ºr; H. -T. Dilek; L. Heck","University of Pennsylvania, Philadelphia, PA; Google Research, Mountain View, CA; Google Research, Mountain View, CA; Google Research, Mountain View, CA; Samsung Research, Mountain View, CA","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","6034","6038","Spoken language understanding (SLU) is a component of goal-oriented dialogue systems that aims to interpret user's natural language queries in system's semantic representation format. While current state-of-the-art SLU approaches achieve high performance for English domains, the same is not true for other languages. Approaches in the literature for extending SLU models and grammars to new languages rely primarily on machine translation. This poses a challenge in scaling to new languages, as machine translation systems may not be reliable for several (especially low resource) languages. In this work, we examine different approaches to train a SLU component with little supervision for two new languages - Hindi and Turkish, and show that with only a few hundred labeled examples we can surpass the approaches proposed in the literature. Our experiments show that training a model bilingually (i.e., jointly with English), enables faster learning, in that the model requires fewer labeled instances in the target language to generalize. Qualitative analysis shows that rare slot types benefit the most from the bilingual training.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461905","Spoken Language Understanding;Cross-Lingual;Slot-Filling;Intent Classification","Training;Training data;Data models;Task analysis;Google;Semantics;Reliability","interactive systems;language translation;natural language processing;query processing;speech recognition","Turkish;semantic representation format;natural language queries;SLU approaches;almost zero-shot cross-lingual spoken language understanding;Hindi;SLU component;machine translation systems;grammars;goal-oriented dialogue systems","","20","","25","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Scaling shrinkage-based language models","S. F. Chen; L. Mangu; B. Ramabhadran; R. Sarikaya; A. Sethy","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","299","304","In we show that a novel class-based language model, Model M, and the method of regularized minimum discrimination information (rMDI) models outperform comparable methods on moderate amounts of Wall Street Journal data. Both of these methods are motivated by the observation that shrinking the sum of parameter magnitudes in an exponential language model tends to improve performance. In this paper, we investigate whether these shrinkage-based techniques also perform well on larger training sets and on other domains. First, we explain why good performance on large data sets is uncertain, by showing that gains relative to a baseline n-gram model tend to decrease as training set size increases. Next, we evaluate several methods for data/model combination with Model M and rMDI models on limited-scale domains, to uncover which techniques should work best on large domains. Finally, we apply these methods on a variety of medium-to-large-scale domains covering several languages, and show that Model M consistently provides significant gains over existing language models for state-of-the-art systems in both speech recognition and machine translation.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373380","","Natural languages;Performance gain;Predictive models;Automatic speech recognition;Speech recognition;Interpolation;Acoustic testing;Training data;Large-scale systems","language translation;natural language processing;speech recognition","language model;regularized minimum discrimination information models;shrinkage-based techniques;speech recognition;machine translation","","14","","13","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"Computational Linguistic Models and Language Technologies for Croatian","B. D. Basic; Z. Dovedan; I. Raffaelli; S. Seljan; M. Tadic","Faculty of Humanities and Social Sciences, University of Zagreb, Zagreb, Croatia; zdravko.dovedan@ffzg.hr; ida.raffaelli@ffzg.hr; sanja.seljan@ffzg.hr; marko.tadic@ffzg.hr","2007 29th International Conference on Information Technology Interfaces","8 Aug 2007","2007","","","521","528","This paper gives an overview of the scientific program ""Computational linguistic models and language technologies for Croatian"" that has been launched recently. Its short and long term goals, its composition as well as methodology and expected results are presented.","1330-1012","953-7138-09-7","10.1109/ITI.2007.4283826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4283826","computational linguistics;natural language processing;Croatian language;corpus linguistics;POS/MSD tagging;lemmatization;parsing;WordNet;machine translation;machine aided translation;computer assisted language learning;automatic document indexing;document classification;document summarization","Computational linguistics;Computational modeling;Speech processing;Dictionaries;Natural language processing;Tagging;Machine learning;Indexing;Frequency;Knowledge representation","computational linguistics;document handling;indexing;language translation;natural language processing","document indexing;machine translation;natural language processing;Croatian language;computational linguistic model","","","1","34","","8 Aug 2007","","","IEEE","IEEE Conferences"
"A factorization network based method for multi-lingual domain classification","Y. Shi; Y. -C. Pan; M. -Y. Hwang; K. Yao; H. Chen; Y. Zou; B. Peng",Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft,"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5276","5280","In many spoken language understanding systems (SLUS), domain classification is the most crucial component, as system responses based on wrong domains often yield very unpleasant user experiences. In multi-lingual domain classification, the training data for some poor-resource languages often comes from machine translation. Some of the higher order n-gram features are distorted during machine translation. Feature co-occurrence becomes reliable feature in multi-lingual domain classification. In this paper, in order to effectively model feature co-occurrences, we propose Factorization Networks that are combinations of Factorization Machines (FMs) with Neural Networks (NNs). FNs extend the linear connections from the input feature layer to the hidden layer in NNs to factorization connections that represent the weights of feature co-occurrences using factorized method. In addition to FNs, we also propose a hybrid model that integrates FNs, NNs and Maximum Entropy (ME) models together. The component models in the hybrid model share the same input features. Based on two data sets (ATIS data set and Microsoft Cortana Chinese data ), the proposed models shows promising results. Especially for large Microsoft Cortana Chinese data which is translated from well annotated English data, FNs using unigram, class and query length features achieve more than 20% relative error reduction over linear (SVMs).","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7178978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178978","Factorization Networks;Spoken Language Understanding;Domain Classification","Artificial neural networks;Support vector machines;Training data;Error analysis;Training;Polynomials","language translation;neural nets;pattern classification","multilingual domain classification;factorization network;spoken language understanding system;machine translation;higher order n-gram features;feature cooccurrence;factorization machines;neural networks;linear connections;maximum entropy;Microsoft Cortana Chinese data;ATIS data set","","1","","23","IEEE","6 Aug 2015","","","IEEE","IEEE Conferences"
"Towards Better Word Alignment in Transformer","K. Song; X. Zhou; H. Yu; Z. Huang; Y. Zhang; W. Luo; X. Duan; M. Zhang","Soochow University, Suzhou, China; Soochow University, Suzhou, China; Alibaba DAMO Academy, Hangzhou, China; Alibaba DAMO Academy, Hangzhou, China; Westlake University, Hangzhou, China; Alibaba DAMO Academy, Hangzhou, China; Soochow University, Suzhou, China; Soochow University, Suzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Jun 2020","2020","28","","1801","1812","While neural models based on the Transformer architecture achieve the State-of-the-Art translation performance, it is well known that the learned target-to-source attentions do not correlate well with word alignment. There is an increasing interest in inducing accurate word alignment in Transformer, due to its important role in practical applications such as dictionary-guided translation and interactive translation. In this article, we extend and improve the recent work on unsupervised learning of word alignment in Transformer on two dimensions: a) parameter initialization from a pre-trained cross-lingual language model to leverage large amounts of monolingual data for learning robust contextualized word representations, and b) regularization of the training objective to directly model characteristics of word alignments which results in favorable word alignments receiving more concentrated probabilities. Experiments on benchmark data sets of three language pairs show that the proposed methods can significantly reduce alignment error rate (AER) by at least 3.7 to 7.7 points on each language pair over two recent works on improving the Transformer's word alignment. Moreover, our methods can achieve better alignment results than GIZA++ on certain test sets.","2329-9304","","10.1109/TASLP.2020.2998278","National Natural Science Foundation of China(grant numbers:61525205,61673289); Alibaba Group through Alibaba Innovative Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103090","Neural network;neural machine translation;Transformer;word alignment;language model pre-training;alignment concentration","Decoding;Data models;Training;Context modeling;Standards;Speech processing;Error analysis","language translation;natural language processing;neural nets;probability;unsupervised learning","Transformer architecture;pre-trained cross-lingual language model;alignment error rate;word alignment;translation performance;contextualized word representation learning;GIZA++;unsupervised learning;machine translation","","2","","54","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"Design of Short-Wave Voice Signal Detection Platform Based on Artificial Intelligence","C. Wu; Y. Qiao; H. Li","The 60th team, Unit 32124 of Chinese People‚Äôs Liberation Army, Yanji, China; The 60th team, Unit 32124 of Chinese People‚Äôs Liberation Army, Yanji, China; The 60th team, Unit 32124 of Chinese People‚Äôs Liberation Army, Yanji, China","2022 6th International Conference on Wireless Communications and Applications (ICWCAPP)","14 Jun 2023","2022","","","123","126","Under the condition of short-wave variable parameter channel, the accuracy rate of common Voice Activity Detection method always need to improve because of the influence of low signal-to-noise ratio and complex environmental noise. With the rise of artificial intelligence (AI), signal recognition which is based on statistical machine learning theory of artificial neural networks, is gradually becoming the main direction of signal recognition areas. A short-wave speech signal detection platform is designed in this article, it can finish the better short-wave speech signal detection even under low SNR (signal-to-noise ratio), and it can give the time segmentation of the speech signal.","","978-1-6654-6290-7","10.1109/ICWCAPP57292.2022.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10148432","Artificial Intelligence;Neural Network;Voice Signal;Automatic Detection","Wireless communication;Voice activity detection;Working environment noise;Machine learning;Artificial neural networks;Machine translation;Signal detection","artificial intelligence;learning (artificial intelligence);neural nets;signal detection;speech processing;statistical analysis;voice activity detection","artificial intelligence;artificial neural networks;common voice activity detection method;complex environmental noise;low signal-to-noise ratio;short-wave speech signal detection platform;short-wave variable parameter channel;short-wave voice signal detection platform;signal recognition areas;statistical machine learning theory","","","","9","IEEE","14 Jun 2023","","","IEEE","IEEE Conferences"
"Semi-Supervised end-to-end Speech Recognition via Local Prior Matching","W. -N. Hsu; A. Lee; G. Synnaeve; A. Hannun",Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research,"2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","125","132","For sequence transduction tasks like speech recognition, a strong structured prior model encodes rich information about the target space, implicitly ruling out invalid sequences by assigning them low probability. In this work, we propose local prior matching (LPM), a semi-supervised objective that distills knowledge from a strong prior (e.g. a language model) to provide learning signal to an end-to-end model trained on unlabeled speech. We demonstrate that LPM is simple to implement and superior to existing knowledge distillation techniques under comparable settings. Starting from a baseline trained on 100 hours of labeled speech, with an additional 360 hours of unlabeled data, LPM recovers 54%/82% and 73%/91% of the word error rate on clean and noisy test sets with/without language model rescoring relative to a fully supervised model on the same data.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383552","Semi-supervised ASR;knowledge distillation","Text recognition;Speech recognition;Semisupervised learning;Data models;Noise measurement;Machine translation;Task analysis","speech recognition;supervised learning","semisupervised end-to-end speech recognition;sequence transduction;local prior matching;LPM;language model;end-to-end model;unlabeled speech;knowledge distillation;labeled speech;fully supervised learning;word error rate;noisy test sets","","1","","69","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"Phonology-Augmented Statistical Framework for Machine Transliteration Using Limited Linguistic Resources","G. H. Ngo; M. Nguyen; N. F. Chen","Cornell University, Ithaca, USA; National University of Singapore, Singapore; Institute for Infocomm Research, A*STAR, Singapore","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Nov 2018","2019","27","1","199","211","Transliteration converts words in a source language (e.g., English) into words in a target language (e.g., Vietnamese). This conversion considers the phonological structure of the target language, as the transliterated output needs to be pronounceable in the target language. For example, a word in Vietnamese that begins with a consonant cluster is phonologically invalid and thus would be an incorrect output of a transliteration system. Most statistical transliteration approaches, albeit being widely adopted, do not explicitly model the target language's phonology, which often results in invalid outputs. The problem is compounded by the limited linguistic resources available when converting foreign words to transliterated words in the target language. In this paper, we present a phonology-augmented statistical framework suitable for transliteration, especially when only limited linguistic resources are available. We propose the concept of pseudo-syllables as structures representing how segments of a foreign word are organized according to the syllables of the target language's phonology. We performed transliteration experiments on Vietnamese and Cantonese. We show that the proposed framework outperforms the statistical baseline by up to 44.68% relative, when there are limited training examples (587 entries).","2329-9304","","10.1109/TASLP.2018.2875269","Multilingual Speech Recognition: Mismatched Crowdsourcing for Spoken Languages in Singapore at the Institute for Infocomm Research (I2R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488567","Transliteration;machine translation;cross-lingual information retrieval;named entity recognition","Speech processing;Training data;Phonetics;Standards;Training;Task analysis;Data models","language translation;linguistics;natural language processing;statistical analysis","linguistic resources;consonant cluster;target languages phonology;phonological structure;source language;machine transliteration;phonology-augmented statistical framework;foreign word;statistical transliteration approaches;transliteration system","","7","","76","IEEE","10 Oct 2018","","","IEEE","IEEE Journals"
"An Innovative Method for Automatic American Sign Language Interpretation using Machine Learning and Leap Motion Controller","J. Jenkins; S. Rashad","School of Engineering and Computer Science, Morehead State University, Morehead, KY, USA; School of Engineering and Computer Science, Morehead State University, Morehead, KY, USA","2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","10 Jan 2022","2021","","","0633","0638","Millions of people globally use some form of sign language in their everyday lives. There is a need for a method of gesture recognition that is as easy to use and ubiquitous as voice recognition is today. In this paper we explore a way to translate from sign language to speech using an innovative method, utilizing the Leap Motion Controller and machine learning algorithms to capture and analyze hand movements in real time, then converting the interpreted signs into spoken word. We seek to build a system that is easy to use, intuitive to understand, adaptable to the individual, and usable in everyday life. This system will be able to work in an adaptive way to learn new signs to expand the dictionary of the system and allow higher accuracy on an individual level. It will have a wide range of applications for healthcare, education, gamification, communication, and more. An optical hand tracking piece of hardware, the Leap Motion Controller will be used to capture hand movements and information to create supervised machine learning models that can be trained to accurately guess American Sign Language (ASL) symbols being signed in real time. Experimental results show that the proposed method is promising and provides a high level of accuracy in recognizing ASL.","","978-1-6654-0690-1","10.1109/UEMCON53757.2021.9666640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666640","machine learning;sign language recognition;artificial neural networks;leap motion controller;human computer interaction","Vocabulary;Tracking;Gesture recognition;Machine learning;Speech recognition;Assistive technologies;Mobile communication","dictionaries;image motion analysis;language translation;motion control;natural language processing;object tracking;palmprint recognition;sign language recognition;speech recognition;supervised learning","innovative method;leap motion controller;gesture recognition;voice recognition;hand movements;interpreted signs;supervised machine learning;American sign language symbols;automatic American sign language interpretation;sign language translation;spoken word;dictionary;optical hand tracking;ASL","","1","","9","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Automatic prediction of vocabulary knowledge for learners of Chinese as a foreign language","J. Lee; C. Y. Yeung","Department of Linguistics and Translation, City University of Hong Kong, Hong Kong; Department of Linguistics and Translation, City University of Hong Kong, Hong Kong","2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)","7 Jun 2018","2018","","","1","4","Since extensive reading is beneficial for learning a foreign language, students are encouraged to seek additional reading materials from sources beyond their textbooks. The materials should be difficult enough to stretch the student's language proficiency, but not too difficult as to hinder comprehension. A complex word identification (CWI) system can identify texts that optimize these criteria by estimating the student's proficiency level. We present a personalized CWI model for Chinese as a foreign language. This model predicts whether the student knows a Chinese word or not, based on a small training set from the student. In empirical evaluation, a support vector machine (SVM) classifier with features based on graded vocabulary lists yielded the best performance, outperforming a label propagation approach that is state-of-the-art for personalized CWI for English.","","978-1-5386-4543-7","10.1109/ICNLSP.2018.8374392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374392","complex word identification;computer-assisted language learning;Chinese as a foreign language;vocabulary modeling","Vocabulary;Training;Support vector machines;Guidelines;Task analysis;Classification algorithms;Standards","computer aided instruction;linguistics;pattern classification;support vector machines;text analysis;vocabulary;word processing","automatic prediction;vocabulary knowledge;complex word identification system;personalized CWI model;Chinese word;foreign language learning;reading materials;support vector machine classifier;SVM classifier;label propagation","","5","","15","IEEE","7 Jun 2018","","","IEEE","IEEE Conferences"
"A Survey of Morphological Analysis for Marathi Language","S. Gokhale; P. Deshpande","Department of Computer Engineering, MKSSS‚Äôs Cummins College of Engineering, Pune, India; Department of Computer Engineering, MKSSS‚Äôs Cummins College of Engineering, Pune, India","2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)","17 Mar 2023","2022","","","1","4","Natural language processing is a field which studies how machines can understand the natural languages used in human-to-human interaction. Any language consists of meaningful sentences. Sentences are made of elementary parts called words. Morphemes are the building blocks of words. Marathi is a morphologically rich language since it has various root words and affixes that come together to form a word. The properties of words change according to their role in the sentence (gerund, adjective, verb, preposition, etc.). Morphology studies the formation of words. The paper focuses on the survey of morphological features of Marathi, which have proven useful in applications like speech synthesis, machine translation, information retrieval and spell checking.","","978-1-6654-9902-6","10.1109/AIST55798.2022.10065304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065304","Natural language processing;Morphology;Marathi;Affixes;Morphemes;Machine Translation;Information retrieval;Spellchecking.","Analytical models;Sentiment analysis;Soft sensors;Morphology;Speech recognition;Tagging;Data models","natural language processing","human-to-human interaction;information retrieval;Marathi language;morphological analysis;morphological features;morphologically rich language;morphology studies;natural language processing;root words;speech synthesis;spell checking","","","","12","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Multilingual Global Translation using Machine Learning","H. Dandge; M. Lokhande; V. Jadhao; S. Patil; S. Powar","Department of Computer Engineering, Pimpri Chinchwad College of Engineering; Department of Computer Engineering, Symbiosis Institute of Technology, Symbiosis International University, Lavale, Pune, India; Department of Computer Engineering, Pimpri Chinchwad College of Engineering; Department of Computer Engineering, Pimpri Chinchwad College of Engineering; Department of Computer Engineering, Pimpri Chinchwad College of Engineering","2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA)","20 Apr 2023","2023","","","101","106","Language remains as a communication between two people while allowing them to understand and express each other's emotions, ideas, and views. In light of the globalization of the IT business, platforms that support several languages have become a standard. Text recognition is one of the most researched topics and is done with the help of Deep Neural Networks (DNN). People find it easy to read text, but computers have a hard time doing it. However, there are also several challenges associated with creating and managing multilingual websites, including: Content creation, Technical implementation, implementing a multilingual website can be technically challenging, content optimization, Maintenance and updates. By taking a paragraph as input and processing the handwritten English characters, the article hopes to convert them into a computer-readable format paragraph with support for symbols and cursive writing, then train a neural network system to do so. Machine Learning algorithms (LSTM, FLITE, VAD, etc.) are studied for the various multilingual language conversion. The model is based on numerous natural language processing concepts and was created by adding multilingual capabilities to Google's previous Speech Recognition model. The article aims to develop a multilingual recognition model that will make it possible for non-readers to interact with digital devices in their native tongue.","","979-8-3503-9720-8","10.1109/ICIDCA56705.2023.10100287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100287","Globalization;Multilingual text;Natural Language Human Text;Deep Learning;Machine Learning","Machine learning algorithms;Tongue;Text recognition;Neural networks;Hidden Markov models;Symbols;Personal digital devices","deep learning (artificial intelligence);learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition","computer-readable format paragraph;Content creation;content optimization;creating managing multilingual websites;cursive writing;Deep Neural Networks;DNN;globalization;Google's previous Speech Recognition model;handwritten English characters;hard time;IT business;machine Learning;multilingual capabilities;multilingual global translation;multilingual language conversion;multilingual recognition model;multilingual website;neural network system;numerous natural language processing concepts;researched topics;Technical implementation;text recognition","","","","20","IEEE","20 Apr 2023","","","IEEE","IEEE Conferences"
"A Hybrid Approach for Word Alignment in English-Hindi Parallel Corpora with Scarce Resources","J. Srivastava; S. Sanyal","Information Technology, Indian Institute of Information Technology, Allahabad, Allahabad, India; Information Technology, Indian Institute of Information Technology, Allahabad, Allahabad, India","2012 International Conference on Asian Language Processing","7 Mar 2013","2012","","","185","188","This paper presents an approach which improves the performance of the word alignment with scarce resources for English-Hindi language pair. We obtain an improvement in the performance of IBM Model 1-2 algorithm by applying part of speech (POS) tag prior to the computation of word alignment probability. This paper demonstrates the increase of precision, recall and F-measure by approximately 15%, 11%, 14% respectively and reduction in Alignment Error Rate (AER) by approximately 14% with IBM Model 1. Similarly it shows an increase of precision, recall and F-measure by approximately 6%, 6% and 6% respectively and reduction in Alignment Error Rate (AER) by approximately 6% with IBM Model 2. Experiments of this paper are based on TDIL corpus.","","978-0-7695-4886-9","10.1109/IALP.2012.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473727","Word alignment;Statistical Machine Translation;POS tagger;Scarce resources","Hidden Markov models;Computational modeling;Training;Tagging;Computational linguistics;Information technology;Error analysis","language translation;natural language processing;probability;statistical analysis","word alignment probability;English-Hindi parallel corpora;scarce resources;performance improvement;English-Hindi language pair;IBM model 1-2 algorithm;part-of-speech tag;POS;increase precision;increase recall;increase F-measure;alignment error rate;AER;TDIL corpus;statistical machine translation;natural language processing","","3","","14","IEEE","7 Mar 2013","","","IEEE","IEEE Conferences"
"Automatic evaluation of sentence fluency","Ding Liu; Yu Zhou; Chengqing Zong; Fuji Ren","National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Beijing, China; Department of Information Science and Intelligent Systems, The University of Tokushima, Japan","SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)","17 Nov 2003","2003","2","","1687","1692 vol.2","In the machine translation (MT) system, how to evaluate the sentence fluency of the translation results is an important research topic. Most of the current methods are based on the similarity of output words compared with the reference translations, which don't specially address the evaluation of the sentence fluency according to the syntactic structure. This paper proposes a statistical approach to the problem, which is based on the n-gram language model and reference-independent. Our approach is a beneficial compensation to the reference-dependent methods and has better robustness than those methods based on the syntactic analysis. The preliminary experimental results indicate that the approach basically reflects the reality of human's judgment.","1062-922X","0-7803-7952-7","10.1109/ICSMC.2003.1244655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244655","","NIST;Frequency;Laboratories;Automation;Information science;Intelligent systems;Machine intelligence;Equations;Humans;Statistics","language translation;computational linguistics;speech intelligibility;linguistics","automatic sentence fluency evaluation;MT system;machine translation;reference translations;syntactic structure;n-gram language model;reference dependent methods;robustness;syntactic analysis;human judgment","","","","9","IEEE","17 Nov 2003","","","IEEE","IEEE Conferences"
"Machine Learning-Based Automated Tool to Detect Sinhala Hate Speech in Images","E. Silva; M. Nandathilaka; S. Dalugoda; T. Amarasinghe; S. Ahangama; G. T. Weerasuriya","Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka","2021 6th International Conference on Information Technology Research (ICITR)","4 Jan 2022","2021","","","1","7","Social media platforms have emerged rapidly with technological advancements. Facebook, the most widely used social media platform has been the primary reason for the spread of hatred in Sri Lanka in the recent past. When a post with Sinhala hate content is reported on Facebook, it is translated to the English language before the review of the moderators. In most instances, the translated content has a different context compared to the original post. This results in concluding that the reported post does not violate the established policies and guidelines concerning hate content. Hence, an effective approach needs to be in place to address the aforementioned problem. This research project proposes a solution through an automated tool that is capable of detecting hate content presented in Sinhala phrases extracted from Facebook posts/memes. The tool accepts an image that contains Sinhala texts, extracts the text using a Convolutional Neural Network (CNN) model, preprocesses the text using Natural Language Processing (NLP) techniques, analyzes the preprocessed text to identify hate intensity level and finally classifies the text into four main domains named Political, Race, Religion and Gender using a text classification model.","","978-1-6654-2000-6","10.1109/ICITR54349.2021.9657453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657453","Hate content;Facebook;Sinhala language;Convolutional Neural Network;Natural Language Processing;Text classifier model","Analytical models;Social networking (online);Text categorization;Hate speech;Machine learning;Natural language processing;Convolutional neural networks","convolutional neural nets;Internet;language translation;learning (artificial intelligence);natural language processing;social networking (online);text analysis","machine learning-based automated tool;social media;Sri Lanka;Sinhala hate content;English language;content translation;Sinhala phrases;Sinhala texts;convolutional neural network;natural language processing;text preprocessing;hate intensity level;text classification;Sinhala hate speech detection;Facebook posts;Facebook memes;political domain;race domain;religion domain;gender domain","","1","","22","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Using Formants for Human Speech Recognition by Artificial Intelligence","D. V. Gadasin; A. V. Shvedov; K. A. Panteleeva; V. V. Maklachkova","Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia","2023 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO","17 Jul 2023","2023","","","1","7","The global transition to the digital space and digital transformation form the needs and challenges not only in the creation of new digital devices, but also in the translation of business processes (for example, Data Governance), which allows a significant reduction of transaction costs and a significant increase in the volume of economic activity. Today, many large companies from many areas of production use a voice assistant in their services, thus reducing the burden on operators, but the need to talk to a human to solve most problems is still relevant, which means that there is still no ideal, fully satisfying all customer needs conversational artificial intelligence (AI). To improve the quality of service by the robot it is necessary to improve the quality of speech recognition of the interlocutor by the machine, for which neural networks are used. This article discusses the current state of development of neural networks, as well as artificial intelligence, which can be used to recognize human speech using a formant approach.","2832-0514","979-8-3503-4831-6","10.1109/SYNCHROINFO57872.2023.10178431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178431","Formant;formant approach;artificial intelligence;neural networks;artificial neural networks;AI;ANN","Training;Text recognition;Speech recognition;White noise;Mathematical models;Telecommunications;Arrays","artificial intelligence;neural nets;quality of service;service robots;speech processing;speech recognition","business processes;conversational artificial intelligence;digital devices;digital transformation form;economic activity;formant approach;global transition;human speech recognition;neural networks;quality of service;robot;transaction costs;voice assistant","","","","14","IEEE","17 Jul 2023","","","IEEE","IEEE Conferences"
"Adversarial Continual Learning to Transfer Self-Supervised Speech Representations for Voice Pathology Detection","D. Park; Y. Yu; D. Katabi; H. K. Kim","AI Graduate School and School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; AI Graduate School and School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea; Department of Electrical Engineering and Computer Science, the Lab for Computer Science and Artificial Intelligence, MIT Center for Wireless Networks and Mobile Computing MIT, Cambridge, MA, USA; AI Graduate School and School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju, South Korea","IEEE Signal Processing Letters","28 Jul 2023","2023","30","","932","936","In recent years, voice pathology detection (VPD) has received considerable attention because of the increasing risk of voice problems. Several methods, such as support vector machine and convolutional neural network-based models, achieve good VPD performance. To further improve the performance, we use a self-supervised pretrained model as feature representation instead of explicit speech features. When the pretrained model is fine-tuned for VPD, an overfitting problem occurs due to a domain shift from conversation speech to the VPD task. To mitigate this problem, we propose an adversarial task adaptive pretraining (A-TAPT) approach by incorporating adversarial regularization during the continual learning process. Experiments on VPD using the Saarbrucken Voice Database show that the proposed A-TAPT improves the unweighted average recall (UAR) by an absolute increase of 12.36% and 15.38% compared with SVM and ResNet50, respectively. It is also shown that the proposed A-TAPT achieves a UAR that is 2.77% higher than that of conventional TAPT learning.","1558-2361","","10.1109/LSP.2023.3298532","Institute of Information & Communications Technology Planning & Evaluation; Korea Government; Localization Technology Development on Spoken Language Synthesis and Translation of OTT Media Contents(grant numbers:2022-0-00963); GIST-MIT Research Collaboration; GIST in 2023; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192328","Adversarial regularization;continual learning;fine-tuning;self-supervised pretrained model;voice pathology detection;wav2vec 2.0","Task analysis;Pathology;Adaptation models;Feature extraction;Context modeling;Data models;Support vector machines","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;learning (artificial intelligence);signal classification;speech processing;supervised learning;support vector machines","A-TAPT;adversarial continual learning;adversarial regularization;adversarial task adaptive;continual learning process;conventional TAPT learning;conversation speech;convolutional neural network-based models;explicit speech features;feature representation;good VPD performance;increasing risk;overfitting problem;Saarbrucken Voice Database show;self-supervised pretrained model;self-supervised speech representations;support vector machine;voice pathology detection;voice problems;VPD task","","","","39","IEEE","24 Jul 2023","","","IEEE","IEEE Journals"
"Attention Analysis and Calibration for Transformer in Natural Language Generation","Y. Lu; J. Zhang; J. Zeng; S. Wu; C. Zong","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China; Department Tencent Cloud Xiaowei, Beijing, China; Department Tencent Cloud Xiaowei, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences (CAS), Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Jun 2022","2022","30","","1927","1938","Attention mechanism has been ubiquitous in neural machine translation by dynamically selecting relevant contexts for different translations. Apart from performance gains, attention weights assigned to input tokens are often utilized to explain that high-attention tokens contribute more to the prediction. However, many works question whether this assumption holds in text classification by manually manipulating attention weights and observing decision flips. This article extends this question to Transformer-based neural machine translation, which heavily relies on cross-lingual attention to produce accurate translations but is relatively understudied in this context. We first design a mask perturbation model which automatically assesses each input‚Äôs contribution to model outputs. We then test whether the token contributing most to the current translation receives the highest attention weight. We find that it sometimes does not, which closely depends on the entropy of attention weights, the syntactic role of the current generation, and language pairs. We also rethink the discrepancy between attention weights and word alignments from the view of unreliable attention weights. Our observations further motivate us to calibrate the cross-lingual multi-head attention by attaching more attention to indispensable tokens, whose removal leads to a dramatic performance drop. Empirical experiments on different-scale translation tasks and text summarization tasks demonstrate that our calibration methods significantly outperform strong baselines.","2329-9304","","10.1109/TASLP.2022.3180678","National Natural Science Foundation of China(grant numbers:62122088,U1836221,62006224); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792313","Attention mechanism;interpretability;Transformer;attention calibration","Graphics;Magnetization;Symbols;Magnetostatics;Speech processing;Permeability;Image color analysis","calibration;language translation;natural language processing;natural languages;text analysis","current translation;highest attention weight;unreliable attention weights;cross-lingual multihead attention;different-scale translation tasks;attention mechanism;different translations;high-attention tokens;Transformer-based neural machine translation;cross-lingual attention;accurate translations;token contributing","","1","","57","IEEE","9 Jun 2022","","","IEEE","IEEE Journals"
"Tunisian Dialect-Modern Standard Arabic Bilingual Lexicon","M. A. Sghaier; M. Zrigui","Department of Computer Science, LaTICE Lab, Monastir, Tunisia; Department of Computer Science, LaTICE Lab, Monastir, Tunisia","2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)","12 Mar 2018","2017","","","973","979","The present paper aims to present a Tunisian Dialect (TD) to Modern Standard Arabic (MSA) bilingual lexicon. This type of resource is valuable and essential for language processing and more particularly for machine translation systems builders. To do this, we firstly collected data from different sources such as social networks (Facebook and Youtube), blogs, forums and other dialectal content sources available on the web. Secondly, we performed a grammatical categorization to the extracted words from this data. Thirdly, we organized them according to an ISO international standard i.e. Lexical Markup Framework (LMF) for Natural Language Processing (NLP) lexicons. Finally, we verified their translation with multiple Arabic dictionaries. Our proposed TD to MSA lexicon provides almost 14K of clean and manually approved words.","2161-5330","978-1-5386-3581-0","10.1109/AICCSA.2017.125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308395","Natural Language Processing;Translation;Lexicon;Lexical Markup Framework;Tunisian Dialect;Modern Standard Arabic","Standards;Tools;Natural language processing;Dictionaries;Pragmatics;Speech;Blogs","dictionaries;language translation;natural language processing","machine translation systems builders;multiple Arabic dictionaries;MSA lexicon;ISO international standard;Tunisian dialect;modern standard Arabic bilingual lexicon;TDA;natural language processing lexicons;NLP lexicons;Lexical Markup Framework;LMF","","1","","12","IEEE","12 Mar 2018","","","IEEE","IEEE Conferences"
"REVEAL THIS: retrieval of multimedia multilingual content for the home user in an information society","S. Piperidis; H. Papageorgiou","IRIS Artemidos 6 & Epidavrou, Institute of Language and Speech Processing, Athens, Greece; IRIS Artemidos 6 & Epidavrou, Institute of Language and Speech Processing, Athens, Greece","The 2nd European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology, 2005. EWIMT 2005. (Ref. No. 2005/11099)","16 Jan 2006","2005","","","461","465","REVEAL THIS addresses a basic need underlying content organisation, filtering, consumption and enjoyment by developing content programming systems that will help European citizens keep up with the explosion of digital content scattered over different platforms (radio, TV, World Wide Web, etc), different media (speech, text, image, video) and different languages. REVEAL THIS aims at developing content programming technology able to capture, semantically index, categorize and cross-link multiplatform, multimedia and multilingual digital content, as well as provide the system user with semantic search, retrieval, summarization and translation functionalities. The innovative aspects of the project spring out of the main scientific and technological challenges: 1) semantic enrichment of multilingual multimedia content with topic, entity and fact information relevant to user profiles; 2) development of suitable cross-language, cross-media representations; and 3) deployment of the above in building search, retrieval, classification and summarization capabilities. We exploit explicit cross-media links and implicit links uncovered by methods such as latent semantic analysis and kernel canonical correlation analysis. Adequate cross-language capabilities (cross-language information retrieval, categorization and machine translation of indicative summaries) will be provided by the latest statistical machine translation technology.","0537-9989","0-86341-595-4","10.1049/ic.2005.0775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1576057","","","content-based retrieval;language translation;multimedia systems;statistical analysis;text analysis","multimedia multilingual content retrieval;information society;content programming system;cross-link multiplatform;multilingual digital content;semantic search;semantic retrieval;semantic summarization;translation functionality;cross-media representation;latent semantic analysis;kernel canonical correlation analysis;cross-language information retrieval;statistical machine translation technology","","","","","","16 Jan 2006","","","IET","IET Conferences"
"Research and Development of English Intelligent Proofreading System based on Modern Intelligent Recognition Technology","L. Jing","Guangdong Business and Technology University, Zhaoqing, China","2022 International Conference on Information System, Computing and Educational Technology (ICISCET)","3 Oct 2022","2022","","","237","241","With the continuous development of modern information technology, English learning methods and approaches are gradually diversified, especially the English electronic dictionary, which makes English learning easier. The quality of translation is an important criterion to measure the results of English interpret, which is mainly characterized by errors in translation characters or spelling, inconsistent expression, lexical and grammatical errors, etc. At present, the most advanced machine translation method, the so-called phrase-based model, is only limited to the mapping of small text blocks. Because language information is not explicitly used, it may be morphological, syntactic or semantic. This paper designs an English machine translation model based on modern intelligent recognition technology. Through the direct maximum entropy model, we can get the best combination of different features in complex English sentences, eliminate some structural ambiguities and improve the accuracy of English machine translation. Based on modern intelligent identification technology, this paper studies the development of English intelligent proofreading system, grasps the user's behavior data through behavior log, and optimizes the system. The software part of the system PP. adopts the computer intelligent proofreading method based on the improved phrase translation model to find the correct words to replace the words to be proofread and realize the intelligent proofreading of English interpret.","","978-1-6654-6044-6","10.1109/ICISCET56785.2022.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898813","Modern intelligent recognition technology;English intelligence;proofreading system","Text recognition;Computational modeling;Measurement uncertainty;Speech recognition;Syntactics;Software;Behavioral sciences","dictionaries;entropy;language translation;maximum entropy methods","modern intelligent recognition technology;direct maximum entropy model;complex English sentences;modern intelligent identification technology;English intelligent proofreading system;computer intelligent proofreading method;improved phrase translation model;continuous development;modern information technology;English electronic dictionary;English learning;English interpret;inconsistent expression;lexical errors;grammatical errors;advanced machine translation method;phrase-based model;English machine translation model","","","","15","IEEE","3 Oct 2022","","","IEEE","IEEE Conferences"
"Normalization Methods for Multiple Sources of Data","K. S. Prathyusha; B. E. Reddy","Department of Computer Science and Engineering, JNTUA College of Engineering, Anantapur, India; Department of Computer Science and Engineering, JNTUA College of Engineering, Anantapur, India","2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS)","26 May 2021","2021","","","1013","1019","Data is important part of any kind of applications, specifically in World Wide Web and Electronic documents. As the ICT (Information & Communication Technology) industry intensifies, the need for managing real data processing exceeds. In the other sense, with multiplying of mission critical applications in Artificial Intelligence like machine translation and text to speech conversion, speech to text conversion, data supplication etc. which are anxious with raw text data, data processing has become one of vital part in the research fields Normalization is the process of converting a series of real-valued numeric attributes into a 0 to 1 scale. In machine learning, data normalization is used to enable model testing less reactive to feature size. Data Normalization is the job of discovering these specific words and transferring them to the subsequent series of words that is defined as normalized words. A novel characteristic-based mining technique is used in similarity propagation framework and point wise mutual information pruning is proposed to extract characteristics systematically and precipitously. Primarily, characteristics named entity sets are acquired from the corpus corresponding to predefined syntactical guidelines. Then, the characteristics named entity sets are represented by frequency counting word vectors and calculating the similarity of characteristics words to cluster the characteristics named entity sets.","","978-1-6654-1272-8","10.1109/ICICCS51141.2021.9432142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432142","Text Processing;Term Frequency - Inverse Document Frequency (TF-IDF);Data Normalization;Text Duplication","Text recognition;Process control;Numerical models;Data mining;Web sites;Speech processing;Frequency control","data fusion;data mining;language translation;learning (artificial intelligence);natural language processing;text analysis","normalized words;similarity propagation framework;point wise mutual information pruning;normalization methods;ICT industry;information and communication technology;data processing;machine translation;speech conversion;text conversion;raw text data;machine learning;data normalization;data supplication;characteristic-based mining technique;multiple data sources","","","","15","IEEE","26 May 2021","","","IEEE","IEEE Conferences"
"Virtual Assistant in Native Language","P. M. Dias; K. Jayakody","Department of Electronics, Wayamba University of Sri Lanka, Kuliyapitiya, Sri Lanka; Department of Computer & Information Systems, Wayamba University of Sri Lanka, Kuliyapitiya, Sri Lanka","2020 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)","22 Jun 2021","2020","","","16","18","In the modern area of fast-moving technology we can do things that we never thought we couldn't do before and automation has taken over everything in day-to-day life. One such creation is the virtual assistant. It has become a boon for everyone in this new era of the 21st century. The improvement has gone up to the capabilities of becoming a personal companion to humans. We can ask questions from machines and can interact with machines using this technology of virtual assistance. This technology spread rapidly in smartphones, laptops, computers, etc. Some iconic virtual assistants are Siri, Google Assistant, Cortana, and Alexa. Voice recognition, speech identification, and the relevant reaction is the basis for virtual assistance. Some of the issues regarding the available virtual assistances are the incompatibility with certain languages. Sinhala is one such language at the moment not included in a virtual assistance environment. The proposed project is designed to associate some Sinhala commands with their corresponding responses to guide a Sinhala user. Furthermore, for global reach, the project will be deployed on a cloud basis for the public to reach the global Sinhala language users.","","978-1-6654-1464-7","10.1109/AGERS51788.2020.9452751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452751","Virtual Assistant;Sinhala;Cloud Deployment;Speech recognition;Translation","Computers;Tongue;Portable computers;Virtual assistants;Conferences;Speech recognition;Geoscience","cloud computing;mobile computing;natural language processing;speech recognition;virtual reality","iconic virtual assistants;Google Assistant;virtual assistance environment;native language;personal companion;Cortana;Alexa;Siri;voice recognition;speech identification;Sinhala language users;smartphones;laptops;cloud basis","","3","","9","IEEE","22 Jun 2021","","","IEEE","IEEE Conferences"
"Generating human-like discussion by paraphrasing a translation by the AIWolf protocol using werewolf BBS logs","H. Nakamura; D. Katagami; F. Toriumi; H. Osawa; M. Inaba; K. Shinoda; Y. Kano","Dept. of Applied Computer Science, Tokyo Polytechnic University, Kanagawa, Japan; Dept. of Applied Computer Science, Tokyo Polytechnic University, Kanagawa, Japan; Graduate School of Engineering, The University of Tokyo, Tokyo, Japan; Faculty of Engineering, Information and Systems University of Tsukuba, Tsukuba-shi, Ibaraki, Japan; Graduate School of Information Science, Hiroshima City University, Hiroshima-shi, Hiroshima, Japan; Graduate School of Information Systems, The University of Electro-Communications, Chofu-shi, Tokyo, Japan; Dept. of Behavior Informatics, Shizuoka university, Hamamatsu-shi, Shizuoka, Japan","2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","24 Aug 2017","2017","","","1","6","‚ÄúAre you a werewolf?‚Äù is one of the most popular communication games and is played globally. The AIWolf Project developed an agent, named ‚Äúthe AIWolf,‚Äù that can play ‚ÄúAre you a werewolf?‚Äù. An AIWolf utters its thoughts using an AIWolf Protocol. As it is difficult for humans to understand the AIWolf Protocol, translation into natural language is required when human players are involved. However, the conventional method of translation uses a word-to-word method, creating the impression that the utterances have been generated by a machine. This study aimed make the utterances of AIWolf sound more human. The authors set the target that a human player would be unable to distinguish human speech from that generated by AIWolves (the Turing test). The authors define the situation as the maximum value of humanity. The output of translated AIWolf Protocol was paraphrased using data from Werewolf BBS Logs. This study considers making the utterances of AIWolf sound more human using Werewolf BBS Logs and a possibility assignment equation with fuzzy sets. In this paper, an experiment was conducted to confirm whether paraphrasing the utterances of AIWolf using Werewolf BBS Logs for human-like speech is useful or not. It was shown that the experimental method produced slightly more human-like speech than the conventional method.","1558-4739","978-1-5090-6034-4","10.1109/FUZZ-IEEE.2017.8015538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8015538","Are you a werewolf?;Werewolf BBS;AIWolf;Paraphrase;Fuzzy Sets;Possibility assignment equation","Protocols;Games;Speech;Natural languages;Fuzzy sets;Artificial intelligence;Servers","computer games;fuzzy set theory;natural language processing","human-like discussion generation;translation paraphrasing;AIWolf protocol;Werewolf BBS logs;communication games;AIWolf Project;Are-you-a-werewolf?;natural language;Turing test;AIWolf sound utterances;possibility assignment equation;fuzzy sets;human-like speech","","","1","19","IEEE","24 Aug 2017","","","IEEE","IEEE Conferences"
"Towards a Ubiquitous Language Resource Management System","M. S. T≈°oeu; S. Maxaku; T. Chemvura; N. Ramchunder; Z. Mabuza","Department of Electrical Engineering, University of Cape Town, P.Bag X3, Rondebosch, Cape Town, South Africa; Department of Electrical Engineering, University of Cape Town, P.Bag X3, Rondebosch, Cape Town, South Africa; Department of Electrical Engineering, University of Cape Town, P.Bag X3, Rondebosch, Cape Town, South Africa; Department of Electrical Engineering, University of Cape Town, P.Bag X3, Rondebosch, Cape Town, South Africa; Department of Electrical Engineering, University of Cape Town, P.Bag X3, Rondebosch, Cape Town, South Africa","2019 IEEE AFRICON","7 Jul 2020","2019","","","1","6","Natural Language Processing has a strong need for large scale quality speech, sign and text resources. The problem in developing countries in African and globally, is the lack of such resources. Significant progress has been made by research groups in South Africa under the coordination Language Resource Management Agency, to collect language resources for the 11 official languages of South Africa. Existing limitations are that resources have been collected manually, in a unidirectional manner that only benefits research institutions and not the wider community that needs them the most and the data is statically pre-packaged and not dynamic. In this paper we detail the design and initial implementations of a ubiquitous language resource management system. The system is aimed at supporting manual collection, crowdsourcing and distribution of language resources and to provide a dynamic resource management system that allows submission, acquisition and quality auditing of languages resources based on user specifications as opposed to pre-packaged data formats. We present initial implementations of the server side and mobile application clients and outline future developments and system integration.","2153-0033","978-1-7281-3289-1","10.1109/AFRICON46755.2019.9134045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134045","machine language processing;language resources;client server systems;crowd sourcing;translation, automatic speech recognition;data warehousing;mobile applications development","Assistive technology;Speech recognition;Speech processing;Resource management;Tools;Mobile applications;Machine learning","mobile computing;natural language processing","ubiquitous language resource management system;natural language processing;South Africa;dynamic resource management system;Language Resource Management Agency;user specifications;mobile application clients","","","","","IEEE","7 Jul 2020","","","IEEE","IEEE Conferences"
"A deep-learning approach to translate between brain structure and functional connectivity","V. D. Calhoun; M. F. Amin; D. Hjelm; E. Damaraju; S. M. Plis",The Mind Research Network; The Mind Research Network; The Mind Research Network; The Mind Research Network; The Mind Research Network,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","6155","6159","While the majority of exploratory approaches search for correlations among features of different modalities, indirect/nonlinear relations between structure and function have not yet been fully investigated. In this work, we employ a neural machine translation model [1] to relate two modalities: structural MRI (sMRI) spatial components and functional MRI (fMRI) brain states estimated using a dynamic connectivity model. We consider each of the modalities as different ‚Äúlanguages‚Äù of the same brain and fit a translation model to estimate a model for how structure influences function. Results identify multiple aligned aspects of brain structure and functional brain states showing significantly more or less alignment in the patient group as well as interesting links to other variables such as cognitive scores and symptom assessments. Our novel approach provides a new perspective on combining brain structure and function by incorporating indirect/nonlinear effects and enabling the algorithm to learn the interplay between structural and the functional networks.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953339","multimodal fusion;deep learning;psychosis;schizophrenia","Brain modeling;Imaging;Context;Artificial neural networks;Machine learning;Feature extraction","biomedical MRI;brain;language translation;learning (artificial intelligence)","symptom assessments;cognitive scores;brain structure;dynamic connectivity model;fMRI brain states;functional MRI brain states;sMRI spatial components;structural MRI spatial components;neural machine translation model;indirect-nonlinear relations","","3","","26","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Natural Language Understanding for Simultaneous Conference Interpretation","M. Farghaly; W. Ahmed; N. Shorim; A. AbdelRaouf; S. Dawood","Faculty Of Computer Science, Misr International University, Cairo, Egypt; Faculty Of Computer Science, Misr International University, Cairo, Egypt; Faculty Of Computer Science, Misr International University, Cairo, Egypt; Faculty Of Computer Science, Misr International University, Cairo, Egypt; Department of Al Alsun, Faculty of Al Alsun and Mass Communication, Misr International University, Cairo, Egypt","2019 14th International Conference on Computer Engineering and Systems (ICCES)","16 Apr 2020","2019","","","342","347","Conference interpretation is an active area of linguistics with growing challenges in technological integration. Despite advancement in information technology, simultaneous interpreters have not yet been provided with adequate tools to bring down the stress level that accompanies their profession. The booth setting and the way they perform have not been changed a lot over the years. Although a number of computer approaches have been presented to make the task of conference interpreters less challenging, most of them fail to meet their actual needs. Some of those approaches add to the pressure that interpreters are already under as they require human input, while others are restricted to certain languages. This paper proposes a new approach that makes use of automatic speech recognition (ASR) combined with a cloud-based machine translation (MT) that transcribes spoken words and provides in-depth translation in a contextual manner through the use of a compiled glossary. The proposed approach provides for the first time an instantaneous transcription of a speech, a domain detection through a part-of-speech tagger, and an adequate translation of the terminology used. Our approach has been tested in terms of transcription accuracy, domain extraction, and terminology identification and retrieval using English and Arabic speeches that cover different domains.","","978-1-7281-5260-8","10.1109/ICCES48960.2019.9068179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068179","Simultaneous interpreting;Natural Language Understanding (NLU);computer-assisted interpreting;named-entity-recognition;context extraction;part-of-speech tagger","Tools;Natural language processing;Automatic speech recognition;Dictionaries","language translation;natural language processing;speech recognition","simultaneous interpreters;automatic speech recognition;cloud-based machine translation;natural language understanding;simultaneous conference interpretation;information technology;ASR","","","","19","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Gesture2Vec: Clustering Gestures using Representation Learning Methods for Co-speech Gesture Generation","P. J. Yazdian; M. Chen; A. Lim","Simon Fraser University, Burnaby, Canada; Simon Fraser University, Burnaby, Canada; Simon Fraser University, Burnaby, Canada","2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","26 Dec 2022","2022","","","3100","3107","Co-speech gestures are a principal component in conveying messages and enhancing interaction experiences between humans and critical ingredients in human-agent interaction, including virtual agents and robots. Existing machine learning approaches have yielded only marginal success in learning speech-to-motion at the frame level. Current methods generate repetitive gesture sequences that lack appropriateness with respect to the speech context. To tackle this challenge, we take inspiration from successes in natural language processing on context and long-term dependencies, and propose a new framework that views text-to-gesture as machine translation, where gestures are words in another (non-verbal) language. We propose a vector-quantized variational autoencoder structure as well as training techniques to learn a rigorous representation of gesture sequences. We then translate input text into a discrete sequence of associated gesture chunks in the learned gesture space. Ultimately, we use translated gesture tokens from the input text as an input to the autoencoder's decoder to produce gesture sequences. Subjective and objective evaluations confirm the success of our approach in terms of appropriateness, human-likeness, and diversity. We also introduce new objective metrics using the quantized gesture representation.","2153-0866","978-1-6654-7927-1","10.1109/IROS47612.2022.9981117","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN/06908-2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981117","","Representation learning;Training;Vocabulary;Vector quantization;Semantics;Clustering algorithms;Extraterrestrial measurements","gesture recognition;human computer interaction;human-robot interaction;language translation;learning (artificial intelligence);natural language processing","associated gesture chunks;Co-speech Gesture Generation;Co-speech gestures;critical ingredients;discrete sequence;enhancing interaction experiences;gesture tokens;gesture2vec;human-agent interaction;human-likeness;input text;learned gesture space;machine learning approaches;machine translation;marginal success;natural language processing;principal component;quantized gesture representation;repetitive gesture sequences;representation learning methods;rigorous representation;speech context;speech-to-motion;text-to-gesture;vector-quantized variational autoencoder structure;virtual agents","","3","","72","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Which Apple Keeps Which Doctor Away? Colorful Word Representations With Visual Oracles","Z. Zhang; H. Yu; H. Zhao; M. Utiyama","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; National Institute of Information and Communications Technology (NICT), Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","29 Dec 2021","2022","30","","49","59","Recent pre-trained language models (PrLMs) offer a new performant method of contextualized word representations by leveraging the sequence-level context for modeling. Although the PrLMs generally provide more effective contextualized word representations than non-contextualized models, they are still subject to a sequence of text contexts without diverse hints from multimodality. This paper thus proposes a visual representation method to explicitly enhance conventional word embedding with multiple-aspect senses from visual guidance. In detail, we build a small-scale word-image dictionary from a multimodal seed dataset where each word corresponds to diverse related images. Experiments on 12 natural language understanding and machine translation tasks further verify the effectiveness and the generalization capability of the proposed approach. Analysis shows that our method with visual guidance pays more attention to content words, improves the representation diversity, and is potentially beneficial for enhancing the accuracy of disambiguation.","2329-9304","","10.1109/TASLP.2021.3130972","National Natural Science Foundation of China(grant numbers:U1836222,61733011); Chinese National Key Laboratory of Science and Technology on Information System Security; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627795","Multimodal learning;natural language processing;pre-trained models;vision-language modeling;word representations","Task analysis;Visualization;Dictionaries;Context modeling;Machine translation;Image representation;Speech processing","language translation;natural language processing;natural languages;text analysis","conventional word;multiple-aspect senses;visual guidance;word-image dictionary;multimodal seed dataset;diverse related images;12 natural language understanding;machine translation tasks;generalization capability;content words;representation diversity;apple keeps which doctor away;colorful word representations;visual oracles;pre-trained language models;PrLMs;performant method;sequence-level context;effective contextualized word representations;noncontextualized models;diverse hints;visual representation method","","","","53","IEEE","26 Nov 2021","","","IEEE","IEEE Journals"
"Benchmarking performance of massively parallel AI architectures","R. F. DeMara; H. Kitano","Department of EE-Systems, University of Southern California, Los Angeles, CA, USA; NEC Corporation Limited, Japan","[Proceedings 1992] The Fourth Symposium on the Frontiers of Massively Parallel Computation","6 Aug 2002","1992","","","517","520","The authors address the architectural evaluation of massively parallel machines suitable for artificial intelligence (AI). The approach is to identify the impact of specific algorithm features by measuring execution time on a SNAP-1 and a Connection Machine-2 using different knowledge base and machine configurations. Since a wide variety of parallel AI languages and processing architectures are in use, the authors developed a portable benchmark set for Parallel AI Computational Efficiency (PACE). PACE provides a representative set of processing workloads, knowledge base topologies, and performance indices. The authors also analyze speedup and scalability of fundamental AI operations in terms of the massively parallel paradigm.<>","","0-8186-2772-7","10.1109/FMPC.1992.234865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=234865","","Artificial intelligence;Parallel processing;Computer architecture;Time measurement;Speech analysis;Computer science;Topology;Parallel machines;Kernel;Computer languages","artificial intelligence;knowledge based systems;parallel architectures;parallel processing;performance evaluation","benchmarking performance;parallel languages;massively parallel AI architectures;architectural evaluation;execution time;SNAP-1;Connection Machine-2;knowledge base;machine configurations;portable benchmark set;Parallel AI Computational Efficiency;speedup;scalability","","","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"A new collocation extraction method combining multiple association measures","Jian-Fang Lin; Sheng Li; Yuhan Cai","MOE-MS Key Laboratory of NLP & Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of NLP & Speech, Harbin Institute of Technology, Harbin, China; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA","2008 International Conference on Machine Learning and Cybernetics","5 Sep 2008","2008","1","","12","17","As an important linguistic resource, collocation represents a significant relation between words. Automatic collocation extraction is very important for many natural language processing applications, such as word sense disambiguation, machine translation and information retrieval etc. While traditional collocation extraction approaches use only one single statistical measure, they may not be optimal in that they can not take advantage of multiple statistical measures. In this paper, we propose a logistic linear regression model (LLRM) that combines five classical lexical association measures: x2-test, t-test, co-occurrence frequency, log-likelihood ratio and mutual information. Experiments show that our approach leads to a significant performance improvement in comparison with individual basic methods in both precision and recall.","2160-1348","978-1-4244-2095-7","10.1109/ICMLC.2008.4620370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620370","Collocation;Co-occurrence frequency;X2-test;T-test;Mutual information;Log-likelihood ratio","Data mining;Mutual information;Frequency measurement;Machine learning;Cybernetics;Linear regression;Logistics","natural language processing;regression analysis;text analysis","collocation extraction method;multiple association measures;automatic collocation extraction;natural language processing applications;machine translation;information retrieval;logistic linear regression model;log-likelihood ratio","","","","12","IEEE","5 Sep 2008","","","IEEE","IEEE Conferences"
"Lexical classes based stop words categorization for Gujarati language","R. M. Rakholia; J. R. Saini","School of Computer Science, R K University, Rajkot Gujarat-India; Narmada College of Computer Application, Bharuch Gujarat-India","2016 2nd International Conference on Advances in Computing, Communication, & Automation (ICACCA) (Fall)","21 Nov 2016","2016","","","1","5","Stop words elimination is important pre-processing step in Natural Language Processing (NLP) and text mining applications. Stop words removal improves the performance and quality of classifications system. In the context of classification task it is possible to reduce number of dimensions in the term of space by removing most common words which has less significant meaning and irrelevant. But it does not mean stop words removal can improve the performance of all types of applications in the area of NLP, Artificial Intelligence, Text Mining and Machine Translation. In context of Machine Translation (MT) stop words elimination process will lead to loss of accuracy, because each token has specific meaning which will be converted into target language. As on date there is no unique stop words list is available for Gujarati language with its lexical classes (Part-of-Speech Tags) to improve the performance of MT system. This paper present construction and categorization of stop words list for Gujarati language based on its lexical classes (nouns, verbs, adjectives, adverbs, etc.) of Part-of-Speech family. We have prepared 126 raw text documents written in Gujarati language in which each document contained more than 260 tokens. After tokenization process, we got list of 32840 tokens. From the total number of tokens we created list of 1125 unique stop words with its lexical classes by manual inspection and help of linguistic experts. The stop words list and specifically categorization thereof is released herewith for NLP applications, particularly MT systems, in Gujarati language by the research community.","","978-1-5090-3480-2","10.1109/ICACCAF.2016.7749005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749005","Categorization;Gujarati Language;Natural Language Processing (NLP);Part-of-Speech (POS);Stop Words","Natural language processing;Context;Syntactics;Grammar;Pragmatics;Cleaning;Text mining","data mining;language translation;natural language processing;text analysis;word processing","lexical class based stop word categorization;Gujarati language;stop word elimination;natural language processing;NLP;text mining;artificial intelligence;machine translation;language conversion;part-of-speech tags;MT system;raw text documents;tokenization process","","13","","","IEEE","21 Nov 2016","","","IEEE","IEEE Conferences"
"Part of Speech Tagging for Indonesian Language using Bidirectional Long Short-Term Memory","D. Handrata; C. N. Purwanto; F. H. Chandra; J. Santoso; Gunawan","Department of Information Technology, Institut Sains dan Teknologi Terpadu Surabaya, Surabaya, Indonesia; Department of Informatics, Institut Sains dan Teknologi Terpadu Surabaya, Surabaya, Indonesia; Department of Electronics Engineering, Institut Sains dan Teknologi Terpadu Surabaya, Surabaya, Indonesia; Department of Information Technology, Institut Sains dan Teknologi Terpadu Surabaya, Surabaya, Indonesia; Department of Information Technology, Institut Sains dan Teknologi Terpadu Surabaya, Surabaya, Indonesia","2019 1st International Conference on Cybernetics and Intelligent System (ICORIS)","21 Oct 2019","2019","1","","85","88","Part of Speech (POS) is a label to distinguish a word based on its grammatical and morphological form. By providing the POS label, we can get the contextual meaning. This label can be used as contextual features for several computational linguistic research - for example, word sense disambiguation, chunking, machine translation, and sequence classification. Our work is done by using bidirectional long short-term memory to do the part of speech tagging task for Bahasa Indonesia. We use deep learning model for Indonesia language POS tagging because deep learning can achieve excellent performance on it. We could reach 96.92% of F1 Score based on our approach.","","978-1-7281-1474-3","10.1109/ICORIS.2019.8874871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874871","part of speech tagging;Indonesian language;bidirectional long short-term memory;natural language processing;deep learning","Tagging;Computer architecture;Hidden Markov models;Training;Task analysis;Deep learning;Artificial neural networks","computational linguistics;grammars;learning (artificial intelligence);natural language processing;recurrent neural nets","POS label;contextual meaning;contextual features;computational linguistic research;word sense disambiguation;machine translation;sequence classification;bidirectional long short-term memory;deep learning model;Indonesian language;grammatical form;morphological form;part of speech tagging;Bahasa Indonesia;Indonesia language;POS tagging","","2","","22","IEEE","21 Oct 2019","","","IEEE","IEEE Conferences"
"Deep Learning Based Hand Gesture Translation System","S. K. Shareef; I. V. S. L. Haritha; Y. L. Prasanna; G. K. Kumar","Department of Information Technology, MLR Institute of Technology; Department of Information Technology, MLR Institute of Technology; Department of Information Technology, MLR Institute of Technology; Department of Information Technology, MLR Institute of Technology","2021 5th International Conference on Trends in Electronics and Informatics (ICOEI)","21 Jun 2021","2021","","","1531","1534","Sign language plays an important role for the people who have the hearing and speech problems For the Non-verbal communication between the deaf-mute people. For the Kinetics gestures plays a crucial role in our day-to-day life. There are different sign languages that are available Hand Gesture Translation is one among them. The sign language is a collection of different Hand Gesture symbols and Each Hand Gesture symbol have special meaning. It is mostly used by the deaf-mute persons to express their views and thoughts very quickly with the normal people. Main problem with this system is very difficult to translate the symbols and required special training on sign language. To overcome this problem we have implemented a Hand Gesture Translation System it provides an ability to interact with the machine efficiently. It will help the deaf-mute to express their feelings and views more effectively with the normal people. In this paper implemented software prototype that will automatically recognize the hand gestures with an accuracy of 93.4% for gesture Translation which will help the deaf and dumb people to interact easily with the normal people. The main aim of the project is to provide the easy way of communication between the normal people and the deaf-mute through gestures.","","978-1-6654-1571-2","10.1109/ICOEI51242.2021.9452947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452947","Hand Gestures;Deep learning;Machine learning;Pre-Processing Image;Feature Extraction of Image;Image Processing;Deaf-mute","Training;Deep learning;Assistive technology;Prototypes;Gesture recognition;Auditory system;Market research","gesture recognition;handicapped aids;human computer interaction;learning (artificial intelligence);sign language recognition","Hand Gesture Translation System;sign language;hearing;speech problems;Nonverbal communication;deaf-mute people;Kinetics gestures;different sign languages;available Hand Gesture Translation;different Hand Gesture symbols;Hand Gesture symbol;deaf-mute persons;required special training;hand gestures;deaf people;dumb people","","7","","15","IEEE","21 Jun 2021","","","IEEE","IEEE Conferences"
"An end-to-end interpolated Automatic speech recognition system with punctuated transcripts for the Hindi language","S. Joshi; V. Kumar","Machine Learning Engineer, Convin.ai India Research Group, Dehradun, India; Dept. of Computer Science & Eng., Bipin Tripathi Kumaon Institute of Technology, Dwarahat, India","2021 6th International Conference on Computing, Communication and Security (ICCCS)","24 May 2022","2021","","","1","7","The Automatic Speech Recognition System (ASR) produces transcripts that often are misinterpreted and confuses the reader due to lack of context and punctuations. The presence of punctuation in the text improves readability and helps in better cognitive understanding. A wide variety of work has been done on English but Hindi which is the third-largest spoken language in the world, after English and Mandarin, still remains in the shadows. This paper aims to extend the technology to a wider section and introduces an end-to-end system, interpolating an automatic speech recognition system and natural language processing to produce high-quality punctuated transcriptions for the Hindi language. An ASR is implemented using the Kaldi toolkit leveraging the hybrid deep neural networks and the punctuation restoration is done with Bidirectional RNNs with an attention network.","","978-1-6654-4039-4","10.1109/ICCCS51487.2021.9776324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776324","Automatic speech recognition (ASR);Speech-To-Text translation;Kaldi;End-to-End ASR System;Punctuation Prediction;Neural Network;Attention Network","Deep learning;Interpolation;Recurrent neural networks;Computational modeling;Production;Natural language processing;Security","deep learning (artificial intelligence);natural language processing;speech recognition;text analysis","end-to-end system;automatic speech recognition system;Hindi language;punctuation restoration;punctuated transcripts;English but Hindi;high quality punctuated transcription;ASR;cognitive understanding;Mandarin;natural language processing;Kaldi toolkit;Bidirectional RNN","","","","31","IEEE","24 May 2022","","","IEEE","IEEE Conferences"
"Unwritten languages demand attention too! Word discovery with encoder-decoder models","M. Z. Boito; A. B√©rard; A. Villavicencio; L. Besacier","Laboratoire d'Informatique de Grenoble, Univ. Grenoble Alpes (UGA), France; Laboratoire d'Informatique de Grenoble, Univ. Grenoble Alpes (UGA), France; Universidade Federal do Rio Grande do Sul, Porto Alegre, RS, BR; Laboratoire d'Informatique de Grenoble, Univ. Grenoble Alpes (UGA), France","2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","25 Jan 2018","2017","","","458","465","Word discovery is the task of extracting words from un-segmented text. In this paper we examine to what extent neural networks can be applied to this task in a realistic unwritten language scenario, where only small corpora and limited annotations are available. We investigate two scenarios: one with no supervision and another with limited supervision with access to the most frequent words. Obtained results show that it is possible to retrieve at least 27% of the gold standard vocabulary by training an encoder-decoder neural machine translation system with only 5,157 sentences. This result is close to those obtained with a task-specific Bayesian nonparametric model. Moreover, our approach has the advantage of generating translation alignments, which could be used to create a bilingual lexicon. As a future perspective, this approach is also well suited to work directly from speech.","","978-1-5090-4788-8","10.1109/ASRU.2017.8268972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268972","Word Discovery;Computational Language Documentation;Neural Machine Translation;Attention models","Speech;Task analysis;Training;Documentation;Computational modeling;Smoothing methods;Vocabulary","Bayes methods;language translation;neural nets","unwritten languages;word discovery;encoder-decoder models;un-segmented text;gold standard vocabulary;encoder-decoder neural machine translation system;task-specific Bayesian nonparametric model;neural networks;bilingual lexicon","","4","","31","IEEE","25 Jan 2018","","","IEEE","IEEE Conferences"
"Machine Learning at Facebook: Understanding Inference at the Edge","C. -J. Wu; D. Brooks; K. Chen; D. Chen; S. Choudhury; M. Dukhan; K. Hazelwood; E. Isaac; Y. Jia; B. Jia; T. Leyvand; H. Lu; Y. Lu; L. Qiao; B. Reagen; J. Spisak; F. Sun; A. Tulloch; P. Vajda; X. Wang; Y. Wang; B. Wasti; Y. Wu; R. Xian; S. Yoo; P. Zhang","Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook Inc., Menlo Park, CA, USA; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Facebook, Inc.; Seoul National University; Facebook, Inc.","2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)","28 Mar 2019","2019","","","331","344","At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized data-center infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a data-driven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms.","2378-203X","978-1-7281-1444-6","10.1109/HPCA.2019.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675201","Machine learning;Edge Inference","Facebook;Smart phones;Performance evaluation;Graphics processing units;Optimization;Hardware;Machine learning","augmented reality;computer centres;inference mechanisms;language translation;learning (artificial intelligence);object detection;social networking (online)","text translations;machine learning models;customized data-center infrastructure;Facebook;machine learning inference;user experience;reduced latency;inference time;deep learning;edge platforms;ranking posts;content understanding;object detection;augmented reality;virtual reality;speech","","202","2","66","IEEE","28 Mar 2019","","","IEEE","IEEE Conferences"
"End-to-end speech topic classification based on pre-trained model Wavlm","T. Cao; L. He; F. Niu","School of Information Science and Engineering, Xinjiang University, Urumqi, China; School of Information Science and Engineering, Xinjiang University, Urumqi, China; School of Information Science and Engineering, Xinjiang University, Urumqi, China","2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP)","8 Feb 2023","2022","","","369","373","Speech topic classification (STC) is the task of automatically classifying audio segments into predefined categories, and has an increasingly wide application in the field of speech indexing, retrieval, surveillance, etc. Currently, the typical STC is a pipeline method consisting of automatic speech recognition (ASR), possible machine translation (MT), and text classification (TC). Although each component in the pipeline has a clear function and mature solutions, it suffers from error propagation and scarcity of annotated training data. To solve it, we propose a monolithic network based on pre-trained models to accomplish the speech topic classification task. The end-to-end training strategy based on the unified network structure avoids error propagation. And the pre-trained models reduce the requirements for a large amount of annotated data. Besides, the proposed method can take advantage of the intrinsic semantic feature of the speech for better performance. Our method carried out a series of experiments on the Fisher dataset. Compared with the traditional pipeline method, we also achieved an accuracy of 7 percentage points better than the traditional method without a large number of voice annotation data, so our method has huge advantages.","","979-8-3503-9796-3","10.1109/ISCSLP57327.2022.10037815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10037815","speech topic classification;end-to-end;pre-trained models;Hubert;Wav2vec2.0;Wavlm","Training;Surveillance;Pipelines;Text categorization;Semantics;Training data;Data models","audio signal processing;feature extraction;image classification;learning (artificial intelligence);pattern classification;speech recognition;text analysis","annotated training data;end-to-end speech topic classification;end-to-end training strategy;error propagation;increasingly wide application;possible machine translation;speech indexing;speech topic classification task;text classification;traditional pipeline method;typical STC","","","","27","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Neural Machine Translation for Malay Text Normalization using Synthetic Dataset","M. A. Hakim bin Sazali; N. B. Idris","Department of Artificial Intelligence Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia; Department of Artificial Intelligence Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia","2022 10th International Conference on Information and Communication Technology (ICoICT)","21 Oct 2022","2022","","","386","390","Social media contains valuable information about any individual, political entity, or brand since users posted impromptu and honest opinions which benefit a better data-driven. Social media text is unstructured and contains some issues such as slang, dialect, and short form. Most work addresses the text issue using an approach called text normalization. Text normalization is a preprocessing step to transform noisy words into their standard form and improve the performance of NLP applications, such as Sentiment Analysis, Part-of-speech Tagging, and Name Entity Recognition. However, less work utilizes Neural Machine Translation (NMT) approach to address text problems, specifically for short form and slang. In this work, three different NMT architectures are used to address the text problems. We conducted our experiments by training three different models under the same synthetic dataset. We prepared the dataset by collecting a local Malay news site which contains 46k of sentences and 800k words. The sentences are further processed using rule-based to create a parallel text. We then split the parallel text into training, validation, and test sets. The best model that utilize Transformer architecture achieved 99% accuracy on the validation set which provides an advantage in taking the sentence context during the text normalization process.","","978-1-6654-8165-6","10.1109/ICoICT55009.2022.9914841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914841","Text normalization;Machine Translation;Synthetic Data","Training;Sentiment analysis;Social networking (online);Text recognition;Transforms;Tagging;Transformers","natural language processing;social networking (online);text analysis","Malay text normalization;synthetic dataset;individual entity;political entity;impromptu opinions;honest opinions;social media text;slang;short form;text issue;approach called text normalization;preprocessing step;noisy words;standard form;NLP applications;Part-of-speech Tagging;Entity Recognition;work utilizes Neural Machine Translation approach;text problems;different NMT architectures;local Malay news site;parallel text;utilize Transformer architecture;text normalization process","","","","21","IEEE","21 Oct 2022","","","IEEE","IEEE Conferences"
"Sinhala Part of Speech Tagger using Deep Learning Techniques","M. W. A. R. Sathsarani; T. P. A. B. Thalawaththa; N. K. Galappaththi; J. N. Danthanarayana; A. Gamage","Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","2022 6th International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)","31 Jan 2023","2022","","","1","6","Natural Language Processing (NLP) is a sub-field of Artificial Intelligence (AI) that consists of a collection of computational methods motivated by theory for the automated classification and reflection of human languages. The foundation for many sophisticated applications of NLP, including named entity recognition, sentiment analysis, machine translation, in-formation retrieval, and information processing, is laid by Part of Speech (POS) tagging, which is part of the lexical layer of NLP systems. In contrast to English, French, German, and other languages from the same geographical region, the development of high-accuracy, stable POS taggers for the Sinhala language is still in its early stages. Hence, Sinhala is identified as a low-resource language. The main objective of this research is to create a POS tagger for the Sinhala language to solve this issue. An innovative and novel strategy that has never been used with the Sinhala language has been designed. This approach has been suggested specifically to evaluate the possibility of enhancing the accuracy compared to other methodologies. So, deep learning algorithms have been applied in this study, which has a significant impact on improving tagger performance. First, highly accurate individual classifiers for primary POS tags were implemented, and then they were combined into one composite model. As expected, all individual classifiers and the final composite model have achieved a higher accuracy level. Thus, it demonstrates that the proposed solution using deep learning algorithms outperformed other methods, such as rule-based and stochastic, in terms of accuracy.","","978-1-6654-5699-9","10.1109/CSITSS57437.2022.10026395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026395","Natural Language Processing;NLP;Parts of Speech tagging;POS tagging;Sinhala;Deep Learning;Machine Learning;Artificial Intelligence;Computational Linguistics","Deep learning;Sentiment analysis;Stochastic processes;Speech recognition;Tagging;Reflection;Classification algorithms","language translation;learning (artificial intelligence);natural language processing;natural languages;pattern classification;text analysis","automated classification;deep learning algorithms;deep learning techniques;human languages;information processing;low-resource language;named entity recognition;Natural Language Processing;NLP systems;POS tagger;primary POS tags;reflection;Sinhala language;Sinhala Part;Speech tagger;Speech tagging;stable POS taggers;tagger performance","","","","23","IEEE","31 Jan 2023","","","IEEE","IEEE Conferences"
"Hybrid language processing in the Spoken Language Translator","M. Rayner; D. Carter","SRI International, Cambridge, UK; SRI International, Cambridge, UK","1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","1","","107","110 vol.1","We present an overview of the Spoken Language Translator (SLT) system's hybrid language-processing architecture, focusing on the way in which rule-based and statistical methods are combined to achieve robust and efficient performance within a linguistically motivated framework. In general, we argue that rules are desirable in order to encode domain-independent linguistic constraints and achieve high-quality grammatical output, while corpus-derived statistics are needed if systems are to be efficient and robust; further, that hybrid architectures are superior from the point of view of portability to architectures which only make use of one type of information. We address the topics of ""multi-engine"" strategies for robust translation; robust bottom-up parsing using pruning and grammar specialization; rational development of linguistic rule-sets using balanced domain corpora; and efficient supervised training by interactive disambiguation. All work described is fully implemented in the current version of the SLT-2 system.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.599559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=599559","","Natural languages;Robustness;Milling machines;Statistical analysis;Statistics;Speech;Costs;Investments;Personnel;Engines","speech recognition;speech synthesis;language translation;natural language interfaces;statistical analysis;computational linguistics;knowledge based systems;performance evaluation;grammars;learning (artificial intelligence)","hybrid language processing;Spoken Language Translator;rule-based methods;statistical methods;performance;domain-independent linguistic constraints;high-quality grammatical output;corpus-derived statistics;portability;multi-engine strategies;robust translation;robust bottom-up parsing;pruning;grammar specialization;linguistic rule-sets;balanced domain corpora;supervised training;interactive disambiguation;SLT-2 system","","4","28","31","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Audio to Indian and American Sign Language Converter using Machine Translation and NLP Technique","A. Dixit; S. Sharma; P. Dhamini Rao; V. Reddy; M. Janaki; R. Thirumalaivasan; M. Monica Subashini","School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India","2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)","18 Oct 2022","2022","","","874","879","People with hearing loss use Sign Language as their mother tongue communication. Unlike acoustic hearing, sign language is a visual language that uses body language and physical communication to communicate effectively thoughts. It usually consists of hand gestures and facial expressions. Generally, communicating with different disabled people seems very difficult. This is because it takes a long time to learn the language, not just the language, people who are not disabled but also people who are. Establishment to communicate in such cases, both parties need to know sign language, or they use a human translator to make communication possible. Information Technologies with their own modern methods such as artificial intelligence, cloud computing has an amazing role to play in improving communication people with speech impairments and ordinary people. Sign language recognition can be done in two ways, glove-based or vision-based recognition. The solution proposed in this paper will produce software that takes over input in the form of speech and indicates appropriate Sign Language. The software is developed in Python platform to convert speech to Indian and American sign languages (ISL and ASL) which provides hearing impairment assistant. This software can be useful in many areas, such as in educational institutes, hospitals, police stations, and for general everyday life conversation.","","978-1-6654-1005-2","10.1109/ICICICT54557.2022.9917614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917614","NLP;SLR;GUI;style;styling;insert","Visualization;Tongue;Law enforcement;Hospitals;Gesture recognition;Auditory system;Oral communication","cloud computing;feature extraction;gesture recognition;handicapped aids;hearing;human computer interaction;language translation;natural language processing;sign language recognition;visual languages","different disabled people;communication people;ordinary people;Sign language recognition;appropriate Sign Language;indian sign language converter;american sign language converter;mother tongue communication;visual language;body language;physical communication","","3","","15","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Dynamics of tongue gestures extracted automatically from ultrasound","J. Berry; I. Fasel","University of Arizona Tucson, Tucson, AZ, USA; University of Arizona Tucson, Tucson, AZ, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","557","560","We describe a system for automatically extracting dynamics of tongue gestures from ultrasound images of the tongue using translational deep belief networks (tDBNs). In tDBNs, a joint model of the input and output vectors are learned during a generative pretraining stage, and then a translation step is used to transform input-only vectors into this joint representation. A final fine-tuning stage is then used to reconstruct the desired outputs given input vectors. We show that this technique dramatically improves performance on segmenting ultrasound image sequences of continuous speech into individual consonant gestures compared with the original DBN method of as well as alternative methods using PCA and support vector machines.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5946464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946464","Deep Belief Networks;Ultrasound","Tongue;Ultrasonic imaging;Shape;Speech;Training;Support vector machines;Feature extraction","belief networks;gesture recognition;image sequences;medical image processing;principal component analysis;support vector machines","tongue gesture extraction;translational deep belief network;PCA;DBN method;support vector machine;tDBN;ultrasound image sequence","","6","","13","IEEE","11 Jul 2011","","","IEEE","IEEE Conferences"
"Robust Method to Convert HIRAGANA Sequences into Japanese Text","T. Yamaguchi; K. Hara; I. Suzuki","Yamagata University, Yamagata City, Japan; Yamagata University, Yamagata City, Japan; Nagasaki University, Nagasaki City, Japan","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","6058","6060","We apply an attention-based sequence-to-sequence model for the Japanese HIRAGANA-KANJI conversion task in spontaneous speech transcripts. Experimental results indicate that short HIRAGANA sequences containing speech-specific errors can be converted into error-free HIRAGANA-KANJI mixed Japanese text.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671804","Japanese HIRAGANA-KANJI conversion;speech-specific errors;neural machine translation;seq2seq","Conferences;Big Data;Task analysis","natural language processing;speech recognition;text analysis","robust method;attention-based sequence-to-sequence model;Japanese HIRAGANA-KANJI conversion task;spontaneous speech transcripts;short HIRAGANA sequences;speech-specific errors;error-free HIRAGANA-KANJI mixed Japanese text","","","","8","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Limits on the Application of Frequency-Based Language Models to OCR","R. Smith","Google Inc., Mountain View, USA","2011 International Conference on Document Analysis and Recognition","3 Nov 2011","2011","","","538","542","Although large language models are used in speech recognition and machine translation applications, OCR systems are ""far behind""¬ù in their use of language models. The reason for this is not the laggardness of the OCR community, but the fact that, at high accuracies, a frequency-based language model can do more damage than good, unless carefully applied. This paper presents an analysis of this discrepancy with the help of the Google Books n-gram Corpus, and concludes that noisy-channel models that closely model the underlying classifier and segmentation errors are required.","2379-2140","978-0-7695-4520-2","10.1109/ICDAR.2011.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065369","OCR;Language Models","Text analysis","language translation;natural language processing;speech recognition","frequency based language models;OCR;speech recognition;machine translation;Google Books n-gram corpus;noisy channel models","","22","","14","IEEE","3 Nov 2011","","","IEEE","IEEE Conferences"
"Performance of Three Slim Variants of The Long Short-Term Memory (LSTM) Layer","D. Kent; F. Salem","Wireless and Video Communications Lab., Michigan State University, East Lansing, MI, USA; Wireless and Video Communications Lab., Michigan State University, East Lansing, MI, USA","2019 IEEE 62nd International Midwest Symposium on Circuits and Systems (MWSCAS)","31 Oct 2019","2019","","","307","310","The Long Short-Term Memory (LSTM) layer is an important advancement in the field of neural networks and machine learning, allowing for effective training and impressive inference performance. LSTM-based neural networks have been successfully employed in various applications such as speech processing and language translation. The LSTM layer can be simplified by removing certain components, potentially speeding up training and runtime with limited change in performance. In particular, several recently introduced variants, called Slim LSTMs, have shown success in initial experiments to support this view. In this paper, we perform computational analysis of the validation accuracy of a convolutional plus recurrent neural network architecture designed to analyze sentiment, using comparatively the standard LSTM and three Slim LSTM layers. We found that some realizations of the Slim LSTM layers can potentially perform as well as the standard LSTM layer for our considered architecture targeted at sentiment analysis.","1558-3899","978-1-7281-2788-0","10.1109/MWSCAS.2019.8885035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885035","","Logic gates;Training;Computer architecture;Recurrent neural networks;Standards;Electric breakdown","convolutional neural nets;language translation;learning (artificial intelligence);neural net architecture;recurrent neural nets;sentiment analysis;speech processing","long short-term memory layer;machine learning;LSTM-based neural networks;convolutional plus recurrent neural network architecture;Slim LSTM layers;speech processing;language translation;sentiment analysis;three slim variants performance","","17","","15","IEEE","31 Oct 2019","","","IEEE","IEEE Conferences"
"Sequence to sequence networks for Roman-Urdu to Urdu transliteration","M. Alam; S. ul Hussain","Computer Science Department, NUCES, Islamabad, Pakistan; Computer Science Department, NUCES, Islamabad, Pakistan","2017 International Multi-topic Conference (INMIC)","12 Feb 2018","2017","","","1","7","Neural Machine Translation models have replaced the conventional phrase based statistical translation methods since the former takes a generic, scalable, data-driven approach rather than relying on manual, hand-crafted features. The neural machine translation system is based on one neural network that is composed of two parts, one that is responsible for input language sentence and other part that handles the desired output language sentence. This model based on encoder-decoder architecture also takes as input the distributed representations of the source language which enriches the learnt dependencies and gives a warm start to the network. In this work, we transform Roman-Urdu to Urdu transliteration into sequence to sequence learning problem. To this end, we make the following contributions. We create the first ever parallel corpora of Roman-Urdu to Urdu, create the first ever distributed representation of Roman-Urdu and present the first neural machine translation model that transliterates text from Roman-Urdu to Urdu language. Our model has achieved the state-of-the-art results using BLEU as the evaluation metric. Precisely, our model is able to correctly predict sentences up to length 10 while achieving BLEU score of 48.6 on the test set. We are hopeful that our model and our results shall serve as the baseline for further work in the domain of neural machine translation for Roman-Urdu to Urdu using distributed representation.","","978-1-5386-2303-9","10.1109/INMIC.2017.8289449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8289449","sequence to sequence models;parallel corpora;neural network;natural language processing;deep learning;distributed representation","Decoding;Natural language processing;Neural networks;Machine learning;Predictive models;Speech recognition;Vocabulary","language translation;learning (artificial intelligence);natural language processing;neural nets","Urdu language;Roman-Urdu;sequence networks;Urdu transliteration;Neural Machine Translation models;statistical translation methods;neural machine translation system;neural network;input language sentence;desired output language sentence;BLEU score","","12","","37","IEEE","12 Feb 2018","","","IEEE","IEEE Conferences"
"A Literature Survey on Various Approaches of Word Sense Disambiguation","G. Chandra; S. K. Dwivedi","Computer Science Department BBAU, Central University, Lucknow; Computer Science Department BBAU, Central University, Lucknow","2014 2nd International Symposium on Computational and Business Intelligence","11 Jun 2015","2014","","","106","109","Word Sense Disambiguation (WSD) is the process of selecting the correct sense for a word in a context. WSD has become a growing research area in the field of Natural Language Processing (NLP). Over the decades, lot of studies had been carried out to suggest different approaches for WSD process. A break-through in this field would have a significant impact on many relevant web-based applications, such as information retrieval (IR), information extraction etc. This paper describes various approaches of WSD like knowledge based approach, supervised approach, unsupervised approach and semi-supervised approach. It also describes various applications of WSD like information retrieval (IR), machine translation (MT), speech recognition, computational advertising, text processing, classification of documents and biometrics.","","978-1-4799-7552-5","10.1109/ISCBI.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119544","Knowledge based approach;supervised approach;unsupervised approach and semi supervised approach","Context;Semantics;Knowledge based systems;Training;Training data;Computational linguistics;Information retrieval","information retrieval;Internet;knowledge based systems;language translation;natural language processing;pattern classification;speech recognition;text analysis;unsupervised learning","word sense disambiguation;natural language processing;WSD process;web-based applications;information retrieval;information extraction;knowledge based approach;supervised approach;unsupervised approach;semisupervised approach;machine translation;speech recognition;computational advertising;text processing;document classification;biometrics","","7","","38","IEEE","11 Jun 2015","","","IEEE","IEEE Conferences"
"Implementation of Minority Language Translation System Based on Android","Y. Q. Mu; X. s. Tan; W. Xiang","College of Electronic and Information Engineering, Southwest Minzu University, Chengdu, Sichuan, P.R. China; Southwest Minzu University; College of Electronic and Information Engineering, Southwest Minzu University, Chengdu, Sichuan, P.R. China","2019 4th International Conference on Computational Intelligence and Applications (ICCIA)","21 Nov 2019","2019","","","93","97","This paper designs a minority language translation system based on Android, which combines the development technology of TensorFlow building neural network. In addition, the Python-based flask framework helps us achieve data transmission, so that the translation of minority languages into Chinese or Chinese into minority languages, and in the form of deep learning, the above system can be used in practical applications to solve the communication barriers between ethnic minority areas and language lovers. We try to use the LSTM algorithm to realize the background translation of the system, and by comparing the translation results with the original text, we find that the translation results have a certain quality improvement over the traditional machine translation, rather than rigid mechanical translation. Therefore, it is hoped that a good translation system of minority languages can be realized.","","978-1-7281-2128-4","10.1109/ICCIA.2019.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908135","android;flask;TensorFlow;translation of minority languages;LSTM","Software;Servers;Computer architecture;Speech recognition;Mobile handsets;Cameras;Data models","","","","1","","9","IEEE","21 Nov 2019","","","IEEE","IEEE Conferences"
"Development and evaluation of AnHitz, a prototype of a basque-speaking virtual 3D expert on science and technology","I. Leturia; A. d. Pozo; K. Arrieta; U. Iturraspe; K. Sarasola; A. D. d. Ilarraza; E. Navas; I. Odriozola","Elhuyar Foundation, Osinalde Industrialdea, Usurbil, Spain; VICOMTech, 57 Miramon Teknologia Parkea, Donostia-SanSebastian, Spain; VICOMTech, 57 Miramon Teknologia Parkea, Donostia-SanSebastian, Spain; Robotiker 202. eraikina, Zamudioko Teknologia Parkea, Zamudio, Spain; IXA Group, University of the Basque Country Informatika Fakultatea, Donostia-San Sebastian, Spain; IXA Group, University of the Basque Country, Informatika Fakultatea, 649 posta-kutxa, 20080 Donostia-San Sebastian, Spain; Ingeniaritza Goi Eskola Politeknikoa, University of Basque Country (UPV-EHU), Bilbao, Spain; Ingeniaritza Goi Eskola Politeknikoa, University of Basque Country (UPV-EHU), Bilbao, Spain","2009 International Multiconference on Computer Science and Information Technology","11 Dec 2009","2009","","","235","242","The aim of the AnHitz project, whose participants are research groups with very different backgrounds, is to carry out research on language, speech and visual technologies for Basque. Several resources, tools and applications have been developed in AnHitz, but we have also integrated many of these into a prototype of a 3D virtual expert on science and technology. It includes Question Answering and Cross Lingual Information Retrieval systems in those areas. The interaction with the system is carried out in Basque (the results of the CLIR module that are not in Basque are translated through Machine Translation) and is speech-based (using Speech Synthesis and Automatic Speech Recognition). The prototype has received ample media coverage and has been greatly welcomed by Basque society. The system has been evaluated by 50 users who have completed a total of 300 tests, showing good performance and acceptance.","2157-5525","978-1-4244-5314-6","10.1109/IMCSIT.2009.5352720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352720","","Virtual prototyping","information retrieval;language translation;natural language processing;speech recognition;speech synthesis","AnHitz project evaluation;Basque speaking virtual 3D expert;language research;speech research;visual technologies research;question answering system;cross lingual information retrieval system;CLIR module;machine translation;speech synthesis;automatic speech recognition","","2","","23","IEEE","11 Dec 2009","","","IEEE","IEEE Conferences"
"An expert system driven approach to generating natural language in Romanized Urdu from English documents","S. Khan; Z. Pervez; M. Mahmood; F. Mustafa; U. Hasan","NUST Institute of Information Technology, National University of Science and Technology, Rawalpindi, Pakistan; NUST Institute of Information Technology, National University of Science and Technology, Rawalpindi, Pakistan; NUST Institute of Information Technology, National University of Science and Technology, Rawalpindi, Pakistan; NUST Institute of Information Technology, National University of Science and Technology, Rawalpindi, Pakistan; Middlesex University, UK","7th International Multi Topic Conference, 2003. INMIC 2003.","11 Apr 2005","2003","","","361","366","Machine translation of English text to its Urdu equivalent is a difficult challenge, and one that has not been tackled with much success to date. We present a direct approach, using an expert system, which successfully generates Romanized Urdu from English text. The expert system works with a knowledge base that contains grammatical patterns of English and Urdu, as well as a tense and gender-aware dictionary of Urdu words (with their English equivalents). The Romanized Urdu generated by this system is understandable by non-English speaking individuals who are familiar with the English alphabets. It is also suitable to be used as input in text-to-speech synthesizers for the benefits of individuals that are unable to read.","","0-7803-8183-1","10.1109/INMIC.2003.1416751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416751","","Expert systems;Natural languages;Information technology;Dictionaries;Software libraries;Speech synthesis;Synthesizers;Educational institutions;Distance learning;Law","language translation;expert systems;text analysis;natural languages;dictionaries","expert system;natural language generation;machine translation;English text;Romanized Urdu;English documents;knowledge base;grammatical patterns;tense-aware dictionary;gender-aware dictionary;text-to-speech synthesizers","","1","","16","IEEE","11 Apr 2005","","","IEEE","IEEE Conferences"
"Representation Matters: The Case for Diversifying Sign Language Avatars","M. Kopf; R. Omardeen; D. Van Landuyt","Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; European Union of the Deaf, Brussels, Belgium; European Union of the Deaf, Brussels, Belgium","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","As interest in sign language machine translation grows, there is more focus on developing and refining avatar technology to present translation outputs. While signing avatar technology research has focused on legibility and appearance, there has been little attention paid to representing diversity in signing avatars, and the default is often a white female animation. We present data from focus groups conducted in two ongoing sign language machine translation projects in Europe that give insight into deaf end-users‚Äô desires for diversity in avatar representations. Our results reveal a strong desire for full customisability, including options for representing diversity in gender expression and ethnicity, as well as accommodating sociolinguistic variation and personal identity through modified avatar signing styles. This work provides initial insights, but considerable future research is necessary, particularly with minorities and sub-groups within deaf communities.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193409","avatar;sign language;diversity and inclusion;sign language machine translation;deaf communities","Visualization;Video games;Avatars;Europe;Gesture recognition;Assistive technologies;Skin","avatars;gender issues;handicapped aids;language translation;natural language processing","avatar representations;avatar technology research;deaf end-users;diversifying sign language avatars;focus groups;legibility;modified avatar signing styles;ongoing sign language machine translation projects;representation matters;representing diversity;signing avatars;strong desire;translation outputs;white female animation","","","","25","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Probabilistic and grouping methods for morphological root identification for Afaan Oromo","G. M. Wegari; M. Melucci; S. Teferra","IT Doctoral Program, Addis Ababa University, Addis Ababa, Ethiopia; Department of Information Engineering, University of Padua, Padua, Italy; Department of Information Science, Addis Ababa University, Addis Ababa, Ethiopia","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","9 Jul 2016","2016","","","12","15","Morphological models are used in many natural language processing tasks including machine translation and speech recognition. We investigated probabilistic and grouping methods to develop a morphological root identification model for Afaan Oromo. In this paper, we have experimentally shown that the proposed methods can improve the morphological root identification performance of some state-of-the-art methods.","","978-1-4673-8203-8","10.1109/CONFLUENCE.2016.7508039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508039","root identification;probabilisic;grouping;Afaan Oromo","Probabilistic logic;Natural language processing;Testing;Speech recognition;Information retrieval;Writing;Europe","group theory;language translation;mathematical morphology;natural language processing;probability","probabilistic methods;grouping methods;Afaan Oromo;natural language processing tasks;machine translation;speech recognition;morphological root identification model","","","","20","IEEE","9 Jul 2016","","","IEEE","IEEE Conferences"
"A System Architecture to Support Cost-Effective Transcription and Translation of Large Video Lecture Repositories","J. A. Silvestre-Cerd√†; A. P√©rez; M. Jim√©nez; C. Turr√≥; A. Juan; J. Civera","DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain; DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain; DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain; DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain; DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain; DSIC/ITI/ASIC, Universitat Polit√®cnica de Val√®ncia, Val√®ncia, Spain","2013 IEEE International Conference on Systems, Man, and Cybernetics","27 Jan 2014","2013","","","3994","3999","Online video lecture repositories are rapidly growing and becoming established as fundamental knowledge assets. However, most lectures are neither transcribed nor translated because of the lack of cost-effective solutions that can give accurate enough results. In this paper, we describe a system architecture that supports the cost-effective transcription and translation of large video lecture repositories. This architecture has been adopted in the EU project transLectures and is now being tested on a repository of more than 9000 video lectures at the Universitat Politecnica de Valencia. Following a brief description of this repository and of the transLectures project, we describe the proposed system architecture in detail. We also report empirical results on the quality of the transcriptions and translations currently being maintained and steadily improved.","1062-922X","978-1-4799-0652-9","10.1109/SMC.2013.682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722435","Language Technologies;Machine Translation;Automatic Speech Recognition;Massive Adaptation;Intelligent Interaction;Education;Video Lectures;Multilingualism;Accessibility;Opencast Matterhorn","Web services;Databases;Streaming media;Adaptation models;Training;Multimedia communication;Computer architecture","language translation;video databases","system architecture;cost-effective transcription;large video lecture repositories translations;online video lecture repositories;cost-effective solutions;EU project transLectures","","3","","22","IEEE","27 Jan 2014","","","IEEE","IEEE Conferences"
"Discriminative reranking for SMT using various global features","C. -L. Goh; T. Watanabe; A. Finch; E. Sumita","MASTAR Project, National Institute of Information and Communications Technology, Keihanna Science City, Japan; MASTAR Project, National Institute of Information and Communications Technology, Keihanna Science City, Japan; MASTAR Project, National Institute of Information and Communications Technology, Keihanna Science City, Japan; MASTAR Project, National Institute of Information and Communications Technology, Keihanna Science City, Japan","2010 4th International Universal Communication Symposium","13 Dec 2010","2010","","","8","14","In this paper, we propose to use various global features for discriminative reranking in an SMT framework. We employ an online large-margin based training algorithm for the structural output support vector machines based on the margin infused relaxed algorithm. Besides the standard features used, such as decoder's scores, source and target sentences, alignments and part-of-speech tags, we include sentence type probabilities, posterior probabilities and back translation features for reranking. These features have been proved to be useful in other approaches in statistical machine translation but it is the first attempt to apply them in reranking. Our experimental results using 160K BTEC corpus show an improvement of 1-4 BLEU percentage points on Japanese/Chinese to English translation.","","978-1-4244-7820-0","10.1109/IUCS.2010.5666776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666776","","Hidden Markov models;Training;Support vector machines;Data models;Training data;Probability;Feature extraction","language translation;learning (artificial intelligence);natural language processing;statistical analysis;support vector machines","discriminative reranking;statistical machine translation;training algorithm;structural output support vector machines;part-of-speech tags;sentence type probabilities;posterior probabilities;back translation features;160K BTEC corpus;BLEU percentage points","","2","1","21","IEEE","13 Dec 2010","","","IEEE","IEEE Conferences"
"A Token-Level Contrastive Framework for Sign Language Translation","B. Fu; P. Ye; L. Zhang; P. Yu; C. Hu; X. Shi; Y. Chen","Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Sign Language Translation (SLT) is a promising technology to bridge the communication gap between the deaf and the hearing people. Recently, researchers have adopted Neural Machine Translation (NMT) methods, which usually require large-scale corpus for training, to achieve SLT. However, the publicly available SLT corpus is very limited, which causes the collapse of the token representations and the inaccuracy of the generated tokens. To alleviate this issue, we propose Con-SLT, a novel token-level Contrastive learning framework for Sign Language Translation , which learns effective token representations by incorporating token-level contrastive learning into the SLT decoding process. Concretely, ConSLT treats each token and its counterpart generated by different dropout masks as positive pairs during decoding, and then randomly samples K tokens in the vocabulary that are not in the current sentence to construct negative examples. We conduct comprehensive experiments on two benchmarks (PHOENIX14T and CSL-Daily) for both end-to-end and cascaded settings. The experimental results demonstrate that ConSLT can achieve better translation quality than the strong baselines1.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095466","National Natural Science Foundation of China; Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095466","Sign language translation;low-resource;contrastive learning","Training;Representation learning;Vocabulary;Gesture recognition;Assistive technologies;Signal processing;Decoding","","","","1","","28","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Label Attention Network for Structured Prediction","L. Cui; Y. Li; Y. Zhang","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; School of Engineering, Westlake University, Hangzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Apr 2022","2022","30","","1235","1248","Sequence labeling assigns a label to each token in a sequence, which is a fundamental problem in natural language processing (NLP). Many NLP tasks, including part-of-speech tagging and named entity recognition, can be solved in a form of sequence labeling problem. Other tasks such as constituency parsing and non-autoregressive machine translation can also be transformed into sequence labeling tasks. Neural models have been shown powerful for sequence labeling by employing a multi-layer sequence encoding network. Conditional random field (CRF) is proposed to enrich information over label sequences, yet it suffers large computational complexity and over-reliance on Marko assumption. To this end, we propose label attention network (LAN) to hierarchically refine representation of marginal label distributions bottom-up, enabling higher layers to learn more informed label sequence distribution based on information from lower layers. We demonstrate the effectiveness of LAN through extensive experiments on various NLP tasks including POS tagging, NER, CCG supertagging, constituency parsing and non-autoregressive machine translation. Empirical results show that LAN not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to CRF.","2329-9304","","10.1109/TASLP.2022.3145311","National Natural Science Foundation of China(grant numbers:61976180); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699087","Label attention;label dependency;sequence labeling","Labeling;Task analysis;Tagging;Artificial neural networks;Machine translation;Natural language processing;Encoding","grammars;language translation;learning (artificial intelligence);Markov processes;natural language processing","label attention network;fundamental problem;natural language processing;NLP tasks;sequence labeling problem;constituency parsing;nonautoregressive machine translation;sequence labeling tasks;multilayer sequence encoding network;label sequences;LAN;informed label sequence distribution;conditional random field;CRF;Marko assumption;hierarchically refine representation;marginal label distributions;POS tagging;NER;CCG supertagging","","1","","85","IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"DetTrans: A Lightweight Framework to Detect and Translate Noisy Inputs Simultaneously","Z. Xue; X. Zhang; T. Shi; D. Xiong","Tianjin University, Tianjin, China; OPPO Research Institute, Beijing, China; OPPO Research Institute, Beijing, China; Tianjin University, Tianjin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2023","PP","99","1","10","Neural machine translation (NMT) systems trained on clean data usually suffer from performance degradation when translating noisy inputs. Existing works attempt to improve the robustness of NMT normally via data augmentation, where synthetic noisy data are mixed with original clean data, either for training NMT with the standard NMT loss alone, or for tuning auxiliary tasks in a multi-task learning manner. Typical auxiliary tasks include detecting and correcting noises, exploiting noisy outputs for contrastive learning etc. The aforementioned two auxiliary tasks are generally designed independently, and the modules for detecting and correcting noises are heavyweight. In this article, we propose a new framework, DetTransNet (Detector-Translator Network), aiming to detect positions of noises in the input and translate the input simultaneously. The newly introduced noise detector module is essentially a lightweight binary classifier built upon the final layer of the encoder of the original Transformer model for the translation task, which is to identify at which position of the input has potential noise. The module has a very few parameters. In order to help the model capture the relationship between clean instances and their noisy counterparts, an extra loss is further introduced to enhance the interaction between clean and noisy data. In this way, we combine noise detection and contrastive learning together. As the model is able to identify and locate noises, a heuristic method is proposed to correct detected noises, in order to achieve better translations. Experiments show that DetTransNet is robust to four types of noises (deletion, insertion, swapping, keyboard), and obtain a substantial improvement of up to 1.6 BLEU points across different datasets.","2329-9304","","10.1109/TASLP.2023.3284513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147355","Neural machine translation;robust translation","Noise measurement;Machine translation;Task analysis;Training;Detectors;Transformers;Robustness","","","","","","","IEEE","9 Jun 2023","","","IEEE","IEEE Early Access Articles"
"Generative Adversarial Neural Machine Translation for Phonetic Languages via Reinforcement Learning","A. Kumar; A. Pratap; A. K. Singh","Department of Computer Science and Engineering, Indian Institute of Technology (Banaras Hindu University), Varanasi, India; Department of Computer Science and Engineering, Indian Institute of Technology (Banaras Hindu University), Varanasi, India; Department of Computer Science and Engineering, Indian Institute of Technology (Banaras Hindu University), Varanasi, India","IEEE Transactions on Emerging Topics in Computational Intelligence","31 Jan 2023","2023","7","1","190","199","Neural Machine Translation (NMT) heavily depends on the context vectors generated via attention network for the target word prediction. Existing works primarily focus on generating context vectors from words or subwords of sentences, limiting NMT models' ability to learn sufficient information about the source sentence representations. These situations are even worse when languages belong to extremely low-resource categories due to rare word problem. To improve the learning of source sentence representations and handle the rare word problem of Low Resource Languages (LRLs), we propose a novel improvement in Generative Adversarial Networks (GAN)-NMT by incorporating deep reinforcement learning-based optimised attention in generator and convolutional neural network in discriminator. We also create the novel joint embedding of subwords and sub-phonetic representation of sentences as input to GAN that helps models to learn the better representations and generate suitable context vectors compared to existing traditional approaches for LRLs. To show the effectiveness of our method, we demonstrate experiments on LRLs pairs, e.g., Gujarati $\leftrightarrow$ Hindi, Nepali $\leftrightarrow$ Hindi, Punjabi $\leftrightarrow$ Hindi, Maithili $\leftrightarrow$ Hindi and Urdu $\leftrightarrow$ Hindi. Our proposed novel approach suppress the existing state-of-the-art techniques with considerable improvement.","2471-285X","","10.1109/TETCI.2022.3209394","Science and Engineering Research Board(grant numbers:SRG/2020/000318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950318","NMT;adversarial training;deep reinforcement learning;phonetic embedding;international phonetic alphabet","Phonetics;Transformers;Context modeling;Generative adversarial networks;Computational modeling;Reinforcement learning;Generators","deep learning (artificial intelligence);language translation;natural language processing;reinforcement learning;speech processing;vectors","attention network;context vectors;convolutional neural network;deep reinforcement learning-based optimised attention;GAN-NMT;generative adversarial networks-NMT;generative adversarial neural machine translation;low resource languages;LRLs;phonetic languages;source sentence representations;target word prediction","","","","40","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Joint optimization of LCMV beamforming and acoustic echo cancellation","W. Herbordt; W. Kellermann; S. Nakamura","Telecommunications Laboratory, University Erlangen-Nuremberg, Erlangen, Germany; Telecommunications Laboratory, University Erlangen-Nuremberg, Erlangen, Germany; ATR Spoken Language Translation Research Laboratories, Kyoto, Japan","2004 12th European Signal Processing Conference","6 Apr 2015","2004","","","2003","2006","Full-duplex hands-free acoustic human/machine interfaces often require the combination of acoustic echo cancellation and speech enhancement in order to suppress acoustic echoes, local interference, and noise. In order to optimally exploit positive synergies between acoustic echo cancellation and speech enhancement, we present in this contribution a combined least-squares (LS) optimization criterion for the integration of acoustic echo cancellation and adaptive linearly-constrained minimum variance (LCMV) beamforming. Based on this optimization criterion, we derive a computationally efficient system based on the generalized sidelobe canceller (GSC), which effectively deals with scenarioes with time-varying acoustic echo paths and simultaneous presence of double-talk of acoustic echoes, local interference, and desired speakers.","","978-320-0001-65-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7080052","","Integrated circuits;Array signal processing;Echo cancellers;Abstracts;Nickel;Acoustic distortion","array signal processing;echo suppression;least squares approximations;man-machine systems;speech enhancement","linearly-constrained minimum variance beamforming;acoustic echo cancellation;full-duplex hands-free acoustic human/machine interfaces;speech enhancement;least-squares optimization;generalized sidelobe canceller","","","5","12","","6 Apr 2015","","","IEEE","IEEE Conferences"
"Dualformer: A Unified Bidirectional Sequence-to-Sequence Learning","J. -T. Chien; W. -H. Chang","Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7718","7722","This paper presents a new dual domain mapping based on a unified bidirectional sequence-to-sequence (seq2seq) learning. Traditionally, dual learning in domain mapping was constructed with intrinsic connection where the conditional generative models in two directions were mutually leveraged and combined. The additional feedback from the other generation direction was used to regularize sequential learning in original direction of domain mapping. Domain matching between source sequence and target sequence was accordingly improved. However, the reconstructions for knowledge in two domains were ignored. The dual information based on separate models in two training directions was not sufficiently discovered. To cope with this weakness, this study proposes a closed-loop seq2seq learning where domain mapping and domain knowledge are jointly learned. In particular, a new feature-level dual learning is incorporated to build a dualformer where feature integration and feature reconstruction are further performed to bridge dual tasks. Experiments demonstrate the merit of the proposed dualformer for machine translation based on the multi-objective seq2seq learning.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413402","sequence-to-sequence learning;dual learning;transformer;domain mapping;machine translation","Training;Bridges;Conferences;Supervised learning;Signal processing;Semisupervised learning;Acoustics","feature extraction;feedback;language translation;learning (artificial intelligence)","source sequence;target sequence;dual information;domain knowledge;feature-level dual learning;dualformer;multiobjective seq2seq learning;bidirectional sequence-to-sequence learning;dual domain mapping;conditional generative models;closed-loop seq2seq learning","","3","","31","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A Refined POS Tag Sequence Finder for Tamil Sentences","R. Sakuntharaj; S. Mahesan","Centre for Information & Communication Technology, Eastern University, Sri Lanka; Department of Computer Science, Faculty of Science, University of Jaffna, Sri Lanka","2018 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)","28 Nov 2019","2018","","","1","6","Part-of-Speech (POS) tagging is the process of labelling syntactic categories to each word in a sentence. Identifying POS tags for words is the fundamental task for various natural language processing applications such as grammar checking and machine translation etc. A novel POS tagging approach for Tamil language is proposed in this paper to determine the hierarchical POS tags for words. This approach uses Hidden Markov Model, Viterbi algorithm, Tamil grammar rules, N-gram and Stemming techniques. Test results show that the POS tags for Tamil words determined by this approach are found to be with 96% accuracy as approved by a Scholar in Tamil.","2151-1810","978-1-5386-9418-3","10.1109/ICIAFS.2018.8913350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913350","Part-of-Speech tag;N-gram;Viterbi;Hidden Morkov Model;Hierarchical tag-set","Tagging;Grammar;Viterbi algorithm;Hidden Markov models;Task analysis;Mathematical model;Natural language processing","grammars;hidden Markov models;language translation;natural language processing","Tamil words;refined POS tag sequence finder;Tamil sentences;part-of-Speech tagging;syntactic categories;natural language processing applications;grammar checking;machine translation etc;novel POS tagging approach;Tamil language;hierarchical POS tags;Tamil grammar rules","","3","","25","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Bangla Parts-of-Speech tagging using Bangla stemmer and rule based analyzer","M. N. Hoque; M. H. Seddiqui","Dept. of Computer Science and Engineering, Port City International University, Chittagong, Bangladesh; Dept. of Computer Science and Engineering, University of Chittagong, Chittagong, Bangladesh","2015 18th International Conference on Computer and Information Technology (ICCIT)","9 Jun 2016","2015","","","440","444","Parts-of-Speech (POS) tagging plays vital roles in the field of Natural Language Processing (NLP), such as - machine translation, spell checker, information retrieval, speech processing, emotion analysis and so on. Bangla is a very inflectional language that induces many variants from a single word. Although there is a few POS Tagger in Bangla language, very small of them address the essence of suffices to identify tag of the words. In this regard, we propose an automated POS Tagging system for Bangla language based on word-suffixes. In our system, we use our own stemming technique to retrieve a possible minimum root words and apply rules according to different forms of suffixes. Moreover, we incorporate a Bangla vocabulary that contains more than 45,000 words with their default tag and a patterned based verb-data-set. These facilitate to improve tagging efficiency of Bangla POS Tagger. We experiment our proposed system on a Bangla text corpus. The result shows that our proposed Bangla POS Tagger has outperformed the known related tagging systems.","","978-1-4673-9930-2","10.1109/ICCITechn.2015.7488111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488111","Bangla Parts-of-Speech (POS) Tagger;Natural Language Processing (NLP);Morphology Bangla Stemmer","Dictionaries;Tagging;Hidden Markov models;Natural language processing;Vocabulary;Training data;Speech","computational linguistics;knowledge based systems;natural language processing;vocabulary","Bangla parts-of-speech tagging;Bangla stemmer;rule based analyzer;natural language processing;NLP;inflectional language;automated POS tagging system;word-suffixes;Bangla vocabulary;patterned based verb-data-set;Bangla text corpus","","16","","14","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Multi-sense based neural machine translation","Z. Yang; W. Chen; F. Wang; B. Xu","Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences","2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","3491","3497","Attention mechanism advances the neural machine translation (NMT) by reducing the confusion introduced by irrelevant words in long sentences. However, the confusion caused by ambiguous words hasn't been handled yet and it may be a bottleneck for the NMT model. This paper validates the hypothesis and proposes a simple and flexible framework, which enables the NMT model to only focus on the relevant sense type of the input word in current context. Experiments show that the proposed model achieves substantial improvements on every test set over competitive baselines. Our contributions come from twofold. Firstly, to the best of our knowledge, this is the first effort to introduce the multi-sense representation, which represents each sense type of the word with a sense-specific embedding, into NMT. Secondly, We propose a sense search module which can detect the sense type of the word automatically. Flexibility and versatility are the most attractive characteristic of the proposed sense search module. It can be applied to any other semantic related NLP tasks with little modification.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7966295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966295","","Context;Context modeling;Computational modeling;Semantics;Neural networks;Speech recognition;History","language translation;natural language processing;recurrent neural nets;text analysis","multisense based neural machine translation;attention mechanism;NMT model;irrelevant words;confusion reduction;ambiguous words;hypothesis validation;multisense representation;sense-specific embedding;sense search module;automatic word sense type detection;semantic related NLP tasks","","2","","25","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Phonetic-Attention Scoring for Deep Speaker Features in Speaker Verification","L. Li; Z. Tang; Y. Shi; D. Wang","Center for Speech and Language Technologies, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Tsinghua University, Beijing, China; Center for Speech and Language Technologies, Tsinghua University, Beijing, China","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","284","288","Recent studies have shown that frame-level deep speaker features can be derived from a deep neural network with the training target set to discriminate speakers by a short speech segment. By pooling the frame-level features, utterance-level representations, called d-vectors, can be derived and used in the automatic speaker verification (ASV) task. This simple average pooling, however, is inherently sensitive to the phonetic content of the utterance. An interesting idea borrowed from machine translation is the attention-based mechanism, where the contribution of an input word to the translation at a particular time is weighted by an attention score. This score reflects the relevance of the input word and the present translation. We can use the same idea to align utterances with different phonetic contents. This paper proposes a phonetic-attention scoring approach for d-vector systems. By this approach, an attention score is computed for each frame pair. This score reflects the similarity of the two frames in phonetic content, and is used to weigh the contribution of this frame pair in the utterance-based scoring. This new scoring approach emphasizes the frame pairs with similar phonetic contents, which essentially provides a soft alignment for utterances with any phonetic contents. Experimental results show that compared with the naive average pooling, this phonetic-attention scoring approach can deliver consistent performance improvement in ASV tasks of both text-dependent and text-independent.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023352","","Phonetics;Task analysis;Decoding;Biological system modeling;Analytical models;Feature extraction;Neural networks","speaker recognition;vectors","d-vector systems;ASV;average pooling;attention score mechanism;utterance-based scoring;machine translation;automatic speaker verification;utterance-level representations;frame-level features;speech segmentation;deep neural network;frame-level deep speaker features;phonetic-attention scoring approach","","1","","27","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Chinese-English transliteration using weighted finite-state transducers","Pang Wei; Xu Bo","Digital Media Content Technology Research Center, China; Digital Media Content Technology Research Center, China","2008 International Conference on Audio, Language and Image Processing","8 Aug 2008","2008","","","1328","1333","This paper proposes a novel method for Chinese-English transliteration based on multiple models by using weighted finite-state transducers (WFST). WFST provide a unified framework for integrating the various components of a speech-to-speech translation system, such as speech recognition and machine translation, motivated by their flexibility in integrating multiple sources of information and other interesting properties. We built a grapheme-based model, a phoneme-based model, an extended phoneme-based model and so on. Combining those models with unified framework of WFST, we can build a combining transliteration model for Chinese-English. The advantage of this method lies in that we can better account for such behavior by combining those information sources from different model to maximize the use of the data available. Our experiments show that the resulting system outperforms single-model systems with the model directly trained with Chinese-English name pairs for Chinese-English translation.","","978-1-4244-1723-0","10.1109/ICALIP.2008.4590109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4590109","","Training;Dictionaries;Transducers;Data models;Context modeling;Error analysis;Data mining","finite state machines;language translation;natural language processing","Chinese-English transliteration;weighted finite-state transducer;speech-to-speech translation system;grapheme-based model;phoneme-based model","","","","19","IEEE","8 Aug 2008","","","IEEE","IEEE Conferences"
"Hallucinated n-best lists for discriminative language modeling","K. Sagae; M. Lehr; E. Prud'hommeaux; P. Xu; N. Glenn; D. Karakos; S. Khudanpur; B. Roark; M. Sara√ßlar; I. Shafran; D. Bikel; C. Callison-Burch; Y. Cao; K. Hall; E. Hasler; P. Koehn; A. Lopez; M. Post; D. Riley","USC, USA; OHSU, USA; OHSU, USA; JHU, USA; BYU, USA; JHU, USA; JHU, USA; OHSU, USA; Boƒüazi√ßi University, Turkey; OHSU, USA; Google, Inc., USA; JHU, USA; JHU, USA; Google, Inc., USA; Edinburgh, UK; Edinburgh, UK; JHU, USA; JHU, USA; Rochester, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","5001","5004","This paper investigates semi-supervised methods for discriminative language modeling, whereby n-best lists are ‚Äúhallucinated‚Äù for given reference text and are then used for training n-gram language models using the perceptron algorithm. We perform controlled experiments on a very strong baseline English CTS system, comparing three methods for simulating ASR output, and compare the results with training with ‚Äúreal‚Äù n-best list output from the baseline recognizer. We find that methods based on extracting phrasal cohorts - similar to methods from machine translation for extracting phrase tables - yielded the largest gains of our three methods, achieving over half of the WER reduction of the fully supervised methods.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289043","language modeling;automatic speech recognition;discriminative training;semi-supervised methods","Training;Hidden Markov models;Speech;Speech recognition;Transducers;Data models;Training data","language translation;natural language processing","hallucinated n-best lists;discriminative language modeling;semisupervised method;reference text;training n-gram language model;perceptron algorithm;English CTS system;simulating ASR output;baseline recognizer;machine translation","","6","4","19","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"Language style and domain adaptation for cross-language SLU porting","E. A. Stepanov; I. Kashkarev; A. O. Bayer; G. Riccardi; A. Ghosh","Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","9 Jan 2014","2013","","","144","149","Automatic cross-language Spoken Language Understanding porting is plagued by two limitations. First, SLU are usually trained on limited domain corpora. Second, language pair resources (e.g. aligned corpora) are scarce or unmatched in style (e.g. news vs. conversation). We present experiments on automatic style adaptation of the input for the translation systems and their output for SLU. We approach the problem of scarce aligned data by adapting the available parallel data to the target domain using limited in-domain and larger web crawled close-to-domain corpora. SLU performance is optimized by reranking its output with Recurrent Neural Network-based joint language model. We evaluate end-to-end SLU porting on close and distant language pairs: Spanish - Italian and Turkish - Italian; and achieve significant improvements both in translation quality and SLU performance.","","978-1-4799-2756-2","10.1109/ASRU.2013.6707720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707720","Spoken Language Understanding;Statistical Machine Translation;Domain Adaptation","Adaptation models;Google;Training;Joints;Data models;Numerical models;Speech","language translation;natural language processing;recurrent neural nets","automatic cross-language spoken language understanding porting;cross-language SLU porting;language pair resources;automatic style adaptation;parallel data;Web crawled close-to-domain corpora;recurrent neural network-based joint language model;end-to-end SLU porting;Spanish language;Italian language;Turkish language;statistical machine translation","","4","","14","IEEE","9 Jan 2014","","","IEEE","IEEE Conferences"
"A text-to-speech system for italian","R. Delmonte; G. Mian; G. Tisato","Centro linguistico Interfacolt√†Ist.Fisica Applicata, University Venezia, Italy; NA; Centro di Sonologia Computazionale, Universit√† di Padova, Italy","ICASSP '84. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1984","9","","81","84","A system for the automatic translation of any text of Italian into naturally fluent speech is presented. The system, planned for use in a reading machine for the blind, is build up around a Phonological Processor (hence FP) and synthesizes speech-by joining LPC coded diphones. The FP maps into prosodic structures the phonological rules of Italian. Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (MF), Phonological Word (PW), Intonationa T Group (IG). Onto these structures, phonological rules are applied such as the ""letter-to-sound"" rules, automatic word stress rules, internal stress hierarchy rules indicating secondary stress, external sandhi rules, phonological focus assignment rules, logical focus assignment rules.","","","10.1109/ICASSP.1984.1172428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1172428","","Speech synthesis;Internal stresses;Computational modeling;Natural languages;Computer interfaces;Speech processing;Linear predictive coding;Foot;Vocabulary;Graphics","","","","2","","18","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Exploiting Inactive Examples for Natural Language Generation With Data Rejuvenation","W. Jiao; X. Wang; S. He; Z. Tu; I. King; M. R. Lyu","Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Microsoft Research Lab-Asia, Beijing, China; Tencent AI Lab, Shenzhen, Guangdong, China; Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","4 Mar 2022","2022","30","","931","943","Recent years have witnessed the success of natural language generation (NLG) accomplished by deep neural networks, which require a large amount of training data for optimization. With the constant increase of data scale, the complex patterns and potential noises make training NLG models difficult. In order to fully utilize large-scale training data, we explore inactive examples in the training data and propose to rejuvenate the inactive examples for improving the performance of NLG models. Specifically, we define inactive examples as those sentence pairs that contribute less to the performance of NLG models, and show that their existence is independent of model variants but mainly determined by the data distribution. We further introduce data rejuvenation to improve the training of NLG models by re-labeling the inactive examples. The rejuvenated examples and active examples are combined to train a final NLG model. We evaluate our approach by experiments on machine translation (MT) and text summarization (TS) tasks, and achieve significant improvements of performance. Extensive analyses reveal that inactive examples are more difficult to learn than active ones and rejuvenation can reduce the learning difficulty, which stabilizes and accelerates the training process of NLG models and results in models with better generalization capability.","2329-9304","","10.1109/TASLP.2022.3153269","National Key Research and Development Program of China(grant numbers:2018AAA0100204); Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:CUHK 14210717); General Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721161","Natural language generation;inactive example;data rejuvenation;machine translation;text summarization","Training;Data models;Task analysis;Training data;Transformers;Computational modeling;Speech processing","deep learning (artificial intelligence);language translation;natural language processing;text analysis","data distribution;data rejuvenation;final NLG model;natural language generation;data scale;large-scale training data;deep neural networks;optimization;machine translation;text summarization tasks","","","","56","IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"A hybrid Noun Phrase translation system","A. R. Nabhan; A. Rafea","Faculty of Computers and Information, Fayoum University, Egypt; Computer Science Department, American University, Cairo, Cairo, Egypt","2010 The 7th International Conference on Informatics and Systems (INFOS)","6 May 2010","2010","","","1","7","We present a hybrid based Noun Phrase (NP) translator that combines rule-based transfer technique with a statistical n-gram language model for selecting the best translation. Noun Phrase is the dominating construct in natural language text and targeting it for focused processing increases effectiveness of language processing systems. Manipulation of Noun Phrases is an effective subtask in Statistical Machine Translation, Multilingual Information Retrieval and Information Extraction. In this work, we make use of knowledge about Arabic language morphology regarding the translation of Verbal Nouns (Masader) and Annexation Constructs.","","978-1-4244-5828-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5461780","","Natural languages;Magnetic heads;Computer science;Information retrieval;Data mining;Morphology;Decoding;Parameter estimation;Speech recognition;Process control","information retrieval;knowledge based systems;language translation;natural language processing","hybrid noun phrase translation system;rule-based transfer technique;statistical n-gram language model;natural language text;language processing systems;statistical machine translation;multilingual information retrieval;information extraction;arabic language morphology;verbal nouns;annexation constructs","","","","12","","6 May 2010","","","IEEE","IEEE Conferences"
"ATTS2S-VC: Sequence-to-sequence Voice Conversion with Attention and Context Preservation Mechanisms","K. Tanaka; H. Kameoka; T. Kaneko; N. Hojo","NTT Communication Science Laboratories, NTT Corporation, Japan; NTT Communication Science Laboratories, NTT Corporation, Japan; NTT Communication Science Laboratories, NTT Corporation, Japan; NTT Communication Science Laboratories, NTT Corporation, Japan","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","16 Apr 2019","2019","","","6805","6809","This paper describes a method based on a sequence-to-sequence learning (Seq2Seq) with attention and context preservation mechanism for voice conversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving sequence modeling such as speech synthesis and recognition, machine translation, and image captioning. In contrast to current VC techniques, our method 1) stabilizes and accelerates the training procedure by considering guided attention and proposed context preservation losses, 2) allows not only spectral envelopes but also fundamental frequency contours and durations of speech to be converted, 3) requires no context information such as phoneme labels, and 4) requires no time-aligned source and target speech data in advance. In our experiment, the proposed VC framework can be trained in only one day, using only one GPU of an NVIDIA Tesla K80, while the quality of the synthesized speech is higher than that of speech converted by Gaussian mixture model-based VC and is comparable to that of speech generated by recurrent neural network-based text-to-speech synthesis, which can be regarded as an upper limit on VC performance.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683282","Voice conversion;deep learning;sequence-to-sequence;attention mechanism;context preservation mechanism","Training;Acoustics;Task analysis;Decoding;Linguistics;Artificial neural networks;Speech recognition","Gaussian processes;learning (artificial intelligence);mixture models;recurrent neural nets;speech synthesis","Gaussian mixture model-based VC;recurrent neural network-based text-to-speech synthesis;VC performance;ATTS2S-VC;sequence-to-sequence voice conversion;context preservation mechanism;sequence-to-sequence learning;Seq2Seq;voice conversion tasks;machine translation;current VC techniques;context preservation losses;context information;time-aligned source;target speech data;VC framework;context preservation mechanisms;attention preservation mechanisms;image captioning;training procedure;spectral envelopes;fundamental frequency contours;NVIDIA Tesla K80;GPU;phoneme labels","","52","","52","IEEE","16 Apr 2019","","","IEEE","IEEE Conferences"
"It's time for your evaluation","K. Knight",USC/ISI,"IEEE Expert","6 Aug 2002","1996","11","5","10","11","If there is a word that strikes fear into the hearts of speech and natural-language researchers, it is ""evaluation"". It's not that we don't like evaluation-mostly, we do-it's just that we've developed this knee-jerk response: ""Another evaluation? Already? But we just did an evaluation!"" Well, that's an exaggeration. But funder-promoted, common evaluations have profoundly affected our research in years past. In these evaluations, researchers agree on a number of tasks and criteria for success, then work independently on those tasks. In many cases, funding agencies provide sample inputs and outputs in advance, but withhold test data until ""evaluation day"". Several months later, participating researchers gather at a specialized workshop to discuss results and techniques. Whether you're interested in machine translation, morphology, speech-query processing, information extraction, or information retrieval, there is a common evaluation somewhere where you can test your ideas in an empirical, quantitatively scored setting. The author looks at some strengths and weaknesses of these common evaluations.","2374-9407","","10.1109/64.539008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=539008","","Testing;Artificial intelligence;Information retrieval;Intersymbol interference;Heart;Speech analysis;Data mining;Robustness;Software engineering;Knowledge acquisition","research and development management;natural languages;speech recognition","evaluation;natural-language research;machine translation;morphology;speech-query processing;information extraction;information retrieval;common evaluation","","","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"The IBM 2011 GALE Arabic speech transcription system","L. Mangu; H. -K. Kuo; S. Chu; B. Kingsbury; G. Saon; H. Soltau; F. Biadsy","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Department of Computer Science, Columbia University, New York, USA","2011 IEEE Workshop on Automatic Speech Recognition & Understanding","5 Mar 2012","2011","","","272","277","We describe the Arabic broadcast transcription system fielded by IBM in the GALE Phase 5 machine translation evaluation. Key advances over our Phase 4 system include a new Bayesian Sensing HMM acoustic model; multistream neural network features; a MADA vowelized acoustic model; and the use of a variety of language model techniques with significant additive gains. These advances were instrumental in achieving a word error rate of 7.4% on the Phase 5 evaluation set, and an absolute improvement of 0.9% word error rate over our 2009 system on the unsequestered Phase 4 evaluation data.","","978-1-4673-0367-5","10.1109/ASRU.2011.6163943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163943","large vocabulary speech recognition","Hidden Markov models;Acoustics;Computational modeling;Training;Dictionaries;Transforms;Lattices","Bayes methods;hidden Markov models;neural nets;speech processing","IBM 2011 GALE Arabic speech transcription system;GALE Phase 5 machine translation evaluation;phase 4 system;Bayesian sensing HMM acoustic model;multistream neural network features;MADA vowelized acoustic model;language model techniques;unsequestered phase 4 evaluation data","","15","2","25","IEEE","5 Mar 2012","","","IEEE","IEEE Conferences"
"Closed-form range-based posture estimation based on decoupling translation and orientation","F. Beutler; U. D. Hanebeck","Intelligent Sensor-Actuator-Systems, Institute of Computer Design and Fault Tolerance, Universit√§t Karlsruhe, Karlsruhe, Germany; Intelligent Sensor-Actuator-Systems, Institute of Computer Design and Fault Tolerance, Universit√§t Karlsruhe, Karlsruhe, Germany","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","4","","iv/989","iv/992 Vol. 4","For estimating the posture, i.e., position and orientation, of an extended target based on range measurements, a new closed-form solution is proposed; it is based on decoupling position and orientation. For decoupling, any procedure for range-based localization of point targets, i.e., for mere position estimation, can be used. The new solution is suboptimal, but nevertheless provides good accuracy and is very practical from an application point of view.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1416177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416177","","Closed-form solution;Position measurement;Loudspeakers;Microphones;Coordinate measuring machines;Propagation delay;Nonlinear equations;Intelligent sensors;Fault tolerance;Acoustic measurements","parameter estimation;target tracking;audio signal processing","range-based posture estimation;closed-form solution;target translation;target orientation;target position;range-based point target localization;position estimation;acoustic localization systems;audio signals;acoustic propagation delays;telepresence scenario","","11","","5","IEEE","9 May 2005","","","IEEE","IEEE Conferences"
"Survey of Literature on Machine Intelligence and Deep learning for Smart Grid Applications","V. Garg; B. Agarwal","Research and Development, Genus Power Infrastructures Limited, Jaipur, India; Research and Development, Genus Power Infrastructures Limited, Jaipur, India","2019 8th International Conference on Power Systems (ICPS)","16 Apr 2020","2019","","","1","5","Deep neural networks (DNNs) are currently the state of the art for many modern artificial intelligence (AI) applications. You have surely been touched by one of its influences. Automatic tagging of contacts in Facebook, personal digital assistants (Siri/Cortana/Alexa/ Google Assistant), or automatic creation of albums by Google Photos; all use variants of a DNN. Since the breakthrough application of DNNs to image recognition, Natural Language processing, Machine Translation, Spam & Fraud detection and Speech Recognition, the number of applications that use DNNs has exploded.Smart Grid is fundamentally defined as a system that has bi-directional information flow in addition to bi-directional flow of Energy. The Energy flow is bidirectional as there is a mechanism through which distributed generation and distributed storage can be integrated with the grid. The Information flow is bidirectional as the information flows from the devices to the central server and control signals flow from the server to the smart devices.This paper seeks to explore the modern Deep Leaning (DL) and Deep Neural Networks (DNN) landscape, and point to some of the possible directions in the space of Smart metering and Smart Grid.","","978-1-7281-4103-9","10.1109/ICPS48983.2019.9067351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067351","Artificial intelligence;Machine learning;Supervised learning;Unsupervised learning;Artificial neural networks;Feedforward neural networks;Smart Grids;Smart Devices","","distributed power generation;learning (artificial intelligence);neural nets;power engineering computing;smart meters;smart power grids","DNNs;image recognition;Natural Language processing;Machine Translation;Spam & Fraud detection;Speech Recognition;bi-directional information flow;Energy flow;distributed generation;distributed storage;smart devices;Smart metering;Smart Grid applications;Deep neural networks;artificial intelligence applications;AI;personal digital assistants;Google Photos;machine intelligence;deep learning","","2","","38","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Web-Service for Translation of Pictogram Messages into Russian Coherent Text","M. Kultsova; D. Matyushechkin; A. Anikin","Volgograd State Technical University, Volgograd, Russia; Volgograd State Technical University, Volgograd, Russia; Volgograd State Technical University, Volgograd, Russia","2018 9th International Conference on Information, Intelligence, Systems and Applications (IISA)","3 Feb 2019","2018","","","1","5","This paper examines design and implementation of a web-service for translating pictogram messages into text messages in Russian. Also, we considered the contemporary approaches to such translation and proposed effective methods solving this task. The developed web-service for the translation of pictogram messages into text messages can have a wide range of applications in the field of augmentative and alternative communication for people with mental and speech disorders. Also, this web-service can be used by third-party software developers, which will enable their programs for people with special needs to translate pictogram messages.","","978-1-5386-8161-9","10.1109/IISA.2018.8633677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633677","","Machine learning;Neural networks;Natural languages;Training;Servers;Python;Task analysis","electronic messaging;handicapped aids;software engineering;text analysis;Web services","third-party software developers;mental disorders;speech disorders;pictogram message translation;web-service;text messages;Russian coherent text","","","","12","IEEE","3 Feb 2019","","","IEEE","IEEE Conferences"
"RNNDROP: A novel dropout for RNNS in ASR","T. Moon; H. Choi; H. Lee; I. Song","Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, South Korea; Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, South Korea; Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, South Korea; Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, South Korea","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","11 Feb 2016","2015","","","65","70","Recently, recurrent neural networks (RNN) have achieved the state-of-the-art performance in several applications that deal with temporal data, e.g., speech recognition, handwriting recognition and machine translation. While the ability of handling long-term dependency in data is the key for the success of RNN, combating over-fitting in training the models is a critical issue for achieving the cutting-edge performance particularly when the depth and size of the network increase. To that end, there have been some attempts to apply the dropout, a popular regularization scheme for the feed-forward neural networks, to RNNs, but they do not perform as well as other regularization scheme such as weight noise injection. In this paper, we propose rnnDrop, a novel variant of the dropout tailored for RNNs. Unlike the existing methods where dropout is applied only to the non-recurrent connections, the proposed method applies dropout to the recurrent connections as well in such a way that RNNs generalize well. Our experiments show that rnnDrop is a better regularization method than others including weight noise injection. Namely, when deep bidirectional long short-term memory (LSTM) RNNs were trained with rnnDrop as acoustic models for phoneme and speech recognition, they significantly outperformed the current state-of-the-arts; we achieved the phoneme error rate of 16.29% on the TIMIT core test set for phoneme recognition and the word error rate of 5.53% on the Wall Street Journal (WSJ) dataset, dev93, for speech recognition, which are the best reported results on both of the datasets.","","978-1-4799-7291-3","10.1109/ASRU.2015.7404775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404775","Recurrent neural networks;LSTM;Dropout;rnnDrop","Training;Speech recognition;Recurrent neural networks;Computer architecture;Microprocessors;Logic gates","feedforward neural nets;recurrent neural nets;speech recognition","RNNS;ASR;recurrent neural network;popular regularization scheme;feed-forward neural network;rnnDrop;weight noise injection;deep bidirectional long short-term memory RNN;deep bidirectional LSTM RNN;acoustic model;speech recognition;phoneme error rate;TIMIT core test set;phoneme recognition;Wall Street Journal dataset;WSJ dataset","","38","","24","IEEE","11 Feb 2016","","","IEEE","IEEE Conferences"
"Hiformer: Sequence Modeling Networks with Hierarchical Attention Mechanisms","X. Wu; H. Lu; K. Li; Z. Wu; X. Liu; H. Meng","Department of Systems Engineering and Engineering Mangement, The Chinese University of Hong Kong, Hong Kong SAR, China; Department of Systems Engineering and Engineering Mangement, The Chinese University of Hong Kong, Hong Kong SAR, China; Department of Systems Engineering and Engineering Mangement, The Chinese University of Hong Kong, Hong Kong SAR, China; Tsinghua Shenzhen International Graduate School of Tsinghua University, Shenzhen, China; Department of Systems Engineering and Engineering Mangement, The Chinese University of Hong Kong, Hong Kong SAR, China; Department of Systems Engineering and Engineering Mangement, The Chinese University of Hong Kong, Hong Kong SAR, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2023","PP","99","1","11","The attention-based encoder-decoder structure, such as the Transformer, has achieved state-of-the-art performance on various sequence modeling tasks, e.g., machine translation (MT) and automatic speech recognition (ASR), benefited from the superior capability of layer-wise self-attention mechanism in the encoder/decoder to access long-distance contextual information. Recently, analysis on the Transformer layers has shown that different levels of information, e.g., phoneme level, word level and semantic level, are represented at different layers. Effectively integrating information from various levels is important for structured prediction. However, the self-attention in the conventional Transformer structure only focuses on intra-layer integration, and does not explicitly model inter-layer information relationships. Also, attention across the encoder and decoder (cross-coder) only focuses on the top encoder layer but ignores the intermediate layers. In this paper, we propose a sequence modeling structure equipped with a hierarchical attention mechanism, named Hiformer, that can consider the inter-layer and cross-coder hierarchical information to improve structured prediction performance. Extensive experiments conducted on both MT and ASR tasks demonstrate the effectiveness of the proposed Hiformer model.","2329-9304","","10.1109/TASLP.2023.3313428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10244068","hierarchical attention mechanism;Transformer;automatic speech recognition;neural machine translation","Transformers;Decoding;Task analysis;Machine translation;Training;Semantics;Computational modeling","","","","","","","IEEE","8 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Sound-to-Sound Translation Using Generative Adversarial Network and Sound U-Net","Y. Kunisada; C. Premachandra","Dept. of Electrical Engineering and Computer Science, Graduate School of Engineering and Science, Shibaura Institute of Technology, Tokyo, Japan; Dept. of Electrical Engineering and Computer Science, Graduate School of Engineering and Science, Shibaura Institute of Technology, Tokyo, Japan","2022 2nd International Conference on Image Processing and Robotics (ICIPRob)","21 Jun 2022","2022","","","1","4","In this paper, we propose a generic learning method for training conditional generative adversarial networks on audio data. This makes it possible to apply the same generic approach as described in this study to problems that previously required completely different loss formulations when learning audio data. This method can be useful for labeling noises with a certain number of identical frequencies, generating speech labels corresponding to each frequency, and generating audio data for noise cancellation. To achieve this, we propose a sound restoration process based on U-Net, called Sound U-net. In this study, we realized a wide applicability of our system, owing to its ease of implementation without a parameter adjustment, as well as a reduction in the training time for audio data. During the experiment, reasonable results were obtained without manually adjusting the loss function.","","978-1-6654-0771-7","10.1109/ICIPRob54042.2022.9798737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798737","Generative Adversarial Networks;Conditional GAN;Sound U-Net;Audio Processing;Machine Leaning","Training;Learning systems;Generative adversarial networks;Noise cancellation;Image restoration;Labeling;Robots","audio signal processing;learning (artificial intelligence);neural nets;signal denoising;speech recognition","generic learning method;conditional generative adversarial networks;sound restoration process;sound-to-sound translation;audio data learning;Sound U-net","","1","","11","IEEE","21 Jun 2022","","","IEEE","IEEE Conferences"
"Arabic Speech Dialect Classification using Deep Learning","M. Alrehaili; T. Alasmari; A. Aoalshutayri","Department of Computer Science and Artificial Intelligence, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Department of Computer Science and Artificial Intelligence, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Department of Computer Science and Artificial Intelligence, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia","2023 1st International Conference on Advanced Innovations in Smart Cities (ICAISC)","3 Apr 2023","2023","","","1","5","The growing use of dialect around the country has recently drawn interest from speech technology and research communities in dialect detection. This article aimed to identify Arabic speech dialects and classify them according to the country of speaking. This study presents an analysis and preprocessing system for audio inputs that express the Arabic dialects within 8 Arab dialects. The dataset contains 672 data and eight main subgroups, 84 samples for each of the eight Arabic dialects. Arabic dialect features are extracted and modeled using Convolutional Neural Network (CNN) techniques. The study shows the suitability and efficiency of the system, deep learning models are used instead of machine learning models. The overall results reveal that CNN‚Äôs implementation of our proposed system for identifying Arabic dialects reaches a degree of accuracy of 83%. This paper has proposed a system that showed its superiority in performance. The system converts the speech into images using the spectrogram feature, and CNN is used because it can extract features from images automatically. The study contributes to enhancing the classification process of Arabic speech dialects which is an essential issue as many of the studies working on Modern Standard Arabic (MSA), while the majority of Arabs speak local dialects, it is necessary to identify the dialect used by speakers in order to communicate with one another or before machine translation takes place.","","978-1-6654-7275-3","10.1109/ICAISC56366.2023.10085647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10085647","Arabic Dialects;Convolutional Neural Networks;Dialect Identification;Automatic Dialect classification","Deep learning;Technological innovation;Smart cities;Speech enhancement;Feature extraction;Speech;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;learning (artificial intelligence);natural language processing","8 Arab dialects;Arabic dialect features;Arabic dialects;Arabic speech dialect classification;Arabic speech dialects;dialect detection;local dialects;Modern Standard Arabic","","","","18","IEEE","3 Apr 2023","","","IEEE","IEEE Conferences"
"Isarn Dharma Alphabets lexicon for natural language processing","N. Phaiboon; P. Seresangtakul","Department of Computer Science, Khon Kaen University, Khon Kaen, Thailand; Department of Computer Science, Khon Kaen University, Khon Kaen, Thailand","2017 9th International Conference on Knowledge and Smart Technology (KST)","27 Mar 2017","2017","","","211","215","Lexicon is a collection of individual words in the language, which is essential for NLP (Natural Language Processing) research such as machine translation, word segmentation and speech processing. According to the computerize system applying to Isarn Dharma Alphabets, this research aims to collect important features to support research in natural language and speech processing field. In the study, Isarn Dharma Alphabets lexicon using Trie structure was constructed. The lexicon consists of Isarn Dharma Alphabets words, Thai words, English words, phonemes, parts of speech, sub-parts of speech, special characteristics, Thai descriptions, and English descriptions. The lexicon contains approximately 8,000 words. Moreover, Isarn Dharma Alphabets transcription system has been proposed based on linguistic rules.","","978-1-4673-9077-4","10.1109/KST.2017.7886108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886108","Isarn Dharma Alphabets;Lexicon;NLP","Speech;Natural language processing;Speech processing;Pragmatics;Speech recognition;Computers","natural language processing;speech processing;text analysis","Isarn Dharma Alphabet lexicon;natural language processing;NLP;speech processing;Thai words;Isarn Dharma Alphabet words;Thai words;English words;phonemes;part-of-speech;sub-part-of-speech;special characteristics;Thai descriptions;English descriptions;Isarn Dharma Alphabet transcription system;linguistic rules","","1","","20","IEEE","27 Mar 2017","","","IEEE","IEEE Conferences"
"Foreign language audio information management system","M. Shichman; M. Gaffney; E. C. Fake; L. Sokol","Veridian, Arlington, VA, USA; Veridian, Arlington, VA, USA; Veridian, Arlington, VA, USA; Veridian, Arlington, VA, USA","Proceedings of the Fifth International Conference on Information Fusion. FUSION 2002. (IEEE Cat.No.02EX5997)","7 Nov 2002","2002","2","","1492","1498 vol.2","Veridian created a prototype of a foreign language audio information management system that integrates speech recognition technology, machine translation and advanced information retrieval and extraction for Mandarin Chinese. The system automatically processes audio recordings to create a data warehouse of derived information using speech recognition and machine translation technology components. The data warehouse can then be further exploited using information retrieval technology components. The prototype system provides the following capabilities: Automatically transforming foreign audio files into electronic text; Transforming foreign text into English text; Matching transcribed and translated text and the topics of interest to the analyst; Displaying transcribed text by speaker. The conclusions imply that while automatic speech processing technology is far from perfect for mass market distribution, it is sufficiently advanced to help with the overload of audio and video data.","","0-9721844-1-4","10.1109/ICIF.2002.1020993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1020993","","Information management;Natural languages;Prototypes;Speech recognition;Information retrieval;Data warehouses;Data mining;Audio recording;Consumer electronics;Speech processing","speech recognition;information retrieval;data mining;multimedia databases;data warehouses","foreign language audio information management;speech recognition;machine translation;advanced information retrieval;Mandarin Chinese;data warehouse;information retrieval;audio mining;automatic speech recognition","","","","5","","7 Nov 2002","","","IEEE","IEEE Conferences"
"Capitalization Feature and Learning Rate for Improving NER Based on RNN BiLSTM-CRF","Warto; Muljono; Purwanto; E. Noersasongko","Faculty of Computer Science, Dian Nuswantoro University, Semarang; Faculty of Computer Science, Dian Nuswantoro University, Semarang; Faculty of Computer Science, Dian Nuswantoro University, Semarang; Faculty of Computer Science, Dian Nuswantoro University, Semarang","2022 IEEE International Conference on Cybernetics and Computational Intelligence (CyberneticsCom)","29 Aug 2022","2022","","","398","403","Entity extraction in the natural language processing research field is still a widely researched topic. It can be a data source for the next NLP stage, such as text summarization, sentiment analysis, chatbot, machine translation, information retrieval, opinion mining, speech recognition, etc. Named Entity Recognition (NER) is the task of detecting named entities on the corpus. The detection process of entities can use various features, one of which is capital letters. Capital letters that appear at the beginning of a sentence indicate the name of a person, place, organization, geolocation, etc. The experiment uses the deep learning approach with Recurrent Neural Network Bidirectional Long Short Term Conditional Random Field (RNN-BiLSTM-CRF). Our comparing three optimization algorithms: Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam), and Adadelta, with the CoNLL2003 dataset. The experiment results using capital letter features showed an increase in the value of F1-Score by 2.9 higher compared to test results that did not use capital letter features. The highest F1-score score was 92.82 in testing using Adam's algorithm, with a 0.001 learning rate.","","978-1-6654-9742-8","10.1109/CyberneticsCom55287.2022.9865660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865660","Adadelta;Adaptive Moment Estimation;Letter Capitalization;Named Entity Recognition;Optimization;Stochastic Gradient Descent","Sentiment analysis;Recurrent neural networks;Text recognition;Soft sensors;Speech recognition;Organizations;Machine translation","data mining;gradient methods;information retrieval;learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition;text analysis","learning rate;capitalization feature;improving NER;RNN BiLSTM-CRF;Entity extraction;natural language;widely researched topic;data source;NLP stage;text summarization;sentiment analysis;machine translation;information retrieval;opinion mining;speech recognition;Named Entity Recognition;named entities;capital letters;deep learning approach;Recurrent Neural Network Bidirectional Long Short Term Conditional Random Field;RNN-BiLSTM-CRF;comparing three optimization algorithms;capital letter features","","1","","46","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Query based information retrieval and knowledge extraction using Hadith datasets","A. Mahmood; H. U. Khan; Zahoor-ur-Rehman; W. Khan","COMSATS University Islamabad, Islamabad, Islamabad, PK; COMSATS University Islamabad, Islamabad, Islamabad, PK; COMSATS University Islamabad, Islamabad, Islamabad, PK; International Islamic University, Islamabad, PK","2017 13th International Conference on Emerging Technologies (ICET)","8 Feb 2018","2017","","","1","6","In Natural language processing, one of the fundamental tasks is Named Entity Recognition (NER) that include identifying names of peoples, locations and other entities. Applications of NER include catboats, speech recognition, machine translation, knowledge extraction and intelligent search systems. NER is an active research domain for the last 10 years. In this paper, we propose a knowledge extraction framework to extract Named entities from Sahih AlBukhari Urdu translation which is a world known Hadith book. The proposed framework is based on finite state transducer system to extract entities and process the Hadith content using Part of Speech (POS) tagging. Conditional Random Field, an ensemble based algorithm, processes the extracted nouns for NER and classification. In the future, we aim to implement the proposed framework to rank the hadith content and apply the Vector Space Model.","","978-1-5386-2260-5","10.1109/ICET.2017.8281714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281714","Data Extraction;Hadith Information Retrieval;Framework;Named Entity Recognition","Data mining;Task analysis;Tagging;Speech;Natural languages;Transducers","knowledge acquisition;language translation;natural language processing;query processing","NER;knowledge extraction framework;Sahih AlBukhari Urdu translation;finite state transducer system;query based information retrieval;Hadith datasets;part of speech tagging;nouns extraction;natural language processing;named entity recognition;Hadith content processing;POS tagging","","14","","29","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Machine translation system to convert Sinhala and English Braille documents into voice","K. S. Anuradha; S. Thelijjagoda","Faculty of Graduate Studies & Research, Sri Lanka Institute of Information Technology, Sri Lanka; SLIIT Business School, Sri Lanka Institute of Information Technology, Sri Lanka","2020 International Research Conference on Smart Computing and Systems Engineering (SCSE)","12 Jan 2021","2020","","","7","16","Reading Braille documents are a time-consuming and labor-intensive task. A blind person should touch every Braille letter by his or her fingers. Therefore, high sensitivity in fingers and memorizing every Braille letters are key factors in Braille reading. Due to enhancement in technology, several Optical Character Recognition (OCR) systems have been introduced for different languages in different parts of the world. However, in Sri Lanka, there are no systems that extract Sinhala or English Braille characters using OCR and convert those Braille codes to sound output. The main purpose of the research is to create a system that extracts both Sinhala and English Braille segments from a given Braille document and makes the Braille content into voice. At the beginning Embossed Sinhala or English Braille image which took from webcam or a high-resolution phone will use for image processing techniques such as gray scaling, thresholding, erosion, and dilation. Erosion and gray scaling help to eliminate the noise and the noise-free image used to detect contour. After pre-processing, the image using an OpenCV library do the segmentation and character extraction. Braille character recognition has done by taking binary to decimal equivalent numbers. Before generating voice output, recognized Braille letters should convert to corresponding language letters (either Sinhala letter or English) and mapping English letters can directly generate English words but in Sinhala need an additional step called Unicode Mapping to generate a Sinhala word. When a Braille document is in high quality, the system will reach for 95 percent accuracy level and generally, results have around 85.30 percent success rate. This software provides many usability characteristics to increase simplicity when it's using OpenCV and related technologies. Even teachers can use this to improve their teaching terminologies and explain things more clearly in their lessons.","2613-8662","978-1-7281-7249-1","10.1109/SCSE49731.2020.9313020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313020","Braille letters;Image processing;OCR;Segmentation;Unicode mapping","Image processing;Libraries;Kernel;Image segmentation;Blindness;Thresholding (Imaging);Optical character recognition software","document image processing;feature extraction;handicapped aids;image segmentation;language translation;natural language processing;optical character recognition;speech processing","optical character recognition systems;OCR;English Braille characters;Braille codes;English Braille segments;English Braille image;high-resolution phone;image processing techniques;gray scaling;noise-free image;character extraction;Braille character recognition;voice output;Sinhala letter;English words;Sinhala word;machine translation system;English Braille documents;labor-intensive task;Braille letter;OpenCV;Webcam","","3","","10","IEEE","12 Jan 2021","","","IEEE","IEEE Conferences"
"Parla! A proposal for a Brain-Computer Interface assistive communication software protocol to translate thought to speech for deaf, hard of hearing or individuals with severe paralysis by using brain waves signal datasets obtained from a brain implant","C. I. Da Silva; J. L. S. Filho; J. De Almeida; D. R. B. Da Silva; B. Stalbaum; M. De Abreu Borges","Educational design, Federal University of S√£o Paulo (Unifesp), S√£o Paulo, Brazil; PPG-EAHC, Catholic University of S√£o Paulo (PUC-SP), S√£o Paulo, Brazil; ICAM/Speculative design, University of California, San Diego (UCSD), La Jolla, US; PPGI/CI-UFPB, Federal University of Para√≠ba (UFPB), Jo√£o Pessoa, Brazil; PPGEEC/CT-UFRN, Federal University of Rio Grande do Norte (UFRN), Natal, Brazil; A&W Tecnologia, Rio de Janeiro, Brazil","2023 18th Iberian Conference on Information Systems and Technologies (CISTI)","15 Aug 2023","2023","","","1","5","The goal of the article is to introduce a speculative educational design project based on an assistive communication approach to provide deaf, hard of hearing or individuals with paralysis an unmodified electronic commercial speech synthesis voice assistant software tool. The system will work as a prosthesis capable of translating thoughts to speech by processing brain waves signals from a brain implant. The article also aims to create a thought to speech translation dictionary database and protocol by using machine learning, neural nets and LLMs.","2166-0727","978-989-33-4792-8","10.23919/CISTI58278.2023.10211462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211462","assistive communication;brain implant;speech processing;thought to speech synthesis voice assistant;brain-computer interface","Protocols;Neural implants;Auditory system;Paralysis;Sensors;Speech synthesis;Reliability","","","","","","32","","15 Aug 2023","","","IEEE","IEEE Conferences"
"Automatic Lecture Subtitle Generation and How It Helps","X. Che; S. Luo; H. Yang; C. Meinel","Hasso Plattner Institute, Potsdam, Germany; Hasso Plattner Institute, Potsdam, Germany; Hasso Plattner Institute, Potsdam, Germany; Hasso Plattner Institute, Potsdam, Germany","2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)","7 Aug 2017","2017","","","34","38","In this paper we propose an integrated framework of automatic bilingual subtitle generation for lecture videos, especially for MOOCs. The framework consists of Automatic Speech Recognition (ASR), Sentence Boundary Detection (SBD), and Machine Translation (MT). Then we quantitatively evaluate the auto-generated subtitles, the manually produced subtitles from scratch, and the auto-generated subtitles with manual modification in term of accuracy and time expenditure, in both original and target languages. The result shows that the auto-generated subtitles in the original language (English) are fairly accurate already. By using them as the draft, human subtitle producers can save 54% of the working time and simultaneously reduce the error rate by 54.3%, which is a significant improvement. However, the effectiveness of machine translated subtitles (English to Chinese) is limited. In the end, if the proposed framework is applied, the total working time in preparing bilingual subtitles can be shortened by approximately 1/3, with no decline in quality.","2161-377X","978-1-5386-3870-5","10.1109/ICALT.2017.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001709","Automatic Subtitling;Sentence Boundary Detection;Lecture Videos;MOOC","Videos;Manuals;Acoustics;Mathematical model;Data models;Automatic speech recognition;Electronic learning","computer aided instruction;language translation;natural language processing;speech recognition","automatic lecture subtitle generation;integrated framework;automatic bilingual subtitle generation;lecture videos;MOOC;automatic speech recognition;ASR;sentence boundary detection;SBD;machine translation;MT;autogenerated subtitle generation;original languages;target languages;human subtitle producers;English-Chinese translation","","10","","24","IEEE","7 Aug 2017","","","IEEE","IEEE Conferences"
"An improved recurrent neural network language model with context vector features","J. Zhang; D. Qu; Z. Li","National Digital Switching System, Engineering & Technological R&D Center, Zhengzhou, Henan Province, China; National Digital Switching System, Engineering & Technological R&D Center, Zhengzhou, Henan Province, China; National Digital Switching System, Engineering & Technological R&D Center, Zhengzhou, Henan Province, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","23 Oct 2014","2014","","","828","831","Recurrent neural network language models have solved the problems of data sparseness and dimensionality disaster which exist in traditional N-gram models. RNNLMs have recently demonstrated state-of-the-art performance in speech recognition, machine translation and other tasks. In this paper, we improve the model performance by providing contextual word vectors in association with RNNLMs. This method can reinforce the ability of learning long-distance information using vectors training from Skip-gram model. The experimental results show that the proposed method can improve the perplexity performance significantly on Penn Treebank data. And we further apply the models to speech recognition task on the Wall Street Journal corpora, where we achieve obvious improvements in word-error-rate.","2327-0594","978-1-4799-3279-5","10.1109/ICSESS.2014.6933694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933694","Speech Recognition;Language Model;Recurrent Neural Network;Skip-gram","Vectors;Context modeling;Recurrent neural networks;Hidden Markov models;Neurons;Computational modeling;Context","recurrent neural nets;speech recognition","recurrent neural network language model;context vector features;data sparseness;data dimensionality;N-gram models;RNNLM;machine translation;contextual word vectors;long-distance information learning;perplexity performance;Penn Treebank data;speech recognition task;Wall Street Journal corpora","","2","","16","IEEE","23 Oct 2014","","","IEEE","IEEE Conferences"
"Advances in Arabic Speech Transcription at IBM Under the DARPA GALE Program","H. Soltau; G. Saon; B. Kingsbury; H. -K. J. Kuo; L. Mangu; D. Povey; A. Emami","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA","IEEE Transactions on Audio, Speech, and Language Processing","16 Jun 2009","2009","17","5","884","894","This paper describes the Arabic broadcast transcription system fielded by IBM in the GALE Phase 2.5 machine translation evaluation. Key advances include the use of additional training data from the Linguistic Data Consortium (LDC), use of a very large vocabulary comprising 737 K words and 2.5 M pronunciation variants, automatic vowelization using flat-start training, cross-adaptation between unvowelized and vowelized acoustic models, and rescoring with a neural-network language model. The resulting system achieves word error rates below 10% on Arabic broadcasts. Very large scale experiments with unsupervised training demonstrate that the utility of unsupervised data depends on the amount of supervised data available. While unsupervised training improves system performance when a limited amount (135 h) of supervised data is available, these gains disappear when a greater amount (848 h) of supervised data is used, even with a very large (7069 h) corpus of unsupervised data. We also describe a method for modeling Arabic dialects that avoids the problem of data sparseness entailed by dialect-specific acoustic models via the use of non-phonetic, dialect questions in the decision trees. We show how this method can be used with a statically compiled decoding graph by partitioning the decision trees into a static component and a dynamic component, with the dynamic component being replaced by a mapping that is evaluated at run-time.","1558-7924","","10.1109/TASL.2009.2022966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5075768","Dialect modeling;discriminative training;speech recognition;vowelization","Speech;Broadcasting;Decision trees;Training data;Vocabulary;Error analysis;Large-scale systems;System performance;Performance gain;Decoding","decision trees;natural language processing;speech processing;unsupervised learning","speech transcription;DARPA GALE program;dialect modelling;pronunciation variants;automatic vowelization;flat-start training;unsupervised training;decision trees;decoding graph","","22","3","23","IEEE","16 Jun 2009","","","IEEE","IEEE Journals"
"Languages and the computing profession","N. Holmes","University of Tasmania, Australia","Computer","2 Aug 2004","2004","37","3","104","103","When involved in a project to develop an intermediary language, translators, interpreters, and linguists will need to work closely together, as grammar and vocabulary are closely interdependent. In this case, both must cope with the translation of many hundreds of wildly different languages. What role would computing professionals play in such a team? Given the project's purpose - to make general machine translation possible - computing professionals would be of vital importance, but in a supporting role. Using different approaches to evaluate the intermediate language and its use for a variety of languages would require a succession of translation programs.","1558-0814","","10.1109/MC.2004.1274016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1274016","","Natural languages;Speech;Biomembranes;Vocabulary;Taxonomy;Concrete","language translation;computational linguistics","intermediary language;machine translation;translation programs;linguists","","","","","IEEE","2 Aug 2004","","","IEEE","IEEE Magazines"
"Parsing for prosody: what a text-to-speech system needs from syntax","E. Fitzpatrick; J. Bachenko","AT&T Bell Lab., Murray Hill, NJ, USA; AT&T Bell Labaratories, Murray Hill, NJ, USA","[1989] Proceedings. The Annual AI Systems in Government Conference","6 Aug 2002","1989","","","188","194","The authors describe an experimental text-to-speech system that uses a syntactic parser and prosody rules to determine prosodic phrasing for synthesized speech. It is shown that many aspects of sentence analysis that are required for other parsing applications, e.g. machine translation and question answering, become unnecessary in parsing for text-to-speech. It is possible to generate natural-sounding prosodic phrasing by relying on information about syntactic category type, partial constituency, and length; information about clausal and verb phrase constituency, predicate-argument relations, and prepositional phrase attachment can by bypassed.<>","","0-8186-1934-1","10.1109/AISIG.1989.47324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=47324","","Speech synthesis;Speech processing;Synthesizers;Stress;Dictionaries;Robustness;System testing;Information retrieval;Character generation;Dairy products","grammars;natural languages;speech synthesis","experimental text-to-speech system;syntactic parser;prosody rules;synthesized speech;sentence analysis;natural-sounding prosodic phrasing;syntactic category type;partial constituency","","3","9","19","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Structure learning for natural language processing","Y. Ni; C. J. Saunders; S. Szedmak; M. Niranjan","ISIS Group, School of Electronics and Computer Science, University of Southampton, Southampton, UK; ISIS Group, School of Electronics and Computer Science, University of Southampton, Southampton, UK; ISIS Group, School of Electronics and Computer Science, University of Southampton, Southampton, UK; ISIS Group, School of Electronics and Computer Science, University of Southampton, Southampton, UK","2009 IEEE International Workshop on Machine Learning for Signal Processing","30 Oct 2009","2009","","","1","6","We applied a structure learning model, Max-Margin Structure (MMS), to natural language processing (NLP) tasks, where the aim is to capture the latent relationships within the output language domain. We formulate this model as an extension of multi-class Support Vector Machine (SVM) and present a perceptron-based learning approach to solve the problem. Experiments are carried out on two related NLP tasks: part-of-speech (POS) tagging and machine translation (MT), illustrating the effectiveness of the model.","2378-928X","978-1-4244-4947-7","10.1109/MLSP.2009.5306193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306193","","Natural language processing;Tagging;Support vector machines;Hidden Markov models;Machine learning;Entropy;Surface-mount technology;Maximum likelihood estimation;Intersymbol interference;Computer science","learning (artificial intelligence);natural language processing;support vector machines","natural language processing;structure learning model;Max-Margin Structure model;support vector machine;perceptron-based learning approach;part-of-speech tagging task;machine translation task","","","","18","IEEE","30 Oct 2009","","","IEEE","IEEE Conferences"
"CONVEX: Conjunct Verb extraction from parallel corpus: A hybrid approach","S. K. Choudhury; B. Kundu","Language Technology, Centre for Development of Advanced Computing, Kolkata, India; Language Technology, Centre for Development of Advanced Computing, Kolkata, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","21 Mar 2013","2012","","","1","6","Conjunct Verbs (CVs) are one of the special forms of Complex Predicates that behave as a single verbal unit but maintain a multiword structure. CVs play an important role in Natural Language Processing applications like Speech to Speech Translation, Machine Translation and lexical resource creation. But due to their distinct construction, detection and extraction of CVs is a challenging task. This paper presents a hybrid approach for mining CVs from parallel corpus combining rule-based and statistical approach. Though the proposed approach has been applied on Bangla-English parallel corpus to extract Bangla CVs, the methodology is equally applicable to other Indian languages of Indo-Aryan family, in presence of parts of speech tagger and sufficient amount of parallel corpus. Evaluation on Bangla-English parallel corpus of 50,000 sentences, the proposed approach yields an accuracy of 76% that can be improved by increasing the number of sentence pairs in the parallel corpus.","","978-1-4673-4369-5","10.1109/IHCI.2012.6481852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481852","Parallel Corpus;Conjunct Verbs;Word Alignment;Expectation Maximization","Helium;Pragmatics;Hidden Markov models;Speech;Speech processing;Accuracy;Periodic structures","data mining;grammars;natural language processing","CONVEX;conjunct verb extraction;conjunct verb mining;complex predicates;single verbal unit;multiword structure;natural language processing;speech to speech translation;machine translation;lexical resource creation;rule-based approach;statistical approach;Bangla-English parallel corpus","","","","9","IEEE","21 Mar 2013","","","IEEE","IEEE Conferences"
"Operation-Augmented Numerical Reasoning for Question Answering","Y. Zhou; J. Bao; Y. Wu; X. He; T. Zhao","Machine Intelligence and Translation Laboratory at the School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China; JD AI Research, Beijing, China; Machine Intelligence and Translation Laboratory at the School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","","2023","PP","99","1","15","Question answering requiring numerical reasoning, which generally involves symbolic operations such as sorting, counting, and addition, is a challenging task. To address such a problem, existing mixture-of-experts (MoE)-based methods design several specific answer predictors to handle different types of questions and achieve promising performance. However, they ignore the modeling and exploitation of fine-grained reasoning-related operations to support numerical reasoning, encountering the inadequacy in reasoning capability and interpretability. To alleviate this issue, we propose OPERA, an operation-augmented numerical reasoning framework. Concretely, we systematically define a scalable operation set to model numerical reasoning. We first identify reasoning-related operations based on context and then softly execute them to imitate the answer reasoning procedure via an operation-aware cross-attention mechanism. Finally, we utilize the operation-augmented semantic representation of execution results to support answer prediction. We verify the effectiveness and generalization of OPERA in two scenarios with different knowledge sources and reasoning capabilities. Specifically, we conduct extensive experiments on two textual datasets, DROP and RACENum, and a table-text hybrid dataset TAT-QA. Experiment results show that OPERA outperforms previous strong methods on the DROP, RACENum, and TAT-QA datasets. Further, we statistically and visually analyze its interpretability.","2329-9304","","10.1109/TASLP.2023.3316448","National Natural Science Foundation of China(grant numbers:U1908216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254290","Numerical Reasoning;Symbolic Operations;Semantic Augmentation;Mixture-of-Experts","Cognition;Task analysis;Semantics;Speech processing;Sorting;Question answering (information retrieval);Predictive models","","","","","","","IEEE","18 Sep 2023","","","IEEE","IEEE Early Access Articles"
"Interest points from the radial mass transform [image representation applications]","P. B. Albee; G. C. Stockman","Department of CS, Central Michigan University, Mount Pleasant, MI, USA; Department of CSE, Michigan State University, East Lansing, MI, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","2 May 2005","2005","2","","ii/261","ii/264 Vol. 2","The radial mass transform (RMT) is defined to produce a rotation and translation invariant vector representation of the neighborhood structure of points. The RMT transforms 3D (or 2D) data sets into a 1D signal where m(p, r) gives the total mass (or intensity) of sensed data at distance r from the point p. A support vector machine can be trained on example signals to detect salient points in an entire image or volume. Results show the method to be effective in multiple applications. The method is computation intensive but highly parallelizable and feasible for high value data sets.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415391","","Support vector machines;Signal detection;Concurrent computing;Image processing;Imaging phantoms;Magnetic resonance imaging;Pixel;Contracts;Integral equations;Clustering algorithms","feature extraction;transforms;image representation;support vector machines","interest point detection;radial mass transform;RMT;rotation vector representation;translation invariant vector representation;point neighborhood structure representation;3D data sets;2D data sets;support vector machine;salient point detection","","1","","9","IEEE","2 May 2005","","","IEEE","IEEE Conferences"
"Disambiguation and Error Resolution in Call Transcripts","J. Hosier; V. K. Gurbani; N. Milstead","Vail Systems, Inc.; Vail Systems, Inc.; Vail Systems, Inc.","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","4602","4607","Ambiguity is inherent to human language and poses a unique challenge both to human listeners as well as natural language processing (NLP) systems. Ambiguity is understood as a type of uncertainty which allows for more than one plausible interpretation of utterances. Ambiguity can introduce problems for NLP systems designed to, for example, execute machine translation, determine sentiment, and perform automatic speech recognition (ASR). We seek to identify and resolve mis-transcriptions that arise from phonetic ambiguity and degraded acoustic signals in 87,000 call transcripts. We first present an alignment algorithm which identifies mis-transcriptions generated by ASR systems when compared against verified human transcriptions. This method not only allows for a general evaluation of ASR performance but also highlights specific areas of difficulty for such systems (e.g. ‚Äúconsiderate‚Äù vs. ‚Äúconsider it‚Äù). We further present an error resolution algorithm which, given a mis-transcribed word, uses contextual cues to suggest a more likely, phonetically similar word. This work has the potential to not only evaluate existing ASR systems, but also to immediately improve their performance.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005993","homophone disambiguation;automatic speech recognition;text processing","Phonetics;Encoding;Natural language processing;Engines;Approximation algorithms;Big Data;Acoustics","natural language processing;speech recognition","call transcripts;human language;human listeners;natural language processing systems;NLP systems;machine translation;automatic speech recognition;mis-transcriptions;phonetic ambiguity;acoustic signals;alignment algorithm;verified human transcriptions;ASR performance;error resolution algorithm;mis-transcribed word;phonetically similar word;ASR systems","","","","10","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"A Bi-directional Interactive System of Sign Language and Visual Speech Based on Portable Devices","F. Wang; S. Sun; Y. Liu","Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Faulty of Robot Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Faulty of Robot Science and Engineering, Northeastern University, Shenyang, China","2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20 Jan 2020","2019","","","1071","1076","At present, the natural communication between deaf and normal people is a major challenge both in theoretical research and application. In addition, in the field of Human-Machine Interaction, there is little work on bi-directional communication based on sign language and visual speech. In this paper, a portable bi-directional interactive system of sign language and visual speech was proposed to help deaf people communicate naturally under certain circumstances. In the section of modeling, Network in Network model was employed to classify sign language words, while the method using a network combined DenseNet and LSTM together was used for lip reading. By wearing the portable device, people can achieve bi-directional translation and communication of visual speech and sign language. We built our own Chinese sign language database containing more than 100 categories of words and conducted experiments in the context of the airport for experimental verification. Eventually, the average time of each round of conversation is within 15 seconds, faster than normal communication without system assistance, which verified the effectiveness and advantage of the purposed system.","","978-1-7281-6321-5","10.1109/ROBIO49542.2019.8961831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961831","Chinese Sign Language Recognition;Visual Speech Recognition;Network in Network;DenseNet;LSTM","","handicapped aids;human computer interaction;image classification;interactive systems;language translation;natural language processing;recurrent neural nets;sign language recognition","visual speech;deaf people;portable device;bi-directional translation;Chinese sign language database;normal communication;natural communication;human-machine interaction;bi-directional communication;portable bi-directional interactive system;network in network model;sign language words classification;DenseNet;LSTM;lip reading","","1","","13","IEEE","20 Jan 2020","","","IEEE","IEEE Conferences"
"A Rule-based Lemmatizing Approach for Sinhala Language","M. Nandathilaka; S. Ahangama; G. T. Weerasuriya","Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka; Faculty of Information Technology, University of Moratuwa, Moratuwa, Sri Lanka","2018 3rd International Conference on Information Technology Research (ICITR)","13 Jun 2019","2018","","","1","5","Speech recognition, natural language processing, language translation and deep learning researches are bridging the communication gap between humans as well as between humans and machines. Sinhala is a native language in Sri Lanka which is being used by 19 million people approximately. The growth of Sinhala natural language processing tools is less when compared to European and other Asian Languages. A lemmatizer for Sinhala can be used for the morphological analysis and is an essential module in Sinhala language processing mechanisms. Lemmatizing is a complex process in morphological analyzing where base/root of words are derived. There is not much work published focusing on lemmatizer approaches for Sinhala. This paper presents a rule based lemmatizing approach which can be used to determine the base form of Sinhala words with an accuracy of 77.3%. It differs from similar works because the data used in the research are extracted from social media.","","978-1-7281-1470-5","10.1109/ICITR.2018.8736134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736134","Sinhala Morphology;Lemmatization;Inflection;Rule-based;Social media data","Social networking (online);Europe;Grammar;Morphology;Tools;Information technology","knowledge based systems;language translation;learning (artificial intelligence);natural language processing;social networking (online);speech recognition;text analysis;word processing","language translation;deep learning researches;native language;Sinhala natural language processing tools;European Languages;Sinhala words;speech recognition;rule-based lemmatizing approach;Asian Languages;social media","","7","","21","IEEE","13 Jun 2019","","","IEEE","IEEE Conferences"
"IEEE Standard Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversion Version 1.2","",,"IEEE Std 3300-2022","28 Apr 2023","2023","","","1","108","This standard adopts MPAI Technical Specification Version 1.2 as an IEEE Standard. Multimodal Conversation (MPAI-MMC) is an MPAI Standard comprising five use cases, all sharing the use of artificial intelligence (AI) to enable a form of human-machine conversation in completeness and intensity.","","978-1-5044-9330-7","10.1109/IEEESTD.2023.10112603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112603","AI Modules;Bidirectional Speech Translation;BST;Conversation with Emotion;CWE;IEEE 3300;MQA;MST;Multimodal Conversation;Multimodal Question Answering;One-to-Many Speech Translation;Unidirectional Speech Translation;Use Cases;UST","IEEE Standards;Artificial intelligence;Audio recording;Motion pictures;MPEG Standards","artificial intelligence;audio coding;human computer interaction;IEEE standards","artificial intelligence;audio coding;data coding;human-machine conversation;IEEE Standard adoption;moving picture;MPAI Standard;MPAI Technical Specification Version 1.2;MPAI-MMC;Multimodal Conversation","","","","0","","28 Apr 2023","","","IEEE","IEEE Standards"
"A Novel approach to improve rule based Telugu morphological analyzer","K. V. N. Sunitha; N. Kalyani","Computer Science Department, G. Narayanamma Institute of Technology and Science, Hyderabad, India; Computer Science Department, G. Narayanamma Institute of Technology and Science, Hyderabad, India","2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)","22 Jan 2010","2009","","","1649","1652","Telugu is an Indian language spoken by more than 50 million people in the country. Language is very rich in literature, and it requires advancements in computational approaches. Applications like machine translation, speech recognition, speech synthesis and information retrieval need a powerful morphological generator to give morphological forms of nouns and verbs. The existing Telugu morphological analyzer (TMA) is rule based. The performance of it is further improved by our novel approach which provides a system that gives information about possible decompositions of the word inflected by many morphemes. Using these possible decompositions the root word could be extracted for those words which were unrecognized by rule based morphological analyzer. The experiment is conducted on Telugu text corpus from CIIL Mysore and the improvement in the performance is checked by the rule based morphological analyzer developed by LTRC group, IIIT and HCU,Hyderabad. In this present work we present an unsupervised stemmer for improving the performance of Telugu rule based morph analyzer. The observed increase in performance of rule based is from 77% to 84.2% for words which are in hundreds. It can still be improved if the corpus is increased.","","978-1-4244-5053-4","10.1109/NABIC.2009.5393637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393637","Unsupervised stemmer;morphological analyzer;clusterin;morphemes;Segmentatio","Natural languages;Data mining;Performance analysis;Information analysis;Speech analysis;Iterative algorithms;Computer science;Application software;Speech recognition;Speech synthesis","knowledge based systems;natural language processing;text analysis","rule based Telugu morphological analyzer;Indian language;machine translation;speech recognition;speech synthesis;information retrieval;morphological generator;Telugu text corpus;CIIL Mysore;unsupervised stemmer","","2","","11","IEEE","22 Jan 2010","","","IEEE","IEEE Conferences"
"Word clustering with parallel spoken language corpora","Ye-Yi Wang; J. Lafferty; A. Waibel","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","4","","2364","2367 vol.4","We introduce a word clustering algorithm which uses a bilingual, parallel corpus to group together words in the source and target language. Our method generalizes previous mutual information clustering algorithms for monolingual data by incorporating a statistical translation model. Preliminary experiments have shown that the algorithm can effectively employ the constraints implicit in bilingual data to extract classes which are well suited to machine translation tasks.","","0-7803-3555-4","10.1109/ICSLP.1996.607283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607283","","Natural languages;Clustering algorithms;Mutual information;Data mining;Books;Scheduling;Entropy;Greedy algorithms;Merging;Bridges","speech processing;language translation;natural languages;word processing;statistical analysis","word clustering algorithm;parallel spoken language corpora;bilingual parallel corpus;mutual information clustering algorithms;monolingual data;statistical translation model;bilingual data;machine translation tasks","","1","","6","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Building a Part of Speech tagger for the Tamil Language","K. Sarveswaran; G. Dias","University of Moratuwa, Sri Lanka; University of Moratuwa, Sri Lanka","2021 International Conference on Asian Language Processing (IALP)","19 Jan 2022","2021","","","286","291","Identifying the lexical category or Part of Speech (POS) of words is critical for developing Natural Language Processing (NLP) systems. Existing Tamil POS taggers are either not publicly available, inaccurate or use a non-standard POS tagset. In this paper, we present a state-of-the-art, contextual neural POS tagger named ThamizhiPOSt. It tags words in a sentence with their lexical category, using the Universal Part of Speech tagset. Each tag is based on the word's context in the sentence. ThamizhiPOSt also integrates a tokeniser and a sentence segmenter so that it can handle raw Tamil text. This paper introduces this POS tagger and compares it with existing tools and resources for Tamil POS tagging. ThamizhiPOSt reports an accuracy of 93.27% for unseen data, which constitutes the best score in comparison to the publicly available Tamil POS taggers. ThamizhiPOSt and its associated resources are all made public. Further, ThamizhiPOSt has been published as a Python library for others to build upon.","","978-1-6654-8311-7","10.1109/IALP54817.2021.9675195","World Bank; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675195","POS tagging;Tamil language;contextual tagging;NLP","Buildings;Tagging;Libraries;Machine translation;Speech processing;Engines","natural language processing;text analysis;word processing","natural language processing systems;contextual neural POS tagger;ThamizhiPOSt;lexical category;sentence segmenter;raw Tamil text;Tamil POS tagging;part of speech tagger;Tamil Language;Tamil POS taggers;Universal Part of Speech tagset;Python library;tokeniser","","","","18","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"Implementation of Chinese-Uyghur Bilateral EBMT System","K. Abiderexiti; T. Yao; T. Yibulayin; A. Wumaier; Y. Yiming","School of Information Science and Engineering, Xinjiang University, Urumuqi, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Information Science and Engineering, Xinjiang University, Urumuqi, China; School of Information Science and Engineering, Xinjiang University, Urumuqi, China; Xinjiang Uyghur Autonomous Regional Working Committee, Urumuqi, China","2013 International Conference on Asian Language Processing","24 Oct 2013","2013","","","87","90","This work makes first attempt towards Chinese-Uyghur bilateral Machine Translation (MT) using an example based approach. We have developed a Chinese-Uyghur bilateral EBMT system by constructing bilingual corpus, implementing its storage format, developing similarity computation algorithm and recombining similar sentence. Finally, we evaluate our system by human judges, experimental result shows that there is still room for improvement for translation quality of our system.","","978-0-7695-5063-3","10.1109/IALP.2013.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646010","Uyghur;EBMT;biligual corpus;sentence recombination","Indexes;Libraries;Computer science;Educational institutions;Speech;Computer architecture;Algorithm design and analysis","language translation;natural language processing","Chinese-Uyghur bilateral EBMT system;machine translation;bilingual corpus;storage format;similarity computation algorithm;similar sentence recombination;human judges;translation quality","","1","","15","IEEE","24 Oct 2013","","","IEEE","IEEE Conferences"
"Question Mark Prediction By Bert","Y. Cai; D. Wang","Babel Technology, Beijing, China; Center for Speech and Language Technologies, Tsinghua University, Beijing, China","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","363","367","Punctuation resotration is important for Automatic Speech Recognition and the down-stream applications, e.g., speech translation. Despite the continuous progress on punctuation restoration, discriminating question marks and periods remains very hard. This difficulty can be largely attributed to the fact that interrogatives and narrative sentences are mostly characterized and distinguished by long-distance syntactic and semantic dependencies, which are cannot well modeled by existing models (e.g., RNN or n-gram). In this paper we propose to solve this problem by the self-attention mechanism of the Bert model. Our experiments demonstrated that compared the best baseline, the new approach improved the F1 score of question mark prediction from 30% to 90%.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023090","","Machine-to-machine communications","natural language processing;neural nets;speech recognition","narrative sentences;long-distance syntactic;Bert model;question mark prediction;punctuation resotration;Automatic Speech Recognition;punctuation restoration;semantic dependencies","","1","","23","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Different Models of Transliteration - A Comprehensive Review","M. Yadav; I. Kumar; A. Kumar","Department of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Department of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Software engineer, Noida, India","2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA)","20 Apr 2023","2023","","","356","363","Accessing information because of language barriers has grown more crucial as globalization accelerates. The field of study called machine transliteration transforms words from one language to another while preserving their phonetic properties. With the rise of the Internet and the rapid expansion of online data, Information Retrieval (IR) has become a more significant field. Foreign terms are increasingly being incorporated into languages due to the development of new technologies and the flood of information available online. This generally indudes changing the adopted word's orthographical form as well as its original pronunciation to match the phonological norms of the target language. Transliteration describes this phonetic ‚Äútranslation‚Äù of a foreign language. Machine translation and cross-language information retrieval both benefit from transliteration. Ambiguity, variability, and out-of-vocabulary (OOV) words are a few of the crucial challenges that transliteration systems must deal with. This paper represents the transliteration models of the last decade used for transliteration-related research carried out by researchers.","","979-8-3503-9720-8","10.1109/ICIDCA56705.2023.10099632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099632","Transliteration;Transliteration models;Grapheme;Phoneme;Hybrid;Transliteration approaches","Training;Support vector machines;Hidden Markov models;Training data;Transforms;Phonetics;Information retrieval","information retrieval;Internet;language translation;natural language processing;speech processing","cross-language information retrieval;foreign language;foreign terms;globalization;language barriers;machine translation;machine transliteration;online data;OOV word;out-of-vocabulary word;phonetic properties;phonetic translation;target language;transliteration systems","","1","","43","IEEE","20 Apr 2023","","","IEEE","IEEE Conferences"
"Part of Speech Tagging for Setswana African Language","M. A. Dibitso; P. A. Owolawi; S. O. Ojo","Department of Computer Systems Engineering, Tshwane University of Technology, Soshanguve South, South Africa; Department of Computer Systems Engineering, Tshwane University of Technology, Soshanguve South, South Africa; Department of Computer Science, Tshwane University of Technology, Soshanguve South, South Africa","2019 International Multidisciplinary Information Technology and Engineering Conference (IMITEC)","28 Feb 2020","2019","","","1","6","Part of speech (POS) tagging is the technique that assigns appropriate lexical categories to words in a sentence. It is a crucial step in Natural Language Processing (NLP) applications such as Machine Translation, Spell and Grammar checking, Word Predictions, Information Retrieval, etc.. A lot of work has been done on POS tagging mainly for European and Asiatic languages, while in Africa, more work is needed mostly due to the lack of the annotated corpus. Some significant works have been done on African languages, such as Arabic, Igbo, Swahili and Yoruba, South African official languages. However, African languages are generally under-resourced, in particular, in terms of lexical semantics annotated corpora, necessary for effective NLP tools and applications. Hence, advances in this direction have been limited. The main aim of the work reported in this paper is the development of a POS tagger model for an under-resourced Setswana African language. A review of some POS taggers for different African languages is conducted, challenges and techniques used in creating the POS taggers are elicited, and a POS tagger model for Setswana language using SVMTool is presented.","","978-1-7281-0040-1","10.1109/IMITEC45504.2019.9015871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9015871","NLP;POS tagging;African languages;Annotated corpus;Machine learning","Tagging;Hidden Markov models;Tools;Machine learning;Data models;Support vector machines;Linguistics","learning (artificial intelligence);natural language processing;support vector machines;text analysis","lexical semantics;Setswana African language;part of speech tagging;natural language processing;European languages;Asiatic languages;POS tagging;SVMTool;machine learning","","1","","22","IEEE","28 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-style adaptive training for robust cross-lingual spoken language understanding","X. He; L. Deng; D. Hakkani-Tur; G. Tur","Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8342","8346","Given the increasingly available machine translation (MT) services nowadays, one efficient strategy for cross-lingual spoken language understanding (SLU) is to first translate the input utterance from the second language into the primary language, and then call the primary language SLU system to decode the semantic knowledge. However, errors introduced in the MT process create a condition similar to the ‚Äúmismatch‚Äù condition encountered in robust speech recognition. Such mismatch makes the performance of cross-lingual SLU far from acceptable. Motivated by successful solutions developed in robust speech recognition, we in this paper propose a multi-style adaptive training method to improve the robustness of the SLU system for cross-lingual SLU tasks. For evaluation, we created an English-Chinese bilingual ATIS database, and then carried out a series of experiments on that database to experimentally assess the proposed methods. Experimental results show that, without relying on any data in the second language, the proposed method significantly improves the performance on a cross-lingual SLU task while producing no degradation for input in the primary language. This greatly facilitates porting SLU to as many languages as there are MT systems without any human effort. We further study the robustness of this approach to another type of mismatch condition, caused by speech recognition errors, and demonstrate its success also.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639292","spoken language understanding;cross-lingual;adaptive training;multi-style training","Training;Training data;Robustness;Data models;Testing;Semantics;Degradation","decoding;language translation;natural language processing;speech recognition","cross-lingual spoken language understanding;machine translation services;MT services;second language;primary language;SLU system;semantic knowledge decoding;robust speech recognition;multistyle adaptive training method;cross-lingual SLU tasks;English-Chinese bilingual ATIS database","","7","1","30","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Multi-modal Dense Video Captioning","V. lashin; E. Rahtu",Tampere University; Tampere University,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","28 Jul 2020","2020","","","4117","4126","Dense video captioning is a task of localizing interesting events from an untrimmed video and producing textual description (captions) for each localized event. Most of the previous works in dense video captioning are solely based on visual information and completely ignore the audio track. However, audio, and speech, in particular, are vital cues for a human observer in understanding an environment. In this paper, we present a new dense video captioning approach that is able to utilize any number of modalities for event description. Specifically, we show how audio and speech modalities may improve a dense video captioning model. We apply automatic speech recognition (ASR) system to obtain a temporally aligned textual description of the speech (similar to subtitles) and treat it as a separate input alongside video frames and the corresponding audio track. We formulate the captioning task as a machine translation problem and utilize recently proposed Transformer architecture to convert multi-modal input data into textual descriptions. We demonstrate the performance of our model on ActivityNet Captions dataset. The ablation studies indicate a considerable contribution from audio and speech components suggesting that these modalities contain substantial complementary information to video frames. Furthermore, we provide an in-depth analysis of the ActivityNet Caption results by leveraging the category tags obtained from original YouTube videos. Code is publicly available: github.com/v-iashin/MDVC.","2160-7516","978-1-7281-9360-1","10.1109/CVPRW50498.2020.00487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150672","","Feature extraction;Proposals;Visualization;Task analysis;Decoding;Natural languages;Generators","audio signal processing;computer vision;language translation;object detection;social networking (online);speech recognition;video signal processing","untrimmed video;localized event;event description;dense video captioning model;automatic speech recognition system;temporally aligned textual description;video frames;audio track;captioning task;multimodal input data;textual descriptions;ActivityNet Captions dataset;YouTube videos;multimodal dense video captioning approach;visual information;speech modalities;ASR system;machine translation problem;transformer architecture;speech components;audio components;in-depth analysis","","40","","59","IEEE","28 Jul 2020","","","IEEE","IEEE Conferences"
"Enhanced Amharic-Arabic Cross-Language Information Retrieval System using Part of Speech Tagging","I. Gashaw; H. Shashirekha","Department of Computer Science, Mangalore University, Mangalagangotri, Mangalore; Department of Computer Science, Mangalore University, Mangalagangotri, Mangalore","2019 International Conference on Advances in Computing, Communication and Control (ICAC3)","16 Mar 2020","2019","","","1","7","In this paper we extended our first experiment on Neural Machine Translation (NMT) based query translation for Amharic-Arabic Cross Language Information Retrieval (CLIR) task to retrieve relevant documents from Amharic and Arabic text collections in response to a query expressed in the Amharic language by modifying the ranking algorithm with Parts of speech Tags (POS). We used a pre-trained NMT model, to map a query in the source language into an equivalent query in the language of the target document collection. The relevant documents are then retrieved using a Language Modeling (LM) based retrieval algorithm by substituting lambda with POS based LM. The experimental result is compared with four conventional IR models, namely Uni-gram and Bi-gram LM, Probabilistic model and Vector Space Model (VSM). The proposed POS based LM ranking algorithm outperform all others for both Amharic and Arabic language document collections.","","978-1-7281-2386-8","10.1109/ICAC347590.2019.9036807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036807","","Dictionaries;Large scale integration;Probabilistic logic;Predictive models;Vocabulary","information retrieval systems;language translation;natural language processing;neural nets;query processing;text analysis","target document collection;language modeling;retrieval algorithm;LM ranking algorithm;pre-trained NMT model;source language;Amharic-Arabic cross language information retrieval task;enhanced Amharic-Arabic cross-language information retrieval system;neural machine translation based query translation;Arabic text collections;POS based LM;parts of speech tagging","","1","","37","IEEE","16 Mar 2020","","","IEEE","IEEE Conferences"
"Incremental parsing for interactive natural language interface","D. Mori; S. Matsubara; Y. Inagaki","Graduate School of Engineering, University of Nagoya, Nagoya, Aichi, Japan; Graduate School of Engineering, University of Nagoya, Nagoya, Aichi, Japan; Graduate School of Engineering, University of Nagoya, Nagoya, Aichi, Japan","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)","6 Aug 2002","2001","5","","2880","2885 vol.5","The paper proposes an effective incremental parsing method for such interactive natural language processing systems as real-time dialogue systems, simultaneous machine interpreting systems, etc. This method produces the analysis of the input while it is being received. It can efficiently deal with not only the normal input, which is piecemeal addition to the input from left to right, but also such changes of the input as insertion and deletion. For such changes of the input, this method exploits parts of the previous analyses. We implemented this method on a workstation and conducted an experiment. We confirm that the method can be expected to be useful for an online language processing system.","1062-922X","0-7803-7087-2","10.1109/ICSMC.2001.971946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971946","","Natural languages;Real time systems;Speech analysis;Natural language processing;Workstations;Internet;Keyboards;Speech recognition","natural language interfaces;grammars;interactive systems;real-time systems;language translation;Internet","incremental parsing;interactive natural language interface;interactive natural language processing systems;real-time dialogue systems;simultaneous machine interpreting systems;normal input;workstation;online language processing system;interactive parsing;machine translation;multilingual on-line chat","","2","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"ERIMM: a platform for supporting and collecting multimodal spontaneous bilingual dialogues","G. Fafiotte; C. Boitet","GETA, CLIPS, Grenoble, France; GETA, CLIPS, Grenoble, France","International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003","22 Mar 2004","2003","","","484","491","We have developed several platforms to handle various aspects of spontaneous, general-purpose spoken bilingual dialogues on the Web: human interpretation, data collection, and integration of machine aids including server-based speech translation based on commercial products and user interaction. All platforms are being integrated into the single ERIMM platform, which should then be extended to be usable for e-learning in interpretation.","","0-7803-7902-0","10.1109/NLPKE.2003.1275954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275954","","Humans;Speech;Lakes;Expert systems;Automation;Automatic testing;Dictionaries;National electric code;Europe","language translation;Internet;natural language interfaces;speech-based user interfaces","ERIMM platform;multimodal spontaneous bilingual dialogues;Web data;human interpretation;dialogue data collection;machine aid integration;server-based speech translation;e-learning;machine interpretation","","","","20","IEEE","22 Mar 2004","","","IEEE","IEEE Conferences"
"Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the ‚ÄúSpeaking Rosetta‚Äù JSALT 2017 Workshop","O. Scharenborg; L. Besacier; A. Black; M. Hasegawa-Johnson; F. Metze; G. Neubig; S. St√ºker; P. Godard; M. M√ºller; L. Ondel; S. Palaskar; P. Arthur; F. Ciannella; M. Du; E. Larsen; D. Merkx; R. Riad; L. Wang; E. Dupoux",Radboud University; LIG - Univ Grenoble Alpes (UGA); Carnegie Mellon University; University of Illinois; Carnegie Mellon University; Carnegie Mellon University; Karlsruhe Institute of Technology; LIMSI CNRS; Karlsruhe Institute of Technology; Brno University.; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; ENS/CNRS/EHESS/INRIA; ENS/CNRS/EHESS/INRIA; Radboud University; ENS/CNRS/EHESS/INRIA; University of Illinois; ENS/CNRS/EHESS/INRIA,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","4979","4983","We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461761","unwritten languages;multi-modal data;unsupervised unit discovery;image retrieval;machine translation","Task analysis;Feature extraction;Acoustics;Hidden Markov models;Visualization;Linguistics;Training","linguistics;natural language processing;speech processing;text analysis","subwords;orthographic transcriptions;unsupervised discovery;linguistic unit discovery;multimodal inputs;unwritten languages;speaking rosetta JSALT 2017 workshop;raw speech;text translation;text translation","","12","","38","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A General Machine Learning Framework for Solving Real Applications with a Texture Perception Case Study","Q. Zhang; J. Liu; H. Sun; J. Liu","Shandong Provincial Key Laboratory of Computer Network, Jinan, China; College of Science and Information Qingdao Agricultural University, Qingdao, China; College of Science and Information Qingdao Agricultural University, Qingdao, China; Shandong Provincial Key Laboratory of Computer Network, Jinan, China","2021 6th International Conference on Image, Vision and Computing (ICIVC)","14 Sep 2021","2021","","","393","397","Machine learning, especially deep learning, has made great achievements in application domains such as computer vision, speech recognition, machine translation, etc. This great success not only depends on the progress of model design but also on the emergence of large-scale annotated benchmark datasets. However, in some application domains, it is difficult to apply machine learning methods to solve the new arising problems due to the lack of standardized and annotated datasets. People have to find their way from the very beginning of the data collecting, cleaning, and labeling. Unfortunately, there is not a guiding framework to deal with such a situation. This paper pays attention to it and proposes a general machine learning framework to solve the pratical problems from the scratch. It contains two main stages implemented by an unsupervised clustering method and a semi-supervised learning method representatively. In the first stage, an unsupervised clustering method is utilized to find representative data samples which are more important and can be used to do data cleaning and manual labeling. In the second stage, a semi-supervised method is adopted to predict the labels for the rest data samples and to construct a larger annotated dataset. A case study in the texture perception field has been done to confirm the effectiveness and efficiency of the proposed application framework.","","978-1-6654-4368-5","10.1109/ICIVC52351.2021.9526959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526959","machine learning;application framework;unsupervised;semi-supervised;texture perception","Deep learning;Computer vision;Clustering methods;Computational modeling;Manuals;Speech recognition;Semisupervised learning","deep learning (artificial intelligence);image texture;pattern clustering;supervised learning;unsupervised learning","semisupervised learning method;unsupervised clustering method;annotated datasets;standardized datasets;deep learning;texture perception case study;machine learning framework","","","","16","IEEE","14 Sep 2021","","","IEEE","IEEE Conferences"
"Towards Incorporating 3D Space-Awareness Into an Augmented Reality Sign Language Interpreter","F. Nunnari; E. Avramidis; V. Yadav; A. Pagani; Y. Hamidullah; S. Mollanorozy; C. Espa√±a-Bonet; E. Woop; P. Gebhard",German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI); German Research Center for Artificial Intelligence (DFKI),"2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","This paper describes the concept and the software architecture of a fully integrated system supporting a dialog between a deaf person and a hearing person through a virtual sign language interpreter (aka avatar) projected in the real space by an Augmented Reality device. In addition, a Visual Simultaneous Localization and Mapping system provides information about the 3D location of the objects recognized in the surrounding environment, allowing the avatar to orient, look and point towards the real location of discourse entities during the translation. The goal being to provide a modular architecture to test single software components in a fully integrated framework and move virtual sign language interpreters beyond the standard ‚Äúfront-facing‚Äù interaction paradigm.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10193194","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193194","sign language;machine translation;augmented reality.","Visualization;Three-dimensional displays;Software architecture;Avatars;Gesture recognition;Speech recognition;Assistive technologies","augmented reality;avatars;handicapped aids;SLAM (robots);software architecture;virtual reality","aka avatar;Augmented Reality device;Augmented Reality sign language interpreter;deaf person;fully integrated framework;fully integrated system;hearing person;Mapping system;modular architecture;single software components;software architecture;space-awareness;virtual sign language interpreter;Visual Simultaneous Localization","","","","23","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Morphological analysis of Bangla words for Universal Networking Language","M. N. Y. Ali; S. M. A. Al-Mamun; J. K. Das; A. M. Nurannabi","East West University, Dhaka, Dhaka District, BD; Ahsanullah University of Science and Technology, Dhaka, Dhaka District, BD; Department of CSE, Jahangir Nagar University, Dhaka, Bangladesh; Senior Programmer Computer and ICT unit, National University, Dhaka, Bangladesh","2008 Third International Conference on Digital Information Management","9 Jan 2009","2008","","","532","537","This paper focuses on morphological analysis of Bangla words to incorporate them into Bangla to Universal Networking Language (UNL) processors. Researchers have been working on morphological structure of Bangla for machine translation and a considerable volume of work is available. So far, no attempt has been made to integrate the works for a concrete computational output. In this paper we particularly emphasize on bringing previous works on morphological analysis in the framework of UNL, with the goal to produce a Bangla-UNL dictionary, as UNL structures can provide, for any morphological analysis, a unified base to fit into already developed universal conversion systems of UNL. We explain the morphological rules of Bangla words for UNL structures. These rules tend to expose the modifications of parts of speech with regards to tense, person, subject etc. of the words of a sentence. Here we outline the morphology of nouns, verbs and adjective phrases only.","","978-1-4244-2916-5","10.1109/ICDIM.2008.4746734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4746734","Morphology;Bangla;Universal Networking Language (UNL);UNL-Bangla dictionary","Morphology;Dictionaries;Internet;Natural languages;Programming profession;Computer networks;Concrete;Speech;Acceleration;Information technology","language translation;mathematical morphology","morphological analysis;Bangla words;universal networking language;morphological structure;machine translation;Bangla-UNL dictionary","","16","","15","IEEE","9 Jan 2009","","","IEEE","IEEE Conferences"
"A Hybrid Approach to Sentence Alignment Using Genetic Algorithm","M. Gautam; R. M. K. Sinha","Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India","2007 International Conference on Computing: Theory and Applications (ICCTA'07)","12 Mar 2007","2007","","","480","484","Sentence alignment in bilingual corpora has been an active research topic in the machine translation research groups. There have been multiple works in the past to align sentences in bilingual corpus in English and European languages and some Asian languages like Chinese and Japanese. This work introduces a novel approach for sentence alignment in bilingual corpora using lexical and statistical information about the language pair using genetic algorithm. The only lexical information used in this work is a restricted form of bilingual dictionary (incomplete). The algorithm works based on the weighted sum of a set of statistical parameters and the parameter denoting degree of dictionary match. No other lexical information like part of speech tagging, chunking, n-gram statistics etc has been used in this work. Our approach has been tested for structurally dissimilar language pair of English-Hindi and is shown to yield a high performance even under noisy conditions. We compare our results with that of Microsoft alignment tool on the same corpus and we find our results to be superior","","0-7695-2770-1","10.1109/ICCTA.2007.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127416","","Genetic algorithms;Natural languages;Dictionaries;Testing;Speech;Tagging;Statistics;Equations;Probability;Dynamic programming","dictionaries;genetic algorithms;language translation;natural language processing","hybrid approach;sentence alignment;genetic algorithm;bilingual corpora;machine translation;statistical information;lexical information;Microsoft alignment tool","","4","","8","IEEE","12 Mar 2007","","","IEEE","IEEE Conferences"
"Design and Implementation of Intelligent Audio Broadcast Monitoring System","D. Zhang; R. Fu; T. Niu; L. Chang; Y. Wang; L. Tuo","Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China; Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China; Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China; Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China; Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China; Television Institute, Academy of Broadcasting Science, NRTA, Beijing, China","2020 International Wireless Communications and Mobile Computing (IWCMC)","27 Jul 2020","2020","","","1590","1593","In order to improve the accuracy and effectiveness of the real-time monitoring on illegal broadcasting and violative broadcasting, the author used artificial intelligence technology to realize the intelligent audio broadcast monitoring system, including frequency domain template comparison, speech recognition, text comparison and text classification. The system has been deployed in the provincial monitoring center, which can realize the automatic processing of the whole flow of illegal broadcasting and violative broadcasting monitoring from discovery, recording and evidence collection, voice translation, to intelligent recognition, it can greatly improve the efficiency and timeliness of audio broadcast supervision. This paper mainly analyzed the requirements and functions of the system, described the design scheme and implementation methods of the system, and put forward the deployment scheme of the system according to the actual situation.","2376-6506","978-1-7281-3129-0","10.1109/IWCMC48107.2020.9148485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148485","wireless communication;illegal broadcasting;monitoring;speech recognition;artificial intelligence;SVM","Monitoring;Frequency synchronization;Time-frequency analysis;Broadcasting;Speech recognition;Artificial intelligence;Support vector machines","artificial intelligence;computerised monitoring;frequency-domain analysis;speech recognition;text analysis","intelligent audio broadcast monitoring system;real-time monitoring;illegal broadcasting;artificial intelligence technology;frequency domain template comparison;text comparison;text classification;provincial monitoring center;violative broadcasting monitoring;intelligent recognition;audio broadcast supervision","","1","","7","IEEE","27 Jul 2020","","","IEEE","IEEE Conferences"
"Word Activation Forces-Based Language Modeling and Smoothing","M. Qin; G. Liu; B. Li; Y. Lu","School of Information and Communication Engineering, BUPT, Beijing, China; School of Information and Communication Engineering, BUPT, Beijing, China; School of Information and Communication Engineering, BUPT, Beijing, China; Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, BUPT, China","2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics","24 Oct 2013","2013","1","","564","567","N-gram language models are useful for modeling the local dependencies of word occurrences but not for capturing global word dependencies. When the window size n is limited, the n-gram is weak in terms of capturing long distance dependencies. Long-distance Dependency information has long been proven useful in language model. However, the improved performance of long-distance LMs over conventional n-gram models generally comes at the cost of increased decoding complexity and model size. Word Activation Forces has been proven a simple and human-comparable accurate measure to identify word closest associates. In this paper, Word Activation Forces-Based language model is proposed to capture the long distance dependency between words, but which is as fast for decoding as a conventional word n-gram. As shown by experiments on broadcast news, the proposed language modeling and smoothing can significantly reduce the perplexity of language models and word error rate with moderate computational cost.","","978-0-7695-5011-4","10.1109/IHMSC.2013.140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643952","long-distance dependency;language model;Word Activation Forces","Smoothing methods;Computational modeling;Training;Interpolation;Error analysis;Semantics;History","computational complexity;language translation;natural language processing;probability;smoothing methods;speech recognition;statistical analysis","word activation forces-based language modeling;word activation forces-based language smoothing;N-gram language models;word occurrence local dependencies;window size;long-distance dependency information;decoding complexity;model size;human-comparable accurate measure;word closest associate identification;perplexity reduction;word error rate;computational cost;automatic speech recognition","","1","","13","IEEE","24 Oct 2013","","","IEEE","IEEE Conferences"
"Multimodal Machine Learning: A Survey and Taxonomy","T. Baltru≈°aitis; C. Ahuja; L. -P. Morency","Microsoft Corporation, Cambridge, United Kingdom; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA","IEEE Transactions on Pattern Analysis and Machine Intelligence","8 Jan 2019","2019","41","2","423","443","Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.","1939-3539","","10.1109/TPAMI.2018.2798607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8269806","Multimodal;machine learning;introductory;survey","Speech recognition;Visualization;Media;Speech;Multimedia communication;Streaming media;Hidden Markov models","artificial intelligence;learning (artificial intelligence);signal processing;user interfaces","multimodal signals;specific multimodal applications;vibrant multidisciplinary field;multimodal machine learning","","1105","","261","IEEE","25 Jan 2018","","","IEEE","IEEE Journals"
"Investigation of Viterbi Algorithm Performance on Part-of-Speech Tagger of Natural Language Processing","Y. Liu","ICB Economics Department, China Agricultural University, Beijing, China","2017 International Conference on Computer Systems, Electronics and Control (ICCSEC)","26 Aug 2018","2017","","","1430","1433","Many tasks of NLP need to preprocess the words and sentences because it can make the future work convenient. Nowadays, the demand of POS tagging is increasing. POS tagging is an efficient method as preprocessing tagging, even meaningful in text to speech, syntactic analysis and machine translation. When it comes to the POS tagger, they need to know every word POS. It is available for human because it is easy for us to describe it. But when we increase the count of words to the million number, it is not possible to make people do POS tag. In this paper, we introduce Viterbi algorithm to help computer do the better job in tagging lexical categories. Viterbi algorithm is an algorithm that used dynamic programming to solve the POS of sentence. As we know, the word is sensitive about the position of the word. The POS of the word is related about the nearby words. We make simulations about how Viterbi Algorithms work in POS tagger and get the accuracy performance.","","978-1-5386-3573-5","10.1109/ICCSEC.2017.8446837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8446837","component;Viterbi;NLP;Possibility Distribution","Viterbi algorithm;Tagging;Fish;Dynamic programming;Hidden Markov models;Heuristic algorithms;Natural language processing","computational linguistics;dynamic programming;natural language processing","viterbi algorithm performance;part-of-speech tagger;natural language processing;POS tagging;machine translation;POS tagger;lexical categories;text to speech;dynamic programming","","1","","6","IEEE","26 Aug 2018","","","IEEE","IEEE Conferences"
"Malware classification with LSTM and GRU language models and a character-level CNN","B. Athiwaratkun; J. W. Stokes","Department of Statistical Science, Cornell University, Ithaca, NY; Microsoft Research, Redmond, WA, USA","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","2482","2486","Malicious software, or malware, continues to be a problem for computer users, corporations, and governments. Previous research [1] has explored training file-based, malware classifiers using a two-stage approach. In the first stage, a malware language model is used to learn the feature representation which is then input to a second stage malware classifier. In Pascanu et al. [1], the language model is either a standard recurrent neural network (RNN) or an echo state network (ESN). In this work, we propose several new malware classification architectures which include a long short-term memory (LSTM) language model and a gated recurrent unit (GRU) language model. We also propose using an attention mechanism similar to [12] from the machine translation literature, in addition to temporal max pooling used in [1], as an alternative way to construct the file representation from neural features. Finally, we propose a new single-stage malware classifier based on a character-level convolutional neural network (CNN). Results show that the LSTM with temporal max pooling and logistic regression offers a 31.3% improvement in the true positive rate compared to the best system in [1] at a false positive rate of 1%.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7952603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952603","Malware Classification;Neural Language Model;LSTM;GRU;CNN","Malware;Logistics;Engines;Training;Standards;Recurrent neural networks","file organisation;invasive software;language translation;pattern classification;recurrent neural nets;regression analysis","malware classification;character-level CNN;malicious software;feature representation;recurrent neural network;RNN;echo state network;ESN;long short-term memory language model;LSTM language model;gated recurrent unit language model;GRU language model;machine translation;file representation;convolutional neural network;temporal max pooling;logistic regression","","128","2","23","IEEE","19 Jun 2017","","","IEEE","IEEE Conferences"
"Building a free, general-domain paraphrase database for Japanese","M. Mizukami; G. Neubig; S. Sakti; T. Toda; S. Nakamura","Nara Institute of Science and Technology, Ikoma-shi, Nara, Japan; Nara Institute of Science and Technology, Ikoma-shi, Nara, Japan; Nara Institute of Science and Technology, Ikoma-shi, Nara, Japan; Nara Institute of Science and Technology, Ikoma-shi, Nara, Japan; Nara Institute of Science and Technology, Ikoma-shi, Nara, Japan","2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)","2 Mar 2015","2014","","","1","4","Previous works have used parallel corpora and alignment techniques from phrase-based statistical machine translation to extract and generate paraphrases. In Japanese, paraphrases for a number of paraphrase categories or domains have been extracted by this method. However, most of these resources focus on a particular phenomenon in Japanese, and there are still no Japanese paraphrase resources that cover all varieties of phrases from several domains, and are freely available. In addition, because Japanese and English vary in grammar and word ordering, we perform syntax-based preprocessing to reduce this mismatch and extract paraphrases similar in quality to those extracted using more similar language pairs. The data used in creating the Japanese paraphrases is either in the public domain, or available under the Creative Commons license, and spans a variety of genres for wide coverage.","","978-1-4799-7094-0","10.1109/ICSDA.2014.7051433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051433","Paraphrasing;Free Data;General-Domain","Grammar;Licenses","audio databases;computational linguistics;grammars;language translation;natural language processing;statistical analysis","general-domain paraphrase database;Japanese;parallel corpora;phrase-based statistical machine translation;English;grammar;word ordering;syntax-based preprocessing;Creative Commons license","","4","","21","IEEE","2 Mar 2015","","","IEEE","IEEE Conferences"
"Bidirectional Temporal Convolution with Self-Attention Network for CTC-Based Acoustic Modeling","J. Sun; W. Guo; B. Gu; Y. Liu","University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China; University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China; University of Science and Technology of China, National Engineering Laboratory for Speech and Language Information Processing, Hefei, China; China General Technology Research Institute","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","1262","1266","Connectionist temporal classification (CTC) based on recurrent (RNNs) or convolutional neural networks (CNNs) is a method for end-to-end acoustic modeling. Inspired by the recent success of the self-attention network (SAN) in machine translation and other domains such as images, we apply the SAN to CTC acoustic modeling in this paper. SAN has powerful capabilities for capturing global dependencies, but it cannot model the sequential information and local interactions of utterances. The bidirectional temporal convolution with self-attention network (BTCSAN) is proposed in order to capture both the global and local dependencies of utterances. Furthermore, the down- and upsampling strategies are adopted in the proposed BTCSAN in order to achieve computational efficiency and high recognition accuracy. Experiments are carried out using the King-ASR-117 Japanese corpus. The proposed BTCSAN can obtain a 15.87% relative improvement in the CER over the BLSTM-based CTC baseline.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023020","‚îÄconnectionist temporal classification;bidirectional temporal convolution;self-attention","Convolution;Hidden Markov models;Training;Acoustics;Task analysis;Speech recognition;Decoding","recurrent neural nets;speech recognition","SAN;sequential information;bidirectional temporal convolution;self-attention network;BTCSAN;BLSTM-based CTC baseline;CTC-based acoustic modeling;connectionist temporal classification;convolutional neural networks;end-to-end acoustic modeling;CTC acoustic modeling","","1","","27","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Statistical Correction of Transcribed Melody Notes Based on Probabilistic Integration of a Music Language Model and a Transcription Error Model","Y. Hiramatsu; G. Shibata; R. Nishikimi; E. Nakamura; K. Yoshii","Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","256","260","This paper describes a statistical post-processing method for automatic singing transcription that corrects pitch and rhythm errors included in a transcribed note sequence. Although the performance of frame-level pitch estimation has been improved drastically by deep learning techniques, note-level transcription of singing voice is still an open problem. Inspired by the standard framework of statistical machine translation, we formulate a hierarchical generative model of a transcribed note sequence that consists of a music language model describing the pitch and onset transitions of a true note sequence and a transcription error model describing the addition of deletion, insertion, and substitution errors to the true sequence. Because the length of the true sequence might be different from that of the observed transcribed sequence, the most likely sequences with possible different lengths are estimated with Viterbi decoding and the most likely length is then selected with a sophisticated language model based on a long short-term memory (LSTM) network. The experimental results show that the proposed method can correct musically unnatural transcription errors.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414249","Singing transcription;music language models;statistical modeling;symbolic music processing","Viterbi algorithm;Hidden Markov models;Markov processes;Rhythm;Probabilistic logic;Decoding;Multiple signal classification","acoustic signal processing;audio signal processing;language translation;learning (artificial intelligence);music;Viterbi decoding","statistical correction;transcribed melody notes;probabilistic integration;music language model;transcription error model;statistical post-processing method;automatic singing transcription;corrects pitch;rhythm errors;transcribed note sequence;frame-level pitch estimation;deep learning techniques;note-level transcription;statistical machine translation;hierarchical generative model;onset transitions;substitution errors;observed transcribed sequence;sophisticated language model;musically unnatural transcription errors","","","","17","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Creation of a multi-paraphrase corpus based on various elementary operations","J. Effendi; S. Sakti; S. Nakamura","Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Graduate School of Information Science, Nara Institute of Science and Technology, Japan","2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)","14 Jun 2018","2017","","","1","6","Paraphrases resemble monolingual translations from a source sentence into other sentences that must preserve the original meaning. To build automatic paraphrasing, a collection of paraphrased expressions is required. However, manually collecting paraphrases is expensive and time-consuming. Most existing paraphrases corpora cover only one-to-one parallel sentences and neglect the fact that possible variants of paraphrases can be generated from a single source sentence. The manipulation applied to the original sentences is also difficult to track. Furthermore, a single corpus is mostly dedicated to a single application that is not reusable in other applications. In this research, we construct a paraphrase corpus based on various elementary operations (reordering, substitution, deletion, insertion) in a crowdsourcing platform to generate multi- paraphrase sentences from a source sentence. These elementary paraphrase operations can be utilized for various applications (i.e., deletion for summarization and reordering for machine translation). Our evaluations show the richness and effectiveness of our created corpus.","2472-7695","978-1-5386-3333-5","10.1109/ICSDA.2017.8384465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8384465","corpus generation;paraphrasing;crowd- sourcing","Task analysis;Cats;Crowdsourcing;IEEE Sections;Standardization;Nails;Databases","language translation;natural language processing;text analysis","multiparaphrase corpus;elementary operations;monolingual translations;automatic paraphrasing;paraphrased expressions;parallel sentences;single source sentence;original sentences;single application;paraphrase corpus;elementary paraphrase operations;multiparaphrase sentences","","1","","24","IEEE","14 Jun 2018","","","IEEE","IEEE Conferences"
"Software Vulnerability Detection Using Deep Neural Networks: A Survey","G. Lin; S. Wen; Q. -L. Han; J. Zhang; Y. Xiang","School of Information Engineering, Sanming University, Fujian, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia","Proceedings of the IEEE","28 Sep 2020","2020","108","10","1825","1848","The constantly increasing number of disclosed security vulnerabilities have become an important concern in the software industry and in the field of cybersecurity, suggesting that the current approaches for vulnerability detection demand further improvement. The booming of the open-source software community has made vast amounts of software code available, which allows machine learning and data mining techniques to exploit abundant patterns within software code. Particularly, the recent breakthrough application of deep learning to speech recognition and machine translation has demonstrated the great potential of neural models‚Äô capability of understanding natural languages. This has motivated researchers in the software engineering and cybersecurity communities to apply deep learning for learning and understanding vulnerable code patterns and semantics indicative of the characteristics of vulnerable code. In this survey, we review the current literature adopting deep-learning-/neural-network-based approaches for detecting software vulnerabilities, aiming at investigating how the state-of-the-art research leverages neural techniques for learning and understanding code semantics to facilitate vulnerability discovery. We also identify the challenges in this new field and share our views of potential research directions.","1558-2256","","10.1109/JPROC.2020.2993293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108283","Cybersecurity;deep neural network (DNN);machine learning (ML);representation learning;software vulnerability","Computer security;Semantics;Feature extraction;Open source software;Neural networks;Deep learning;Computer bugs","data mining;language translation;learning (artificial intelligence);natural languages;neural nets;public domain software;security of data;software engineering;speech recognition","neural network-based approaches;natural languages;vulnerability discovery;code semantics;vulnerable code patterns;cybersecurity communities;software engineering;machine translation;speech recognition;deep learning;data mining techniques;machine learning;software code;open-source software community;software industry;security vulnerabilities;deep neural networks;software vulnerability detection","","177","","128","IEEE","4 Jun 2020","","","IEEE","IEEE Journals"
"Code-Mixing: A Brief Survey","S. Thara; P. Poornachandran","Dept of Computer Science and Engineering, Amrita Center for Cyber Security Systems and Networks, India; Dept of Computer Science and Engineering, Amrita Center for Cyber Security Systems and Networks, India","2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","2 Dec 2018","2018","","","2382","2388","Indians and many other non-English speakers across the world, prefer not to use single code in their messaging texts on social media platforms. They make use of transliteration and randomly merged English words using code-mixing, two or more languages to show their linguistic proficiency (English-Spanish, Arabic-English, etc.). Code-mixing (CM) is a dynamically progressive area of research in the domain of text mining. Present time communications in social media, blogs, reviews are abuzz with creative, crafty code-mixed messages. This paper highlights a comprehensive study of CM in the diverse fields of Natural Language Processing (NLP) including language identification, Part-of-Speech (POS) tagging, Named Entity Recognition (NER), Polarity Identification, Question Answering. CM has also been sought after in studies involving Machine Translation, Dialect identification, Speech technologies etc. Most of the applications of code mixing are scrutinized and presented briefly in this survey. This study purports to articulate tends and, techniques pursued in languages used and also unique evaluation measures to give accuracy.","","978-1-5386-5314-2","10.1109/ICACCI.2018.8554413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554413","NER;code mixed;SVM;Logistic Regression;Gaussian Naive Bayes;KNN;Random Forest;Adaboost;feature vectors","Feature extraction;Tagging;Support vector machines;Task analysis;Twitter;Switches","data mining;natural language processing;question answering (information retrieval);social networking (online);text analysis","code mixing;social media platforms;transliteration;randomly merged English words;linguistic proficiency;text mining;code-mixed messages;natural language processing;language identification;part-of-speech tagging;named entity recognition;polarity identification;question answering","","20","","63","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Collocation Extraction Using Web Feedback Data","J. Lin; S. Li; Y. Cai","MOE-MS Key Laboratory of NLP & Speech, Harbin Institute of Technology, Harbin, China; MOE-MS Key Laboratory of NLP & Speech, Harbin Institute of Technology, Harbin, China; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA","Chinese Journal of Electronics","25 May 2023","2009","18","2","312","316","As an important linguistic resource, collocation represents a significant relation between words. Automatic collocation extraction is very important for many natural language processing applications such as machine translation, information extraction, and information retrieval. While traditional collocation extraction approaches are based on linguistic corpus, we propose to acquire collocations from the Web. Three classical lexical association measures (co-occurrence frequency, mutual information and $t$-test) are used to automatically extract collocation. Based on the experimental results, the benchmarks indicate that superior performance of this new Web-based approach in both high precision and recall.","2075-5597","","","National Natural Science Foundation of China(grant numbers:60736044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136595","Collocation;Co-occurrence frequency;Mutual information;T-test;Corpora;Web;Google","Linguistics;Benchmark testing;Information retrieval;Frequency measurement;Data mining;Machine translation;Research and development","","","","","","13","","25 May 2023","","","CIE","CIE Journals"
"A method of automatic recognition of attributive clauses in Chinese language","L. Wang; W. Qu; H. Wang; S. Yu","Key Lab of Computational Linguistics of Ministry of Education, Peking University, Beijing; School of Computer Science, Nanjing Normal University, Nanjing; Institute of Computational Linguistics, Peking University, Beijing; Institute of Computational Linguistics, Peking University, Beijing","2016 International Conference on Asian Language Processing (IALP)","13 Mar 2017","2016","","","172","175","Influenced by the grammatical system of western languages, there are more and more syntactic structures in modern Chinese that can be translated into English attributive clauses. But for the great differences of the syntactic structures, parsing Chinese by western grammatical rules usually does not lead to satisfactory results, which will result in poor translation performance in complex syntactic structures. This paper first attempts to recognize the attributive clauses by using conditional random fields (‚ÄúCRFs‚Äù) theory by selecting representative features combined with grammatical information unique in Chinese language. The experiment proves that this method will produce a better result than simply by statistical machine translation method.","","978-1-5090-0922-0","10.1109/IALP.2016.7875961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875961","Chinese language;Attributive Clause;Automatic Recognition","Syntactics;Hidden Markov models;Computational linguistics;Compounds;Feature extraction;Probabilistic logic;Semantics","language translation;natural language processing;speech recognition","automatic recognition;Chinese language;grammatical system;western languages;English attributive clauses;parsing Chinese;western grammatical rules;complex syntactic structures;conditional random fields;CRF theory;grammatical information;statistical machine translation","","","","7","IEEE","13 Mar 2017","","","IEEE","IEEE Conferences"
"An Improved LSTM Structure for Natural Language Processing","L. Yao; Y. Guan","Qingdao No.2 Middle School, Qingdao, China; Intelligence and Big Data Research Center, Global Wisdom Inc., Beijing, China","2018 IEEE International Conference of Safety Produce Informatization (IICSPI)","14 Apr 2019","2018","","","565","569","Natural language processing technology is widely used in artificial intelligence fields such as machine translation, human-computer interaction and speech recognition. Natural language processing is a daunting task due to the variability, ambiguity and context-dependent interpretation of human language. The current deep learning technology has made great progress in NLP technology. However, many NLP systems still have practical problems, such as high training complexity, computational difficulties in large-scale content scenarios, high retrieval complexity and lack of probabilistic significance. This paper proposes an improved NLP method based on long short-term memory (LSTM) structure, whose parameters are randomly discarded when they are passed backwards in the recursive projection layer. Compared with baseline and other LSTM, the improved method has better F1 score results on the Wall Street Journal dataset, including the word2vec word vector and the one-hot word vector, which indicates that our method is more suitable for NLP in limited computing resources and high amount of data.","","978-1-5386-5514-6","10.1109/IICSPI.2018.8690387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8690387","NLP;LSTM;machine translation;deep learning;recursive projection layer","Natural language processing;Deep learning;Grammar;Task analysis;Semantics;Neural networks","learning (artificial intelligence);natural language processing;neural nets;speech recognition","natural language processing;artificial intelligence fields;human-computer interaction;speech recognition;context-dependent interpretation;human language;high training complexity;computational difficulties;high retrieval complexity;improved NLP method;short-term memory structure;improved LSTM structure;deep learning technology","","33","","12","IEEE","14 Apr 2019","","","IEEE","IEEE Conferences"
"General labelled data generator framework for network machine learning","K. Kim; Y. -G. Hong; Y. -H. Han","ETRI, 218 Gajeong-ro, Yuseong-gu, Daejeon, 34129, Korea; ETRI, 218 Gajeong-ro, Yuseong-gu, Daejeon, 34129, Korea; KOREATECH, 1600, Chungjeol-ro, Byeongcheon-myeon, Dongnam-gu, Cheonan-si, Chungcheongnam-do, 31253, Korea","2018 20th International Conference on Advanced Communication Technology (ICACT)","26 Mar 2018","2018","","","1","1","Artificial Intelligence (AI) technology has made remarkable achievements in various fields. Especially, deep learning technology that is the representative technology of AI, showed high accuracy in speech recognition, image recognition, pattern recognition, natural language processing and translation. In addition, there are many interesting research results such as art, literature and music that cannot be distinguished whether it was made by human or AI. In the field of networks, attempts to solve problems that have not been able to be solved or complex problems using AI have started to become a global trend. However, there is a lack of data sets to apply machine learning to the network and it is difficult to know network problem to solve. So far, there have been a lot of efforts to study network machine learning, but there are few studies to make a necessary dataset. In this paper, we introduce basic network machine learning technology and propose a method to easily generate data for network machine learning. Based on the data generation framework proposed in this paper, the results of automatic generation of labelled data and the results of learning and inferencing from the corresponding dataset are also provided.","","979-11-88428-01-4","10.23919/ICACT.2018.8323669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323669","machine learning;data generator;deep learning;network machine learning;supervised learning","Machine learning;Deep learning;Standardization;Internet of Things;Generators;Computer science;Wireless networks","","","","2","","","","26 Mar 2018","","","IEEE","IEEE Conferences"
"IEEE Approved Draft Standard - Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","",,"IEEE P3300/D3, October 2022","17 Oct 2022","2022","","","1","106","This standard adopts MPAI Technical Specification Version 1.2 as an IEEE Standard. Multimodal Conversation (MPAI-MMC) is an MPAI Standard comprising five use cases, all sharing the use of artificial intelligence (AI) to enable a form of human-machine conversation in completeness and intensity.","","978-1-5044-9069-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921240","AI Modules;Bidirectional Speech Translation;BST;Conversation with Emotion;CWE;IEEE 3300‚Ñ¢;MQA;MST;Multimodal Conversation;Multimodal Question Answering;One-to-Many Speech Translation;Unidirectional Speech Translation;Use Cases;UST","IEEE Standards;Motion pictures;Codes;Imaging;Artificial intelligence","","","","","","","","17 Oct 2022","","","IEEE","IEEE Standards"
"Complex autoregressive model for shape recognition","I. Sekita; T. Kurita; N. Otsu","Information Science Division, Electrotechnical Laboratory, M.I.T.I., Tsukuba, Ibaraki, Japan; Information Science Division, Electrotechnical Laboratory, M.I.T.I., Tsukuba, Ibaraki, Japan; Machine Understanding Division, Electrotechnical Laboratory, M.I.T.I., Tsukuba, Ibaraki, Japan","IEEE Transactions on Pattern Analysis and Machine Intelligence","6 Aug 2002","1992","14","4","489","496","A complex autoregressive model for invariant feature extraction to recognize arbitrary shapes on a plane is presented. A fast algorithm to calculate complex autoregressive coefficients and complex PARCOR coefficients of the model is also shown. The coefficients are invariant to rotation around the origin and to choice of the starting point in tracing a boundary. It is possible to make them invariant to scale and translation. Experimental results that the complicated shapes like nonconvex boundaries can be recognized in high accuracy, even in the low-order model. It is seen that the complex PARCOR coefficients tend to provide more accurate classification than the complex AR coefficients.<>","1939-3539","","10.1109/34.126809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=126809","","Shape;Pattern recognition;Speech analysis;Predictive models;Signal analysis;Vectors;Sampling methods;Laboratories;Feature extraction;Computer vision","computer vision;filtering and prediction theory;statistics","computer vision;statistics;rotation invariance;scale invariance;translation invariance;shape recognition;complex autoregressive model;invariant feature extraction;complex autoregressive coefficients;complex PARCOR coefficients;nonconvex boundaries","","68","","20","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"Keynote 3: Weighted transducers in speech and language processing","M. D. Riley","Google Inc New York, USA","2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)","2 Mar 2015","2014","","","1","1","This talk will describe the use of weighted transducers in speech and language processing. Weighted transducers are automata where each transition has an input label, an output label, and a weight. They have key applications in speech recognition and synthesis, machine translation, optical character recognition, pattern matching, string processing, machine learning, information extraction and retrieval among others. We give a brief history of the field, an overview of the theory and algorithms of weighted finite-state and pushdown transducers, a discussion of available software libraries, a description of applications to speech recognition, speech synthesis, machine translation and NLP, and a short overview of current research problems.","","978-1-4799-7094-0","10.1109/ICSDA.2014.7051411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051411","","","","","","","","","IEEE","2 Mar 2015","","","IEEE","IEEE Conferences"
"Automatic speech recognition errors detection using supervised learning techniques","R. Errattahi; A. E. Hannani; H. Ouahmane; T. Hain","Laboratory of Information Technologies, University of Chouaib Doukkali, El Jadida, Morocco; Laboratory of Information Technologies, University of Chouaib Doukkali, El Jadida, Morocco; Laboratory of Information Technologies, University of Chouaib Doukkali, El Jadida, Morocco; Speech and Hearing Group, University of Sheffield, UK","2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)","12 Jun 2017","2016","","","1","6","Over the last years, many advances have been made in the field of Automatic Speech Recognition (ASR). However, the persistent presence of ASR errors is limiting the widespread adoption of speech technology in real life applications. This motivates the attempts to find alternative techniques to automatically detect and correct ASR errors, which can be very effective and especially when the user does not have access to tune the features, the models or the decoder of the ASR system or when the transcription serves as input to downstream systems like machine translation, information retrieval, and question answering. In this paper, we present an ASR errors detection system targeted towards substitution and insertion errors. The proposed system is based on supervised learning techniques and uses input features deducted only from the ASR output words and hence should be usable with any ASR system. Applying this system on TV program transcription data leads to identify 40.30% of the recognition errors generated by the ASR system.","2161-5330","978-1-5090-4320-0","10.1109/AICCSA.2016.7945669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945669","","Feature extraction;Speech;Decoding;Context;Supervised learning;Acoustics;Training","error correction;error detection;learning (artificial intelligence);speech recognition","automatic speech recognition error detection;supervised learning;ASR error detection system;ASR error correction;substitution errors;insertion errors;ASR output words;TV program transcription data","","4","","19","IEEE","12 Jun 2017","","","IEEE","IEEE Conferences"
"Upper and Lower Tight Error Bounds for Feature Omission with an Extension to Context Reduction","R. Schl√ºter; E. Beck; H. Ney","Computer Science Department, RWTH Aachen University, Aachen, Germany; Computer Science Department, RWTH Aachen University, Aachen, Germany; Computer Science Department, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","8 Jan 2019","2019","41","2","502","514","In this work, fundamental analytic results in the form of error bounds are presented that quantify the effect of feature omission and selection for pattern classification in general, as well as the effect of context reduction in string classification, like automatic speech recognition, printed/handwritten character recognition, or statistical machine translation. A general simulation framework is introduced that supports discovery and proof of error bounds, which lead to the error bounds presented here. Initially derived tight lower and upper bounds for feature omission are generalized to feature selection, followed by another extension to context reduction of string class priors (aka language models) in string classification. For string classification, the quantitative effect of string class prior context reduction on symbol-level Bayes error is presented. The tightness of the original feature omission bounds seems lost in this case, as further simulations indicate. However, combining both feature omission and context reduction, the tightness of the bounds is retained. A central result of this work is the proof of the existence, and the amount of a statistical threshold w.r.t. the introduction of additional features in general pattern classification, or the increase of context in string classification beyond which a decrease in Bayes error is guaranteed.","1939-3539","","10.1109/TPAMI.2017.2788434","H2020 European Research Council(grant numbers:694537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242665","Error bound, Bayes error, feature selection, language model, perplexity, context reduction, pattern classification, sequence classification","Context modeling;Analytical models;Upper bound;Feature extraction;Measurement uncertainty;Automatic speech recognition","Bayes methods;feature extraction;language translation;pattern classification;statistical analysis","string classification;automatic speech recognition;string class prior context reduction;handwritten character recognition;simulation framework;Bayes error;pattern classification;feature omission bounds;lower tight error bounds;upper tight error bounds;statistical machine translation","","1","","22","IEEE","1 Jan 2018","","","IEEE","IEEE Journals"
"Crop Yield Management System Using Machine Learning Techniques","S. B; N. D; M. G; J. T. M; D. A; D. P","Department. of IST, College of Engineering, Anna University, Chennai, India; Department. of IST, College of Engineering, Anna University, Chennai, India; Department. of IST, College of Engineering, Anna University, Chennai, India; Department of ECE, Sri Manakula Vinayagar Engg College, Puducherry, India; Department of ECE, IFET College of Engineering, Villupuram, India; Department of CSE, Pondicherry Technological University, Puducherry, India","2021 IEEE International Conference on Mobile Networks and Wireless Communications (ICMNWC)","26 Jan 2022","2021","","","1","5","Farming is the backbone of agriculture country like India. Farmers lose their yield due to lack of knowledge about new technologies and plantation parameters which help them to increase their yield. The proposed system, Aruvi, performs machine learning analysis and applies Ontology-based mapping to assist the farmers in order to increase their yield. Aruvi is basically a chatbot that can mimic a virtual conversation with user (farmer) using regional language (Tamil). Aruvi is trained and made to learn on its own using ontology based mapping. Based on the user query it gives relevant answers, which is more useful for farmers in remote places. Using the proposed system, the user can know about the crops, their atmospheric conditions and suitable soil by querying the system in their own regional language to the chatbot. Another advantage of Aruvi, users can converse with it apart through menus or buttons via text or speech on websites or through mobile apps. Based on the trained dataset and real time scenario the accuracy of the system is 83.25%. This can be improved by collecting the real time conditions in that particular region and training Aruvi using them.","","978-1-6654-3883-4","10.1109/ICMNWC52512.2021.9688453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688453","Bigrams;Levenshtein distance;Language Translation;Natural Language Processing;Ontology studies","Wireless communication;Training;Conferences;Crops;Machine learning;Soil;Ontologies","agriculture;chatbots;crops;learning (artificial intelligence);mobile computing;natural language processing;ontologies (artificial intelligence);query processing;user interfaces;Web sites","farming;Aruvi;chatbot;virtual conversation;regional language;ontology based mapping;user query;crop yield management system;machine learning;agriculture;mobile apps;Web sites","","","","15","IEEE","26 Jan 2022","","","IEEE","IEEE Conferences"
"Easier Notation ‚Äì a Proposal for a Gloss-Based Scripting Language for Sign Language Generation Based on Lexical Data","T. Hanke; L. K√∂nig; R. Konrad; M. Kopf; M. Schulder; R. Wolfe","Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; Institute of German Sign Language, Universit√§t Hamburg, Hamburg, Germany; Institute for Language and Speech Processing, Athena Research Center, Athens, Greece","2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)","2 Aug 2023","2023","","","1","5","We introduce EASIER Notation, a gloss-based scripting language to describe sign language content to be signed by an avatar and describe the functionality a lexical database for a sign language needs to provide in order to fully support the notation approach. In addition, we present the prototype of a text editor supporting EASIER Notation for human post-editing of machine translation output as well as pre-scribing signed utterances from scratch.","","979-8-3503-0261-5","10.1109/ICASSPW59220.2023.10192997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192997","sign language;animation;notation;avatar;HamNoSys","Databases;Conferences;Avatars;Prototypes;Gesture recognition;Assistive technologies;Signal processing","authoring languages;avatars;language translation;natural language processing","EASIER Notation;easier Notation;gloss-based scripting language;lexical data;lexical database;notation approach;pre-scribing signed utterances;sign language content;sign language generation","","1","","16","IEEE","2 Aug 2023","","","IEEE","IEEE Conferences"
"Evaluating Open-source Toolkits for Automatic Speech Recognition of South African Languages","A. Naidoo; M. Tsoeu","Dept. of Electrical Engineering, University of Cape Town, Cape Town, South Africa; Dept. of Electrical Engineering, University of Cape Town, Cape Town, South Africa","2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)","2 May 2019","2019","","","160","165","Automatic speech recognition is a critical component of human language technologies. It concerns the translation of speech into textual data which can be processed by computers. Thus, it offers the creation of an intimate link allowing humans to interact with machines on a completely natural level. A variety of open-source toolkits exist for the development of these systems. These toolkits have been successfully implemented and tested for use on well-resourced languages. However, the same level of testing has not been performed for South African languages. This investigation sets out to evaluate popular open-source tools for South African languages and identify optimal toolkit configurations for each language and toolkit. The NCHLT corpora were used to set up automatic speech recognition systems for English and isiXhosa using Kaldi, CMU Sphinx, and HTK. The word error rates achieved during this investigation showed that the best configurations from this investigation achieved better performance than those which were reported by the developers of the NCHLT corpus.","","978-1-7281-0369-3","10.1109/RoboMech.2019.8704774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704774","automatic speech recognition;under-resourced;evaluation;languages;isiXhosa;English","Hidden Markov models;Acoustics;Tools;Feature extraction;Decoding;Open source software;Speech recognition","natural language processing;public domain software;speech recognition","open-source toolkits;South African languages;human language technologies;completely natural level;well-resourced languages;open-source tools;optimal toolkit configurations;automatic speech recognition systems;NCHLT corpora;isiXhosa;Kaldi;CMU Sphinx;HTK","","1","","13","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
"Lexical Semantics: A Newapproach to Analyze the Bangla Sentence with Semantic Features","M. M. Hoque; M. J. Rahman; P. KumarDhar","Department of Computer Science and Engineering, University of Engineering and Technology BUET, Chittagong, Bangladesh; Department of Computer Science and Engineering, University of Engineering and Technology BUET, Chittagong, Bangladesh; Department of Computer Science and Engineering, University of Engineering and Technology BUET, Chittagong, Bangladesh","2007 International Conference on Information and Communication Technology","25 Jun 2007","2007","","","87","91","Semantics is concerned with the meaning of words and a word with semantic features is corresponding to their sense component. Associating words with semantic features is useful in the field of machine translation. This paper present a methodology for analysis the Bangla sentences in semantic manner. This paper also focuses on the several primitive semantic features of Bangla words that are necessary for semantic analysis. Semantic features are shown in a tabular fashion with values indicating whether the features or inverse features are applicable and also whatever the features are redundant. Context free grammar and top-down approach have been used for the purpose of generating annotated parse tree with distinct semantic features in this paper. Model is tested for simple sentence of Bangla and test results reflect the successful results for most of the test cases.","","984-32-3394-8","10.1109/ICICT.2007.375348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4261371","","Testing;Natural languages;Speech analysis;Calculus;Communications technology;Information analysis;Computer science;Application software;Performance analysis;Logic","context-free grammars;language translation;programming language semantics;trees (mathematics)","lexical semantics;Bangla sentence;semantic features;machine translation;context free grammar;annotated parse tree","","5","","9","","25 Jun 2007","","","IEEE","IEEE Conferences"
"Phrase-Based Named Entity Transliteration on Myanmar-English Terminology Dictionary","A. M. Mon; K. M. Soe","Natural Language Processing Lab University of Computer Studies, Yangon, Yangon, Myanmar; Natural Language Processing Lab University of Computer Studies, Yangon, Yangon, Myanmar","2020 23rd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","28 Dec 2020","2020","","","38","43","Named entity (NE) transliteration is mainly a phonetically based transcription of names across languages using different writing systems. For the Myanmar language, robust transliteration of named entities is still a challenging task, because of the complex writing system and the lack of data. The Myanmar NE transliteration dictionary has so far developed over 135,255 NE instance pairs of western person, organization and place names. We apply statistical experiments on Phrase-based statistical machine translation (PBSMT) model using 2-Grams, 3-Grams, 4-Grams, 5-Grams and 6-Grams language models in decoding. Different units in the Myanmar script, i.e., characters and syllables are compared. We perform experiments on 1,000 test data set and 1,000 development data set of our proposed dictionary and measure the performance of our system applying bilingual evaluation understudy (BLEU) score. We discuss detailed observations of our experiments in this paper. According to the evaluations, we got the significant results on syllable unit for Myanmar (Myan) to English (Eng) transliteration direction with 89.3% BLEU score and on character unit for English (Eng) to Myanmar (Myan) transliteration direction with 82.0% BLEU score.","2472-7695","978-1-7281-9896-5","10.1109/O-COCOSDA50338.2020.9295015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9295015","Myanmar;named entity;transliteration;Phrasebased statistical machine translation (PBSMT);language model","Task analysis;Dictionaries;Mathematical model;Probability;Decoding;Data models;Organizations","language translation;natural language processing;statistical analysis","Myanmar-English terminology dictionary;named entity transliteration;writing systems;Myanmar NE transliteration dictionary;phrase based statistical machine translation model;bilingual evaluation understudy score;English transliteration;BLEU score;phonetically based transcription;6-Grams language model;5-Grams language model;4-Grams language model;3-Grams language model;2-Grams language model","","","","18","IEEE","28 Dec 2020","","","IEEE","IEEE Conferences"
"Hybrid approach for aligning parallel sentences for languages without a written form using standard Malay and Malay dialects","Y. -M. J. Khaw; T. -P. Tan","School of Computer Sciences, USM, Penang, Malaysia; School of Computer Sciences, USM, Penang, Malaysia","2014 International Conference on Asian Language Processing (IALP)","4 Dec 2014","2014","","","170","174","Alignment of parallel text is a step for building a machine translation. Parallel text alignment is important because linguistic information can be retrieved from the result of alignment which including bilingual dictionaries and grammars correspondence of each language. In this paper, we propose a hybrid approach for standard Malay-dialectal Malay parallel text alignment. The Malay dialects in Malaysia can be grouped according to the states such as Perak dialect, Kedah dialect and Terengganu dialect. It is important to learn Malay dialects as it is still flourished and widely used in many areas especially for unofficial matters. Kelantanese Malay is used as an example for dialectal Malay in this research. The obtained precision and recall values of the proposed alignment methods are above 90%.","","978-1-4799-5330-1","10.1109/IALP.2014.6973524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973524","parallel text alignment;Kelantanese Malay;linguistic information","Standards;Dictionaries;Probability;Pragmatics;Vocabulary;Speech;Reliability","dictionaries;grammars;language translation;linguistics;natural language processing;text analysis","parallel sentences alignment;machine translation;linguistic information;bilingual dictionaries;grammars;hybrid approach;standard Malay-dialectal Malay parallel text alignment;Malaysia;Perak dialect;Kedah dialect;Terengganu dialect;Kelantanese Malay","","","","13","IEEE","4 Dec 2014","","","IEEE","IEEE Conferences"
"Cross-lingual textual entailment using deep learning approach","W. Belay; M. Meshesha; D. Melesew","Faculty of computing, Bahir Dar Institute of Technology, Bahir Dar University; School of Information Science, Addis Ababa University; Faculty of computing, Bahir Dar Institute of Technology, Bahir Dar University","2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA)","17 Jan 2022","2021","","","48","53","Natural Language processing is dealing with natural language understandings and natural language generation which enable computers to understand and analyze human language. Cross-lingual Textual Entailment (CLTE) is one of the applications of NLU if there exists premise (P) as a source language and hypothesis (H) as a target language. CLTE is challenging for transferring information between under resource (Amharic) language and high resource (English) language. To solve this problem, we have proposed Cross-lingual Textual Entailment model using deep neural network approaches. We have used Bi-LSTM to transfer sequential information, XLNet for handling a position of word and its boundary, MLP for classification and prediction outputs, and FastText to word representations. Neural machine translation is utilized for translating English sentences into Amharic sentences with IBM5 alignment. We have combined Amharic dataset with SNLI dataset and annotated based on multi-way classification. The NMT predicts 96.01% of the testing accuracy. We have obtained 89.92% training and 86.89% testing accuracy for the proposed model. The issue with this research is that it ignores multiple inferences.","","978-1-6654-3666-3","10.1109/ICT4DA53266.2021.9672220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672220","Cross-lingual Textual Entailment;Deep learning algorithms;hybrid model;pre-trained models","Training;Deep learning;Economics;Neural networks;Speech recognition;Tokenization;Information and communication technology","deep learning (artificial intelligence);language translation;natural language processing;pattern classification;recurrent neural nets;text analysis","deep learning;natural language processing;natural language understandings;natural language generation;human language;CLTE;high resource language;cross-lingual textual entailment;deep neural network;Amharic language;English language;MLP;Bi-LSTM;sequential information transfering;XLNet;classification;word representations;neural machine translation;IBM5 alignment;multiway classification","","","","25","IEEE","17 Jan 2022","","","IEEE","IEEE Conferences"
"Bilingual Phrase Extraction from N-Best Alignments","Yong-Zeng Xue; Sheng Li; Tie-Jun Zhao; Mu-Yun Yang; Jun Li","Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)","16 Oct 2006","2006","3","","410","414","Improved approach of phrase extraction was proposed for phrase-based statistical machine translation. The effectiveness was investigated when using n-best alignments instead of one-best for phrase extraction. Bilingual phrase pairs were extracted in the presented approach by combining word-to-word links from n-best alignments between source and target sentences. First, the n-best alignments were divided into hierarchies by frequencies of word co-occurrence. Second, candidates of phrase pairs were extracted from each layer. Experimental results show that the presented approach outperforms the baseline system Pharaoh in both NIST and BLEU scores. Therefore it is effective to use n-best alignments as an extension to one-best alignment for phrase extraction.","","0-7695-2616-0","10.1109/ICICIC.2006.426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692201","","Natural languages;NIST;Context modeling;Laboratories;Speech processing;Computer science;Frequency conversion;Data mining;Encoding;Error analysis","computational linguistics;language translation;linguistics","bilingual phrase extraction;phrase-based statistical machine translation;n-best alignments;bilingual phrase pairs;word-to-word links;word cooccurrence;Pharaoh baseline system;NIST;BLEU scores","","","","12","IEEE","16 Oct 2006","","","IEEE","IEEE Conferences"
"Incorporating syntax-based language models in phrase-based SMT models","Y. Chen; X. Shi; C. Zhou; Q. Hong","Department of Cognitive Science, School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China; Department of Cognitive Science, School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China; Department of Cognitive Science, School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China; Department of Cognitive Science, School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China","2008 3rd International Conference on Intelligent System and Knowledge Engineering","30 Dec 2008","2008","1","","808","812","In this paper, we proposed a method to incorporate syntax-based language models in phrase-based statistical machine translation (SMT) systems. The language model that we used is based on link grammar, which is a high lexical formalism. In order to apply language models based on link grammar in phrase-based models, we brought out the concept of linked phrases, which is an extension of the concept of traditional phrases in phrase-based models. Experiments were carried out and the results showed that the use of syntax-based language models could improve the performance of the phrase-based models greatly.","","978-1-4244-2196-1","10.1109/ISKE.2008.4731040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731040","","Surface-mount technology;Intelligent systems;Knowledge engineering;NIST;Predictive models;Cognitive science;Information science;Natural languages;Speech recognition;Information retrieval","computational linguistics;language translation","syntax-based language models;SMT models;statistical machine translation;link grammar;high lexical formalism;phrase-based models","","","","17","IEEE","30 Dec 2008","","","IEEE","IEEE Conferences"
"Rule base combined linguistics knowledge with corpus","Ying Liu; Chengqing Zong","Lab of Computational Linguistics, Department of Chinese Language and Literature, Tsinghua University, Beijing, China; National Laboratory of Pattem Recognition, Institute of Automation, Chinese Academy and Sciences, Beijing, China","SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)","17 Nov 2003","2003","5","","5022","5027 vol.5","This paper proposes a new approach to construction of rule bases for the transferred-based machine translation. In our approach, the rule bases are constructed in combination of the linguistics knowledge and large scale of corpora. On the one hand, the lexical knowledge, the syntactic knowledge and the semantic knowledge are all used in the rules. On the other hand, the knowledge is used for the statistics and self-learning rules. In each rule base, all rules are scored and ranked. Thus, an impersonal choice for the sentence can be made. The preliminary experimental results show that the approach may increase the speed to build the rule base and improve the quality of rules.","1062-922X","0-7803-7952-7","10.1109/ICSMC.2003.1245779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245779","","Natural languages;Tagging;Large-scale systems;Computational linguistics;Statistics;Laboratories;Natural language processing;Speech recognition","knowledge based systems;language translation;computational linguistics","rule bases;transferred-based machine translation;linguistics knowledge;lexical knowledge;syntactic knowledge;semantic knowledge;statistics;self-learning rules","","","","16","IEEE","17 Nov 2003","","","IEEE","IEEE Conferences"
"Asian Information HUB Project: NICT's R&D Vsion and Strategies for Universal Communication Technology in the Big Data Era","M. Iwazume; H. Fujii; T. Iwase; H. Haraguchi; M. Hijiya","Universal Communication Research Institute, National Institute of Information and Communications Technology, Kyoto, Japan; Universal Communication Research Institute, National Institute of Information and Communications Technology, Kyoto, Japan; Kobe Digital Labo, Inc., Kobe, Japan; Universal Communication Research Institute, National Institute of Information and Communications Technology, Kyoto, Japan; Universal Communication Research Institute, National Institute of Information and Communications Technology, Kyoto, Japan","2012 IEEE 36th Annual Computer Software and Applications Conference Workshops","10 Nov 2012","2012","","","1","6","The Universal Communications Research Institute (UCRI), NICT conducts research and development on universal communication technologies: multi-lingual machine translation, spoken dialogue, information analysis and ultra-realistic interaction technologies, through which people can truly interconnect, anytime, anywhere, about any topic, and by any method, transcending the boundaries of language, culture, ability and distance. To realizing universal communication, UCRI collects diverse information including huge volumes of web pages focusing on information from Asia. This paper introduces NICT's vision and strategies for Asian information hub as a platform for collecting, storing, analyzing large-scale information and providing advanced communication services in Big Data Era.","","978-0-7695-4758-9","10.1109/COMPSACW.2012.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341541","Bid Data;Unicersal Communication Technology;Asian Information Hub;Large-scale Infromation Infrastructure","Information management;Data handling;Data storage systems;Crawlers;Speech;Web pages;Dictionaries","information analysis;information storage;language translation;research and development;Web sites","NICT R&D vision;universal communication technology;big data era;Asian information hub project;Universal Communications Research Institute;UCRI;multilingual machine translation;spoken dialogue;information analysis;ultrarealistic interaction technologies;language boundaries;Web pages;large-scale information;advanced communication services","","","","9","IEEE","10 Nov 2012","","","IEEE","IEEE Conferences"
"Statistical MT/spl ne/stone soup","K. W. Church","AT and T Research, USA","IEEE Expert","6 Aug 2002","1996","11","2","14","15","The statistical approach to machine translation is producing a set of useful terminology and reuse tools. Unlike traditional MT, these tools do not attempt to compete with the human at what the human does best (translating the easy vocabulary and the easy grammar), but complement the human in areas where they know they need help (difficult vocabulary and reuse).","2374-9407","","10.1109/64.491311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=491311","","Statistics;Statistical analysis;Application software;Natural language processing;Humans;Telephony;Speech recognition;Probability;Surface morphology","language translation;humanities;statistical analysis","statistical machine translation;reuse tools;terminology tools;vocabulary","","","","5","IEEE","6 Aug 2002","","","IEEE","IEEE Magazines"
"American Sign Language Recognition using Deep Learning and Computer Vision","K. Bantupalli; Y. Xie","Department of Computer Science, Kennesaw State University, Kennesaw, USA; Department of Computer Science, Kennesaw State University, Kennesaw, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4896","4899","Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.","","978-1-5386-5035-6","10.1109/BigData.2018.8622141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622141","computer science;machine learning;computer vision;sign language","Gesture recognition;Assistive technology;Feature extraction;Training;Hidden Markov models;Conferences;Recurrent neural networks","computer vision;convolutional neural nets;feature extraction;gesture recognition;handicapped aids;image sequences;language translation;learning (artificial intelligence);recurrent neural nets;sign language recognition","deep learning;computer vision;gesture recognition;sign language translation;American sign language recognition;speech impairment;nonsign language;sign language speakers;motion recognition;American sign language dataset;video sequences;temporal feature extraction;spatial feature extraction;Inception;CNN;convolutional neural network;spatial feature recognition;RNN;recurrent neural network","","129","","10","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Tracking dialog states using an Author-Topic based representation","R. Dufour; M. Morchid; T. Parcollet","LIA - University of Avignon, France; LIA - University of Avignon, France; LIA - University of Avignon, France","2016 IEEE Spoken Language Technology Workshop (SLT)","9 Feb 2017","2016","","","544","551","Automatically translating textual documents from one language to another inevitably results in translation errors. In addition to language specificities, this automatic translation appears more difficult in the context of spoken dialogues since, for example, the language register is far from ‚Äúclean speech‚Äù. Speech analytics suffer from these translation errors. To tackle this difficulty, a solution consists in mapping translations into a space of hidden topics. In the classical topic-based representation obtained from a Latent Dirichlet Allocation (LDA), distribution of words into each topic is estimated automatically. Nonetheless, the targeted classes are ignored in the particular context of a classification task. In the DSTC5 main task, this targeted class information is crucial, the main objective being to track dialog states for sub-dialog segments. For this challenge, we propose to apply an original topic-based representation for each sub-dialogue based not only on the sub-dialogue content itself (words), but also on the dialogue state related to the sub-dialogue. This original representation is based on the Author-Topic (AT) model, previously successfully applied on a different classification task. Promising results confirmed the interest of such a method, the AT model reaching performance slightly better in terms of F-measure than baseline ones given by the task's organizers.","","978-1-5090-4903-5","10.1109/SLT.2016.7846316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846316","Author-Topic Model;Dialog State Tracking;Sub-dialog Level","Speech;Resource management;Context;Registers;Support vector machines;Data collection;Protocols","interactive systems;speech processing;text analysis","dialog states;author-topic based representation;textual documents;translation errors;spoken dialogues;clean speech;speech analytics;latent Dirichlet allocation;LDA","","1","","27","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Towards a Unified Training for Levenshtein Transformer","K. Zheng; L. Wang; Z. Wang; B. Chen; M. Zhang; Z. Tu","School of CS, National Key Laboratory for Multimedia Information Processing, Peking University; Tencent AI Lab; Xiamen University; School of CS, National Key Laboratory for Multimedia Information Processing, Peking University; School of CS, National Key Laboratory for Multimedia Information Processing, Peking University; Tencent AI Lab","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Levenshtein Transformer (LevT) is a widely-used text-editing model, which generates a sequence based on editing operations (deletion and insertion) in a non-autoregressive manner. However, it is challenging to train the key refinement components of LevT due to training-inference discrepancy. By carefully designing experiments, our work reveals that the deletion module is under-trained while the insertion module is over-trained due to the imbalance training signals for the two refinement modules. Based on these observations, we further propose a dual learning approach that can remedy the imbalance training by feeding an initial input to both refinement modules, which is consistent with the process in inference. Experimental results on three representative NLP tasks demonstrate the effectiveness and universality of the proposed approach. 1","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094646","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094646","Non-Autoregressive Model;Training Signal;Editing Operation;Machine Translation;Text Summarization;Grammar Error Correction","Training;Transformers;Acoustics;Error correction;Task analysis;Speech processing","","","","","","19","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Bitext Dependency Parsing With Auto-Generated Bilingual Treebank","W. Chen; J. Kazama; M. Zhang; Y. Tsuruoka; Y. Zhang; Y. Wang; K. Torisawa; H. Li","Department of Human Language Technology, Institute for Infocomm Research, Singapore; National Institute of Information and Communications Technology, Kyoto, Japan; Department of Human Language Technology, Institute for Infocomm Research, Singapore; Graduate School of Engineering, University of Tokyo, Tokyo, Japan; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; National Institute of Information and Communications Technology, Kyoto, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; Department of Human Language Technology, Institute for Infocomm Research, Singapore","IEEE Transactions on Audio, Speech, and Language Processing","27 Feb 2012","2012","20","5","1461","1472","This paper proposes a method to improve the accuracy of bilingual texts (bitexts) dependency parsing by using an auto-generated bilingual treebank created with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are costly and troublesome to obtain. In the proposed method, we use an auto-generated bilingual treebank to train the parsing models. First, an SMT system is used to translate a monolingual treebank into the target language; then, a monolingual parser for the target language is used to parse the translated sentences. Since the auto-translated sentences and auto-parsed trees in the auto-generated bilingual treebank are far from perfect, the bilingual constraints are not sufficiently reliable. To overcome this problem, we propose a method to verify the reliability of the constraints using a large amount of target monolingual and bilingual unannotated data. Finally, we design a set of effective bilingual features for parsing models on the basis of the verified constraints. We conduct the experiments using a standard test data. The experimental results show that our bitext parser significantly outperforms monolingual parsers. Moreover, our method is still able to provide improvement when we use a larger monolingual treebank containing over 50 000 sentences. We also test the proposed method with different SMT systems and the results show that our method is very robust to the noise. In particular, the proposed method can be used in a purely monolingual setting with the help of SMT. That is, it does not need the human translation of the test set as previous methods do.","1558-7924","","10.1109/TASL.2011.2180898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111269","Dependency parsing;natural language processing;statistical machine translation;unannotated data","Humans;Reliability;Educational institutions;Data mining;Materials;Training;Noise","speech processing;statistical analysis;trees (mathematics)","bitext dependency parsing;autogenerated bilingual treebank;bilingual text dependency parsing;statistical machine translation systems;SMT systems;human-annotated bilingual treebanks;monolingual treebank;autotranslated sentences;autoparsed trees;bilingual constraints;bilingual unannotated data;bilingual features;bitext parser;monolingual parsers","","1","","37","IEEE","21 Dec 2011","","","IEEE","IEEE Journals"
"Issues in parsing and POS tagging of hybrid language","S. Harsh Atrey; T. V. Prasad; G. Rama Krishna","K.L University, Vijayawada, India; PBR Visvodaya Institute of Technology & Science, Kavali, A.P, India; Dept. of CSE, K.L University, Vijayawada, A.P, India","2012 IEEE International Conference on Computational Intelligence and Cybernetics (CyberneticsCom)","14 Feb 2013","2012","","","20","24","The purpose of a Machine Translation (MT) system is to decode one language into another. Every language has its own different lexical and syntactic structure. A hybrid language does not have its own structure; it is an amalgamation of two or more languages in a sentence. To understand the structure and to decode a hybrid language into a formal language, hybrid parsing techniques are required. Hindi and English have Subject Object Verb (SOV) and Subject Verb Object (SVO) word orders, respectively. The basic requirement of parsers is to transform a SOV word order to a SVO word order and vice versa and Part of Speech (POS) tagging is essential for word grouping. The purpose of this paper is to bring out the concepts of parsers and POS tagging techniques to which hybrid translation can takes place to a formal language.","","978-1-4673-0892-2","10.1109/CyberneticsCom.2012.6381609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381609","Parse Tree;POS;Syntax Model;bilingual dictionary","Tagging;Random access memory;Formal languages;Libraries;Grammar;Syntactics;Dictionaries","formal languages;grammars;language translation;natural language processing","parsing;POS tagging;hybrid language;machine translation system;MT system;language lexical structure;language syntactic structure;language amalgamation;formal language;hybrid parsing technique;Hindi;English;subject object verb;subject verb object;SOV word order;SVO word order;part-of-speech tagging;word grouping","","2","","11","IEEE","14 Feb 2013","","","IEEE","IEEE Conferences"
"Automatic lexico-syntactic classification of noun-adjective relations for Romanian language","M. LazƒÉr; D. Militaru","Military Equipment and Technologies Research, Agency Bucharest, Romania; University Politehnica of Bucharest, Bucharest, Romania","2015 7th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","26 Oct 2015","2015","","","S-85","S-88","The natural language processing became one of the most important fields of artificial intelligence because is related to the area of human-computer interaction using human languages (natural language generation, question answering, machine translation, etc.) or speech understanding (language modeling).To model the relations between words it is necessary to find the syntactic and semantic relations between them. Starting from the property/attribute-holder relation between nouns and adjectives, extracted from Romanian translation of Orwell's novel ‚ÄúNineteen eighty-four‚Äù (part of Multext-East [1]), this paper presents the results of automatic lexico-syntactic classification using three classification methods: decision trees, k nearest neighbors and na√Øve Bayes.","","978-1-4673-6647-2","10.1109/ECAI.2015.7301166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301166","lexico-syntactic pattern;decision trees;Na√Øve Bayes;k-NN;Romanian language","Semantics;Decision trees;Training;Syntactics;Context;Accuracy;Pragmatics","decision trees;human computer interaction;language translation;natural language processing;question answering (information retrieval)","automatic lexico-syntactic classification;noun-adjective relations;Romanian language;natural language processing;artificial intelligence;human-computer interaction;HCI;human languages;natural language generation;question answering;machine translation;language modeling;speech understanding;attribute-holder relation;decision trees;k nearest neighborsand na√Øve Bayes","","","","23","IEEE","26 Oct 2015","","","IEEE","IEEE Conferences"
"Augmenting Kannada Educational Video with Indian Sign Language Captions Using Synthetic Animation","D. S. Jayalakshmi; H. Salpekar; R. H. K. Kiran; R. Rahul; Shobha","Dept. of CSE, M.S. Ramaiah Institute of Technology, Bangalore, India; Dept. of CSE, M S Ramaiah Institute of Technology, Bangalore, India; Dept. of CSE, M.S. Ramaiah Institute of Technology, Bangalore, India; Dept. of CSE, M.S. Ramaiah Institute of Technology, Bangalore, India; Dept. of CSE, M S Ramaiah Institute of Technology, Bangalore, India","2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)","1 Oct 2020","2020","","","324","329","The project aims for a machine translation system for augmenting Kannada educational video by providing a translation to Indian Sign Language (ISL) using virtual animation approach along with the subtitles. The system accepts Kannada educational video clips as input and output is delivered as 3D character animation. The first phase approach of the proposed system makes use of SpeechRecognition like kaldi an open source for converting the input video files to their respective text which are used to generate the subtitles and these texts are further pre-processed in order to meet the sign language norms. The right SiGML file for these pre-processed words is picked from the database. These SiGML files consist of XML tags which correspond to the respective HamNoSys Notations. HamNoSys is an alphabetic system that mostly phonetically describes signs. A synthetic animation of a human in the tool in the URL APP, JA SiGML takes input as these SiGML files and generates signs of the respective pre-processed words. Synchronization of inputted video with the sign language and the subtitle generation is vital for the clear and clean flow. This system is Tested by professional sign language trainer/interpreter for the sign language generated by the avatar and is found effective. The accuracy of sign language shown was found out to be 63%.","","978-1-7281-6823-4","10.1109/WorldS450073.2020.9210385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210385","NLP;ISL;Kaldi;HamNoSYS;SiGML;Speech Recognition","Market research;Security;Sustainable development","avatars;computer aided instruction;computer animation;educational technology;handicapped aids;human computer interaction;language translation;natural language processing;sign language recognition;speech recognition;video signal processing;XML","synthetic animation;machine translation system;virtual animation;3D character animation;kaldi;SiGML file;Indian sign language captions;Kannada educational video augmentation;speech recognition;open source;XML tags;HamNoSys alphabetic system;video synchronization;avatar","","","","11","IEEE","1 Oct 2020","","","IEEE","IEEE Conferences"
"Image Talk: a real time synthetic talking head using one single image with Chinese text-to-speech capability","Woei-Luen Perng; Yungkang Wu; Ming Ouhyoung","Communication and Multimedia Laboratory, Department Of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Communication and Multimedia Laboratory, Department Of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Communication and Multimedia Laboratory, Department Of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan","Proceedings Pacific Graphics '98. Sixth Pacific Conference on Computer Graphics and Applications (Cat. No.98EX208)","6 Aug 2002","1998","","","140","148","Image Talk uses a single image to automatically create talking sequences in real time. The image can be acquired from a photograph, video clip, or hand drawn characters. This interactive system accepts Chinese text and talks back in Mandarin Chinese, generating facial expression in real time. Image Talk analyzes Chinese text by converting it to a standard Pinyin system used in Taiwan and fetches the associated facial expressions from an expression pool. The expressions are synchronized with the sound and played back in a talking sequence. Image Talk also incorporates eye blinking, small scale head rotation and translation perturbations, to make the resulting sequence more natural. It is also easy to switch to any other face images. The result is quite entertaining, and can easily be used as a new human machine interface, as well as for lip sync in computer animated characters.","","0-8186-8620-0","10.1109/PCCGA.1998.732094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=732094","","Speech synthesis;Humans;Facial animation;Head;Read only memory;Interactive systems;Eyes;Motion pictures;Multimedia communication;Laboratories","natural languages;speech synthesis;real-time systems;linguistics;natural language interfaces","Image Talk;real time synthetic talking head;single image;Chinese text-to-speech capability;talking sequences;interactive system;Mandarin Chinese;facial expression;Chinese text;standard Pinyin system;Taiwan;expression pool;eye blinking;small scale head rotation;translation perturbations;face images;human machine interfac;lip sync;computer animated characters","","5","4","13","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic Methods and Neural Networks in Arabic Texts Diacritization: A Comprehensive Survey","M. M. Almanea","College of Languages and Translation, Al Imam Mohammad Ibn Saud Islamic University, Riyadh, Saudi Arabia","IEEE Access","5 Nov 2021","2021","9","","145012","145032","Arabic diacritics are signs used in Arabic orthography to represent essential morphophonological and syntactic information. It is a common practice to leave out those diacritics in written Arabic. Most Arabic electronic texts lack such diacritics. The processing of those texts for various purposes of Natural Language Processing is a complicated task. Diacritized words are necessary for applications such as machine translation, sentiment analysis, and speech synthesis. To address this problem, several studies proposed automatic systems to restore diacritics in Arabic texts. The present paper presents an in-depth survey of 56 most recent Arabic diacritization studies. Based on the diacritization approach, the studies are grouped into four sections in terms of method; rule-based, simple statistical, hybrid, and Neural Networks. While rule-based methods such as morphological analyzers and lexicon retrievals were the earliest approaches, results indicated that they are still valuable tools that can aid in the process of diacritization. Effective statistical methods that produced diacritics with acceptable accuracy include Hidden Markov Model, n-grams, and Support Vector Machines. They are often accompanied by either rule-based or neural networks in hybrid systems. Neural networks, specifically Bidirectional Long Short Term Memory, reached very high diacritization accuracy levels. Studies employing neural networks focused on evaluating and comparing the efficacy of several types of neural networks or a hybrid of them, testing alternatives of input units or suggested schemes for partial daicritization. The study synthesizes the results of the studies, identifies research gaps, and offers recommendations for future research.","2169-3536","","10.1109/ACCESS.2021.3122977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585619","Arabic text diacritization;neural networks;Arabic corpora;deep learning;Arabic natural language processing","Syntactics;Linguistics;Artificial neural networks;Writing;Statistical analysis;Machine translation","computational linguistics;hidden Markov models;knowledge based systems;natural language processing;recurrent neural nets;statistical analysis;support vector machines;text analysis","Arabic diacritics;Arabic electronic texts;Arabic orthography;Arabic texts diacritization;bidirectional long short term memory;hidden Markov model;n-grams;natural language processing;neural networks;rule-based method;statistical method;support vector machines;word diacritization","","4","","77","CCBY","26 Oct 2021","","","IEEE","IEEE Journals"
"Semantic Word Error Rate for Sentence Similarity","C. Spiccia; A. Augello; G. Pilato; G. Vassallo","Italian National Research Council (CNR), Istituto di calcolo e reti ad alte prestazioni (ICAR), Palermo, Italy; Italian National Research Council (CNR), Istituto di calcolo e reti ad alte prestazioni (ICAR), Palermo, Italy; Italian National Research Council (CNR), Istituto di calcolo e reti ad alte prestazioni (ICAR), Palermo, Italy; Dipart. di Ingegneria Chimica, Universit√° degli Studi di Palermo, Palermo, Italy","2016 IEEE Tenth International Conference on Semantic Computing (ICSC)","24 Mar 2016","2016","","","266","269","Sentence similarity measures have applications in several tasks, including: Machine Translation, Paraphrase Identification, Speech Recognition, Question-answering and Text Summarization. However, measures designed for these tasks are aimed at assessing equivalence rather than resemblance, partly departing from human cognition of similarity. While this is reasonable for these activities, it hinders the applicability of sentence similarity measures to other tasks. We therefore propose a new sentence similarity measure specifically designed for resemblance evaluation, in order to cover these fields better. Experimental results are discussed.","","978-1-5090-0662-5","10.1109/ICSC.2016.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439346","Semantic Word Error Rate;SWER;sentence similarity measure;sentence resemblance;word relatedness;Latent Semantic Analysis;LSA;Word Error Rate;WER","Extraterrestrial measurements;Semantics;Current measurement;Measurement uncertainty;Error analysis;Atmospheric measurements;Particle measurements","language translation;question answering (information retrieval);text analysis","semantic word error rate;machine translation;paraphrase identification;speech recognition;question-answering;text summarization;sentence similarity measures;resemblance evaluation","","3","","22","IEEE","24 Mar 2016","","","IEEE","IEEE Conferences"
"Entity Recognition in Assamese Text","N. Mahanta; S. Dhar; S. Roy","Department of CSE, Assam University, Silchar, Assam, India; Department of CSE, Assam University, Silchar, Assam, India; Department of CSE, Assam University, Silchar, Assam, India","2016 International Conference on Communication and Electronics Systems (ICCES)","30 Mar 2017","2016","","","1","5","Entity Recognition detects all the entities present in a document to improve the performance of some high level Natural Language Processing (NLP) tasks like Question Answering, Auto Summarization, Machine Translation, Information Extraction. The task is subdivided into two parts: Parts of Speech Tagging (POS) and Entity Recognition. Each sentence is annotated with part-of-speech tags and then the proper nouns are again classified with our own entity tag set. This paper introduces Entity Recognition in Assamese Text using Conditional Random Fields (CRF). Results are measured with F-measure metric for each different entity class.","","978-1-5090-1066-0","10.1109/CESYS.2016.7890006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890006","POS tagging;Entity Recognition;CRF;Assamese Language","Tagging;Text recognition;Speech recognition;Speech;Organizations;Natural language processing;Random access memory","natural language processing;random processes;text analysis","entity recognition;Assamese text;performance improvemnt;high-level NLP tasks;natural language processing;question answering;autosummarization;machine translation;information extraction;part-of-speech tagging;POS;sentence annotation;proper noun classification;conditional random fields;CRF;F-measure metric","","2","","12","IEEE","30 Mar 2017","","","IEEE","IEEE Conferences"
"Spellchecker for Malayalam using finite state transition models","N. Manohar; P. T. Lekshmipriya; V. Jayan; V. K. Bhadran","CDAC, Language Technology Section, Thiruvananthapuram; Language Technology Section, CDAC -Thiruvananthapuram; Language Technology Section, CDAC -Thiruvananthapuram; Language Technology Section, CDAC -Thiruvananthapuram","2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS)","13 Jun 2016","2015","","","157","161","Finite-state machine is used as a core technology in many fields of natural language processing. The applications include speech recognition and generation, spelling correction, fact extraction, information retrieval and approaches to translation. This paper describes the design of finite state machine in development of a Malayalam spellchecker. Conventional spell checkers has the disadvantage of large dictionary size which can be reduced using finite state models. Morphological factors like word categories and their inflections evolve automatically during training of these models if we train the model with an appropriate training set.","","978-1-4673-6670-0","10.1109/RAICS.2015.7488406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488406","spellchecker;morphology;automata;Malayalam","Automata;Training;Dictionaries;Mathematical model;Computational modeling;Morphology","finite state machines;information retrieval;natural language processing;speech recognition","spellchecker;Malayalam;finite state transition models;finite state machine;natural language processing;speech recognition;speech generation;spelling correction;fact extraction;information retrieval","","2","","6","IEEE","13 Jun 2016","","","IEEE","IEEE Conferences"
"AI-Powered Smart Glasses for Blind, Deaf, and Dumb","S. L. R; S. M; S. K; A. Thilagavathy","Department of Computer Engineering, R.M.K Engineering College, Tamilnadu, India; Department of Computer Engineering, R.M.K Engineering College, Tamilnadu, India; Department of Computer Engineering, R.M.K Engineering College, Tamilnadu, India; Department of Computer Engineering, R.M.K Engineering College, Tamilnadu, India","2022 5th International Conference on Advances in Science and Technology (ICAST)","13 Feb 2023","2022","","","280","285","Back then, technology was far more advanced (developed) than time. We can now determine what a deaf or mute person is saying only by capturing it and comparing it to predetermined datasets, showing what the individual is striving to achieve. In this study, we propose a way of supporting impaired people, such as those who are Deaf, Dumb, or Blind, by giving them a new tech that acts as an eye, ear, and brain to them. Machine learning methods are employed for object recognition with the aid of image processing to give the blind, a pair of eyes. The deaf and the dumb can both benefit from text-to-speech communication using Bluetooth or radio technology and speech-to-text translation. The convergence of all these technologies, together with AI, AR, VR, and IoT technologies, will help in finding solutions to the problems that these people who face disabilities.","","978-1-6654-9263-8","10.1109/ICAST55766.2022.10039557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10039557","Internet of things;Artificial Intelligence;processing of images;wireless communication;Bluetooth communication;Augmented Reality;Virtual Reality","Legged locomotion;Machine learning algorithms;Image resolution;Image communication;Face recognition;Machine learning;Ear","Bluetooth;handicapped aids;Internet of Things;learning (artificial intelligence);object recognition","AI-powered smart glasses;blind;deaf person;Dumb;eye;image processing;impaired people;IoT technologies;machine learning methods;mute person;object recognition;predetermined datasets;speech-to-text translation;text-to-speech communication","","","","16","IEEE","13 Feb 2023","","","IEEE","IEEE Conferences"
"High-Quality Nonparallel Voice Conversion Based on Cycle-Consistent Adversarial Network","F. Fang; J. Yamagishi; I. Echizen; J. Lorenzo-Trueba","National Institute of Informatics, Japan; National Institute of Informatics, Japan; National Institute of Informatics, Japan; National Institute of Informatics, Japan","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","5279","5283","Although voice conversion (VC) algorithms have achieved remarkable success along with the development of machine learning, superior performance is still difficult to achieve when using nonparallel data. In this paper, we propose using a cycle-consistent adversarial network (CycleGAN) for nonparallel data-based VC training. A CycleGAN is a generative adversarial network (GAN) originally developed for unpaired image-to-image translation. A subjective evaluation of inter-gender conversion demonstrated that the proposed method significantly outperformed a method based on the Merlin open source neural network speech synthesis system (a parallel VC system adapted for our setup) and a GAN-based parallel VC system. This is the first research to show that the performance of a nonparallel VC method can exceed that of state-of-the-art parallel VC methods.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462342","Voice conversion;deep learning;cycle-consistent adversarial network;generative adversarial network","Training;Linguistics;Hidden Markov models;Gallium nitride;Generators;Training data;Linear programming","learning (artificial intelligence);neural nets;speech recognition;speech synthesis","GAN-based parallel VC system;nonparallel VC method;high-quality nonparallel voice conversion;cycle-consistent adversarial network;voice conversion algorithms;machine learning;CycleGAN;nonparallel data-based VC training;generative adversarial network;inter-gender conversion;Merlin open source neural network speech synthesis system;image-to-image translation;parallel VC methods","","61","","33","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"The Task of Synthesizing the Kazakh Language Based on the seq2seq Approach for a Question-Answer System","D. Rakhimova; Y. Suleimenov; G. Akhmet","Al-Farabi Kazakh National University, Institute of information and computational technologies, Almaty, Kazakhstan; Institute of information and computational technologies, Almaty, Kazakhstan; Al-Farabi Kazakh National University, Almaty, Kazakhstan","2022 7th International Conference on Computer Science and Engineering (UBMK)","28 Oct 2022","2022","","","289","293","Currently, many texts are synthesized using a computer. Sentence synthesis is becoming more common in various fields. For example, it is used in intelligent systems that can explain to the user the progress of solving a specific problem, decision support systems that help the user make decisions based on the developed alternative, smart homes, voice assistants, chat bots, and so on. Machine learning is one of the most effective approaches to solving the problem of text synthesis. Machine learning algorithms can determine how to perform important tasks by analyzing examples. In the task of synthesizing sentences using machine learning, it is possible to replace a number of components of the entire system with neural networks, which allows not only to approach existing algorithms in quality, but even to significantly surpass them. The article investigates analogous sentence synthesis systems, considers the problem of sentence synthesis in the Kazakh language for the task of a question-answer system. This work consists of two subtasks. The first describes the collection and processing of text resources necessary to solve the problem. For this subtask, an electronic question-answer corpus for the Kazakh language was collected and processed. This corpus consists of 60,000 questions and answers on various topics. The second sentence synthesis problem was solved on the basis of machine learning using the seq2seq method. Based on the chosen method and the assembled corpus, a number of experiments were carried out and the results were obtained. Based on the results obtained, the quality was assessed using the BLEU metric and an expert.","2521-1641","978-1-6654-7010-0","10.1109/UBMK55850.2022.9919436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919436","Kazakh language;sentence synthesis;machine learning;seq2seq method","Measurement;Decision support systems;Computer science;Machine learning algorithms;Neural networks;Machine learning;Smart homes","decision support systems;language translation;learning (artificial intelligence);natural language processing;natural languages;speech synthesis;text analysis","Kazakh language;seq2seq approach;question-answer system;intelligent systems;decision support systems;developed alternative;text synthesis;machine learning algorithms;analogous sentence synthesis systems;text resources;electronic question-answer corpus;000 questions;sentence synthesis problem;seq2seq method","","","","16","IEEE","28 Oct 2022","","","IEEE","IEEE Conferences"
"Automatic detection and translation of text from natural scenes","J. Yang; X. Chen; J. Zhang; Y. Zhang; A. Waibel","Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA, USA; Interactive Systems Laboratory, Carnegie Mellon University, Pittsburgh, PA, USA","2002 IEEE International Conference on Acoustics, Speech, and Signal Processing","7 Apr 2011","2002","2","","II-2101","II-2104","Large amounts of information are embedded in natural scenes. Signs are good examples of natural objects with high information content. In this paper, we discuss problems in automatic detection and translation of text from natural scenes. We describe the chal1enges of automatic text detection and propose methods to address these chal1enges. We extend example based machine translation technology for sign translation and present a prototype system for Chinese sign translation. This system is capable of capturing images, automatically detecting and recognizing text, and translating the text into English. The translation can be displayed on a palm size PDA, or synthesized as a voice output message over the earphones.","1520-6149","0-7803-7402-9","10.1109/ICASSP.2002.5745049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5745049","","Engines;Optical character recognition software;Text recognition;Syntactics","","","","15","1","17","IEEE","7 Apr 2011","","","IEEE","IEEE Conferences"
"NER in Hindi Language Using Transformer Model:XLM-Roberta","A. A. Choure; R. B. Adhao; V. K. Pachghare","Department of Computer Engineering and IT, College of Engineering Pune(COEP), Pune, India; Department of Computer Engineering and IT, College of Engineering Pune(COEP), Pune, India; Department of Computer Engineering and IT, College of Engineering Pune(COEP), Pune, India","2022 IEEE International Conference on Blockchain and Distributed Systems Security (ICBDS)","9 Nov 2022","2022","","","1","5","Natural language processing (NLP) is a computer program that trains computers to read and understand the text and spoken words in the same way that people do. In Natural Language Processing, Named Entity Recognition (NER) is a crucial field. It extracts information from given texts and is used to translate machines, text to speech synthesis, to understand natural language, etc. Its main goal is to categorize words in a text that represent names into specified tags like location, organization, person-name, date, time, and measures. In this paper, the proposed method extracts entities on Hindi Fraud Call (publicly not available) annotated Corpus using XLM-Roberta (base-sized model). By pre-training model to build the accurate NER system for datasets, the Authors are using XLM-Roberta as a multi-layer bidirectional transformer encoder for learning deep bidirectional Hindi word representations. The fine-tuning concept is used in this proposed method. XLM-Roberta Model has been fine-tuned to extract nine entities from sentences based on context of sentences to achieve better performance. An Annotated corpus for Hindi with a tag set of Nine different Named Entity (NE) classes, defined as part of the NER Shared Task for South and Southeast Asian Languages (SSEAL) at IJCNLP. Nine entities have been recognized from sentences. The Obtained F1-score(micro) and F1-score(macro) are 0.96 and 0.80, respectively.","","978-1-6654-2832-3","10.1109/ICBDS53701.2022.9935841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935841","Natural Language Processing;NER;XLM-Roberta;Hindi Fraud Call;NE","Text recognition;System performance;Transformers;Natural language processing;Time measurement;Fraud;Speech synthesis","feature extraction;information retrieval;language translation;learning (artificial intelligence);natural language processing;natural languages;speech synthesis;text analysis","different Named Entity;NER Shared Task;Southeast Asian Languages;Hindi Language;transformer Model:XLM-Roberta;Natural language processing;computer program;spoken words;Natural Language Processing;Named Entity Recognition;crucial field;given texts;person-name;Hindi Fraud Call annotated Corpus;base-sized model;pre-training model;accurate NER system;multilayer bidirectional transformer encoder;deep bidirectional Hindi word representations;fine-tuning concept;XLM-Roberta Model","","","","20","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Efficient Stuttering Event Detection Using Siamese Networks","P. Mohapatra; B. Islam; M. T. Islam; R. Jiao; Q. Zhu","Northwestern University, Illinois, USA; Worcester Polytechnic Institute, Massachusetts, USA; Amazon Labs, Massachusetts, USA; Northwestern University, Illinois, USA; Northwestern University, Illinois, USA","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Speech disfluency research is pivotal to accommodating atypical speakers in mainstream conversational technology. However, the lack of publicly available labeled and unlabeled datasets is a significant bottleneck to such research. While many works use pseudo dysfluency data with proxy labels and formulate a self-supervised task, we see merit in using real-world data. In this work, we consolidate the corpora of publicly available speech disfluency datasets with and without labels and propose DisfluentSiam ‚Äì an efficient siamese network-based small-scale pretraining pipeline using task-specific data from multiple domains with only 10M trainable parameters. We show that with DisfluentSiam, we achieve an average of 15% boost in performance across five types of dysfluency event detection compared to direct wav2vec 2.0 embeddings. In particular, with only 4-5 mins of labeled data for fine-tuning, the DisfluentSiam demonstrates the advantage of task-specific pretraining with up to 25% higher accuracy.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094692","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094692","Dysfluency;Self-supervised Learning","Event detection;Pipelines;Signal processing;Real-time systems;Data models;Acoustics;Machine translation","","","","","","26","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Cross-Language Neural Dialog State Tracker for Large Ontologies Using Hierarchical Attention","Y. Jang; J. Ham; B. -J. Lee; K. -E. Kim","Computer Science Department, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Computer Science Department, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Computer Science Department, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Computer Science Department, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 Aug 2018","2018","26","11","2072","2082","Dialog state tracking, which refers to identifying the user intent from utterances, is one of the most important tasks in dialog management. In this paper, we present our dialog state tracker developed for the fifth dialog state tracking challenge, which focused on cross-language adaptation using a very scarce machine-translated training data when compared to the size of the ontology. Our dialog state tracker is based on the bi-directional long short-term memory network with a hierarchical attention mechanism in order to spot important words in user utterances. The user intent is predicted by finding the closest keyword in the ontology to the attention-weighted word vector. With the suggested methodology, our tracker can overcome various difficulties due to the scarce training data that existing machine learning-based trackers had, such as predicting user intents they have not seen before. We show that our tracker outperforms other trackers submitted to the challenge with respect to most of the performance measures.","2329-9304","","10.1109/TASLP.2018.2852492","Ministry of Trade, Industry and Energy (MOTIE, Korea); Industrial Technology Innovation Program (10063424, Development of distant speech recognition and multitask dialog processing technologies for in-door conversational robots); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8401898","Dialog state tracking;attention mechanism;hierarchical attention mechanism;long short term memory;cross language","Task analysis;Ontologies;Training data;Speech processing;Predictive models;Neural networks;Vocabulary","interactive systems;language translation;learning (artificial intelligence);natural language processing;neural nets;ontologies (artificial intelligence);vectors;word processing","cross-language neural dialog state tracker;ontology;dialog management;cross-language adaptation;hierarchical attention mechanism;user utterances;attention-weighted word vector;machine learning-based trackers;dialog state tracking challenge;machine-translated training data;user intent identification","","5","4","33","IEEE","2 Jul 2018","","","IEEE","IEEE Journals"
"Object Recognition and Auto-annotation In News Videos","Bastan; Duygulu","Bilgisayar M√ºhendisliƒüi B√∂l√ºm√º, Bilkent √úniversitesi, Ankara, Turkey; Bilgisayar M√ºhendisliƒüi B√∂l√ºm√º, Bilkent √úniversitesi, Ankara, Turkey","2006 IEEE 14th Signal Processing and Communications Applications","31 Jul 2006","2006","","","1","4","We propose a new approach to object recognition problem motivated by the availability of large annotated image and video collections. Similar to translation from one language to another, this approach considers the object recognition problem as the translation of visual elements to words. The visual elements represented in feature space are first categorized into a finite set of blobs. Then, the correspondences between the blobs and the words are learned using a method adapted from Statistical Machine Translation. Finally, the correspondences, in the form of a probability table, are used to predict words for particular image regions (region naming), for entire images (auto-annotation), or to associate the automatically generated speech transcript text with the correct video frames (video alignment). Experimental results are presented on TRECVID 2004 data set, which consists of about 150 hours of news videos associated with manual annotations and speech transcript text.","2165-0608","1-4244-0238-7","10.1109/SIU.2006.1659821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1659821","","Object recognition;Videos;Speech;Electrostatic precipitators;Probability;NIST","object recognition;video retrieval","object recognition;news videos;image auto-annotation;statistical machine translation;region naming;speech transcript text;video alignment;blobs","","","","7","IEEE","31 Jul 2006","","","IEEE","IEEE Conferences"
"Leveraging multiple languages to improve statistical MT word alignments","K. Filali; J. Bilmes","Departments of Computer Science & Engineering and Electrical Engineering, University of Washington, Seattle, WA, USA; Departments of Computer Science & Engineering and Electrical Engineering, University of Washington, Seattle, WA, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","92","97","We present a new multilingual statistical MT word alignment model based on a simple extension of the IBM and HMM models and a two-step alignment procedure. Preliminary results on a small hand-aligned subset of the Europarl corpus show a 7% relative improvement over a state of the art alignment model","","0-7803-9478-X","10.1109/ASRU.2005.1566493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566493","","Hidden Markov models;Information resources;Computer science;Robustness;Optimized production technology;Natural languages;Redundancy;Mathematics","hidden Markov models;language translation;natural languages","leveraging multiple languages;statistical MT word alignments;HMM models;hand-aligned subset;machine translation","","","3","13","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Designing Soft-Hardware Complex for Gesture Language Recognition using Neural Network Methods","M. D. Artemov; L. I. Voronova; A. G. Vovik","Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia","2020 International Conference on Engineering Management of Communication and Technology (EMCTECH)","30 Nov 2020","2020","","","1","6","In recent years, the problem of social adaptation of people with disabilities has been actively solved using developments including artificial intelligence methods. The article describes elaboration of a software and hardware complex (SHWC) for recognizing the sign language of disabled people with hearing and speech impairments. The analysis of analogue products for automatic sign language translation is carried out, technical and design requirements for the soft-hardware complex are formulated, the architecture of the SHWC, the functionality of the server and user application subsystems are described. The design and implementation of a subsystem for the collection and processing of photo and video materials with elements of sign language, including static and dynamic fingerprints, gestures, simple phrases, was carried out. The structure of the file data storage and metadata base has been developed. Scenarios and algorithms for sequencing and transforming video data are described. The sequence of data preprocessing when forming a training set using the augmentation method is described. A model for detecting hands in an image is described.","","978-1-6654-0448-8","10.1109/EMCTECH49634.2020.9261564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261564","image recognition methods;fingerprints;sign language;soft-hardware complex for automation of sign language translation","Assistive technology;Gesture recognition;Servers;Neural networks;Auditory system;Cameras;Cloud computing","artificial intelligence;gesture recognition;handicapped aids;man-machine systems;meta data;neural nets;video signal processing","augmentation method;video data transformation;metadata base;file data storage;dynamic fingerprints;static fingerprints;video materials;user application subsystems;technical design requirements;automatic sign language translation;analogue products;hearing speech impairments;disabled people;SHWC;artificial intelligence methods;neural network;gesture language recognition;soft-hardware complex design","","3","","26","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"Russian sign language machine interpreter system based on the analyses of syntax and semantic construction","M. G. Grif; J. S. Manueva","Novosibirsk State Technical University, Novosibirsk, Russia; Novosibirsk State Technical University, Novosibirsk, Russia","2016 13th International Scientific-Technical Conference on Actual Problems of Electronics Instrument Engineering (APEIE)","5 Jan 2017","2016","02","","498","501","In this paper the existing systems of translation into sign language are analyzed. There are ViSiCAST project, Zardoz system, ASL Workbench, TEAM system. Their strengths and weaknesses are identified. A new translation way based on a comparison of syntactic constructions is described. Sentences in sounding language often describe complex situations, including some simple situations (for example, a few actions). The unit, which is processed about, is a proposal for a full participle. As a result of comparison of syntactic constructions is obtained a set of simple sentences in which all participles replace verb. A corresponding library to determine the syntax construction is developed. A new method for constructing a semantic unit of the computer system of sign language is proposed. The result of semantic analysis system is a list of correspondence ‚Äúword-gesture‚Äù. Among the many alternatives of words based on semantic analysis algorithm, every word is assigned a unique lexical meaning. For simple sentences algorithms of semantic analysis are designed and implemented. The most priority directions of modification module semantic analysis may include the following: broadening the base of gesture, the implementation of parsing complex sentences, taking into account specifics of the Russian-language.","","978-1-5090-4069-8","10.1109/APEIE.2016.7806402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806402","Machine interpreter system;syntax construction;semantic analyses;sign language;Russian language","Semantics;Assistive technology;Gesture recognition;Syntactics;Proposals;Dictionaries;Speech recognition","gesture recognition;grammars;sign language recognition","Russian sign language machine interpreter system;semantic construction;ViSiCAST project;Zardoz system;ASL Workbench;TEAM system;syntactic constructions;sounding language;syntax construction;word-gesture;semantic analysis algorithm;modification module semantic analysis;complex sentence parsing","","1","","10","IEEE","5 Jan 2017","","","IEEE","IEEE Conferences"
"Efficient Korean text summarization based on key phrase extraction","W. Liu; L. Wang","Laboratory of Language Engineering and Computing, Guangdong University of Foreign Studies, Guangzhou, China; Center for Translation Studies, Guangdong University of Foreign Studies, Guangzhou, China","2017 International Conference on Machine Learning and Cybernetics (ICMLC)","16 Nov 2017","2017","1","","61","66","Language big data explosion causes a severe divergence of people's attention and a serious scarcity of people's time. This paper addresses the problem of Korean text summarization (KTS) and presents a flexible multi-plugin framework. Within the framework, we design a novel KTS algorithm based on key phrase extraction. Supported by the pluggable components of word stemming and part of speech tagging, the key-phrase-extraction-based KTS algorithm can complete text summarization efficiently. The experimental results show that our KTS algorithm with MMR plugin component can achieve the perfect performance in the Korean summarization task.","2160-1348","978-1-5386-0408-3","10.1109/ICMLC.2017.8107743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107743","Korean Text Summarization;Multi-plugin Framework;Key Phrase Extraction;Maximal Marginal Relevance;ROUGE","Algorithm design and analysis;Manuals;Speech;Tagging;Big Data;Explosions","","","","2","","14","IEEE","16 Nov 2017","","","IEEE","IEEE Conferences"
"Machine Learning based Custom Named Entity Recognition","A. V. S. Siva Rama Rao; A. P. Sreevatsav","Dept. of CSE, Sasi Institute of Technology & Engineering, Tadepalligudem, India; Department of Computing Security, Rochester Institute of Technology, Rochester, NY, USA","2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)","16 Jan 2023","2022","","","231","237","The significant increase of digital data in real world also results in various challenges in extracting the intended entities from the input text. Entities extraction process plays an important role in Natural Language Processing (NLP) applications such as named entity recognition, machine translation, text analytics, parts of speech tagging, sentiment analysis etc. In the real world, there are huge number of Named Entities, the recent named entity recognizer, SpaCy API version 3.0 can extract eighteen types of entities. In order to create and extract the user defined custom entities, this research work has proposed a Custom Named Entity Recognition (CNER) model. This model uses few textual training datasets by including user defined custom entities. By using CNER model, ten user intended custom named entities were trained and extracted from 27,439 English words present in eight different domains and Wikipedia articles and finally the state-of-the-art performance results are obtained.","","978-1-6654-8962-1","10.1109/ICAISS55157.2022.10010847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10010847","Named Entity Recognition;Custom Entity;Natural Language Pprocessing;Information Extraction;Machine Learning","Training;Sentiment analysis;Text recognition;Speech recognition;Machine learning;Tagging;Internet","application program interfaces;learning (artificial intelligence);natural language processing;text analysis","custom named entities;Custom Named Entity Recognition model;digital data;Entities extraction process;input text;intended entities;machine translation;Natural Language Processing applications;recent named entity recognizer;SpaCy API version;text analytics;user defined custom entities","","1","","24","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Noisy SMS text normalization model","G. Jose; N. S. Raj","Department of Computer Science SCMS School of Engineering and Technology Emakulam, Kerala; Department of Computer Science SCMS School of Engineering and Technology Emakulam, Kerala","International Conference for Convergence for Technology-2014","23 Apr 2015","2014","","","1","6","Today digital media such as social networks, chat rooms, and forums have gained much importance in human life for information sharing. Users will share their knowledge and emotions in their own languages. This will create a novel syntax to communicate their messages with as much as pithiness as possible. Noisy text is characterized by unusual forms such as abbreviations, phonetic translations, short forms etc. This led to the emergence of text normalization. Cleaning the noisy text has become an important factor for adequate development and deployment of NLP (Natural Language Processing) services such as text-to-speech and automatic translation. In this paper we introduce a channel based normalization model for cleaning the noisy texts. The normalization is based on the types of distortion such as grapheme distortion, abbreviation and phoneme distortion. The model will explore the type of distortion occurred in the noisy word and replace it by using the different channel list. Precursory evaluation shows that the channel model will normalize the noisy word to their standard peer with 96.43 % accuracy.","","978-1-4799-3759-2","10.1109/I2CT.2014.7092164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092164","Natural Language Processing;Machine Translation;Social Media;Text Normalization;Noisy words","Noise measurement;Hidden Markov models;Databases;Natural language processing;Standards;Computational modeling;Adaptation models","natural language processing;social networking (online);text analysis","noisy SMS text normalization model;digital media;social networks;chat rooms;forums;information sharing;noisy text;text normalization;natural language processing services;text-to-speech;automatic translation;grapheme distortion;phoneme distortion;noisy word;channel model","","1","","16","IEEE","23 Apr 2015","","","IEEE","IEEE Conferences"
"Light weight Real Time Indian Sign Language Symbol Recognition with Captioning and Speech Output","M. M. Varma; T. Kashinath; T. Jain; S. N. Pai","ICT dept., Manipal Institute of Technology Manipal Academy of Higher Education, Manipal, India; University of California Los Angeles; Columbia Engineering Fu Foundation School of Engineering and Applied Science, New York; ICT dept., Manipal Institute of Technology Manipal Academy of Higher Education, Manipal, India","2022 International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)","6 Apr 2023","2022","","","1","8","Sign language is the principal mode of communication for the vision and hearing impaired. It has allowed us to communicate with differently-abled individuals in a manner that is as expressive as a spoken language. Sign language has satisfied all the objectives of a traditional language system, like expressing emotions while also fulfilling a greater humanitarian role. Just like there are different dialects of languages spoken around the world, there also exist dialects of sign language that differ from country to country. Each of these dialects use a different symbol or method of representing the same lexicon. This work proposes a method of dynamic Indian sign language translation. that uses neural networks and image processing. The parameter utilized is minimum with good degree of predicted accuracy is the major contribution of this paper. The trained weights for the model are small enough to be efficiently ported over to a mobile phone or web browser and can be used without an active internet connection. The model is tested on the dataset with 99% accuracy.","","978-1-6654-5499-5","10.1109/SMARTGENCON56628.2022.10083871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083871","Deep Learning;Sign Language;Assistive Technology;Feature Extraction;Machine Learning","Image processing;Computational modeling;Neural networks;Symbols;Gesture recognition;Speech recognition;Assistive technologies","handicapped aids;Internet;language translation;natural language processing;sign language recognition","different dialects;different symbol;differently-abled individuals;dynamic Indian sign language translation;spoken language;time Indian sign language symbol recognition;traditional language system","","","","24","IEEE","6 Apr 2023","","","IEEE","IEEE Conferences"
"Subspace-based phonotactic language recognition using multivariate dynamic linear models","H. -S. Lee; Y. -C. Shih; H. -M. Wang; S. -K. Jeng","National Taiwan University, Taipei, TW; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","6870","6874","Phonotactics, dealing with permissible phone patterns and their frequencies of occurrence in a specific language, is acknowledged to be related to spoken language recognition (SLR). With the assistance of phone recognizers, each speech utterance can be decoded into an ordered sequence of phone vectors filled with likelihood scores contributed by all possible phone models. In this paper, we propose a novel approach to dig the concealed phonotactic structure out of the phone-likelihood vectors through a kind of multivariate time series analysis: dynamic linear models (DLM). In these models, treating the generation of phone patterns in each utterance as a dynamic system, the relationship between adjacent vectors is linearly and time-invariantly modeled, and unobserved states are introduced to capture a temporal coherence intrinsic in the system. Each utterance expressed by the DLM is further transformed into a fixed-dimensional linear subspace so that well-developed distance measures between two subspaces can be applied to linear discriminant analysis (LDA) in a dissimilarity-based fashion. The results of SLR experiments on the OGI-TS corpus demonstrate that the proposed framework outperforms the well-known vector space modeling (VSM)-based methods and achieves comparable performance to our previous subspace-based method.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6638993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638993","phonotactic language recognition","Vectors;Speech;Support vector machine classification;Decoding;Speech recognition;Acoustics;Computational modeling","language translation;speaker recognition;time series","subspace-based method;VSM-based methods;vector space modeling;dissimilarity-based fashion;linear discriminant analysis;fixed-dimensional linear subspace;phone patterns generation;multivariate time series analysis;dynamic linear models;phone-likelihood vectors;phonotactic structure;phone recognizers;spoken language recognition;phone patterns;multivariate dynamic linear models;subspace-based phonotactic language recognition","","2","","30","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Using Chinese part-of-speech patterns for sentiment phrase identification and opinion extraction in user generated reviews","T. -C. Peng; C. -C. Shih","Institute for Information Industry, Taipei, Taiwan; Institute for Information Industry, Taipei, Taiwan","2010 Fifth International Conference on Digital Information Management (ICDIM)","10 Dec 2010","2010","","","120","127","Accelerated growth of the Internet has enabled users worldwide to share their feelings and experiences. User-generated content (UGC) websites are the most abundant sources of user reviews. Accurately identifying sentiment phrases is essential to understand the expressed opinions in user reviews. To achieve this, part-of-speech (POS) patterns of phrases are useful. However, previous studies for Chinese opinion extraction only translate English POS patterns directly into Chinese for this task without considering the feasibility. Therefore, this work proposes a Chinese opinion extraction method that exploits the observed Sinica Treebank POS patterns for sentiment phrase identification. Sinica Treebank is a widely representative POS corpus for Chinese. The results of preliminary experiments indicate that the proposed method is highly effective in extracting opinions from Chinese UGC reviews.","","978-1-4244-7573-5","10.1109/ICDIM.2010.5664631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664631","","Accuracy;Tagging;Classification algorithms;Machine learning;Discussion forums;User-generated content;Speech","Internet;language translation;natural language processing;text analysis;Web sites","sentiment phrase identification;opinion extraction;chinese part-of-speech patterns;Internet;user-generated content Website;Sinica Treebank POS pattern","","13","","19","IEEE","10 Dec 2010","","","IEEE","IEEE Conferences"
"1ST IEEE International Workshop on Electronic Design Automation and Machine Learning (EDAML)","",,"2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","1 Aug 2022","2022","","","1180","1180","Summary form only given. Machine Learning (ML) has evolved substantially over the past decade and is now an integral component of many applications such as classification and object detection in images and video, speech recognition and language translation, data mining and pattern recognition, and cyber security. However, the design of server, edge, and embedded computing platforms to support these ML and other emerging applications remains a significant challenge. In particular, the design of ML-inspired electronic design automation (EDA) tools to efficiently prototype these platforms, and, conversely, the design of platforms to accelerate ML training and inference in EDA tools, represents an interdependent and important problem. This workshop aims to explore the intersection of ML and EDA and define a roadmap to realize the next generation of parallel and distributed computing systems. This year's program will feature a keynote talk by Prof. David Pan from The University of Texas at Austin and ten invited talks from experts in the field of EDA and ML. We thank the keynote and invited speakers for contributing to a high-quality technical program for this inaugural edition of the EDAML workshop. ","","978-1-6654-9747-3","10.1109/IPDPSW55747.2022.00192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835591","","Conferences;Distributed processing;Machine learning;Design automation;Training;Speech recognition;Servers","","","","","","","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"The DIBBS blackboard control architecture and its application to distributed natural language processing","J. R. R. Leavitt; E. Nyberg","Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA; Center for Machine Translation, Carnegie Mellon University, Pittsburgh, PA, USA","[1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence","6 Aug 2002","1990","","","202","208","The authors present DIBBS, a domain-independent blackboard system that has been used for applications in distributed natural language processing (NLP). It is shown how the explicit representation of control knowledge as units on the DIBBS control blackboard allows an application to implement dynamic control strategies and reason about its own problem-solving performance. The DIBBS truth maintenance dependency network allows robust treatment of control issues in conflict resolution. All of these properties makes DIBBS especially appropriate for applications in distributed NLP, such as the DIOGENES and DIANA systems.<>","","0-8186-2084-6","10.1109/TAI.1990.130335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=130335","","Natural languages;Control systems;Natural language processing;Problem-solving;Distributed control;Automatic generation control;Automatic control;Buildings;Speech;Computational modeling","distributed processing;knowledge representation;natural languages","DIBBS blackboard control architecture;distributed natural language processing;domain-independent blackboard system;representation of control knowledge;dynamic control strategies;problem-solving performance;truth maintenance dependency network;conflict resolution;DIOGENES;DIANA systems","","","4","20","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Reinforced Zero-Shot Cross-Lingual Neural Headline Generation","Ayana; Y. Chen; C. Yang; Z. Liu; M. Sun","Department of Computer Information Management, Inner Mongolia University of Finance and Economics, Hohhot, China; School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai, China; Department of Computer Science and Technology State Key Laboratory of Intelligent Technology and Systems Tsinghua, National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology State Key Laboratory of Intelligent Technology and Systems Tsinghua, National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology State Key Laboratory of Intelligent Technology and Systems Tsinghua, National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Sep 2020","2020","28","","2572","2584","Cross-lingual neural headline generation (CNHG), which aims at training a single, large neural network that directly generates a target language headline given a source language news document, has received considerable attention in recent years. Unlike conventional neural headline generation, CNHG faces the problem that there are no large-scale parallel corpora of source language articles and target language headlines. Consequently, CNHG is a zero-shot scenario. To solve this problem, we propose zero resource CNHG with reinforcement learning. We develop a reinforcement learning framework that is composed of two modules: a neural machine translation (NMT) module and a CNHG module. The translation module translates an input document into a source language document, and the headline generation module takes the previous output as input to generate a target language headline. Then, both modules receive a reward for joint training. The experimental results reveal that our method significantly outperforms baseline models.","2329-9304","","10.1109/TASLP.2020.3009487","National Key R&D Program of China(grant numbers:2019AAA0105200); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142327","Neural networks;cross-lingual headline generation (CNHG);reinforcement learning","Data models;Training data;Training;Learning (artificial intelligence);Speech processing;Neural networks;Task analysis","language translation;learning (artificial intelligence);natural language processing;neural nets;text analysis","NMT;neural machine translation module;reinforcement learning framework;zero resource CNHG;source language articles;source language news document;target language headline;large neural network;single network;reinforced zero-shot cross-lingual neural headline generation","","3","","53","IEEE","16 Jul 2020","","","IEEE","IEEE Journals"
"Detection and Correction of Real-Word Errors in Bangla Language","M. Mashod Rana; M. Tipu Sultan; M. F. Mridha; M. Eyaseen Arafat Khan; M. Masud Ahmed; M. Abdul Hamid","Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh; Department of Computer Science and Engineering, University of Asia Pacific, Dhaka, Bangladesh","2018 International Conference on Bangla Speech and Language Processing (ICBSLP)","2 Dec 2018","2018","","","1","4","Detection of spelling error is not so facile in Bangla. To check for real-world error in a sentence, it comes with more difficulties. In this paper, we focus on correcting homophone error in real-word error. We use N-gram Model which is used in many purposes like machine translation, speech recognition, to extract syntactic information etc. We have used a combination of Bi-gram and Tri-gram with candidate word which is going to be detected whether it is a real-word error or not. We have developed corpora which contain: (i) one of them is a collection of sets of homophone (confusing) word, (ii) another two are the collection of bigrams and trigrams using homophone word and (iii) other seven are the test sets. A candidate word extracts the set of homophone words from the corpus. In our proposed method, we create tri-gram and bigram using homophone word, then it checks the validity and takes the frequency of bi-gram or tri-gram, and finally calculates the probability for making the final decision about the candidate word. We have used around a million words to inspect our system. Our proposed method achieves more than 96% accuracy in detecting and correcting real-word errors of Bangla Text.","","978-1-5386-8207-4","10.1109/ICBSLP.2018.8554502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554502","Bangla homophones;NLP;Real-word error;N-gram;Markov model","Feature extraction;Computer science;Asia;Probability;Hidden Markov models;Mathematical model;Tagging","error correction;natural language processing;probability;spelling aids;text analysis","candidate word;million words;homophone error;homophone word;bigram;Bangla language;n-gram model;real-world error detection;real-word errors correction;spelling error;machine translation;speech recognition;syntactic information;trigram;probability;Bangla text","","4","","9","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Deep Learning-based Job Placement in Distributed Machine Learning Clusters","Y. Bao; Y. Peng; C. Wu","Department of Computer Science, The University of Hong Kong; Department of Computer Science, The University of Hong Kong; Department of Computer Science, The University of Hong Kong","IEEE INFOCOM 2019 - IEEE Conference on Computer Communications","17 Jun 2019","2019","","","505","513","Production machine learning (ML) clusters commonly host a variety of distributed ML workloads, e.g., speech recognition, machine translation. While server sharing among jobs improves resource utilization, interference among co-located ML jobs can lead to significant performance downgrade. Existing cluster schedulers (e.g., Mesos) are interference-oblivious in their job placement, causing suboptimal resource efficiency. Interference-aware job placement has been studied in the literature, but was treated using detailed workload profiling and interference modeling, which is not a general solution. This paper presents Harmony, a deep learning-driven ML cluster scheduler that places training jobs in a manner that minimizes interference and maximizes performance (i.e., training completion time). Harmony is based on a carefully designed deep reinforcement learning (DRL) framework augmented with reward modeling. The DRL employs state-of-the-art techniques to stabilize training and improve convergence, including actor-critic algorithm, job-aware action space exploration and experience replay. In view of a common lack of reward samples corresponding to different placement decisions, we build an auxiliary reward prediction model, which is trained using historical samples and used for producing reward for unseen placement. Experiments using real ML workloads in a Kubernetes cluster of 6 GPU servers show that Harmony outperforms representative schedulers by 25% in terms of average job completion time.","2641-9874","978-1-7281-0515-4","10.1109/INFOCOM.2019.8737460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737460","","Training;Servers;Interference;Graphics processing units;Computational modeling;Machine learning;Resource management","cluster computing;interference suppression;learning (artificial intelligence);processor scheduling","distributed machine learning clusters;production machine learning clusters;speech recognition;machine translation;resource utilization;co-located ML jobs;cluster schedulers;interference-oblivious;suboptimal resource efficiency;interference-aware job placement;Harmony;deep learning-driven ML cluster scheduler;training completion time;DRL;reward modeling;job-aware action space exploration;experience replay;auxiliary reward prediction model;unseen placement;ML workloads;Kubernetes cluster;average job completion time;performance downgrade;deep reinforcement learning framework;placement decisions;actor-critic algorithm","","67","","40","IEEE","17 Jun 2019","","","IEEE","IEEE Conferences"
"Comparing deep learning performance on BigData by using CPUs and GPUs","S. I. Baykal; D. Bulut; O. K. Sahingoz","Computer Engineering Department, Istanbul Kultur University, Istanbul, Turkey; Computer Engineering Department, Istanbul Kultur University, Istanbul, Turkey; Computer Engineering Department, Istanbul Kultur University, Istanbul, Turkey","2018 Electric Electronics, Computer Science, Biomedical Engineerings' Meeting (EBBT)","21 Jun 2018","2018","","","1","6","During the last few years, we have witnessed a fast growth of the cyber world in which great attention is focused on the use of Big Data that cannot be managed or analyzed with the use of traditional tools. In the past, to convert raw data into valuable information, machine learning techniques were extensively preferred. However, conventional techniques such as Naive Bayes and support vector machines are not efficient for these huge data. Therefore, deep learning methods which are especially preferred in image/speech recognition, information retrieval, language translation, etc., arise as sophisticated and acceptable choices to process data with high accuracy in a hierarchical representation model. Depending on the size of data, even deep learning techniques can be inadequate. To increase the performance, some parallel processing techniques are needed which can be executed on a multi-core structure of the processor. With the technological improvements, the Graphical Processing Units(GPUs), which can contain a few thousands of cores, can also be used in this parallel execution platform. In this paper, we aimed to use a deep learning approach for processing big data to solve a specific problem in a multi-core platform. The experimental results are compared with a CPU execution, and it is depicted that use of GPU Technologies increases the performance of system up to 10 times depending on the type of the GPUs.","","978-1-5386-5135-3","10.1109/EBBT.2018.8391429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8391429","deep learning;big data;parallel programming;machine learning","Machine learning;Neurons;Graphics processing units;Big Data;Machine learning algorithms;Biological neural networks;Computer architecture","Big Data;graphics processing units;learning (artificial intelligence);multiprocessing systems;parallel processing","deep learning methods;image/speech recognition;information retrieval;language translation;hierarchical representation model;parallel processing techniques;multicore structure;parallel execution platform;Graphical Processing Units;support vector machines;machine learning techniques;big data","","13","","14","IEEE","21 Jun 2018","","","IEEE","IEEE Conferences"
"Indonesian natural voice command for robotic applications","K. T. Putra; D. Purwanto; R. Mardiyanto","Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember Surabaya, Indonesia; Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember Surabaya, Indonesia; Department of Electrical Engineering, Institut Teknologi Sepuluh Nopember Surabaya, Indonesia","2015 International Conference on Electrical Engineering and Informatics (ICEEI)","17 Dec 2015","2015","","","638","643","Human-machine interaction has been growing with the discovery of artificial intelligence technology. The development of human-machine interaction leads to a more natural interaction. In daily interactions, human uses speech, more dominant than the other way such as gestures and eye contact. Speech is the vocalized form of human communication which is closely related to language system. The problem is meaning, ambiguity, and the language that is not according to the rules of syntax, causing the command translation become more complex. To understand the meaning of the voice command, it is necessary to know the semantic and syntactic structure of sentences. An artificial intelligence technology that can understand Indonesian voice commands for robotic applications will be developed in this research. The purpose of this research is to translate voice command into the robots action, to generate human-machine interaction more natural. The voice command will be extracted using bark-frequency cepstral coefficients. Cepstral identified into words using neural networks. Words in a complete sentences will be processed using natural language processing so that, the meaning and appropriate action from the given command can be executed. Speech recognition experiments with 28 sets of speech signal obtain 82 % accuracy, while natural language processing experiments obtain 93 % accuracy with 50 sets of learning data.","2155-6830","978-1-4673-7319-7","10.1109/ICEEI.2015.7352577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352577","bark-frequency cepstral coefficients;neural network;natural language processing","Natural language processing;Speech;Cepstral analysis;Speech recognition;Robots;Artificial neural networks;Vocabulary","human-robot interaction;learning (artificial intelligence);natural language interfaces;natural language processing;speech recognition","Indonesian natural voice command;robotic applications;artificial intelligence;human-machine interaction;human communication;language system;semantic structure;syntactic structure;bark-frequency cepstral coefficients;neural networks;natural language processing;speech recognition;learning data sets","","3","","12","IEEE","17 Dec 2015","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Pruning: A Survey","S. Xu; A. Huang; L. Chen; B. Zhang","School of Automatic Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automatic Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automatic Science and Eletrical Engineering, Beihang University, Beijing, China; School of Automatic Science and Eletrical Engineering, Beihang University, Beijing, China","2020 39th Chinese Control Conference (CCC)","9 Sep 2020","2020","","","7458","7463","Deep convolutional neural networks have enabled remarkable progress over the last years on a variety of visual tasks, such as image recognition, speech recognition, and machine translation. These tasks contribute many to machine intelligence. However, developments of deep convolutional neural networks to a machine terminal remains challenging due to massive number of parameters and float operations that a typical model contains. Therefore, there is growing interest in convolutional neural network pruning. Existing work in this field of research can be categorized according to three dimensions: pruning method, training strategy, estimation criterion.","1934-1768","978-9-8815-6390-3","10.23919/CCC50068.2020.9189610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189610","convolutional neural networks;machine intelligence;pruning method;training strategy;estimation criterion","Training;Kernel;Estimation;Convolutional neural networks;Task analysis;Computational modeling;Computer architecture","convolutional neural nets;learning (artificial intelligence)","convolutional neural network pruning;deep convolutional neural networks;visual tasks;image recognition;speech recognition;machine translation;machine intelligence;machine terminal","","15","","45","","9 Sep 2020","","","IEEE","IEEE Conferences"
"Statistical natural language understanding using hidden clumpings","M. Epstein; K. Papineni; S. Roukos; T. Ward; S. Della Pietra","IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Renaissance Technologies, Inc., USA","1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings","6 Aug 2002","1996","1","","176","179 vol. 1","We present a new approach to natural language understanding (NLU) based on the source-channel paradigm, and apply it to ARPA's Air Travel Information Service (ATIS) domain. The model uses techniques similar to those used by IBM in statistical machine translation. The parameters are trained using the exact match algorithm; a hierarchy of models is used to facilitate the bootstrapping of more complex models from simpler models.","1520-6149","0-7803-3192-3","10.1109/ICASSP.1996.540319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=540319","","Natural languages;Formal languages;Speech recognition;Training data;Knowledge based systems;Labeling;Decision trees;Neural networks;Probability","statistical analysis;natural languages;speech recognition","statistical natural language understanding;hidden clumpings;source-channel paradigm;ARPA's Air Travel Information Service;ATIS;exact match algorithm;bootstrapping","","10","2","8","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Overcoming the Language Barrier (Invited Paper)","M. Nagao","National Diet Library, Japan","2008 Second International Symposium on Universal Communication","22 Dec 2008","2008","","","3","4","We can observe a clear distinction in the approaches to natural language processing before and after the year 2000. Before the twenty first century grammar rules and word dictionaries for computer use were written by the instinct of linguists, while after the turn of the century these have been basically obtained, or havebeen recognized to be obtainable, from linguistic databy proper automatic analyses and processings.","","978-0-7695-3433-6","10.1109/ISUC.2008.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724431","","Dictionaries;Natural languages;Speech;Natural language processing;Supercomputers;Data mining;Thesauri;System testing;Libraries;Internet","language translation;natural language processing;text analysis","language barrier;natural language processing;text data;machine translation","","","","","IEEE","22 Dec 2008","","","IEEE","IEEE Conferences"
"Rule based anaphora resolution in Hindi","D. Singla; P. Kumar","Computer Science and Engineering, Thapar University, Patiala, India; Computer Science and Engineering, Thapar University, Patiala, India","2017 International Conference on Computational Intelligence in Data Science(ICCIDS)","1 Feb 2018","2017","","","1","5","Handling of human languages, especially written texts, requires analysis and implementation at different linguistic levels such as morphological analysis, Parts-of-Speech(POS) labeling at word level; chunking and clause identification at word group level; syntactic parsing at the sentence or structural level; semantic analysis at the level of meaning and finally discourse analysis at the discourse or text level. Discourse level analysis involves various sub-problems that deal with relationships between sentences and larger linguistic units. One such phenomenon is Anaphora Resolution (AR). Anaphora occurs frequently in written texts and spoken languages. Anaphora resolution is required in almost every application of NLP as information extraction (IE), summarization and Machine translation. The main focus of this paper is Entity Resolution (ER) and pronominal forms. Various approaches have been discussed to resolve Entity-pronoun references in Hindi. This paper will discuss the Rule Based approach among all the approaches, used to identify the anaphors and their antecedents. Rules have been framed for five types of pronouns that have been discussed in this paper.","","978-1-5090-5595-1","10.1109/ICCIDS.2017.8272666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8272666","POS(Parts-of-Speech);AR(Anaphora Resolution);NLP(Natural Language Processing);IE(Information Extraction;Entity Resolution)","Computational intelligence;Pragmatics;Semantics;Supervised learning;Syntactics;Concrete;Computer science","computational linguistics;grammars;language translation;linguistics;natural language processing;natural languages;text analysis","linguistic levels;rule based approach;Parts-of-Speech;POS;human languages;Hindi;pronominal forms;summarization;written texts;anaphora resolution;larger linguistic units;discourse level analysis;semantic analysis;sentence;syntactic parsing;word group level;clause identification;chunking;word level;morphological analysis","","4","","5","IEEE","1 Feb 2018","","","IEEE","IEEE Conferences"
"An automatic non-native speaker recognition system","B. Tan; Q. Li; R. Foresta","Li Creative Technologies, Inc., Florham Park, NJ, USA; Li Creative Technologies, Inc., Florham Park, NJ, USA; RDECOM CERDEC, APG, US Army, MD, USA","2010 IEEE International Conference on Technologies for Homeland Security (HST)","3 Dec 2010","2010","","","77","83","Identification of non-native personnel is a critical piece of information for making crucial on-the-spot decisions for security purposes. Identification of a non-native speaker is often readily apparent in normal conversation with a native speaker through speech content and accent. Such identification which requires familiarity with language nuances may not be possible for a non-native interrogator or intelligence analyst or when conversing or listening through a machine language translator. Developing an automatic system to identify speakers as native or non-native, as well as their native language, including dialect, within input audio streams, is the major goal of this project. Such a system may be used alone or with other downstream applications such as machine language translation systems. In this paper we present four approaches to identify native and non-native speakers as a binary recognition problem. The approaches can be further categorized into phonetic-based approaches and non-phonetic-based approaches. These approaches were tested on two separate databases, including text-dependent read speech and text-independent spontaneous speech. The results show that our system is competitive in comparison with other published, state-of-the-art non-native speaker recognition systems. Key metrics for automated non-native recognition systems include: 1) positive identification rates, 2) false alarm/identification rates, and 3) length of captured speech sample required to reach a decision.","","978-1-4244-6048-9","10.1109/THS.2010.5655088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655088","non-native;accent recognition;speaker recognition;Hidden Markov Model;Gaussian Mixture Model;neural network;discriminative training;fusion","Hidden Markov models;Databases;Training;Speaker recognition;Speech;Artificial neural networks;Classification algorithms","language translation;natural language processing;speaker recognition","nonnative speaker recognition system;nonnative personnel identification;machine language translator;binary recognition problem;phonetic-based approach;text-dependent read speech;text-independent spontaneous speech","","3","","18","IEEE","3 Dec 2010","","","IEEE","IEEE Conferences"
"Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads","Y. Bao; Y. Peng; C. Wu","Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong; Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong; Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong","IEEE/ACM Transactions on Networking","17 Apr 2023","2023","31","2","634","647","Nowadays, most leading IT companies host a variety of distributed machine learning (ML) workloads in ML clusters to support AI-driven services, such as speech recognition, machine translation, and image processing. While multiple jobs are executed concurrently in a shared cluster to improve resource utilization, interference among co-located ML jobs can lead to significant performance downgrade. Existing cluster schedulers, such as YARN and Mesos, are interference-agnostic in their job placement, leading to suboptimal resource efficiency and usage. Some literature has studied interference-aware job placement policy, but relies on detailed workload profiling and interference modeling, which is not a general solution. In this work, we present Harmony, a deep learning-driven ML cluster scheduler that places heterogeneous training jobs (either with parameter server architecture or all-reduce architecture) in a manner that minimizes interference and maximizes performance (i.e., training completion time minimization). The design of Harmony is based on a carefully designed deep reinforcement learning (DRL) framework enhanced with reward modeling. The DRL integrates a dynamic sequence-to-sequence model with the state-of-the-art techniques to stabilize training and improve convergence, including actor-critic algorithm, job-aware action space exploration, multi-head attention, and experience replay. In view of a common lack of reward samples corresponding to different placement decisions, we build an auxiliary sequence-to-sequence reward prediction model, which is trained with historical samples and used for producing reward for unseen placement. Experiments using real ML workloads in a Kubernetes cluster of 6 GPU servers show that Harmony outperforms representative schedulers by 16%‚Äì42% in terms of average job completion time.","1558-2566","","10.1109/TNET.2022.3202529","Hong Kong Research Grants Council (RGC)(grant numbers:HKU 17204619,17208920,17207621); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882005","Distributed machine learning systems;job scheduling;deep reinforcement learning","Servers;Training;Interference;Graphics processing units;Data models;Computational modeling;Deep learning","deep learning (artificial intelligence);pattern clustering;reinforcement learning;resource allocation;scheduling","actor-critic algorithm;AI-driven services;auxiliary sequence-to-sequence reward prediction model;average job completion time;carefully designed deep reinforcement learning framework;co-located ML jobs;deep learning-driven ML cluster scheduler;distributed machine learning clusters;distributed machine learning workloads;dynamic sequence-to-sequence model;experience replay;GPU servers;Harmony;heterogeneous workloads;image processing;interference-agnostic;interference-aware job placement policy;job-aware action space exploration;Kubernetes cluster;machine translation;ML clusters;ML workloads;multihead attention;multiple jobs;parameter server architecture;placement decisions;places heterogeneous training jobs;resource utilization;reward modeling;shared cluster;speech recognition;suboptimal resource efficiency;training completion time minimization;unseen placement;workload profiling","","1","","64","IEEE","8 Sep 2022","","","IEEE","IEEE Journals"
"Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things","J. Zhang; D. Tao","School of Computer Science, Faculty of Engineering, University of Sydney, Darlington, NSW, Australia; School of Computer Science, Faculty of Engineering, University of Sydney, Darlington, NSW, Australia","IEEE Internet of Things Journal","7 May 2021","2021","8","10","7789","7817","In the Internet-of-Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the Internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas, including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of AI of things (AIoT). This article presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities.","2327-4662","","10.1109/JIOT.2020.3039359","Australian Research Council Projects(grant numbers:FL-170100117,DP-180103424,IH-180100002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264235","3-D;aged care;artificial intelligence (AI);biometric recognition;causal reasoning;cloud/fog/edge computing;deep learning;human‚Äìmachine interaction;Internet of Things (IoT);machine translation (MT);privacy;security;sensors;smart agriculture;smart city;smart grids;speech recognition","Internet of Things;Sensors;Edge computing;Deep learning;Computer architecture;Cloud computing;Artificial intelligence","artificial intelligence;cloud computing;Internet;Internet of Things;learning (artificial intelligence);natural language processing;speech recognition","artificial intelligence;Internet-of-Things era;IoT;sensors;cloud centers;heterogeneous data;complex environments;smart decisions;deep learning;proven success;computer vision;speech recognition;natural language processing;AIoT architecture;cloud computing;fog computing;edge computing;AI research;potential research opportunities","","223","","269","IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"Plenary talk II: Recent developments in sign language recognition systems","M. F. Tolba","Ain Shams University, Cairo, EG","2013 8th International Conference on Computer Engineering & Systems (ICCES)","9 Jan 2014","2013","","","xxxiii","xxxv","Automated translation systems for sign languages are important in a world that is showing a continuously increasing interest in removing barriers faced by physically challenged individuals in communicating and contributing to the society and the workforce. These systems can greatly facilitate the communication between the vocal and the non-vocal communities. For the hearing-impaired, such systems can serve as the equivalent of speech-recognition systems used by speaking people to interact with machines in a more natural way. Few research projects tried to develop a translation system from and to visual sign language. Very few of these attempts were on Arabic sign language. None of them succeeded to develop a reliable industrial product. This review explores these trials and proposes clear recommendations to convert the lab research to a real product useful to the community.","","978-1-4799-0080-0","10.1109/ICCES.2013.6707156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707156","","","handicapped aids;language translation;sign language recognition","sign language recognition systems;automated sign language translation systems;physically challenged individuals;nonvocal communities;hearing-impaired individuals;speech-recognition systems;visual sign language;Arabic sign language;industrial product","","1","","","IEEE","9 Jan 2014","","","IEEE","IEEE Conferences"
"Adam Induces Implicit Weight Sparsity in Rectifier Neural Networks","A. Yaguchi; T. Suzuki; W. Asano; S. Nitta; Y. Sakata; A. Tanizawa","Toshiba Corporation, Corporate Research and Development Center, Kawasaki, Japan; Graduate School of Information Science and Technology, The University of Tokyo, Japan; Toshiba Corporation, Corporate Research and Development Center, Kawasaki, Japan; Toshiba Corporation, Corporate Research and Development Center, Kawasaki, Japan; Toshiba Corporation, Corporate Research and Development Center, Kawasaki, Japan; Toshiba Corporation, Corporate Research and Development Center, Kawasaki, Japan","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","318","325","In recent years, deep neural networks (DNNs) have been applied to various machine leaning tasks, including image recognition, speech recognition, and machine translation. However, large DNN models are needed to achieve state-of-the-art performance, exceeding the capabilities of edge devices. Model reduction is thus needed for practical use. In this paper, we point out that deep learning automatically induces group sparsity of weights, in which all weights connected to an output channel (node) are zero, when training DNNs under the following three conditions: (1) rectified-linear-unit (ReLU) activations, (2) an L2-regularized objective function, and (3) the Adam optimizer. Next, we analyze this behavior both theoretically and experimentally, and propose a simple model reduction method: eliminate the zero weights after training the DNN. In experiments on MNIST and CIFAR-10 datasets, we demonstrate the sparsity with various training setups. Finally, we show that our method can efficiently reduce the model size and performs well relative to methods that use a sparsity-inducing regularizer.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614079","deep neural networks;model reduction;group sparse;Adam","Training;Reduced order systems;Convergence;Convolution;Neural networks;Linear programming;Computational modeling","image recognition;learning (artificial intelligence);neural nets;optimisation;reduced order systems;speech recognition","zero weights;sparsity-inducing regularizer;rectifier neural networks;deep neural networks;machine leaning tasks;image recognition;speech recognition;machine translation;DNN models;edge devices;deep learning;rectified-linear-unit activations;Adam optimizer;model reduction method;DNN training;Adam induced implicit weight sparsity;L2-regularized objective function;ReLU activations","","8","","31","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Automatic Speech Recognition for Mixed Dialect Utterances by Mixing Dialect Language Models","N. Hirayama; K. Yoshino; K. Itoyama; S. Mori; H. G. Okuno","Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Kyoto, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 May 2017","2015","23","2","373","382","This paper presents an automatic speech recognition (ASR) system that accepts a mixture of various kinds of dialects. The system recognizes dialect utterances on the basis of the statistical simulation of vocabulary transformation and combinations of several dialect models. Previous dialect ASR systems were based on handcrafted dictionaries for several dialects, which involved costly processes. The proposed system statistically trains transformation rules between a common language and dialects, and simulates a dialect corpus for ASR on the basis of a machine translation technique. The rules are trained with small sets of parallel corpora to make up for the lack of linguistic resources on dialects. The proposed system also accepts mixed dialect utterances that contain a variety of vocabularies. In fact, spoken language is not a single dialect but a mixed dialect that is affected by the circumstances of speakers‚Äô backgrounds (e.g., native dialects of their parents or where they live). We addressed two methods to combine several dialects appropriately for each speaker. The first was recognition with language models of mixed dialects with automatically estimated weights that maximized the recognition likelihood. This method performed the best, but calculation was very expensive because it conducted grid searches of combinations of dialect mixing proportions that maximized the recognition likelihood. The second was integration of results of recognition from each single dialect language model. The improvements with this model were slightly smaller than those with the first method. Its calculation cost was, however, inexpensive and it worked in real-time on general workstations. Both methods achieved higher recognition accuracies for all speakers than those with the single dialect models and the common language model, and we could choose a suitable model for use in ASR that took into consideration the computational costs and recognition accuracies.","2329-9304","","10.1109/TASLP.2014.2387414","Japan Society for the Promotion of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001195","Corpus simulation;mixture of dialects;speech recognition","Speech;Hidden Markov models;Accuracy;Acoustics;Vocabulary;Pragmatics;Computational modeling","","","","6","","27","IEEE","5 Jan 2015","","","IEEE","IEEE Journals"
"Memory Network for Linguistic Structure Parsing","Z. Li; C. Guan; H. Zhao; R. Wang; K. Parnow; Z. Zhang","Department of Computer Science, and Engineering, 800 Dongchuan Road, Minhang District, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, and Engineering, 800 Dongchuan Road, Minhang District, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, and Engineering, 800 Dongchuan Road, Minhang District, Shanghai Jiao Tong University, Shanghai, China; Advanced Speech Translation Research, and Development Promotion Center, National Institute of Information, and Communications Technology (NICT), Kyoto, Japan; Department of Computer Science, and Engineering, 800 Dongchuan Road, Minhang District, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, and Engineering, 800 Dongchuan Road, Minhang District, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","26 Oct 2020","2020","28","","2743","2755","Memory-based learning can be characterized as a lazy learning method in machine learning terminology because it delays the processing of input by storing the input until needed. Linguistic structure parsing, which has been in a performance improvement bottleneck since the latest series of works was presented, determines the syntactic or semantic structure of a sentence. In this article, we construct a memory component and use it to augment a linguistic structure parser which allows the parser to directly extract patterns from the known training treebank to form memory. The experimental results show that existing state-of-the-art parsers reach new heights of performance on the main benchmarks for dependency parsing and semantic role labeling with this memory network.","2329-9304","","10.1109/TASLP.2020.3030500","National Key Research, and Development Program of China(grant numbers:2017YFB0304100); National Natural Science Foundation of China(grant numbers:U1836222,61733011); JSPS Grant-in-Aid(grant numbers:19K20354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222278","Memory network;semantic role labeling;syntactic dependency parsing","Semantics;Syntactics;Linguistics;Labeling;Random access memory;Task analysis;Speech processing","computational linguistics;grammars;learning (artificial intelligence)","state-of-the-art parsers;linguistic structure parser;memory component;performance improvement bottleneck;machine learning terminology;lazy learning method;memory-based learning;linguistic structure parsing;memory network;semantic role labeling;dependency parsing","","5","","74","IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"Human vs machine translation of foreign languages","J. M. Lufkin","Honeywell Inc., Minneapolis, Minn.","IEEE Transactions on Engineering Writing and Speech","13 Sep 2013","1965","8","1","8","14","The translation of technical subject matter demands an extensive knowledge of both languages and a considerable acquaintance with the subject. A computer system to give high-quality translations of serious subjects, spoken or written, would be most useful, but neither the cerebral logic used in translation nor the semantic structure of language itself is yet understood. The memory storage required for machine translation may not be a serious obstacle, but the linguistic task of quantifying the effects of syntax and context upon meaning may prove to be insurmountable.","2331-3706","","10.1109/TEWS.1965.6594556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6594556","","Computers;Pragmatics;Educational institutions;Syntactics;Context;Standards;Government","","","","2","","","IEEE","13 Sep 2013","","","IEEE","IEEE Journals"
"What Everybody Should Know about Translation","J. M. Lufkin","Honeywell, Inc., Minneapolis, MN, USA","IEEE Transactions on Engineering Writing and Speech","12 Nov 2007","1969","12","1","3","9","Translation, whether literary or technical, is a creative art, not a mechanical process, and machines have failed to give useful translations of serious texts for this reason. The conventional distinctions between ""literary"" and ""technical"" translation are misleading and perhaps false. Literal or word-for-word translations are not translations at all and their use should be discouraged. Translation requires a full command of both languages and some understanding of the subject. Quality is difficult for a layman to judge, but idiomatic English (or whatever the target language is) is one good sign, and there are others.","2331-3706","","10.1109/TEWS.1969.4322369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4322369","","Educational institutions;Natural languages;Libraries;Testing;Writing;Speech;Art;Handicapped aids;Permission;Inspection","","","","","","8","IEEE","12 Nov 2007","","","IEEE","IEEE Journals"
"The manuscript evaluation through Artificial Intelligence using Natural Language Processing and Machine Learning","T. ul Haq; D. Anand; N. Kapoor","University Institute of Engineering, Chandigarh University, Chandigarh, India; Chandigarh University, Chandigarh, India; Chandigarh University, Chandigarh, India","2022 IEEE 3rd Global Conference for Advancement in Technology (GCAT)","12 Dec 2022","2022","","","1","6","Peer reviewing plays an important role if a re-searcher wants to publish his/her manuscript. Peer review is the impartial evaluation of your research article by subject matter experts in your area. Its objective is to assess the manuscript's quality and publishing readiness. This paper will be considering these peer reviews and use computational linguistics to relate them with acceptance of the manuscript. This paper will focus on text conversions in depth and discuss the flow of natural language processing. Natural language processing is a field of computer science and artificial intelligence that studies how computers interact with human languages, In specific it is an automated process of analyzing human language in order to gain information from it. Automation is the ultimate goal for efficiency driven organisations and Natural language processing is playing an important role in it. A machine can fully understand humans only when it communicates it with human language, its like communicating to someone who speaks your native language versus someone who speaks language you hardly can speak or understand. Natural language processing removes this gap. Text processing and text summarization, Automatic translation of languages, user interfaces, speech recognition and expert systems are examples of natural language processing applications. Chal-lenges for processing the complex language system of humans are far too many as human emotions and expressions through voice and body language are not fully decode in data. In near future everyone will have to communicate with a machine for household work as well as space work, so it becomes important to understand the working of Artificial Intelligence and Natural language processing is an important part of it.","","978-1-6654-6855-8","10.1109/GCAT55367.2022.9971874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9971874","Natural language Processing (NLP);Machine learning;Stemming;Lemmetization;TF-IDF;Artificial Intelligence(AI);Bag of words;Vectorizer","Automation;Text analysis;Publishing;Machine learning;User interfaces;Natural language processing;Task analysis","computational linguistics;expert systems;information retrieval;learning (artificial intelligence);natural language processing;speech recognition;text analysis;user interfaces","artificial intelligence;computational linguistics;human language;natural language processing applications;peer review;speech recognition;text conversions;text processing;text summarization;user interfaces","","","","22","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"Hidden-articulator Markov models for pronunciation evaluation","J. Tepperman; S. Narayanan","Signal Analysis and Interpretation Laboratory Viterbi School of Engineering, University of Southern California, USA; Signal Analysis and Interpretation Laboratory Viterbi School of Engineering, University of Southern California, USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","174","179","The design of a robust language-learning system, intended to help students practice a foreign language along with a machine tutor, must provide for localization of common pronunciation errors. This paper presents a new technique for unsupervised detection of phone-level mispronunciations, created with language-learning applications in mind. Our method uses multiple hidden-articulator Markov models to asynchronously classify acoustic events in various articulatory domains. It requires no human input besides a pronunciation dictionary for all words in the end system's vocabulary, and has been shown to perform as well as a human tutor would, given the same task. For the majority of systematic mispronunciations investigated in this study, precision in detecting the presence of an error exceeded the 70% inter-annotator agreement reported by our test corpus","","0-7803-9478-X","10.1109/ASRU.2005.1566471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566471","","Humans;Natural languages;Acoustic signal detection;Dictionaries;Tongue;Speech analysis;Loudspeakers;Signal analysis;Laboratories;Viterbi algorithm","dictionaries;hidden Markov models;language translation;natural languages;speech recognition","hidden-articulator Markov models;pronunciation evaluation;robust language-learning system;foreign language;machine tutor;unsupervised detection;pronunciation dictionary","","3","","15","IEEE","3 Jan 2006","","","IEEE","IEEE Conferences"
"Organizational intelligence: theory of collectively intelligent behaviors and engineering of effective information systems in the complex organizations","T. Matsuda","Sanno College, Isehara, Japan","Proceedings of IEEE Systems Man and Cybernetics Conference - SMC","6 Aug 2002","1993","1","","664","669 vol.1","The concept of organizational intelligence originates from a naive analogy with the human individual intelligence: namely, as each human individual is supposed to possess a unique intelligence (intellectual faculty), so may each organization as a whole be assumed to have a characteristic collectivity of its members' intelligence. Now that each organization as well as its environment is highly information-sophisticated, the role of machine or artifact intelligence collectively employed in each organization is quite remarkable. Therefore, one may provisionally define organizational intelligence as: an interactive aggregative-coordinative complex of human intelligence and artifact intelligence in the organization.<>","","0-7803-0911-1","10.1109/ICSMC.1993.384820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=384820","","Machine intelligence;Humans;Speech;Network address translation;Intelligent systems;Electrons;Physics;Terminology;Natural languages","management science;management","organizational intelligence;collectively intelligent behaviors;information systems;complex organizations;human individual intelligence;intellectual faculty;interactive aggregative-coordinative complex","","2","","7","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"The MSIIP system for dialog state tracking challenge 5","Y. Su; M. Li; J. Wu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2016 IEEE Spoken Language Technology Workshop (SLT)","9 Feb 2017","2016","","","525","530","We present our work in Dialog State Tracking Challenge 5, the main task of which is to track dialog state on human-human conversations cross language. Firstly a probabilistic enhanced framework is used to represent sub-dialog, which consists of three parts, the input model for extracting features, the enhanced model for updating dialog state and the output model to give the tracking frame. Meanwhile, parallel language systems are proposed to overcome inaccuracy caused by machine translation for cross language testing. We also introduce a new iterative alignment method extended from our work in DSTC4. Furthermore, a slot-based score averaging method is introduced to build an ensemble by combining different trackers. Results of our DSTC5 system show that our method significantly improves tracking performance compared with baseline method.","","978-1-5090-4903-5","10.1109/SLT.2016.7846313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846313","dialog state tracking;probabilistic enhanced frame strcture;parallel language system","Static VAr compensators;Feature extraction;Iterative methods;Training;Support vector machines;Target tracking","natural language processing;speech processing","MSIIP SYSTEM;dialog state tracking challenge 5;human-human conversations;feature extraction;parallel language systems;machine translation;cross language testing;DSTC4;slot-based score averaging method;DSTC5","","1","","9","IEEE","9 Feb 2017","","","IEEE","IEEE Conferences"
"Use of a novel hash-table for speeding-up suggestions for misspelt Tamil words","R. Sakuntharaj; S. Mahesan","Centre for Information & Communication Technology, Eastern University, Sri Lanka; Department of Computer Science Faculty of Science, University of Jaffna, Sri Lanka","2017 IEEE International Conference on Industrial and Information Systems (ICIIS)","22 Feb 2018","2017","","","1","5","Spell checker is a tool that finds and corrects misspelt words in a text document. Spelling error detection and correction techniques are widely used by text editing systems, machine translation systems, optical character recognition systems, search engines and speech recognition systems. Though spell checkers for European languages and Indian languages are well developed, few for Tamil language, perhaps, because the fact that Tamil language is morphologically rich and agglutinative makes it a challenging task. An efficient approach to generating suggestions for misspelt words in Tamil language has been proposed in this paper. The proposed novel approach uses n-gram technique on stemmed form of the words with two different hash-tables and find the better one to generate most suitable alternatives to misspelt words by speeding up the lookup. The use of length of words in hash-table speed up finding appropriate suggestions while reducing the number of inappropriate suggestions. Test results show that the suggestions generated by the system are with 95% accuracy as approved by a Scholar in Tamil.","","978-1-5386-1676-5","10.1109/ICIINFS.2017.8300346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300346","Tamil Language;Error Detection and Correction;N-gram;Hash Table;Stemming;Misspelt Word","Generators;Speech recognition;Automata;Adaptive optics;Optical character recognition software;Character recognition;Task analysis","natural language processing;spelling aids","European languages;Indian languages;Tamil language;hash-table speed;misspelt Tamil words;spell checker;text document;text editing systems;machine translation systems;optical character recognition systems;speech recognition systems;Spelling error detection;Spelling error correction;n-gram technique","","8","","21","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Using Hidden Markov Model to improve the accuracy of Punjabi POS tagger","Sanjeev Kumar Sharma; Gurpreet Singh Lehal","Department of CSE, BIS College of Engineering and Technology, Moga, India; Department of computer science, Punjabi University, Patiala, India","2011 IEEE International Conference on Computer Science and Automation Engineering","14 Jul 2011","2011","2","","697","701","POS tagger is the process of assigning a correct tag to each word of the sentence. Accuracy of all NLP tasks like grammar checker, phrase chunker, machine translation etc. depends upon the accuracy of the POS tagger. We attempted to improve the accuracy of existing Punjabi POS tagger. This POS tagger lacks in resolving the ambiguity of compound and complex sentences. A Bi-gram Hidden Markov Model has been used to solve the part of speech tagging problem. An annotated corpus of 20,000 words was used for training and estimating of HMM parameter. Maximum likelihood method has been used to estimate the parameter. This HMM approach has been implemented by using Viterby algorithm. A module has been developed that takes the existing POS tagger output as input and assign the correct tag to the words having more than one tag. Our module was tested on the corpus containing 26,479 words. The accuracy of 90.11% was evaluated using manual approach.","","978-1-4244-8728-8","10.1109/CSAE.2011.5952600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952600","POS;Tagging;HMM;Punjabi","Hidden Markov models;Tagging;Accuracy;Speech;Training;Natural language processing;Probability","hidden Markov models;maximum likelihood estimation;natural language processing","Punjabi POS tagger;part-of-speech tagger;grammar checker task;phrase chunker task;machine translation task;natural language processing;bi-gram hidden Markov model;maximum likelihood method;Viterbi algorithm","","3","","32","IEEE","14 Jul 2011","","","IEEE","IEEE Conferences"
"Building Statistical Language Models of code","P. Schulam; R. Rosenfeld; P. Devanbu","Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Department of Computer Science, University of California, Davis, USA","2013 1st International Workshop on Data Analysis Patterns in Software Engineering (DAPSE)","19 Sep 2013","2013","","","1","3","We present the Source Code Statistical Language Model data analysis pattern. Statistical language models have been an enabling tool for a wide array of important language technologies. Speech recognition, machine translation, and document summarization (to name a few) all rely on statistical language models to assign probability estimates to natural language utterances or sentences. In this data analysis pattern, we describe the process of building n-gram language models over software source files. We hope that by introducing the empirical software engineering community to best practices that have been established over the years in research for natural languages, statistical language models can become a tool that SE researchers are able to use to explore new research directions.","","978-1-4673-6296-2","10.1109/DAPSE.2013.6603797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603797","","Data models;Vocabulary;Smoothing methods;Speech recognition;Buildings;Software engineering;Natural languages","data analysis;natural languages;software engineering;source coding;statistical analysis","building statistical language models;source code data analysis pattern;speech recognition;machine translation;document summarization;natural language utterances;natural language sentences;n-gram language models;software source files;empirical software engineering community","","","","6","IEEE","19 Sep 2013","","","IEEE","IEEE Conferences"
"Exploring the language modeling toolkits for Arabic text","F. S. Al-Anzi; D. AbuZeina","Department of Computer Engineering, Kuwait University; Department of Computer Engineering, Kuwait University","2017 International Conference on Electrical and Computing Technologies and Applications (ICECTA)","11 Jan 2018","2017","","","1","4","Statistical N-grams language models (LMs) have shown to be very effective in natural language processing (NLP), particularly in automatic speech recognition (ASR) and machine translation. In fact, the successful impact of LMs promote to introduce efficient techniques as well as different types models in various linguistic applications. The LMs mainly include two types that are grammars and statistical language models that is also called N-grams. The main difference between grammars and statistical language models is that the statistical language models are based on the estimation of probabilities for words sequences while the grammars usually do not have probabilities. Despite there are many toolkits that are used to create LMs, however, this work employs two well-known language modeling toolkits with focus on the Arabic text. The implementing toolkits include the Carnegie Mellon University (CMU)-Cambridge Language Modeling Toolkit and the Cambridge University Hidden Markov Model Toolkit (HTK) language modeling toolkits. For clarification, we used a small Arabic text corpus to compute the N-grams for 1-gram, 2-gram, and 3-gram. In addition, this paper demonstrates the intermediate steps that are needed to generate the ARPA-format LMs using both toolkits.","","978-1-5386-0872-2","10.1109/ICECTA.2017.8251935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8251935","Arabic;language model;grammar;N-grams;perplexity","Computational modeling;Hidden Markov models;Grammar;Probability;Vocabulary;Speech recognition;Pragmatics","computational linguistics;grammars;hidden Markov models;natural language processing;text analysis","natural language processing;machine translation;grammars;Arabic text;Carnegie Mellon University-Cambridge Language Modeling Toolkit;Cambridge University Hidden Markov Model Toolkit language modeling toolkits;statistical N-grams language models;automatic speech recognition;words sequences","","","","17","IEEE","11 Jan 2018","","","IEEE","IEEE Conferences"
"Multilabel Classification of Hate Speech and Abusive Words on Indonesian Twitter Social Media","R. Hendrawan; Adiwijaya; S. Al Faraby","School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia; School of Computing, Telkom University, Bandung, Indonesia","2020 International Conference on Data Science and Its Applications (ICoDSA)","6 Oct 2020","2020","","","1","7","Hate speech and abusive words spread widely on social media. The impact of hate speech on social media is hazardous, which can lead to discrimination, social conflict, and even genocide. Hate speech also has target types, categories, and levels. This research discusses the classification of hate speech and abusive words in the text on social media Twitter in Indonesian, English, and a mixture of both up to the types, categories, and levels. Classification of hate speech multilabel text is investigated using RFDT, BiLSTM, and BiLSTM with the pre-trained BERT model. The Classifier Chains, Label Powerset, and Binary Relevance methods are also used as data transformation, and TF-IDF is also used as feature extraction combined with the RFDT classification method. Some scenarios of the preprocessing stage are also carried out to find the best results, namely full preprocess, without stopword removal, and without stemming and without stopword removal. The problem of having Indonesian, English, and a mixture of both is solved in two ways, namely, without being translated and translated into Indonesian. The best results with an accuracy of 76.12% were obtained using the RFDT classification method with Classifier Chains, without translation, without stemming, and without stopword removal. This research also shows that the translation, stemming, and stopword removal are not effective, and the problem of dependencies between labels greatly affects the results of classification.","","978-1-7281-8235-3","10.1109/ICoDSA50139.2020.9212962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212962","hate speech;text classification;multilabel classification;randomforest decision tree;bilstm;bert","Twitter;Support vector machines;Text categorization;Standards;Feature extraction;Dictionaries","feature extraction;Internet;natural language processing;pattern classification;recurrent neural nets;social networking (online);text analysis;word processing","Indonesian Twitter social media;RFDT classification method;stopword removal;hate speech multilabel text classification;abusive words classification;BiLSTM;feature extraction","","10","","24","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"Improving Self-Attention Networks With Sequential Relations","Z. Zheng; S. Huang; R. Weng; X. -Y. Dai; J. Chen","National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","12 Jun 2020","2020","28","","1707","1716","Recently, self-attention networks show strong advantages of sentence modeling in many NLP tasks. However, self-attention mechanism computes the interactions of every pair of words independently regardless of their positions, which makes it not able to capture the sequential relations between words in different positions in a sentence. In this paper, we improve the self-attention networks by better integrating sequential relations, which is essential for modeling natural languages. Specifically, we 1) propose a position-based attention to model the interaction between two words regarding positions; 2) perform separated attention for the context before and after the current position, respectively; and 3) merge the above two parts with a position-aware gated fusion mechanism. Experiments in natural language inference, machine translation and sentiment analysis tasks show that our sequential relation modeling helps self-attention networks outperform existing approaches. We also provide extensive analyses to shed light on what the models have learned about the sequential relations.","2329-9304","","10.1109/TASLP.2020.2996807","National Natural Science Foundation of China(grant numbers:U1836221,61772261,61672277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099090","Sentence modeling;self-attention;sequential relations","Decoding;Computational modeling;Context modeling;Analytical models;Natural languages;Task analysis;Logic gates","language translation;sentiment analysis","sentiment analysis tasks;machine translation;natural language inference;natural language modelling;NLP tasks;sequential relation modeling;position-aware gated fusion mechanism;position-based attention;self-attention mechanism;self-attention networks","","6","","44","IEEE","22 May 2020","","","IEEE","IEEE Journals"
"isiZulu Word Embeddings","S. Dlamini; E. Jembere; A. Pillay; B. van Niekerk","Department of Computer Science, University of KwaZulu-Natal, Durban, South Africa; Department of Computer Science, University of KwaZulu-Natal, Durban, South Africa; Department of Computer Science, University of KwaZulu-Natal, Durban, South Africa; Department of Computer Science, University of KwaZulu-Natal, Durban, South Africa","2021 Conference on Information Communications Technology and Society (ICTAS)","6 Apr 2021","2021","","","121","126","Word embeddings are currently the most popular vector space model in Natural Language Processing. How we encode words is important because it affects the performance of many downstream tasks such as Machine Translation (MT), Information Retrieval (IR) and Automatic Speech Recognition (ASR). While much focus has been placed on constructing word embeddings for English, very little attention is paid to under-resourced languages, especially native African languages. In this paper we select four popular word embedding models (Word2Vec CBOW and Skip-Gram; FastText and GloVe) and train them on the 10 million token isiZulu National Corpus (INC) to create isiZulu word embeddings. To the best of our knowledge, this is the first time that word embeddings in isiZulu have been constructed and made available to the public. We create a semantic similarity data set analogous to WordSim353, which we also make publicly available. This data set is used to conduct an evaluation of the four models to determine which is the best for creating isiZulu word embeddings in a low-resource (small corpus) setting. We found that the Word2Vec Skip-Gram model produced the highest quality embeddings, as measured by this semantic similarity task. However, it was the GloVe model which performed best on the nearest neighbours task.","","978-1-7281-8081-6","10.1109/ICTAS50802.2021.9395011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395011","isiZulu;word embeddings;semantic relatedness;agglutinative language;subword embeddings","Correlation;Semantics;Information retrieval;Data models;Communications technology;Machine translation;Task analysis","learning (artificial intelligence);natural language processing;text analysis","Word2Vec Skip-gram model;vector space model;natural language processing;Word2Vec CBOW;isiZulu word embeddings;native African languages;FastText;GloVe;WordSim353;Word2Vec Continuous Bag of Words;machine learning","","1","","23","IEEE","6 Apr 2021","","","IEEE","IEEE Conferences"
"Deep Learning Approach for Classifying the Aggressive Comments on Social Media: Machine Translated Data Vs Real Life Data","M. S. Akter; H. Shahriar; N. Ahmed; A. Cuzzocrea","Department of Computer Science, Kennesaw State University, USA; Department of Information Technology, Kennsaw State University, USA; Department of Electrical and Computer Engineering (ECE), North South University, Bangladesh; iDEA Lab, University of Calabria, Rende, Italy","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","5646","5655","Aggressive comments on social media negatively impact human life. Such offensive contents are responsible for depression and suicidal-related activities. Since online social networking is increasing day by day, the hate content is also increasing. Several investigations have been done on the domain of cyberbullying, cyberaggression, hate speech, etc. The majority of the inquiry has been done in the English language. Some languages (Hindi and Bangla) still lack proper investigations due to the lack of a dataset. This paper particularly worked on the Hindi, Bangla, and English datasets to detect aggressive comments and have shown a novel way of generating machine-translated data to resolve data unavailability issues. A fully machine-translated English dataset has been analyzed with the models such as the Long Short term memory model (LSTM), Bidirectional Long-short term memory model (BiLSTM), LSTM-Autoencoder, word2vec, Bidirectional Encoder Representations from Transformers (BERT), and generative pre-trained transformer (GPT-2) to make an observation on how the models perform on a machine-translated noisy dataset. We have compared the performance of using the noisy data with two more datasets such as raw data, which does not contain any noises, and semi-noisy data, which contains a certain amount of noisy data. We have classified both the raw and semi-noisy data using the aforementioned models. To evaluate the performance of the models, we have used evaluation metrics such as F1-score, accuracy, precision, and recall. We have achieved the highest accuracy on raw data using the gpt2 model, semi-noisy data using the BERT model, and fully machine-translated data using the BERT model. Since many languages do not have proper data availability, our approach will help researchers create machine-translated datasets for several analysis purposes.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020249","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020249","Cyber-bullying;Augmentation;Data preprocessing;AutoEncoder;LSTM;BiLSTM;Word2vec;BERT;GPT-2","Deep learning;Training;Analytical models;Bit error rate;Organizations;Big Data;Transformers","language translation;learning (artificial intelligence);natural language processing;neural nets;social networking (online)","aforementioned models;aggressive comments;BERT model;Bidirectional Long-short term memory model;data unavailability issues;English language;fully machine-translated English dataset;generative pre-trained transformer;gpt2 model;hate content;lack proper investigations;languages;Long Short term memory model;machine translated data vs real life data;machine-translated data;machine-translated datasets;machine-translated noisy dataset;noisy data;offensive contents;online social networking;proper data availability;raw data;seminoisy data;social media negatively impact human life;suicidal-related activities","","4","","42","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Sign Language Conversion using Hand Gesture Recognition","Y. Dhamecha; R. Pawar; A. Waghmare; S. Ghosh","Department of Computer Engineering & IT, College of Engineering, Pune, Pune, India; Department of Computer Engineering & IT, College of Engineering, Pune, Pune, India; Department of Computer Engineering & IT, College of Engineering, Pune, Pune, India; Department of Computer Engineering & IT, College of Engineering, Pune, Pune, India","2023 2nd International Conference for Innovation in Technology (INOCON)","19 Apr 2023","2023","","","1","6","Communication is difficult for speech and hearing impaired when interacting with others. Such people struggle to communicate their ideas because not everyone can understand sign language. Gesture recognition is a perceptual computing user interface that enables computers to record and interpret human motions and execute commands based on those movements. Gesture recognition can be seen of as a method for computers to start comprehending body language, creating a more complex connection between machines and people than simple text user interfaces. To facilitate easier communication between the two, we provide experiments of gesture recognition and text translation in this study.","","979-8-3503-2092-3","10.1109/INOCON57975.2023.10101099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10101099","component;formatting;style;styling;insert","Computers;Technological innovation;Codes;Gesture recognition;Auditory system;User interfaces;Assistive technologies","handicapped aids;image motion analysis;language translation;natural language processing;sign language recognition;user interfaces","body language;hand gesture recognition;human motions;perceptual computing user interface;sign language conversion;simple text user interfaces;speech and hearing impaired;text translation","","","","9","IEEE","19 Apr 2023","","","IEEE","IEEE Conferences"
"Intent detection and semantic parsing for navigation dialogue language processing","Y. Zheng; Y. Liu; J. H. L. Hansen","Electrical Engineering Department, University of Texas at Dallas, Richardson, TX, USA; Electrical Engineering Department, University of Texas at Dallas, Richardson, TX, USA; Electrical Engineering Department, University of Texas at Dallas, Richardson, TX, USA","2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)","15 Mar 2018","2017","","","1","6","Voice-based human-machine interface has become a prevalent feature for modern intelligent vehicles, especially in navigation and infotainment applications. Automatic Speech Recognition (ASR) converts spoken audio streams to plain texts, but a follow-up Natural Language Processing (NLP) sub-system is needed to understand the contextual meaning from the text and act. For the specific human-vehicle navigation dialogue application, the two major tasks include (1) intent detection - decide whether a sentence is navigation-related, and (2) semantic parsing - retrieve important information (e.g., point-of-interest destinations) from the words. To address these two tasks, this study proposes a Recurrent Neural Network (RNN) architecture, with the consideration of (1) one joint model vs. two separate models, (2) a context window approach vs. sequence-to-sequence translation approach, as well as (3) alternate model hyper-parameter selections. The experiment is conducted with both the benchmark ATIS dataset and the CU-Move in-vehicle dialogue corpus, and the result is compared against related state-of-the-art methods. Overall, the proposed solution on the CU-Move data achieves accuracies of 98.24% for intent detection and 99.60% for semantic parsing, outperforming other related methods.","2153-0017","978-1-5386-1526-3","10.1109/ITSC.2017.8317620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317620","","Task analysis;Navigation;Semantics;Recurrent neural networks;Logic gates;Computer architecture","audio streaming;information retrieval;interactive systems;natural language processing;recurrent neural nets;speech processing;speech recognition;speech-based user interfaces;text analysis;traffic engineering computing","infotainment applications;Automatic Speech Recognition;audio streams;plain texts;contextual meaning;specific human-vehicle navigation dialogue application;point-of-interest destinations;Recurrent Neural Network architecture;intelligent vehicles;semantic parsing;navigation dialogue language processing;voice-based human-machine interface;Natural Language Processing subsystem;intent detection;one joint model-vs.-two separate models;context window approach-vs.-sequence-to-sequence translation approach;alternate model hyperparameter selections;ATIS dataset;CU-Move in-vehicle dialogue corpus","","5","","31","IEEE","15 Mar 2018","","","IEEE","IEEE Conferences"
"Using Vector Proximity for NLP Analysis of Specialized Texts","A. N. Sak","Moscow state university of civil engineering, Moscow, Russia","2022 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)","4 Aug 2022","2022","","","1","6","The paper considers some latest approaches to develop a machine translation system to handle texts specialized in construction industry both in Russian and English. Graph theory is the basis for automatic sentence analysis and representation of its structure. At the first stage, the sentence is represented by a line graph. The article discusses how to assign attributes to each word in accordance with its prospective belonging to different parts of speech for their subsequent processing using graph convolutional neural networks (GCN). The initial features of each element of the sentence are compared with the calculated values obtained, and by means of certain parameters for the compiled vectors, they are classified as parts of speech. The paper contains parts of the software listing for the computer implementation of the proposed technology.","2832-0514","978-1-6654-7064-3","10.1109/SYNCHROINFO55067.2022.9840981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840981","graph theory;syntax;semantics;graph convolutional neural networks;object-oriented programming;natural language processing;vector space;Euclidean distance between vectors","Reliability theory;Software;Graph theory;Telecommunications;Convolutional neural networks;Telecommunication network reliability;Synchronization","construction industry;feature extraction;graph theory;language translation;natural language processing;neural nets;text analysis","vector proximity;NLP analysis;specialized texts;latest approaches;machine translation system;construction industry;graph theory;automatic sentence analysis;line graph;prospective belonging;subsequent processing;graph convolutional neural networks;initial features;compiled vectors","","1","","16","IEEE","4 Aug 2022","","","IEEE","IEEE Conferences"
"A Toolkit for Text Extraction and Analysis for Natural Language Processing Tasks","T. J. Sefara; M. Mbooi; K. Mashile; T. Rambuda; M. Rangata","Data Science Department Council for Scientific and Industrial Research, Pretoria, South Africa; Data Science Department Council for Scientific and Industrial Research, Pretoria, South Africa; Data Science Department Council for Scientific and Industrial Research, Pretoria, South Africa; Data Science Department Council for Scientific and Industrial Research, Pretoria, South Africa; Data Science Department Council for Scientific and Industrial Research, Pretoria, South Africa","2022 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)","22 Aug 2022","2022","","","1","6","Text extraction is an important part of natural language processing (NLP) tasks. Most NLP tasks like text classification, machine translation, text-to-speech, text-based language identification, text summarization, and named-entity recognition involve the use of textual data. Such data is limited for low-resourced languages making it difficult to experiment advanced NLP techniques on these languages. This paper presents a Python-based toolkit for text analysis and text extraction from different types of images, documents, and audio files. The toolkit is built as a library that has functions that can be imported and utilized for text extraction.","","978-1-6654-8422-0","10.1109/icABCD54961.2022.9856269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856269","natural language processing;text extraction;text analysis","Text recognition;Text categorization;Big Data;Natural language processing;Libraries;Machine translation;Data communication","language translation;natural language processing;text analysis","text summarization;low-resourced languages;text analysis;text extraction;natural language processing tasks;NLP tasks;text classification;text-to-speech;text-based language identification","","","","37","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Keynotes","",,"2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)","16 Jan 2017","2016","","","xx","xxii","Provides an abstract for each of the keynote presentations and may include a brief professional biography of each","2167-6445","978-1-5090-0981-7","10.1109/ICFHR.2016.0012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814028","","Handwriting recognition;Speech recognition;Hidden Markov models;Training;Machine translation;User interfaces;Deep learning","","","","","","","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"Morphological analysis of Kokborok for universal networking language dictionary","K. Debbarma; B. G. Patra; S. Debbarma; L. Kumari; B. S. Purkayastha","Department of CSE, National Institute of Technology, Agartala, India; Department of CSE, National Institute of Technology, Agartala, India; Department of CSE, National Institute of Technology, Agartala, India; Department of CSE, National Institute of Technology, Agartala, India; Department of Computer Science, Assam University, India","2012 1st International Conference on Recent Advances in Information Technology (RAIT)","7 May 2012","2012","","","474","477","This paper focuses on morphological analysis of Kokborok words to incorporate them into Kokborok dictionary and Kokborok Machine translator. So far, no attempt has been made to integrate the works for a concrete computational output. In this paper we particularly emphasize on bringing works on morphological analysis in the frame, with the goal to produce a Kokborok-dictionary, as well as Machine Translator, which will provide a unified base to fit into already developed universal conversion systems of UNL. We explain the morphological rules of Kokborok words for UNL structures. These rules tend to expose the modifications of parts of speech with regards to tense, person, subject etc. of the words of a sentence. Here we outline the morphology of nouns, verbs and adjective phrases only.","","978-1-4577-0697-4","10.1109/RAIT.2012.6194624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6194624","Morphology;Kokborok;Universal Net-working Language (UNL);UNL-Kokborok dictionary","Morphology;Dictionaries;Compounds;Speech;Information technology;Magnetic heads;Vegetation","dictionaries;language translation;natural language processing","morphological analysis;universal networking language dictionary;Kokborok words;Kokborok dictionary;Kokborok Machine translator;universal conversion system;morphological rules;UNL structure;sentence words;noun morphology;verb morphology;adjective phrases","","2","","11","IEEE","7 May 2012","","","IEEE","IEEE Conferences"
"Grammatical category disambiguation based on second order hidden Markov model","Sun Jian; Wang Wei; Zhong Yixin","Research Center of Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Research Center of Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; Research Center of Intelligence, Beijing University of Posts and Telecommunications, Beijing, China","2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)","6 Aug 2002","2001","2","","887","891 vol.2","Grammatical category disambiguation is an important field because of its basis in many applications, for example, parsing, machine translation, phrase recognition and so on. We put forward an improved second-order hidden Markov model that can capture more context information and develop one part-of-speech tagging system based on the model. In order to reduce the number of model parameters, word equivalence classes are used. The parameters of model are achieved by the Baum-Welch algorithm using untagged text. Results show that it improves the accuracy of tagging.","1062-922X","0-7803-7087-2","10.1109/ICSMC.2001.973029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=973029","","Hidden Markov models;Context modeling;Tagging;Natural languages;Parameter estimation;Equations;Machine intelligence;Statistical analysis;Probability;Robustness","grammars;hidden Markov models;word processing;natural languages;linguistics;equivalence classes","grammatical category disambiguation;second order hidden Markov model;parsing;machine translation;phrase recognition;context information;part-of-speech tagging system;model parameters;word equivalence classes;Baum-Welch algorithm;untagged text;POS tagging","","","1","9","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Audio Caption: Listen and Tell","M. Wu; H. Dinkel; K. Yu","MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence SpeechLab, Shanghai Jiao Tong University, Shanghai, China","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","830","834","Increasing amount of research has shed light on machine perception of audio events, most of which concerns detection and classification tasks. However, human-like perception of audio scenes involves not only detecting and classifying audio sounds, but also summarizing the relationship between different audio events. Comparable research such as image caption has been conducted, yet the audio field is still quite barren. This paper introduces a manually-annotated dataset for audio caption. The purpose is to automatically generate natural sentences for audio scene description and to bridge the gap between machine perception of audio and image. The whole dataset is labelled in Mandarin and we also include translated English annotations. A baseline encoder-decoder model is provided for both English and Mandarin. Similar BLEU scores are derived for both languages: our model can generate understandable and data-related captions based on the dataset.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682377","Audio Caption;Audio Databases;Natural Language Generation;Recurrent Neural Networks","Feature extraction;Task analysis;Videos;Hospitals;Training;Music;Mathematical model","audio signal processing;language translation;natural language processing;recurrent neural nets;signal classification;video signal processing","audio sounds classification;audio events;manually-annotated dataset;translated English annotations;encoder-decoder model;data-related captions;machine perception;audio scene description;audio caption;image caption","","17","","22","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Language Modeling at Scale","M. Patwary; M. Chabbi; H. Jun; J. Huang; G. Diamos; K. Church","Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA; Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA; Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA; Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA; Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA; Silicon Valley AI Lab, Baidu Research, Sunnyvale, California, USA","2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","2 Sep 2019","2019","","","590","599","We show how Zipf's Law can be used to scale up language modeling (LM) to take advantage of more training data and more GPUs. LM plays a key role in many important natural language applications such as speech recognition and machine translation. Scaling up LM is important since it is widely accepted by the community that there is no data like more data. Eventually, we would like to train on terabytes (TBs) of text (trillions of words). Modern training methods are far from this goal, because of various bottlenecks, especially memory (within GPUs) and communication (across GPUs). This paper shows how Zipf's Law can address these bottlenecks by grouping parameters for common words and character sequences, because U ‚â™ N, where U is the number of unique words (types) and N is the size of the training set (tokens). For a local batch size K with G GPUs and a D-dimension embedding matrix, we reduce the original per-GPU memory and communication asymptotic complexity from Œò(GKD) to Œò(GK + UD). Empirically, we find U ‚àù (GK)^0.64 on four publicly available large datasets. When we scale up the number of GPUs to 64, a factor of 8, training time speeds up by factors up to 6.7√ó (for character LMs) and 6.3√ó (for word LMs) with negligible loss of accuracy. Our weak scaling on 192 GPUs on the Tieba dataset shows a 35% improvement in LM prediction accuracy by training on 93 GB of data (2.5√ó larger than publicly available SOTA dataset), but taking only 1.25√ó increase in training time, compared to 3 GB of the same dataset running on 6 GPUs.","1530-2075","978-1-7281-1246-6","10.1109/IPDPS.2019.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820976","Language Modeling;Deep Learning;High Performance Computing;Communication Optimization","Training;Vocabulary;Graphics processing units;Computational modeling;Data models;Neural networks;Natural languages","graphics processing units;language translation;learning (artificial intelligence);natural language processing;speech recognition;text analysis","common words;unique words;training set;training time;weak scaling;LM prediction accuracy;language modeling;training data;important natural language applications;speech recognition;machine translation;modern training methods;Zipf law;GPU;word LM","","","","44","IEEE","2 Sep 2019","","","IEEE","IEEE Conferences"
"Coarse-to-fine trained multi-scale Convolutional Neural Networks for image classification","Haobin Dou; Xihong Wu","Speech and Hearing Research Center, Peking University, Beijing, China; College of Computer Science and Technology, Jilin University, Changchun, China","2015 International Joint Conference on Neural Networks (IJCNN)","1 Oct 2015","2015","","","1","7","Convolutional Neural Networks (CNNs) have become forceful models in feature learning and image classification. They achieve translation invariance by spatial convolution and pooling mechanisms, while their ability in scale invariance is limited. To tackle the problem of scale variation in image classification, this work proposed a multi-scale CNN model with depth-decreasing multi-column structure. Input images were decomposed into multiple scales and at each scale image, a CNN column was instantiated with its depth decreasing from fine to coarse scale for model simplification. Scale-invariant features were learned by weights shared across all scales and pooled among adjacent scales. Particularly, a coarse-to-fine pre-training method imitating the human's development of spatial frequency perception was proposed to train this multi-scale CNN, which accelerated the training process and reduced the classification error. In addition, model averaging technique was used to combine models obtained during pre-training and further improve the performance. With these methods, our model achieved classification errors of 15.38% on CIFAR-10 dataset and 41.29% on CIFAR-100 dataset, i.e. 1.05% and 2.97% reduction compared with single-scale CNN model.","2161-4407","978-1-4799-1960-4","10.1109/IJCNN.2015.7280542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280542","","Training;Laplace equations;Airplanes;Automobiles;Birds;Atmospheric modeling;Marine vehicles","feature extraction;image classification;learning (artificial intelligence);neural nets","coarse-to-fine trained multiscale convolutional neural networks;image classification;feature learning;translation invariance;spatial convolution mechanism;pooling mechanism;scale variation problem;multiscale CNN training;depth-decreasing multicolumn structure;input images;scale-invariant feature learning;spatial frequency perception;classification error reduction;model averaging technique;performance improvement;CIFAR-IO dataset;CIFAR-100 dataset","","2","4","32","IEEE","1 Oct 2015","","","IEEE","IEEE Conferences"
"Named entity recognition on real data: A preliminary investigation for Turkish","G. √áelikkaya; D. Torunoƒülu; G. Eryiƒüit","R&D Department Huawei Technologies Co., Ltd Istanbul, 34768, Turkey; Istanbul Technical University, Institute of Science and Technology, Istanbul, Turkey; Dep. of Computer Engineering, Istanbul Technical University, Istanbul, Turkey","2013 7th International Conference on Application of Information and Communication Technologies","27 Jan 2014","2013","","","1","5","Named Entity Recognition (NER) is a well-studied area in natural language processing (NLP) and the reported results in the literature are generally very high (‚àº>%95) for most of the languages. Today, the focus area of most practical natural language applications (i.e. web mining, sentiment analysis, machine translation) is real natural language data such as Web2.0 or speech data. Nevertheless, the NER task is rarely investigated on this type of data which differs severely from formal written text. In this paper, we present 3 new Turkish data sets from different domains (on this focused area; namely from Twitter, a Speech-to-Text Interface and a Hardware Forum) annotated specifically for NER and report our first results on them. We believe, the paper draws light to the difficulty of these new domains for NER and the possible future work.","","978-1-4673-6420-1","10.1109/ICAICT.2013.6722801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722801","Named Entity Recognition;Turkish;Conditional Random Fields;ENAMEX;Speech Data;Twitter","Data models;Hidden Markov models;Twitter;Organizations;Speech;Media;Natural language processing","natural language processing","named entity recognition;Turkish data sets;NER;natural language processing;NLP;natural language data;Web2.0;speech data;NER task","","18","1","17","IEEE","27 Jan 2014","","","IEEE","IEEE Conferences"
"MiReLa: a musical robot","U. Esnaola; T. Smithers","San Sebasti√°n Technology Park, Donostia San Sebastian, Spain; San Sebasti√°n Technology Park, Donostia San Sebastian, Spain","2005 International Symposium on Computational Intelligence in Robotics and Automation","12 Dec 2005","2005","","","67","72","A music based robot-human interaction system is presented. A musical language was developed to make robot-human interaction reliable in noisy environments. This language can be whistled and played on simple instruments. Mobile phones and PDAs can also be used to generate phrases in this language. It was implemented as part of a ""society of agents"" where each agent helps the others, and all of them together make a mass of skills that grow in power and efficiency.","","0-7803-9355-4","10.1109/CIRA.2005.1554256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1554256","Mobile Robotics;Agent Technology;robot-human;interaction","Frequency;Instruments;Natural languages;Software agents;Mobile handsets;Personal digital assistants;Human robot interaction;Working environment noise;Animals;Birds","man-machine systems;language translation;music;human computer interaction;speech processing","MiReLa;musical robot;music-based robot-human interaction system;musical language;multiagent society","","3","","9","IEEE","12 Dec 2005","","","IEEE","IEEE Conferences"
"Spoken Language Identification System for Kashmiri and Related Languages Using Mel-Spectrograms and Deep Learning Approach","I. A. Thukroo; R. Bashir","Department of Computer Science, Islamic University of Science & Technology Awantipora, Pulwama, Srinagar, India; Department of Computer Science, Islamic University of Science & Technology Awantipora, Pulwama, Srinagar, India","2021 7th International Conference on Signal Processing and Communication (ICSC)","14 Jan 2022","2021","","","250","255","Language identification, being the front-end for various natural language processing tasks, plays an important role in language translation. Owing to this, the focus has been on the field of speech recognition involving the identification & recognition of languages by a machine. Spoken language identification is the identification of language present in a speech segment despite its size (duration & speed), ambiance (topic & emotion), and moderator (gender, age, demographic region). In this model, six languages have been identified that include five officially spoken languages of Jammu and Kashmir (Kashmiri, Urdu, Dogri, Hindi, and English) along with Ladakhi language by applying Convolutional Neural Network (CNN) approach. The data is recorded from TV channels of languages (Kashmiri, Ladakhi, Urdu, and Dogri) and standard corpora IIIT-H and VoxForge for Hindi and English respectively. Data is pre-processed by removing background noise and segmented into segments of 5 seconds each. Each segment is converted into Mel-spectrograms by applying Fast Fourier Transformations and at last, CNN serves as a classifier. Experimental results showed that an average training and testing accuracies of 100% have been achieved by running the model at 100 epochs.","2643-444X","978-1-6654-2739-5","10.1109/ICSC53193.2021.9673212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673212","Spoken Language Identification;Kashmiri language;deep neural networks;convolutional neural network;Mel-spectrogram;fast Fourier transformation","Deep learning;Training;TV;Neural networks;Speech recognition;Signal processing;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);fast Fourier transforms;feature extraction;natural language processing;signal classification;speech recognition","deep learning approach;natural language processing tasks;language translation;speech recognition;speech segment;Kashmiri language;Ladakhi language;Mel-spectrograms;spoken language identification system;officially spoken languages;language identification;language recognition;Jammu and Kashmir;Urdu language;Dogri language;Hindi language;English language;convolutional neural network approach;CNN approach;TV channels;IIIT-H corpora;VoxForge corpora;background noise removal;fast Fourier transformations","","4","","30","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Short Utterances based real time on-Device Spoken Language Identification","S. Choudhary; K. Jain; T. Bansal; C. Karthik; P. S. Lakshmi; S. Sharma","Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India","2021 IEEE 18th India Council International Conference (INDICON)","1 Feb 2022","2021","","","1","6","Speech based tasks on mobile devices such as voice based interactions with virtual personal assistants, speech translation etc. have become commonplace today. For such tasks, real-time language identification can serve as an important first step. However, due to the less speech content in short utterances, this is a challenging problem. In this paper, we present Short Utterances Language Identification (SULInet) which is an attention-based light-weight CNN model. It uses 3 seconds utterance and is fit for real time analysis for resource constraint environment like mobile devices. We used state of the art systems trained on Voxforge and EuroSpeech datasets as baseline. We showcase the results at frame-level (3 seconds) and file-level (complete audio file) on PC and an Android based mobile device. We significantly improved our accuracy and achieved SOTA results by proposing a voice activity detector (VAD) as a preprocessing step. On Voxforge dataset, we surpassed SOTA system by 2.36% with 65% less training parameters and 70% smaller input. Similarly, on EuroSpeech dataset, we achieved comparable results with 42% less parameters. Our approach achieved on-Device frame level accuracy as 92%, 80% on Voxforge and EuroSpeech respectively with just one-tenth of PC model size.","2325-9418","978-1-6654-4175-9","10.1109/INDICON52576.2021.9691630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691630","Real-Time Spoken Language Identification;Convolutional Neural Networks (CNN);Machine Learning;Attention;TensorFlow Lite)","Voice activity detection;Training;Image edge detection;Neural networks;Europe;Detectors;Real-time systems","audio signal processing;convolutional neural nets;mobile computing;speech processing","speech content;attention-based light-weight CNN model;resource constraint environment;mobile devices;frame-level;file-level;complete audio file;voice activity detector;Voxforge dataset;EuroSpeech dataset;on-Device frame level accuracy;speech based tasks;voice based interactions;virtual personal assistants;speech translation;real-time language identification;short utterances language identification;SULInet;real time on-device spoken language identification;SOTA system;time 3.0 s","","","","31","IEEE","1 Feb 2022","","","IEEE","IEEE Conferences"
"Effective Deep Learning Models for Automatic Diacritization of Arabic Text","M. A. H. Madhfar; A. M. Qamar","Department of Computer Science, College of Computer, Qassim University, Buraydah, Saudi Arabia; Department of Computer Science, College of Computer, Qassim University, Buraydah, Saudi Arabia","IEEE Access","4 Jan 2021","2021","9","","273","288","While building a text-to-speech system for the Arabic language, we found that the system synthesized speeches with many pronunciation errors. The primary source of these errors is the lack of diacritics in modern standard Arabic writing. These diacritics are small strokes that appear above or below each letter to provide pronunciation and grammatical information. We propose three deep learning models to recover Arabic text diacritics based on our work in a text-to-speech synthesis system using deep learning. The first model is a baseline model used to test how a simple deep learning model performs on the corpora. The second model is based on an encoder-decoder architecture, which resembles our text-to-speech synthesis model with many modifications to suit this problem. The last model is based on the encoder part of the text-to-speech model, which achieves state-of-the-art performances in both word error rate and diacritic error rate metrics. These models will benefit a wide range of natural language processing applications such as text-to-speech, part-of-speech tagging, and machine translation.","2169-3536","","10.1109/ACCESS.2020.3041676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274427","Arabic language;Tacotron;diacritization;deep learning;text-to-speech","Hidden Markov models;Deep learning;Adaptation models;Standards;Viterbi algorithm;Task analysis;Tagging","deep learning (artificial intelligence);natural language processing;neural nets;speech synthesis;text analysis;word processing","text-to-speech synthesis model;word error rate;diacritic error rate metrics;natural language processing applications;part-of-speech tagging;effective deep learning models;automatic diacritization;Arabic language;pronunciation errors;primary source;grammatical information;Arabic text diacritics;text-to-speech synthesis system;baseline model;encoder-decoder architecture;standard Arabic writing","","8","","42","CCBY","1 Dec 2020","","","IEEE","IEEE Journals"
"Multi-Head Self-Attention Gated-Dilated Convolutional Neural Network for Word Sense Disambiguation","C. -X. Zhang; Y. -L. Zhang; X. -Y. Gao","School of Computer Science and Technology, Harbin University of Science and Technology, Harbin, China; School of Computer Science and Technology, Harbin University of Science and Technology, Harbin, China; School of Computer Science and Technology, Harbin University of Science and Technology, Harbin, China","IEEE Access","15 Feb 2023","2023","11","","14202","14210","Word sense disambiguation (WSD) is to determine correct sense of ambiguous word based on its context. WSD is widely used in text classification, machine translation and information retrieval and so on. WSD accuracy is low because disambiguation features can not cover more language phenomenon and the discriminative ability of WSD model is not high. In order to improve accuracy of simplified Chinese WSD, a WSD model based on multi-head self-attention and gated-dilated convolutional neural network(AGDCNN) is proposed. Ambiguous word is viewed as the center and 4 adjacent lexical units are extracted successively toward the left and right side. Words, parts of speech, and semantic categories in 4 adjacent lexical units are vectorized and the vectorized results are input into gated-dilated convolutional neural network to get discriminative features. Then, multi-head self-attention is adopted to learn the difference and connection among discriminative features fully. Finally, classification weights are output from adaptive average pooling layer. Experiments are conducted on SemEval-2007: Task#5 and SemEval-2021: Task#2. Experimental results show that AGDCNN model has higher accuracy compared with other methods. Our goal is to improve the quality of simplified Chinese WSD as much as possible based on current linguistic resources and machine learning methods. The challenge we face is to extract effective discriminative features and design disambiguation model in high quality. Our novelty lies in that gated-dilated convolution is combined with multi-head self-attention to extract effective discriminative features, and learn their difference and connection from word form, parts of speech, and semantic categories.","2169-3536","","10.1109/ACCESS.2023.3243574","Heilongjiang Provincial Natural Science Foundation of China(grant numbers:LH2022F031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041152","Word sense disambiguation;multi-head self-attention;convolutional neural network;part of speech;semantic category","Text analysis;Feature extraction;Semantics;Adaptation models;Task analysis;Logic gates;Convolutional neural networks;Speech recognition","computational linguistics;convolutional neural nets;feature extraction;information retrieval;language translation;learning (artificial intelligence);linguistics;natural language processing;pattern classification;sentiment analysis;text analysis;word processing","4 adjacent lexical units;AGDCNN model;ambiguous word;correct sense;current linguistic resources;design disambiguation model;disambiguation features;discriminative ability;effective discriminative features;gated-dilated convolution;gated-dilated convolutional neural network;machine learning methods;multihead self-attention;semantic categories;simplified Chinese WSD;word form;word sense disambiguation;WSD accuracy;WSD model","","1","","32","CCBY","9 Feb 2023","","","IEEE","IEEE Journals"
"Recent developments in sign language recognition systems","M. F. Tolba; A. S. Elons","Ain Shams University, Cairo, EG; Ain Shams University, Cairo, EG","2013 8th International Conference on Computer Engineering & Systems (ICCES)","9 Jan 2014","2013","","","xxxvi","xlii","Automated translation systems for sign languages are important in a world that is showing a continuously increasing interest in removing barriers faced by physically challenged individuals in communicating and contributing to the society and the workforce. These systems can greatly facilitate the communication between the vocal and the nonvocal communities. For the hearing-impaired, such systems can serve as the equivalent of speech-recognition systems used by speaking people to interact with machines in a more natural way. Few research projects tried to develop a translation system from and to visual sign language. Very few of these attempts were on Arabic sign language. None of them succeeded to develop a reliable industrial product. This review explores these trials and proposes clear recommendations to convert the lab research to a real product useful to the community.","","978-1-4799-0080-0","10.1109/ICCES.2013.6707157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707157","Arabic Sign Language (ArSL);hand gesture;hand posture;movement transition","Assistive technology;Gesture recognition;Hidden Markov models;Cameras;Sensors;Three-dimensional displays;Biological neural networks","human computer interaction;natural language processing;sign language recognition;speech recognition","lab research;industrial product;Arabic sign language;visual sign language;speaking people;speech-recognition systems;hearing-impaired individual;nonvocal community;physically challenged individuals;automated translation systems;sign language recognition systems","","21","","30","IEEE","9 Jan 2014","","","IEEE","IEEE Conferences"
"A Kannada Handwritten Character Recognition System Exploiting Machine Learning Approach","S. Vijaya Shetty; R. Karan; H. Sarojadevi","Computer Science and Engineering, Nitte Meenakshi Institute of Technolgy, Bangalore, India; Computer Science and Engineering, Nitte Meenakshi Institute of Technolgy, Bangalore, India; Computer Science and Engineering, Nitte Meenakshi Institute of Technolgy, Bangalore, India","2023 International Conference on Applied Intelligence and Sustainable Computing (ICAISC)","9 Aug 2023","2023","","","1","7","Handwritten character recognition plays an important role when the handwritten text on paper, postcards, etc. requires conversion of the handwritten text into digitized form. The difference between a digitized handwritten document and a scanned document is that the prior one can be edited, and the latter cannot. Significant developments have been made on the handwritten character recognition of widely used languages like English. India is a multilingual country where there exist multiple regional languages like Kannada, Tamil, Malayalam and other Dravidian Languages with complex scripts. Kannada is spoken in most of the regions of Karnataka State, which is one of the southern regions of India. In the proposed research, a Convolutional Neural Network(CNN) is practiced to recognize Kannada handwritten Characters. The research employs densely connected-convolutional networks or DenseNet variant of CNN to recognize handwritten Kannada characters. DenseNet is preferred in this research for its known advantages such as enhanced feature propagation, improved feature reuse, and minimized vanishing gradient problem. The dataset used in the experimentation is a standard Char74k dataset. The prime objective of this research is to devise a machine learning based application to recognize Kannada handwritten characters with high accuracy and convert them into digitized characters. Digitized documents promote the growth of several other major applications like speech conversion, language translation and conversion of medieval documents. A testing accuracy of 93.87% is observed for 3285 images of handwritten Kannada characters with 5 images from each of the 657 classes. This machine learning model can also be trained to recognize characters of different Indian languages.","","979-8-3503-2379-5","10.1109/ICAISC58445.2023.10199168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10199168","Handwritten Character Recognition;Kannada;Convolution Neural Network;DenseNet;Machine Learning","Handwriting recognition;Symbols;Machine learning;Character recognition;Convolutional neural networks;Standards;Testing","convolutional neural nets;deep learning (artificial intelligence);document image processing;feature extraction;handwritten character recognition;image classification;natural language processing;text detection","CNN;convolutional neural network;densely connected-convolutional networks;DenseNet variant;digitized characters;digitized handwritten document;Dravidian languages;handwritten text;Indian languages;Kannada handwritten character recognition system;language translation;machine learning approach;minimized vanishing gradient problem;multiple regional languages;standard Char74k dataset","","","","19","IEEE","9 Aug 2023","","","IEEE","IEEE Conferences"
"eyeSay: Make Eyes Speak for ALS Patients with Deep Transfer Learning-empowered Wearable","J. Zou; Q. Zhang","Purdue School of Engineering and Technology, Indiana University-Purdue University at Indianapolis; Purdue School of Engineering and Technology, Indiana University-Purdue University at Indianapolis","2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","9 Dec 2021","2021","","","377","381","Eye dynamics, a typical expression of brain activities, is an emerging modality for emerging and promising smart health applications. Electrooculogram (EOG) ‚Äì a natural bio-electric signal generated during eye movements, if decoded, is of great potential to reveal the user‚Äôs mind and enable voice-free communication for patients with amyotrophic lateral sclerosis (ALS). ALS patients usually lose physical movement abilities including speech and handwriting but fortunately can move their eyes. In this study, we propose a novel deep transfer learning-empowered system, called ""eyeSay"", which leverages both deep learning and transfer learning for intelligent eye EOG-to-speech translation. More specifically, we have designed a multi-stage convolutional neural network (CNN) to analyze the eye-written words, named as CNN-word. Moreover, to reveal fundamental patterns of eye movements, we build a transferable feature extractor, CNN-stroke, upon eye strokes that are building components of an eye word. Then, we transfer the CNN-stroke model to the eye word learning task in an innovative way, that is, use CNN-stroke as an additional branch of CNN-word to generate a stroke probability map. The achieved boostCNN-word model, enhanced by the transferable feature extractor, has greatly improved the eye word decoding performance. This novel study will directly contribute to voice-free communications for ALS patients, and greatly advance the ubiquitous eye EOG-based smart health area.","2694-0604","978-1-7281-1179-7","10.1109/EMBC46164.2021.9629874","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629874","Smart Health;Deep Learning;Transfer Learning;Electrooculography;Amyotrophic Lateral Sclerosis","Solid modeling;Wheelchairs;Smart healthcare;Transfer learning;Virtual reality;Speech recognition;Feature extraction","brain;convolutional neural nets;deep learning (artificial intelligence);diseases;electro-oculography;eye;feature extraction;handicapped aids;learning (artificial intelligence);medical signal processing","eyeSay;ALS patients;eye dynamics;brain activities;smart health applications;natural bio-electric signal;eye movements;voice-free communication;amyotrophic lateral sclerosis;physical movement abilities including speech;deep transfer learning-empowered system;deep learning;intelligent eye EOG-to-speech translation;multistage convolutional neural network;eye-written words;CNN-word;transferable feature extractor;eye strokes;CNN-stroke model;eye word learning task;stroke probability map;eye word decoding performance;ubiquitous eye EOG-based smart health area;boostCNN-word model;deep transfer learning-empowered wearable","Amyotrophic Lateral Sclerosis;Electrooculography;Eye Movements;Humans;Machine Learning;Wearable Electronic Devices","4","","17","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"A Novel Approach in the Automatic Generation of Regional Language Subtitles for Videos in English","B. T. R; N. P. Chinnari; P. Hadadi; S. Shanbhogh; V. R. B. Prasad","Computer Science and Enginnering, PES University, Banglore, India; Computer Science and Enginnering, PES University, Banglore, India; Computer Science and Enginnering, PES University, Banglore, India; Computer Science and Enginnering, PES University, Banglore, India; Computer Science and Enginnering, PES University, Banglore, India","2023 IEEE 8th International Conference for Convergence in Technology (I2CT)","23 May 2023","2023","","","1","6","The usage of videos for communication has increased drastically in recent years. Non-native language speakers and those with hearing impairments, on the other hand, are unable to use this strong medium. Subtitles translate the speech to text and hence it makes the video available for a wider audience which includes those who don‚Äôt understand the language and people with hearing problems. With the rapid development of e-Learning technology, geographical boundaries are no longer considered as a major barrier in learning. However, language becomes a barrier in the wave of knowledge globalization. Subtitles in a regional language like Kannada helps to understand concepts better. The major goal of designing the system is to give an automated method for creating Kannada subtitles for videos.","","979-8-3503-3401-2","10.1109/I2CT57861.2023.10126282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126282","Subtitles;Speech Recognition;Vosk;Spleeter;Support Vector Machine;Translation;Media player","Support vector machines;Electronic learning;Globalization;Auditory system;Speech recognition;Synchronization;Videos","computer aided instruction;handicapped aids;hearing;linguistics;natural language processing","automatic generation;e-Learning technology;english;geographical boundaries;hearing impairments;hearing problems;Kannada subtitles;nonnative language speakers;regional language subtitles;strong medium;wider audience","","","","20","IEEE","23 May 2023","","","IEEE","IEEE Conferences"
"A Robot-based Arabic Sign Language Translating System","D. A. Alabbad; N. O. Alsaleh; N. A. Alaqeel; Y. A. Alshehri; N. A. Alzahrani; M. K. Alhobaishi","Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia; Computer Engineering Department, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia","2022 7th International Conference on Data Science and Machine Learning Applications (CDMA)","23 Mar 2022","2022","","","151","156","Services provided to deaf people in the Eastern province of Saudi Arabia were evaluated, which confirmed a high need to support the deaf community. This paper proposes utilizing the Pepper robot in the task of recognizing and translating Arabic sign language (ArSL), by which the robot recognizes static hand gestures of the letters in ArSL from each keyframe extracted from the input video and translate it into written text and vice versa. This project aims to conduct a two-way translation of the Arabic sign language in a way that fulfills the communication gap found in Saudi Arabia among deaf and non-deaf people. The methods proposed in this paper are computer vision to use the pepper robot's camera and sensors, Natural language processing to convert natural speech to sign language and Deep learning to build a convolutional neural network model that classifies the sign language gestures and convert them into their corresponding written and spoken form. Moreover, two datasets were used, first one is a collection of hand gestures for training the model and the other one is 39 animated signs of all the Arabic letters and special letters.","","978-1-6654-1014-4","10.1109/CDMA54072.2022.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736369","computer vision;Arabic sign language;Pepper robot;Convolutional neural network;Natural language processing","Training;Text recognition;Robot vision systems;Gesture recognition;Speech recognition;Assistive technologies;Sensors","computer vision;feature extraction;gesture recognition;handicapped aids;language translation;learning (artificial intelligence);natural language processing;neural nets;sign language recognition","robot-based Arabic sign language translating system;deaf people;Eastern province;Saudi Arabia;deaf community;Pepper robot;ArSL;static hand gestures;written text;two-way translation;nondeaf people;Natural language processing;sign language gestures;39 animated signs;Arabic letters","","","","20","IEEE","23 Mar 2022","","","IEEE","IEEE Conferences"
"Automatic visual to tactile translation. I. Human factors, access methods and image manipulation","T. P. Way; K. E. Barner","Department of Electrical Engineering, University of Delaware, Newark, DE, USA; Alfred I. duPont Institute, Applied Science and Engineering Laboratories, University of Delaware, Newark, DE, USA","IEEE Transactions on Rehabilitation Engineering","6 Aug 2002","1997","5","1","81","94","This is the first part of a two-part paper that motivates and evaluates a method for the automatic conversion of images from visual to tactile form. In this part, a broad-ranging background is provided in the areas of human factors, including the human sensory system, tactual perception and blindness, access technology for tactile graphics production, and image processing techniques and their appropriateness to tactile image creation. In Part II, this background is applied in the development of the TACTile Image Creation System (TACTICS), a prototype for an automatic visual-to-tactile translator. The results of an experimental evaluation are then presented and discussed, and possible future work in this area is outlined.","1558-0024","","10.1109/86.559353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559353","","Human factors;Pervasive computing;Image converters;Computer displays;Blindness;Graphics;Image processing;Internet;Speech synthesis;Graphical user interfaces","mechanoception;human factors;handicapped aids;vision defects;image representation;computer graphics;computer vision;visual perception","automatic visual to tactile translation;human factors;access methods;image manipulation;tactile form;broad-ranging background;human sensory system;tactual perception;blindness;access technology;tactile graphics production;image processing techniques;tactile image creation;TACTile Image Creation System;TACTICS","Algorithms;Blindness;Humans;Image Processing, Computer-Assisted;Man-Machine Systems;Sensory Aids;Software;Touch;User-Computer Interface;Visual Perception","75","6","","IEEE","6 Aug 2002","","","IEEE","IEEE Journals"
"A novel approach to identify polysemy words in Indian regional language","C. Prasad; J. S. Kallimani","Department of Computer Science and Engineering, M S Ramaiah Institute of Technology, Bangalore, India; Department of Computer Science and Engineering, M S Ramaiah Institute of Technology, Bangalore, India","2016 International Conference on Electrical, Electronics, Communication, Computer and Optimization Techniques (ICEECCOT)","26 Jun 2017","2016","","","179","182","There is a tremendous growth in the number of Internet users every day. These users are spread all over the globe belonging to different community speaking different languages. India being a multilingual country has more than six crores people speaking Kannada (south Indian regional) language. There is demand for many applications to be effective to solve problems related to native languages. In this paper, a method to extract various meanings of the polysemy words in Kannada is proposed. Identification of correct meaning of a polysemy word based on the context is very important in abstractive text summarization. The algorithm takes the word as an input and performs pattern matching with the words available in the dictionary and finally displays the result. This idea will be initially developed as a normal application later an attempt will be made to deploy it on the cloud and to make a cross-platform mobile application so that it can be used in any device that the user is comfortable with.","","978-1-5090-4697-3","10.1109/ICEECCOT.2016.7955210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955210","Polysemy words;Regional Languages;Automatic Text Summarization;Abstractive Summary;Parts of Speech (POS) Tagging;Word Sense Disambiguation (WSD)","Context;Dictionaries;Classification algorithms;Support vector machines;Bayes methods;Tagging;Algorithm design and analysis","cloud computing;language translation;mobile computing;natural language processing;pattern matching;text analysis","polysemy word identification;Internet users;multilingual country;Kannada language;south Indian regional language;native languages;polysemy word extraction;abstractive text summarization;pattern matching;cloud;cross-platform mobile application","","","","7","IEEE","26 Jun 2017","","","IEEE","IEEE Conferences"
"PHTI: Pashto Handwritten Text Imagebase for Deep Learning Applications","I. Hussain; R. Ahmad; S. Muhammad; K. Ullah; H. Shah; A. Namoun","Department of Computer Science and Information Technology, University of Malakand, Khyber Pakhtunkhawa, Pakistan; Department of Computer Science, Shaheed Benazir Bhutto University (SBBU), Sheringal, Upper Dir, Khyber Pakhtunkhawa, Pakistan; Department of Computer Science, Shaheed Benazir Bhutto University (SBBU), Sheringal, Upper Dir, Khyber Pakhtunkhawa, Pakistan; Department of Software Engineering, University of Malakand (UOM), Khyber Pakhtunkhawa, Pakistan; Department of Computer Science, King Khalid University, Abha, Saudi Arabia; Faculty of Computer Science and Information Systems, Islamic University of Madinah, Madinah, Saudi Arabia","IEEE Access","2 Nov 2022","2022","10","","113149","113157","Document Image Analysis (DIA) is one of the research areas of Artificial Intelligence (AI) that converts document images into machine-readable codes. In DIA systems, Optical Character Recognition (OCR) plays a key role in digitizing document images. The output of an OCR system is further used in many applications including, Natural Language Processing (NLP), Sentiment Analysis, Speech Recognition, and Translation Services. However, standard datasets are an essential requirement for the development, evaluation and comparison of different text recognition techniques. Pashto is one of such low resource languages that lacks availability regarding standard dataset of handwritten text. This paper therefore, addresses the unavailability of standard dataset for the Pashto handwritten text by developing a dataset named Pashto Handwritten Text Imagebase (PHTI). The PHTI is created by collecting handwritten samples from diverse genre of the Pashto language including poetry, religion, short stories, articles, novels, sports, culture and news. The dataset consists of 4,000 scanned images, written by 400 writers including 200 males and 200 females. These 4,000 images are further segmented into 36,082 text-line images. Each text-line image is annotated/ transcribed with UTF-8 codecs. The dataset can be used for many deep learning-based applications including, text recognition, skew detection, gender classification and age-groups classification.","2169-3536","","10.1109/ACCESS.2022.3216881","Deanship of Scientific Research at King Khalid University through Large Group(grant numbers:RGP 2/212/1443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928191","Artificial intelligence;document image analysis;handwritten text;natural language processing;optical character recognition;speech recognition;Pashto;standard dataset","Optical character recognition;Text recognition;Handwriting recognition;Artificial intelligence;Writing;Text analysis;Image segmentation;Natural language processing;Speech recognition","document image processing;feature extraction;handwriting recognition;handwritten character recognition;image segmentation;learning (artificial intelligence);natural language processing;natural languages;optical character recognition;speech recognition;text analysis","PHTI;Pashto Handwritten Text Imagebase;deep learning applications;Document Image Analysis;machine-readable codes;DIA systems;Optical Character Recognition;digitizing document images;OCR system;Natural Language Processing;Sentiment Analysis;Speech Recognition;standard dataset;different text recognition techniques;low resource languages;handwritten samples;Pashto language including poetry;36 text-line images;082 text-line images;text-line image;deep learning-based applications","","4","","43","CCBY","25 Oct 2022","","","IEEE","IEEE Journals"
"Two view motion analysis under a small perturbation","Xinhua Zhuang; R. Haralick","Machine Vision International, Ann Arbor, MI, USA; NA","ICASSP '85. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1985","10","","929","932","Given a set of corresponding points from a moving object is two perspective projection images. The paper completely solves the two view motion problem. We show how to use the corresponding point set to determine mode of motion, rotation, translation orientation and relative depths. Also we give a noise robust algorithm which works well under small perturbations.","","","10.1109/ICASSP.1985.1168240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1168240","","Motion analysis;Acoustic noise;Noise robustness;Symmetric matrices;Machine vision;Robot kinematics;Robot vision systems;Differential equations;Hydrogen","","","","","","6","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Hastha: Online Learning Platform for Hearing Impaired","D. Wanasinghe; C. Maddugoda; H. Ramawickrama; T. Munasinghe; L. Abeywardhana; Y. Mallawarachchi","Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information technology, Colombo, Sri Lanka","2022 22nd International Conference on Advances in ICT for Emerging Regions (ICTer)","25 Jan 2023","2022","","","130","135","Sign language is the primary means of communication for the hearing-impaired community. Introducing a learning platform can result in many ways to make learning more accessible for the hearing-impaired community of Sri Lanka. Although many approaches are being made to build such systems, the learning platform ‚ÄúHastha‚Äù aims to provide a more interactive outcome with a component that converts Youlhbe videos to sign language and a Chatbot component that acts as an intermediary between a hearing-impaired user and a Google Search Engine. Furthermore, it includes a game-based learning platform and a gesture translation component from Sri Lankan to American Sign Language while the results are displayed to the users in the form of an animation. The proposed methodology is achieved by using Natural Language Processing, speech recognition, and machine learning techniques. This web-based application enables increased interaction between the student and the system making it an effective learning environment for the hearing impaired.","2472-7598","979-8-3503-4613-8","10.1109/ICTer58063.2022.10024093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024093","Sri Lankan Sign Language;online learning platform;Natural Language processing;Chatbot","Learning systems;Video on demand;Gesture recognition;Auditory system;Speech recognition;Machine learning;Assistive technologies","computer aided instruction;handicapped aids;Internet;learning (artificial intelligence);natural language processing;search engines;speech recognition","American Sign Language;Chatbot component;effective learning environment;gesture translation component;hearing-impaired community;hearing-impaired user;learning platform Hastha;machine learning techniques;Natural Language Processing;online learning platform;Sri Lanka","","1","","13","IEEE","25 Jan 2023","","","IEEE","IEEE Conferences"
"AIB's Gesture Language Convertor","M. Iswarya; B. P. C; A. V S","Department of Computer Science and Engineering, Sri Sairam Engineering College, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Sri Sairam Engineering College, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Sri Sairam Engineering College, Chennai, Tamil Nadu, India","2022 1st International Conference on Computational Science and Technology (ICCST)","14 Feb 2023","2022","","","355","358","In recent days the requirement to communicate with a character audibly and freely may be a basic need and should be on the market to which will assist for deaf and dumb persons. Sign language is sometimes the only form of communication used by persons with hearing and speech disabilities. Because of this, communication between the deaf, dumb, and normal people is seriously affected. The likelihood of learning sign language is not very high among those who speak vocal languages. The goal of this project is to create an interpreter that can translate spoken language into text or speech that can be understood as a sentence utilizing CNN's segmentation of the various sign languages into their English alphabets. This project possesses the ability to sight the alphabetic sentence. This project conjointly proposes inclusion of action gif video (from deaf and dumb) to text model (normal people) and vice versa so as to facilitate seamless two way approach communication.","","978-1-6654-7655-3","10.1109/ICCST55948.2022.10040426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040426","Deep Learning;Convolutional Neural Network (CNN);Gesture recognition;Indian Sign Language (ISL)","Computer science;Visualization;Three-dimensional displays;Machine learning algorithms;Scientific computing;Neural networks;Gesture recognition","gesture recognition;handicapped aids;language translation;natural language processing;sign language recognition;text analysis","AIB's gesture language convertor;deaf people;deaf persons;dumb persons;dumb, people;hearing speech disabilities;seamless two way approach communication;sign language;text model;vocal languages","","","","14","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Sentence Correction Incorporating Relative Position and Parse Template Language Models","C. -H. Wu; C. -H. Liu; M. Harris; L. -C. Yu","Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Information Management, Yuan-Ze University, Chungli, Taiwan","IEEE Transactions on Audio, Speech, and Language Processing","16 Aug 2010","2010","18","6","1170","1181","Sentence correction has been an important emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common errors in sentences produced by second language learners. In this paper, a relative position language model and a parse template language model are proposed to complement traditional language modeling techniques in addressing this problem. A corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a state-of-the-art phrase-based statistical machine translation system, the error correction performance of the proposed approach achieves a significant improvement using human evaluation.","1558-7924","","10.1109/TASL.2009.2031237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226606","Computer-Assisted Language Learning (CALL);error correction;language transfer;parse template;relative position","Natural languages;Error correction;Computer errors;Humans;Chaos;Robustness;System testing;Councils;Computer science;Information management","computer aided instruction;grammars;language translation;natural language processing;statistical analysis","sentence correction;parse template language models;computer assisted language learning;grammar rules;statistical machine translation;English Chinese language","","15","","34","IEEE","28 Aug 2009","","","IEEE","IEEE Journals"
"Investigating the role of machine translated text in ASR domain adaptation: Unsupervised and semi-supervised methods","H. Cucu; L. Besacier; C. Burileanu; A. Buzo","LIG, Joseph Fourier University, Grenoble, France; LIG, Joseph Fourier University, Grenoble, France; ETTI, University Politehnica of Bucharest, Bucharest, Romania; ETTI, University Politehnica of Bucharest, Bucharest, Romania","2011 IEEE Workshop on Automatic Speech Recognition & Understanding","5 Mar 2012","2011","","","260","265","This study investigates the use of machine translated text for ASR domain adaptation. The proposed methodology is applicable when domain-specific data is available in language X only, whereas the goal is to develop a domain-specific system in language Y. Two semi-supervised methods are introduced and compared with a fully unsupervised approach, which represents the baseline. While both unsupervised and semi-supervised approaches allow to quickly develop an accurate domain-specific ASR system, the semi-supervised approaches overpass the unsupervised one by 10% to 29% relative, depending on the amount of human post-processed data available. An in-depth analysis, to explain how the machine translated text improves the performance of the domain-specific ASR, is also given at the end of this paper.","","978-1-4673-0367-5","10.1109/ASRU.2011.6163941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163941","","Adaptation models;Domain specific languages;Interpolation;Dictionaries;Google;Training;Hidden Markov models","language translation;text analysis","machine translated text;ASR domain adaptation;unsupervised method;semi-supervised method;domain-specific system","","9","3","21","IEEE","5 Mar 2012","","","IEEE","IEEE Conferences"
"Automatic detection of English words in Benglish text: A statistical approach","B. Kundu; S. Chandra","Language Technology, Centre for Development of Advanced Computing, Kolkata, India; Language Technology, Centre for Development of Advanced Computing, Kolkata, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","21 Mar 2013","2012","","","1","4","Code-mixing and code-switching create challenges in the field of natural language processing applications like Machine Translation and Speech-to-Speech Translation. Detection of foreign words is very much essential for smooth processing of natural language. A statistical language independent approach for automatic detection of foreign words in mixed language has been introduced in this paper. Initially, the proposed approach has been applied on Benglish text which is combination of Bangla text contains English words. The methodology can be easily adopted for other languages where such code mixing exists. The proposed approach yields an accuracy of 71.82% when tested on sentences collected from Bangla blogs and social networking websites.","","978-1-4673-4369-5","10.1109/IHCI.2012.6481827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481827","Foreign phrase fusion;Foreign Inclusion;Code Mixing;Code Switching;Mixed Lingua;Foreign Words in Bangla","Switches;Joints;Pragmatics;Blogs;Social network services;Grammar;Dictionaries","natural language processing;social networking (online);statistical analysis;text analysis","automatic English words detection;Benglish text;code-mixing;natural language processing applications;machine translation;speech-to-speech translation;smooth natural language processing;statistical language independent approach;automatic foreign words detection;mixed language;Bangla text;Bangla blogs;social networking Web sites","","2","","7","IEEE","21 Mar 2013","","","IEEE","IEEE Conferences"
"Audio Example Recognition and Retrieval Based on Geometric Incremental Learning Support Vector Machine System","L. Fan","School of Statistics, Capital University of Economics and Business, Beijing, China","IEEE Access","6 May 2020","2020","8","","78630","78638","With the fast development of computer and information technology, multimedia data has become the most important form of information media. Auditory information plays an important role in information location, this comes from the fact that it can be difficult to find useful information. Thus audio classification becomes more important in audio analysis as it prepares for content-based audio retrieval. There is quite a bit of research on the topic of audio classification methods, audio feature analysis, and extraction based on audio classification. Many works of literature extract features of audio signals based on time or Fourier transform frequency domain. The emergence of the wavelet theory provides a time-frequency analysis tool for signal analysis. Wavelet transformation is a local transformation of the signal in time and frequency which can effectively extract information from the signal, and perform multi-scale refinement analysis on functions or signals through operations such as stretching and translation instead of the traditional Fourier transformation. In the time-frequency analysis of the signal, the wavelet analysis captures the local time and frequency characters of the signal which can improve the ability of signal analysis. It can also change certain locals of the signal without affecting other aspects of it. In this paper, the frequency domain features are combined with the wavelet domain features. At the same time that the MFCC features are extracted, the discrete wavelet transform is used to extract the features of the wavelet domain. Then the statistical features are extracted for each audio example, and the SVM model is used to realize the different forms of audio classification identification.","2169-3536","","10.1109/ACCESS.2020.2988686","Science and Technology Granted by the Beijing Education Committee(grant numbers:KM201810038002); Key Members of the Outstanding Young Teacher by the Capital University of Economics and Business 2017; Top Talent of Youth Teacher by the Beijing Education Committee 2018; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072167","Content audio;wavelet transform;audio feature;audio processing","Support vector machines;Feature extraction;Training;Time-frequency analysis;Artificial neural networks;Speech recognition;Fans","audio signal processing;discrete wavelet transforms;feature extraction;Fourier transforms;learning (artificial intelligence);signal classification;support vector machines;time-frequency analysis","audio example recognition;geometric incremental learning support vector machine system;multimedia data;important form;information media;auditory information;information location;audio analysis;content-based audio retrieval;audio classification methods;audio feature analysis;audio signals;wavelet theory;time-frequency analysis tool;signal analysis;wavelet transformation;local transformation;multiscale refinement analysis;traditional Fourier transformation;wavelet analysis;local time;frequency characters;frequency domain features;wavelet domain features;MFCC features;statistical features;audio classification identification","","6","","32","CCBY","20 Apr 2020","","","IEEE","IEEE Journals"
"GBC: An Energy-Efficient LSTM Accelerator With Gating Units Level Balanced Compression Strategy","B. Wu; Z. Wang; K. Chen; C. Yan; W. Liu","School of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Circuits and Systems I: Regular Papers","29 Aug 2022","2022","69","9","3655","3665","Recurrent Neural Networks (RNNs) have emerged as one of the most popular neural networks for processing time-series problems, widely used in machine translation, automatic speech recognition, and other natural language processing applications. However, conventional RNNs suffered from vanishing and exploding gradients, resulting in poor network performance in applications with long-term input information. As a variant of RNN, Long Short-Term Memory (LSTM) had been proposed to tackle this issue. Nevertheless, at the same time, LSTM introduces gating units and many additional parameters, which makes it challenging to be implemented directly on resource-limited platforms, such as Field Programmable Gate Arrays (FPGAs). This work first investigated the overall maximum achievable compression rates of different gating units and their correlations. Then, Gating Units Level Balanced Compression (GBC) strategy is proposed. After Top- $k$  pruning, the proposed GBC strategy can attain a compression rate of  $36.6\times $  for LSTM. Further, the theoretical analysis indicates that for the existing gating units level LSTM compression variants, the GBC strategy still has further potential for compression. A complementary compression of the GBC strategy is performed on the existing coupled-gate LSTM to verify the analysis. Experimental results show that GBC achieves an additional  $32\times $  (overall  $42.7\times $ ) compression rate with negligible accuracy loss. Finally, hardware experiments conducted on Xilinx ADM-PCIE-7V3 FPGAs also demonstrate that the accelerator designed in this paper achieves an improvement of 7.4%-191.5% in energy efficiency compared to the state-of-the-art designs.","1558-0806","","10.1109/TCSI.2022.3181975","Natural Science Foundation of Jiangsu Province(grant numbers:BK20210286); National Natural Science Foundation of China(grant numbers:61704005,62022041); Fundamental Research Funds for the Central Universities(grant numbers:NP2022103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803872","RNN;LSTM;neural network compression;FPGAs","Field programmable gate arrays;Recurrent neural networks;Logic gates;Correlation;Compression algorithms;Task analysis;Machine translation","data compression;field programmable gate arrays;recurrent neural nets;speech recognition","energy-efficient LSTM accelerator;recurrent neural networks;automatic speech recognition;long-term input information;long short-term memory;field programmable gate arrays;compression rates;gating units;GBC strategy;compression rate;LSTM compression variants;complementary compression;energy efficiency;gating unit level balanced compression strategy;coupled-gate LSTM;top-k pruning;GBC;Xilinx ADM-PCIE-7V3 FPGA","","3","","36","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"Natural Language Processing","A. C. -C. Liu; O. M. K. Law; I. Law",NA; NA; NA,"Understanding Artificial Intelligence: Fundamentals and Applications","","2022","","","29","37","The natural language processing (NLP) market grows rapidly in recent years. However, NLP faces different levels of ambiguity challenges they are word‚Äêlevel ambiguity, sentence‚Äêlevel ambiguity, and meaning‚Äêlevel ambiguity. Various neural networks are used for different NLP applications, including convolutional neural networks (CNN), recurrent neural networks (RNN), recursive neural networks, and reinforcement learning (RL) models. The CNN is targeted for word‚Äêbased prediction and other language tasks, such as name entity recognition, part of speech, and aspect detection. The RNN is useful to process the context‚Äêdependent human language because the words are related to each other in the sentence. The recursive neural networks are applied for parsing, sentimental analysis, and sentence relatedness. The RL is useful for dialogue generation. The chapter lists several interesting NLP applications, virtual assistant, language translation, and machine transcription. Machine transcription can automate the clinical documenting process.","","9781119858348","10.1002/9781119858393.ch4","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9881925.pdf&bkn=9880910&pdfType=chapter","","Logic gates;Natural language processing;Convolutional neural networks;Recurrent neural networks;Task analysis;Machine translation;Training","","","","","","","","8 Sep 2022","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Efficient natural language pre-processing for analyzing large data sets","B. Billal; A. Fonseca; F. Sadat","Department of Computer Science, University of Quebec in Montreal, Montreal, Canada; Department of Computer Science, University of Quebec in Montreal, Montreal, Canada; Department of Computer Science, University of Quebec in Montreal, Montreal, Canada","2016 IEEE International Conference on Big Data (Big Data)","6 Feb 2017","2016","","","3864","3871","The phenomenon of big data is described using five Vs: Volume, Variety, Velocity, Variability and Veracity. In this paper, we are interested by analyzing and pre-processing tweets for NLP and machine learning applications such as machine translation and classification. Collected contents from Twitter (tweets) are considered as unstructured, highly noisy and short (140 characters) texts. Overcoming these complex challenges will help learn from such data and apply traditional NLP and machine learning techniques. In this paper, we propose a pre-processing pipeline for tweets consisting of filtering part-of-speech, named entities recognition, hashtag segmentation and disambiguation. Our proposed approach is also based on the graph theory and group words of tweets using semantic relations of WordNet and the idea of connected components. Evaluations on the task of classification showed promising results when using this proposed preprocessing pipeline, with an increase in the accuracy of the classification up to 87.6%.","","978-1-4673-9005-7","10.1109/BigData.2016.7841060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841060","Natural Language Processing;big data;tweets;Twitter;named entities;hashtag segmentation","Twitter;Tagging;Big data;Semantics;Pipelines;Feature extraction","Big Data;classification;language translation;learning (artificial intelligence);natural language processing;social networking (online)","natural language preprocessing;large data sets;Big Data;NLP;machine learning applications;machine translation;classification;Twitter;tweets;part-of-speech;named entities recognition;hashtag segmentation;graph theory;WordNet","","4","","25","IEEE","6 Feb 2017","","","IEEE","IEEE Conferences"
"Construction of a Japanese-Chinese Bilingual Corpus for Learning Japanese Sentence Patterns through Multi-Level Annotation","J. Liu; L. Zhuang","School of Foreign Languages and Literatures, Guangxi University, Nanning, China; School of Foreign Languages and Literatures, Guangxi University, Nanning, China","2022 3rd International Conference on Pattern Recognition and Machine Learning (PRML)","13 Sep 2022","2022","","","321","327","This work introduces the construction of a Japanese-Chinese bilingual corpus, aiming to help Chinese-speaking learners of Japanese as a second language (JSL) with their study of Japanese sentence patterns in Japanese grammar. For constructing this Japanese-Chinese bilingual corpus, methods of automatic identification of the Japanese sentence patterns were proposed by annotating a dataset on multiple levels for training a model based on Conditional Random Fields (CRF) machine learning algorithm. The multi-level annotation performed is Part-of-speech (POS) tagging, which includes the detailed information, such as surface form, dictionary form, pronunciation, difficulty level and etc. The well-trained model is applied to automatic identification and suggestion of the Japanese sentence patterns with different difficulty levels according to Japanese Language Proficiency Test (JLPT). We evaluated the model on a test dataset in order to show the effectiveness of our proposed methods. We also show how we used this model to construct a Japanese-Chinese bilingual corpus, and manually corrected the example sentences where the Japanese sentence patterns were identified incorrectly or not identified. This model can be effectively used for the Japanese language processing framework, which allows for a fast and easy application for the problem of Japanese grammar extraction. Moreover, the constructed Japanese-Chinese bilingual corpus can be applied as a useful computer-assisted language learning (CALL) tool for learning Japanese sentence patterns.","","978-1-6654-9950-7","10.1109/PRML56267.2022.9882244","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882244","Japanese sentence pattern;bilingual corpus;corpus annotation;linguistic resource;natural language processing;computer-assisted language learning","Training;Machine learning algorithms;Dictionaries;Annotations;Computational modeling;Machine learning;Tagging","computer aided instruction;grammars;language translation;learning (artificial intelligence);linguistics;natural language processing;natural languages;text analysis","Japanese-Chinese bilingual corpus;Japanese sentence patterns;multilevel annotation;Japanese Language Proficiency Test;Japanese language processing framework;Japanese grammar extraction","","","","38","IEEE","13 Sep 2022","","","IEEE","IEEE Conferences"
"An Augmented Transformer Architecture for Natural Language Generation Tasks","H. Li; A. Y. C. Wang; Y. Liu; D. Tang; Z. Lei; W. Li","Hong Kong Applied Science and Technology Research Institute Company Limited, Hong Kong, China; Hong Kong Applied Science and Technology Research Institute Company Limited, Hong Kong, China; Hong Kong Applied Science and Technology Research Institute Company Limited, Hong Kong, China; Hong Kong Applied Science and Technology Research Institute Company Limited, Hong Kong, China; Hong Kong Applied Science and Technology Research Institute Company Limited, Hong Kong, China; Chinese University of Hong Kong, Shenzhen, China","2019 International Conference on Data Mining Workshops (ICDMW)","5 Mar 2020","2019","","","1","7","The Transformer based neural networks have been showing significant advantages on most evaluations of various natural language processing and other sequence-tosequence tasks due to its inherent architecture based superiorities. Although the main architecture of the Transformer has been continuously being explored, little attention was paid to the positional encoding module. In this paper, we enhance the sinusoidal positional encoding algorithm by maximizing the variances between encoded consecutive positions to obtain additional promotion. Furthermore, we propose an augmented Transformer architecture encoded with additional linguistic knowledge, such as the Part-of-Speech (POS) tagging, to boost the performance on some natural language generation tasks, e.g., the automatic translation and summarization tasks. Experiments show that the proposed architecture attains constantly superior results compared to the vanilla Transformer.","2375-9259","978-1-7281-4896-0","10.1109/ICDMW48858.2019.9024754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9024754","Natural language processing;Neural machine translation;Sequence-to-sequence;Temporal dynamics;Transformer attention model","Encoding;Task analysis;Tagging;Computer architecture;Natural language processing;Decoding","linguistics;natural language processing;neural nets","architecture based superiorities;augmented transformer architecture;summarization tasks;automatic translation;encoded consecutive positions;sinusoidal positional encoding algorithm;positional encoding module;sequence-tosequence tasks;natural language processing;neural networks;natural language generation tasks;vanilla transformer","","6","","","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Towards Robust Visual Transformer Networks via K-Sparse Attention","S. Amini; S. Ghaemmaghami","Electronics Research Institute, Sharif University of Technology; Electronics Research Institute, Sharif University of Technology","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","4053","4057","Transformer networks, originally developed in the community of machine translation to eliminate sequential nature of recurrent neural networks, have shown impressive results in other natural language processing and machine vision tasks. Self-attention is the core module behind visual transformers which globally mixes the image information. This module drastically reduces the intrinsic inductive bias imposed by CNNs, such as locality, while encountering insufficient robustness against some adversarial attacks. In this paper we introduce K-sparse attention to preserve low inductive bias, while robustifying transformers against adversarial attacks. We show that standard transformers attend values with dense set of weights, while the sparse attention, automatically selected by an optimization algorithm, can preserve generalization performance of the transformer and, at the same time, improve its robustness.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746916","Visual transformer;self-attention;sparse;adversarial robustness","Visualization;Signal processing algorithms;Transformer cores;Signal processing;Transformers;Robustness;Task analysis","computer vision;convolutional neural nets;generalisation (artificial intelligence);learning (artificial intelligence);optimisation","K-sparse attention;machine translation;recurrent neural networks;natural language processing;machine vision;image information;intrinsic inductive bias;adversarial attacks;inductive bias;robust visual transformer network;CNN;optimization algorithm;generalization performance","","","","23","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network","X. Bai; P. Liu; Y. Zhang","Zhejiang University, Hangzhou, China; Machine Intelligence and Translation Laboratory, School of Computer Science of Technology, Harbin Institute of Technology, Harbin, China; School of Engineering, Westlake University, and also with the Institute of Advanced Technology, Westlake Institute for Advanced Study, Hangzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Jan 2021","2021","29","","503","514","Targeted sentiment classification predicts the sentiment polarity on given target mentions in input texts. Dominant methods employ neural networks for encoding the input sentence and extracting relations between target mentions and their contexts. Recently, graph neural network has been investigated for integrating dependency syntax for the task, achieving the state-of-the-art results. However, existing methods do not consider dependency label information, which can be intuitively useful. To solve the problem, we investigate a novel relational graph attention network that integrates typed syntactic dependency information. Results on standard benchmarks show that our method can effectively leverage label information for improving targeted sentiment classification performances. Our final model significantly outperforms state-of-the-art syntax-based approaches.","2329-9304","","10.1109/TASLP.2020.3042009","National Natural Science Foundation of China(grant numbers:61976180); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276424","Attention mechanism;dependency tree;graph neural networks;targeted sentiment analysis","Syntactics;Context modeling;Bit error rate;Task analysis;Feature extraction;Neural networks;Speech processing","graph theory;learning (artificial intelligence);neural nets;pattern classification;sentiment analysis","typed syntactic dependencies;targeted sentiment classification;graph attention neural network;sentiment polarity;given target;input texts;dominant methods;input sentence;dependency syntax;dependency label information;syntactic dependency information;sentiment classification performances;state-of-the-art syntax-based approaches;relational graph attention network","","40","","61","IEEE","2 Dec 2020","","","IEEE","IEEE Journals"
"Learning networks hyper-parameter using multi-objective optimization of statistical performance metrics","G. Torres; C. Sanchez; D. Gil","Computer Science Department, UAB Computer Vision Center, Barcelona, Spain; Computer Science Department, UAB Computer Vision Center, Barcelona, Spain; Computer Science Department, UAB Computer Vision Center, Barcelona, Spain","2022 24th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","25 May 2023","2022","","","233","238","Deep Learning has enabled remarkable progress over the last years on a several objectives, such as image and speech recognition, and machine translation. Deep neural architectures are a main contribution for this progress. Current architectures have mostly been developed manually by engineers, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. In this paper we present a strategy for the optimization of network hyper-parameters using a multi-objective Non-dominated Sorting Genetic Algorithm combined with a nested cross-validation to optimize statistical metrics of the performance of networks. In order to illustrate the proposed hyper-parameter optimization, we have applied it to a use case that uses transformers to map abstract radiomic features to specific radiological annotations. Results obtained with the LUNA16 public data base show generalization power of the proposed optimization strategy for hyper-parameter setting.","2470-881X","978-1-6654-6545-8","10.1109/SYNASC57785.2022.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130952","meta-learning;optimization;deep learning;network hyper-parameters;multi-objective;HPO","Measurement;Visualization;Annotations;Search methods;Speech recognition;Transformers;Optimization","deep learning (artificial intelligence);feature extraction;genetic algorithms;learning (artificial intelligence);medical image processing;neural net architecture;optimisation;search problems;speech recognition","automated neural architecture search methods;current architectures;Deep Learning;deep neural architectures;error-prone process;hyper-parameter optimization;hyper-parameter setting;Learning networks hyper-parameter;machine translation;map abstract radiomic features;multiobjective optimization;nested cross-validation;network hyper-parameters;optimization strategy;Sorting Genetic Algorithm;speech recognition;statistical metrics;statistical performance metrics","","","","23","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"Canonical Segmentation Using Affix Characters as a Unit on Transformer for Javanese Language","S. H. Wijono; M. R. Alhamidi; M. H. Hilman; W. Jatmiko","Department of Informatics, Faculty of Science and Technology, Sanata Dharma University, Indonesia; Faculty of Computer Science, Universitas Indonesia, Indonesia; University of Melbourne, Australia; Faculty of Computer Science, Universitas Indonesia, Indonesia","2021 6th International Workshop on Big Data and Information Security (IWBIS)","28 Dec 2021","2021","","","67","72","Morphological segmentation for agglutinative languages is the process of getting stems and affixes. Morphological segmentation is a necessary process in various NLP applications such as machine translation, question answering, and speech recognition. Several neural morphological segmentation studies have used the sequence of characters as input to encoder-decoder. However, this can not provide linguistic information. We propose affix characters as a unit to provide affixes feature on Transformer encoder-decoder. We use the Javanese word corpus which consists of affixed, canonical affixed, and non-affixed words. For affixed words, our proposed method obtains 11.2 times higher point of accuracy than the Sequence of Characters. For canonical affixed words, we get 21.9 times higher point of accuracy than the baseline method. The results also show that the use of different affix symbols, which are ‚Äú%%‚Äù, ‚Äú##‚Äù, and ‚Äú@@‚Äù for each type of affix improve accuracy in affix recognition.","","978-1-6654-2451-6","10.1109/IWBIS53353.2021.9631839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631839","canonical segmentation;affix characters as a unit;Javanese language;Transformer","Conferences;Information security;Speech recognition;Linguistics;Big Data;Transformers;Knowledge discovery","computational linguistics;natural language processing;speech recognition","canonical segmentation;affix characters;agglutinative languages;NLP applications;machine translation;question answering;speech recognition;transformer encoder-decoder;Javanese word;nonaffixed words;canonical affixed words;affix recognition;affix symbols;neural morphological segmentation","","","","27","IEEE","28 Dec 2021","","","IEEE","IEEE Conferences"
"Inference of human postures by classification of 3D human body shape","I. Cohen; H. Li","Institute for Robotics and Intelligent Systems Integrated Media Systems Center, University of Southern California, USA; Institute for Robotics and Intelligent Systems Integrated Media Systems Center, University of Southern California, USA","2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443)","27 Oct 2003","2003","","","74","81","We describe an approach for inferring the body posture using a 3D visual-hull constructed from a set of silhouettes. We introduce an appearance-based, view-independent, 3D shape description for classifying and identifying human posture using a support vector machine. The proposed global shape description is invariant to rotation, scale and translation and varies continuously with 3D shape variations. This shape representation is used for training a support vector machine allowing the characterization of human body postures from the computed visual hull. The main advantage of the shape description is its ability to capture human shape variation allowing the identification of body postures across multiple people. The proposed method is illustrated on a set of video streams of body postures captured by four synchronous cameras.","","0-7695-2010-3","10.1109/AMFG.2003.1240827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1240827","","Humans;Shape;Biological system modeling;Cameras;Intelligent robots;Support vector machines;Support vector machine classification;Speech recognition;Joints;Intelligent systems","image representation;gesture recognition;solid modelling;support vector machines;video cameras;image classification","human posture inference;3D human body shape classification;appearance-based method;view-independent method;3D shape description method;support vector machine;body posture identification;synchronous camera;video stream","","77","4","25","IEEE","27 Oct 2003","","","IEEE","IEEE Conferences"
"Semantic Analysis of Bengali Sentences","S. Khatun; M. Moshiul Hoque","Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology (CUET), Chittagong, Bangladesh; Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology (CUET), Chittagong, Bangladesh","2018 International Conference on Bangla Speech and Language Processing (ICBSLP)","2 Dec 2018","2018","","","1","6","Semantic analysis is concerning the structures and occurrences of the words in a sentence and understanding the idea of what's written in a particular sentence. This analysis plays a crucial role to represent the actual interpretation of the sentence structure and hence helps to improve the accuracy of machine translation. In this paper, we propose a semantic analyzer to perform the semantic analysis of Bengali sentences. We introduce a set of rules for composition functions to the constituent parts of a sentence with the semantic features of the word for this purpose. The system has experimented with more than 2000 Bengali sentences and the success rates in over 88%.","","978-1-5386-8207-4","10.1109/ICBSLP.2018.8554726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554726","Bengali language processing;compositional semantics;composition function;semantic features;annotated parse tree;context sensitive grammar","Semantics;Grammar;Compounds;Dictionaries;Production;Generators;Computer science","language translation;natural language processing","semantic features;semantic analysis;sentence structure;Bengali sentences","","1","","14","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Maximum A Posteriori Approximation of Hidden Markov Models for Proportional Sequential Data Modeling With Simultaneous Feature Selection","S. Ali; N. Bouguila","Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada; Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2022","2022","33","10","5590","5601","One of the pillar generative machine learning approaches in time series data study and analysis is the hidden Markov model (HMM). Early research focused on the speech recognition application of the model with later expansion into numerous fields, including video classification, action recognition, and text translation. The recently developed generalized Dirichlet HMMs have proven efficient in proportional sequential data modeling. As such, we focus on investigating a maximum a posteriori (MAP) framework for the inference of its parameters. The proposed approach differs from the widely deployed Baum‚ÄìWelch through the placement of priors that regularizes the estimation process. A feature selection paradigm is also integrated simultaneously in the algorithm. For validation, we apply our proposed approach in the classification of dynamic textures and the recognition of infrared actions.","2162-2388","","10.1109/TNNLS.2021.3071083","Fonds de recherche du Quebec‚ÄîNature et technologies (FRQNT) through the doctoral scholarship; Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420275","Compositional time series;dynamic textures;feature saliency;feature selection;hidden Markov models (HMMs);infrared actions;machine learning;video classification","Hidden Markov models;Feature extraction;Data models;Computational modeling;Speech recognition;Mathematical model;Estimation","feature extraction;hidden Markov models;learning (artificial intelligence);speech recognition;time series;video signal processing","posteriori approximation;hidden Markov model;proportional sequential data modeling;simultaneous feature selection;pillar generative machine;time series data study;speech recognition application;action recognition;recently developed generalized Dirichlet HMMs;posteriori framework;feature selection paradigm","Algorithms;Markov Chains;Neural Networks, Computer","3","","60","IEEE","30 Apr 2021","","","IEEE","IEEE Journals"
"Controllable Time-Delay Transformer for Real-Time Punctuation Prediction and Disfluency Detection","Q. Chen; M. Chen; B. Li; W. Wang","Alibaba Group, DAMO Academy; Alibaba Group, DAMO Academy; Alibaba Group, DAMO Academy; Alibaba Group, DAMO Academy","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8069","8073","With the increased applications of automatic speech recognition (ASR) in recent years, it is essential to automatically insert punctuation marks and remove disfluencies in transcripts, to improve the readability of the transcripts as well as the performance of subsequent applications, such as machine translation, dialogue systems, and so forth. In this paper, we propose a Controllable Time-delay Transformer (CT-Transformer) model that jointly completes the punctuation prediction and disfluency detection tasks in real time. The CT-Transformer model facilitates freezing partial outputs with controllable time delay to fulfill the real-time constraints in partial decoding required by subsequent applications. We further propose a fast decoding strategy to minimize latency while maintaining competitive performance. Experimental results on the IWSLT2011 benchmark dataset and an in-house Chinese annotated dataset demonstrate that the proposed approach outperforms the previous state-of-the-art models on F-scores and achieves a competitive inference speed.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053159","Punctuation prediction;disfluency detection;Transformer;multitask learning;transfer learning","Transfer learning;Predictive models;Benchmark testing;Real-time systems;Decoding;Task analysis;Speech processing","acoustic signal processing;decoding;interactive systems;speech recognition","increased applications;automatic speech recognition;punctuation marks;punctuation prediction;disfluency detection tasks;CT-Transformer model;controllable time delay;real-time constraints;controllable time-delay transformer model","","10","","35","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Towards Energy-Preserving Natural Language Understanding With Spiking Neural Networks","R. Xiao; Y. Wan; B. Yang; H. Zhang; H. Tang; D. F. Wong; B. Chen","Damo Academy, Alibaba Group, Hangzhou, China; NLP CT Lab, University of Macau, Macao, China; Damo Academy, Alibaba Group, Hangzhou, China; Damo Academy, Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China; NLP CT Lab, University of Macau, Macao, China; Damo Academy, Alibaba Group, Hangzhou, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","9 Dec 2022","2023","31","","439","447","Artificial neural networks have shown promising results in a variety of natural language understanding (NLU) tasks. Despite their successes, conventional neural-based NLU models are criticized for high energy consumption, making them laborious to be widely applied in low-power electronics, such as smartphones and intelligent terminals. In this paper, we introduce a potential direction to alleviate this bottleneck by proposing a spiking encoder. The core of our model is bi-directional spiking neural network (SNN) which transforms numeric values into discrete spiking signals and replaces massive multiplications with much cheaper additive operations. We examine our model on sentiment classification and machine translation tasks. Experimental results reveal that our model achieves comparable classification and translation accuracy to advanced Transformer baseline, whereas significantly reduces the required computational energy to 0.82%.","2329-9304","","10.1109/TASLP.2022.3221011","National Key Research and Development Program of China(grant numbers:2020AAA0105900); National Natural Science Foundation of China(grant numbers:62206188); China Postdoctoral Science Foundation(grant numbers:2022M712237); Science and Technology Development Fund, Macau SAR(grant numbers:0070/2022/AMJ); Multi-year Research Grant from the University of Macau(grant numbers:MYRG2020-00054-FST); National Key Research and Development Program of China(grant numbers:2018YFB1403202); Alibaba Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944911","Natural language processing;language model;natural language understanding;spiking neural network","Computational modeling;Task analysis;Neurons;Mathematical models;Transformers;Logic gates;Biological neural networks","language translation;neural nets;sentiment analysis","artificial neural networks;bi-directional spiking neural network;computational energy;discrete spiking signals;energy-preserving natural language understanding;high energy consumption;intelligent terminals;low-power electronics;machine translation tasks;natural language understanding tasks;neural-based NLU models;sentiment classification;spiking encoder","","1","","39","IEEE","10 Nov 2022","","","IEEE","IEEE Journals"
"An efficient algorithm for learning invariance in adaptive classifiers","P. Simard; Y. Le Cun; J. Denker; B. Victorri","AT and T Bell Laboratories, Inc., Holmdel, NJ, USA; AT and T Bell Laboratories, Inc., Holmdel, NJ, USA; AT and T Bell Laboratories, Inc., Holmdel, NJ, USA; Universit√© de Caen Basse Normandie, Caen, France","Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems","6 Aug 2002","1992","","","651","655","In many machine learning applications, one has not only training data but also some high-level information about certain invariances that the system should exhibit. In character recognition, for example, the answer should be invariant with respect to small spatial distortions in the input images (translations, rotations, scale changes, etcetera). The authors have implemented a scheme that minimizes the derivative of the classifier outputs with respect to distortion operators. This not only produces tremendous speed advantages, but also provides a powerful language for specifying what generalizations the network can perform.<>","","0-8186-2915-0","10.1109/ICPR.1992.201861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=201861","","Digital images;Machine learning;Training data;Machine learning algorithms;Character recognition;Image recognition;Speech recognition;Backpropagation algorithms;Learning systems;Smoothing methods","character recognition;image recognition;learning (artificial intelligence)","adaptive classifiers;machine learning applications;invariances;character recognition;spatial distortions;input images;classifier outputs;speed","","8","","4","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"ISLR: Indian Sign Language Recognition","K. B. Prathap; G. D. Swaroop; B. P. Kumar; V. Kamble; M. Parate","IIIT, Nagpur; IIIT, Nagpur; IIIT, Nagpur; VNIT, Nagpur; IIIT, Nagpur","2023 2nd International Conference on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal Processing (PCEMS)","2 Jun 2023","2023","","","1","6","Normal people can readily connect and communicate with one another, however, those with hearing and speech impairments find it difficult to converse with normal-hearing people without the assistance of a translator. The only way deaf and dumb people can communicate is through Sign Language. Indian Sign Language has its own grammar, syntax, vocabulary, and unique language features. We propose two methods, namely Bidirectional LSTM and BERT Transformer to address the problem of sign language translation. The proposed work is validated on standard datasets and provides promising results. The INCLUDE-50 dataset is used to validate the performance of the proposed algorithm. The deep neural network is evaluated using a combination of approaches for augmentation of the data, features extraction using the mediapipe.On the Dataset INCLUDE 50 the best performing model obtained an accuracy of 89.5%. This model employs a feature extractor that has been pre-trained, as well as an encoder and a decoder.","","979-8-3503-1071-9","10.1109/PCEMS58491.2023.10136062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136062","sign language translator;deaf and mute;hand gesture recognition;Indian Sign Language;Mediapipe;Pytorch;CNN;LSTM;Transformers","Vocabulary;Bit error rate;Signal processing algorithms;Gesture recognition;Assistive technologies;Syntactics;Feature extraction","deep learning (artificial intelligence);feature extraction;handicapped aids;language translation;natural language processing;recurrent neural nets;sign language recognition","Dataset INCLUDE 50;deep neural network;dumb people;feature extractor;features extraction;hearing speech impairments;INCLUDE-50 dataset;indian Sign Language recognition;ISLR;normal-hearing people;sign language translation;standard datasets;translator;unique language features;way","","","","32","IEEE","2 Jun 2023","","","IEEE","IEEE Conferences"
"Off-line unconstrained handwritten word recognition","Jinhai Cai; Zhi-Qiang Liu","Computer Vision and Machine Intelligence Lab, Department of Computer Science, University of Melbourne; Computer Vision and Machine Intelligence Lab, Department of Computer Science, University of Melbourne","1996 Australian New Zealand Conference on Intelligent Information Systems. Proceedings. ANZIIS 96","6 Aug 2002","1996","","","199","202","The authors describe their system for writer independent, off-line unconstrained handwritten word recognition. First, they present a new method to automatically determine the parameters of Gabor filters to extract features from slant and tilt corrected images. An algorithm is also developed to translate 2D images to 1D domain. Finally, they propose a modified dynamic programming method with fuzzy inference to recognize words. Their initial experiments have shown encouraging results.","","0-7803-3667-4","10.1109/ANZIIS.1996.573933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=573933","","Handwriting recognition;Feature extraction;Gabor filters;Inference algorithms;Dynamic programming;Speech recognition;Data mining;Image segmentation;Australia;Computer vision","feature extraction;dynamic programming;image recognition;filters;filtering theory;inference mechanisms;fuzzy logic","writer independent off-line unconstrained handwritten word recognition;automatic Gabor filter parameter determination;feature extraction;slant corrected images;tilt corrected images;algorithm;2D image translation;1D domain;modified dynamic programming method;fuzzy inference","","","","10","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"An Improved Learning Experience for English Language Using Big Data Analytics and AAC","P. V. Kumar; D. Balamurugan; K. Suriya; R. Reshma","Department of Computer Science and Engineering, Faculty of Engineering, Karpagam Academy of Higher Education, Coimbatore, Tamil Nadu, India; Department of Computer Science and Engineering, Faculty of Engineering, Karpagam Academy of Higher Education, Coimbatore, Tamil Nadu, India; Department of Computer Science and Engineering, Faculty of Engineering, Karpagam Academy of Higher Education, Coimbatore, Tamil Nadu, India; Department of Computer Science and Engineering, Faculty of Engineering, Karpagam Academy of Higher Education, Coimbatore, Tamil Nadu, India","2023 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)","25 Apr 2023","2023","","","347","353","Teachers and students in the 21st century are shifting their attention from the classroom to digital education, aiming for meaningful, high-quality, and dynamic learning. With the introduction of digital learning, the educational system has undergone significant transformations. Some teachers/instructors, on the other hand, are still experiencing difficulties. For this reason, this research finds a solution named AAC integrated Big Data Analytics Framework (AACBDAF) for people with neural disorders in the English language. AAC-BDAF gathers all the linguistic data and categorizes it based on how frequently it is used. To input data, the AAC system uses eyeball tracking to display symbols and characters on a screen. To help the listener follow along, the selected option/word is converted into a text-to-speech paradigm. The detection algorithms of machine learning and deep learning are evaluated for word detection and suggest the convolutional neural network (CNN) support AACBDAF. According to simulation findings, the proposed AAC-BDAF recognizes phrases and predicts the word from their eye-gazing with higher accuracy of 98.63%.","","978-1-6654-9199-0","10.1109/ICSCDS56580.2023.10104901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10104901","Eyeball Tracking;Neural Network;Augmentation;Big Data Analytics;Eye Gazing;Text-Speech Translation","Deep learning;Computational modeling;Education;Symbols;Big Data;Predictive models;Linguistics","Big Data;computer aided instruction;convolutional neural nets;data analysis;deep learning (artificial intelligence);learning (artificial intelligence)","AAC integrated Big Data Analytics Framework;AAC system;AAC-BDAF gathers;convolutional neural network support AACBDAF;deep learning;digital education;digital learning;dynamic learning;educational system;English language;improved learning experience;linguistic data;machine learning;meaningful quality;neural disorders;significant transformations;word detection","","","","28","IEEE","25 Apr 2023","","","IEEE","IEEE Conferences"
"Hierarchical model for object recognition based on natural-stimuli adapted filters","P. Mishra; B. K. Jenkins","Signal and Image Processing Institute, Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; Signal and Image Processing Institute, Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","950","953","We examine a set of biologically inspired features and apply it to the multiclass object recognition problem. To obtain these features we modify HMAX, which is based on a hierarchical model of visual cortex. Instead of using a set of standard Gabor filters we use a set of natural-stimuli adapted filters. These filters emerge as a result of optimization based in part on smooth L1-norm based sparseness maximization. These filters in conjunction with the HMAX model give more biological plausibility to the model. The features thus obtained are largely scale, translation and rotation invariant, and are fed to a support vector machine classifier. We successfully demonstrate the applicability of this modified HMAX model to the object recognition task by testing it on Caltech-5 and Caltech-101 datasets.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495294","Biologically inspired hierarchical model;object recognition;HMAX;L1-norm;Caltech-101","Object recognition;Brain modeling;Feature extraction;Biological system modeling;Gabor filters;Band pass filters;Dictionaries;Prototypes;Data mining;Cost function","biomimetics;image recognition;neurophysiology;support vector machines;vision","natural stimuli adapted filters;biologically inspired features;multiclass object recognition problem;visual cortex hierarchical model;smooth L1-norm based sparseness maximization;HMAX model;support vector machine classifier","","3","","18","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"Artificial speaking device for aphasic children","M. Abraham; J. Ormrod","Ecole Nationale Sup√©rieure de T√©l√©communications de Bretagne, Brest, France; Ecole Nationale Sup√©rieure de T√©l√©communications de Bretagne, Brest, France","Proceedings of the First Joint BMES/EMBS Conference. 1999 IEEE Engineering in Medicine and Biology 21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering Society (Cat. N","6 Aug 2002","1999","1","","671 vol.1","","The authors propose to help children with cerebral palsy compensate for certain neuro-physical deficiencies in vocalization by building a speaking machine, based on a three-step cognitive model. The first version of the machine developed speaks French but, faced with translation into English, the authors show the limitations of using icons as an intermediate language. Finally, a test with aphasic children shows their interest in the machine.","1094-687X","0-7803-5674-8","10.1109/IEMBS.1999.802749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=802749","","Birth disorders;Testing;Animation;Councils;Natural languages;Communications technology;Mice;Cognition;Visualization;Machine learning","paediatrics;brain;handicapped aids;speech synthesis","artificial speaking device;aphasic children;cerebral palsy;neuro-physical vocalization deficiencies compensation;French;English;icons;intermediate language;speaking machine;three-step cognitive model","","1","","4","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Chinese-English quasi-equivalent noun phrase definition and automatic identification","Yanjun Ma; Ying Liu","Lab of Computational Linguistics, Department of Chinese Language and Literature, Tsinghua University, Beijing, China; Lab of Computational Linguistics, Department of Chinese Language and Literature, Tsinghua University, Beijing, China","2005 International Conference on Natural Language Processing and Knowledge Engineering","27 Feb 2006","2005","","","431","436","After an examination of a Chinese-English bilingual corpus with 2239 sentence pairs, a new definition of Chinese noun phrase (NP), quasi-equivalent noun phrase (equNP), is proposed with a goal of translation from Chinese NPs to English NPs. Firstly, all the equNPs in the corpus are tagged manually according to the definition in this paper. A set of part of speech (POS) templates for equNP is automatically acquired. Secondly, all the possible equNPs in a sentence are identified using the templates. These equNPs are the candidates for equNP identification. Finally, a classification process and a chunking process are carried out. In classification process, the correct equNPs are chosen from the candidates set using a maximum entropy classifier which combined POS, syntactic and semantic information. In chunking process, the equNPs in the sentence are finally chosen. On open test set, the precision is 83.75% and recall is 86.50%.","","0-7803-9361-9","10.1109/NLPKE.2005.1598776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598776","","Natural languages;Entropy;Machine learning;Data mining;Natural language processing;Statistical analysis;Computational linguistics;Speech;Testing;Information retrieval","natural languages;grammars;learning (artificial intelligence)","Chinese-English quasi-equivalent noun phrase definition;automatic identification;Chinese noun phrase;maximum entropy classifier;POS;part of speech;semantic information","","1","","22","IEEE","27 Feb 2006","","","IEEE","IEEE Conferences"
"Parameter Estimation of Statistical Models Using Convex Optimization","H. Jiang; X. Li","University of Science and Technology, China; York University, Canada","IEEE Signal Processing Magazine","15 Apr 2010","2010","27","3","115","127","Discriminative learning methods have achieved many successes in speech and language processing during the past decades. Discriminative learning of generative models is a typical optimization problem, where efficient optimization methods play a critical role. For many widely used statistical models, discriminative learning normally leads to nonconvex optimization problems. In this article we used three representative examples to showcase how to use a proper convex relaxation method to convert discriminative learning of HMMs and MMMs into standard convex optimization problem so that it can be solved effectively and efficiently even for large-scale statistical models. We believe convex optimization will continue to play important role in discriminative learning of other statistical models in other application domains, such as statistical machine translation, computer vision, biometrics, and informatics.","1558-0792","","10.1109/MSP.2010.936018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447069","","Parameter estimation;Optimization methods;Learning systems;Speech processing;Natural languages;Relaxation methods;Hidden Markov models;Large-scale systems;Machine learning;Application software","hidden Markov models;learning (artificial intelligence);optimisation;parameter estimation","parameter estimation;discriminative learning methods;speech-language processing;nonconvex optimization problems;convex relaxation method;HMM;MMM;large-scale statistical models;statistical machine translation;computer vision;biometrics","","12","","41","IEEE","15 Apr 2010","","","IEEE","IEEE Magazines"
"Assamese Word Sense Disambiguation using Supervised Learning","P. P. Borah; G. Talukdar; A. Baruah","Department of Computer Science & Engineering and IT, Assam Don Bosco University, Guwahati, India; Department of Computer Science & Engineering and IT, Assam Don Bosco University, Guwahati, India; Department of Computer Science & Engineering and IT, Assam Don Bosco University, Guwahati, India","2014 International Conference on Contemporary Computing and Informatics (IC3I)","26 Jan 2015","2014","","","946","950","Word sense disambiguation (WSD) can be defined as a task that focuses on estimating the right sense of a word in its context. It is important as a pre-processing step in information extraction, machine translation, question answering and many other natural language processing tasks. Ambiguity in Word Sense arises when a particular word has more than one possible sense. Finding the correct sense requires thorough knowledge regarding words. This information of words is often derived from the sources such as words appearing in the context of the target word, part of speech information of the words in the neighbour, syntactical relations and local collocations. Our main aim in this paper is to develop an automatic system for WSD in Assamese using a Naive Bayes classifier. This is the first work to the best of our knowledge on developing an automatic WSD system for Assamese language. Assamese, the main language of most of the people in North-Eastern part of India is a morphologically very rich language. In Assamese WSD is a challenging task because a word can behave differently when combined with a suffix or a sequence of suffixes to have an entirely different sense. WSD often makes use of lexical resources such as WordNet, lexicon, annotated or unannotated corpora etc for its process of disambiguation.","","978-1-4799-6629-5","10.1109/IC3I.2014.7019726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019726","Lexicon;Wordnet;Local collocations;Polysemic word;Unigram cooccurence","Training;Semantics;Context;Natural language processing;Dictionaries;Speech;Ink","learning (artificial intelligence);natural language processing;pattern classification","Assamese word sense disambiguation;supervised learning;WSD;information extraction;machine translation;question answering;natural language processing task;naive Bayes classifier;Assamese language;India;WordNet;lexicon;disambiguation process","","8","","14","IEEE","26 Jan 2015","","","IEEE","IEEE Conferences"
"Discovering meaningful multimedia patterns with audio-visual concepts and associated text","L. Xie; L. Kennedy; S. . -F. Chang; A. Divakaran; H. Sun; C. . -Y. Lin","Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA; Mitsubishi Electric Research Laboratories, Inc., Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Inc., Cambridge, MA, USA; IBM Thomas J. Watson Research Center, Hawthorne, NY, USA","2004 International Conference on Image Processing, 2004. ICIP '04.","18 Apr 2005","2004","4","","2383","2386 Vol. 4","The work presents the first effort to automatically annotate the semantic meanings of temporal video patterns obtained through unsupervised discovery processes. This problem is interesting in domains where neither perceptual patterns nor semantic concepts have simple structures. The patterns in video are modeled with hierarchical hidden Markov models (HHMM), with efficient algorithms to learn the parameters, the model complexity and the relevant features; the meanings are contained in words of the speech transcript of the video. The pattern-word association is obtained via cooccurrence analysis and statistical machine translation models. Promising results are obtained through extensive experiments on 20+ hours of TRECVID news videos: video patterns that associate with distinct topics such as el-nino and politics are identified; the HHMM temporal structure model compares favorably to a nontemporal clustering algorithm.","1522-4880","0-7803-8554-3","10.1109/ICIP.2004.1421580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1421580","","Hidden Markov models;Speech;Games;Statistics;Sun;Pattern analysis;Clustering algorithms;Supervised learning;Algorithm design and analysis;Tagging","audio-visual systems;multimedia communication;unsupervised learning;hidden Markov models;language translation;pattern clustering;temporal logic;semantic Web;video signal processing","automatic annotation;semantic meaning;temporal video pattern;unsupervised discovery process;hierarchical hidden Markov model;HHMM;cooccurrence analysis;statistical machine translation model;TRECVID news video;multimedia pattern;audio-visual concept","","13","","9","IEEE","18 Apr 2005","","","IEEE","IEEE Conferences"
"Can Computers Help Us to Better Understand Different Cultures? Toward a Computer-Based CULINT","D. Livshits; N. Howard; Y. Neuman","Ben-Gurion University of the Negev, Beersheva, Israel; Massachusetts Institute of Technology, Cambridge, USA; Ben-Gurion University of the Negev, Beersheva, Israel","2012 European Intelligence and Security Informatics Conference","13 Sep 2012","2012","","","172","179","Identifying cultural discrepancies in worldviews is of high priority to Cultural Intelligence (CULINT). This paper presents a CULINT computer-based methodology for increasing cultural awareness. By automatically identifying themes/motifs in textual data and using machine translation, we expose cultural discrepancies in cultural understanding. This novel methodology is empirically tested through the analysis of 30 speeches and illustrated through an in-depth analysis of a case-study.","","978-1-4673-2358-1","10.1109/EISIC.2012.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298828","CULINT;information;technology;interdisciplinary;research","Cultural differences;Speech;Humans;Semantics;Security;Educational institutions;Psychology","cultural aspects;language translation;psychology;text analysis","cultural discrepancies;computer-based CULINT;cultural intelligence;cultural awareness;textual data;machine translation;cultural understanding","","","","24","IEEE","13 Sep 2012","","","IEEE","IEEE Conferences"
"Named Entity Recognition: A Survey for Indian Languages","K. Bhattacharjee; S. K. S; S. Mehta; A. Kumar; R. Mehta; D. Pandya; P. Chaudhari; D. Verma","Applied AI Group (AAI), Centre for Development of Advanced Computing (CDAC), Pune, India; Applied AI Group (AAI), Centre for Development of Advanced Computing (CDAC), Pune, India; Applied AI Group (AAI), Centre for Development of Advanced Computing (CDAC), Pune, India; Applied AI Group (AAI), Centre for Development of Advanced Computing (CDAC), Pune, India; Department of Computer Engineering, Vishwakarma Institute of Information Technology (VIIT), Pune, India; Department of Computer Engineering, Vishwakarma Institute of Information Technology (VIIT), Pune, India; Department of Computer Engineering, Vishwakarma Institute of Information Technology (VIIT), Pune, India; Department of Computer Engineering, Vishwakarma Institute of Information Technology (VIIT), Pune, India","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)","13 Feb 2020","2019","1","","217","220","Named Entity Recognition (NER) is a tool based on principles of Artificial Intelligence (AI) and Natural Language Processing (NLP) for automatically tagging Named Entities from unstructured text. Named Entities, which are generally proper nouns, can be the name of a person, organization, location etc. Named Entity Recognition is a very crucial tool of Natural Language Processing. Some of the application areas of NER include- Information Extraction and Retrieval, Machine Translation, Text Summarization etc. This paper analyses different approaches used in the NER of Indian languages, with emphasis on Hindi. The study compares the different approaches for NER viz. Machine Learning (ML), Rule-based and Hybrid. This study is orchestrated to provide gap analysis in available NER systems for Indian Languages, especially Hindi. It is because existing NER systems are evaluated for accuracies based on predefined datasets, which are not providing universal results on any dataset. Also, there is no standard dataset available for fair comparison of accuracies of Indian Languages. Furthermore, we concentrated our scope to Hindi as it is the official language of India and representative to other Indo-Aryan languages having similar structure, making it a generalized solution. Moreover, the scope of Named Entity tags are limited. Whereas, there is a scope of further classification of Named Entities. As future work, we would like to develop an efficient system, in terms of accuracy, and catering to more Named Entity tags than available presently in Hindi NER tools.","","978-1-7281-0283-2","10.1109/ICICICT46008.2019.8993236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993236","Part of speech;Named Entity Recognition;Machine Learning;Conditional Random Fields;Support Vector Machine;Hidden Markov Models;Hybrid;Rule Based","","learning (artificial intelligence);natural language processing","machine learning;named entity recognition;Indian languages;natural language processing;Hindi NER tools;Indo-Aryan languages","","5","","13","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Reading machines for the blind:The technical problems and the methods adopted for their solution","J. Allen","Research Laboratory of Electronics and the Department of Electrical Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Transactions on Audio and Electroacoustics","29 Jan 2003","1973","21","3","259","264","In order to assess current efforts devoted to reading machine design, it is first necessary to develop a set of requirements for an ideal device. Direct translation aids are then seen to lack several of these desirable features, and more general, linguistically based techniques are then examined. Structural properties of English are found to be obtainable from the orthographic representation, and these abstract relations can then be used to infer structural correlates in the output speech waveform. Current knowledge is centered largely at the word level, but several correlates of higher order units have been studied and rules for their behavior have been implemented in working systems. Finally, direct assessment of speech synthesized by rule has shown that even currently available techniques can yield speech acceptable to blind users.","1558-2582","","10.1109/TAU.1973.1162463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1162463","","Speech synthesis;Character recognition;Books;Costs;Speech analysis;Dictionaries;Equations;Art;Weight control;Packaging","","","","15","2","18","IEEE","29 Jan 2003","","","IEEE","IEEE Journals"
"Information retrieval, extraction and summarisation","Y. Wilks","University of Sheffield, UK","IEE Colloquium on Speech and Language Engineering - State of the Art (Ref. No. 1998/499)","6 Aug 2002","1998","","","8/1","","Summary form only given. The talk distinguishes information detection or retrieval (IR) from information extraction (IE) and describes recent advances in using IE technology for fast access to very large amounts of textual information in, for example, the world wide web and its extraction to a browsable database. This technology is now becoming available commercially and I describe a number of Language Engineering projects incorporating IE technology. It is argued that multilingual applications in IE/IR make the distinction between these information processing technologies and machine translation and automatic question answering and summarisation less clear than before, and they can now be combined in original ways to optimise information access via electronic text. Promising applications are mentioned in security, publishing, communications, finance, science, patents etc. The problems in advancing the field rapidly are described, particularly an appropriate interface, the modelling of the users needs and automatic adaptation of such systems to new domains. Summarisation used to be a traditional linguistic task, tackled by a range of techniques, but is now seen almost exclusively as a by produced of IR or IE technology, creating a text from the set of sentences containing the most improbable terms or by generating text from the content of IE templates.","","","10.1049/ic:19980962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=755358","","","information retrieval","information retrieval;information extraction;information summarisation;information detection;textual information;browsable database;multilingual applications;information processing technologies;machine translation;automatic question answering;electronic text;security;publishing;communications;finance;science;patents","","","","","","6 Aug 2002","","","IET","IET Conferences"
"Benchmarking and Analyzing Deep Neural Network Training","H. Zhu; M. Akrout; B. Zheng; A. Pelegris; A. Jayarajan; A. Phanishayee; B. Schroeder; G. Pekhimenko","University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada; University of British Columbia, Vancouver, Canada; Microsoft Research, Redmond, United States; University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada","2018 IEEE International Symposium on Workload Characterization (IISWC)","13 Dec 2018","2018","","","88","100","The recent popularity of deep neural networks (DNNs) has generated considerable research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference - i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for DNN training, called TBD1, which comprises a representative set of eight DNN models and covers six major machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of performance metrics, and methodologies to analyze the results. We also build a new set of tools for memory profiling in three major frameworks. These tools can shed light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. Using our tools and methodologies, we make several important observations and recommendations on where future DNN training research and optimization should be focused.","","978-1-5386-6780-4","10.1109/IISWC.2018.8573476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8573476","","Training;Benchmark testing;Tools;Graphics processing units;Computational modeling;Speech recognition;Hardware","data handling;data structures;learning (artificial intelligence);neural nets","deep neural network training;deep neural networks;DNN-related computation;trained models;image classification networks;primary benchmark;myopic view;benchmark suite;DNN training;representative set;DNN models;machine translation;speech recognition;object detection;adversarial networks;reinforcement learning;extensive performance analysis;deep learning frameworks;multiGPU;performance analysis tools;performance metrics;optimization;TBD;machine learning applications;hardware configurations","","71","","91","IEEE","13 Dec 2018","","","IEEE","IEEE Conferences"
"Dialogue act segmentation for Vietnamese human-human conversational texts","T. -L. Ngo; S. -B. Pham; K. -L. Pham; X. -H. Phan; M. -S. Cao","University of Information and Communication Technology, Thainguyen University (TNU); University of Engineering and Technology, Vietnam National University, Hanoi (VNU); University of Engineering and Technology, Vietnam National University, Hanoi (VNU); University of Engineering and Technology, Vietnam National University, Hanoi (VNU); University of Engineering and Technology, Vietnam National University, Hanoi (VNU)","2017 9th International Conference on Knowledge and Systems Engineering (KSE)","23 Nov 2017","2017","","","203","208","Dialog act identification plays an important role in understanding conversations. It has been widely applied in many fields such as dialogue systems, automatic machine translation, automatic speech recognition, and especially useful in systems with human-computer natural language dialogue interfaces such as virtual assistants and chatbots. The first step of identifying dialog act is identifying the boundary of the dialog act in utterances. In this paper, we focus on segmenting the utterance according to the dialog act boundaries, i.e. functional segments identification, for Vietnamese utterances. We investigate carefully functional segment identification in two approaches: (1) machine learning approach using maximum entropy (ME) and conditional random fields (CRFs); (2) deep learning approach using bidirectional Long Short-Term Memory (LSTM) with a CRF layer (Bi-LSTM-CRF) on two different conversational datasets: (1) Facebook messages (Message data); (2) transcription from phone conversations (Phone data). To the best of our knowledge, this is the first work that applies deep learning based approach to dialog act segmentation. As the results show, deep learning approach performs appreciably better as to compare with traditional machine learning approaches. Moreover, it is also the first study that tackles dialog act and functional segment identification for Vietnamese.","","978-1-5386-3576-6","10.1109/KSE.2017.8119459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119459","Dialog act segmentation;functional segment;Vietnamese conversation","Speech;Rivers;Marine animals;Machine learning;Electronic mail;ISO Standards","interactive systems;learning (artificial intelligence);maximum entropy methods;natural language processing;random processes;speech processing;text analysis","dialogue act segmentation;human-human conversational texts;dialog act identification;dialog act boundaries;Vietnamese utterances;conditional random fields;machine learning;functional segment identification;utterance segmentation;maximum entropy;deep learning;long short-term memory","","1","","19","IEEE","23 Nov 2017","","","IEEE","IEEE Conferences"
"Transformation Rule Learning without Rule Templates: A Case Study in Part of Speech Tagging","N. X. Bach; L. A. Cuong; N. V. Ha; N. N. Binh","College of Technology, Vietnam National University, Hanoi, Vietnam; College of Technology, Vietnam National University, Hanoi, Vietnam; Information Technology Institute, Vietnam National University, Hanoi, Vietnam; College of Technology, Vietnam National University, Hanoi, Vietnam","2008 International Conference on Advanced Language Processing and Web Information Technology","1 Aug 2008","2008","","","9","14","Part of speech (POS) tagging is an important problem and is one of the first steps included in many tasks in natural language processing. It affects directly on the accuracy of many other problems such as Syntax Parsing, WordSense Disambiguation, and Machine Translation. Stochastic models solve this problem relatively well, but they still make mistakes. Transformation-based learning (TBL) is a solution which can be used to improve stochastic taggers by learning a set of transformation rules. However, its rule learning algorithm has the disadvantages that rule templates must be prepared by hand and only rules are instances of rule templates can be generated. In this paper, we propose a model to learn transformation rules without rule templates. This model considers the rule learning problem as a feature selection problem. Experiments on PennTree Bank showed that the proposal model reduces errors of stochastic taggers with some tags.","","978-0-7695-3273-8","10.1109/ALPIT.2008.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4584333","","Training;Artificial neural networks;Entropy;Tagging;Stochastic processes;Accuracy;Proposals","feature extraction;learning (artificial intelligence);natural language processing;stochastic processes","transformation rule-based learning;part-of-speech tagging;natural language processing;stochastic tagger model;feature selection problem","","3","","15","IEEE","1 Aug 2008","","","IEEE","IEEE Conferences"
"Paraphrase Generation Model Integrating Transformer Architecture, Part-of-Speech Features, and Pointer Generator Network","Y. -C. Tsai; F. -C. Lin","Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan; Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan","IEEE Access","30 Mar 2023","2023","11","","30109","30117","In recent years, hardware advancements have enabled natural language processing tasks that were previously difficult to achieve due to their intense computing requirements. This study focuses on paraphrase generation, which entails rewriting a sentence using different words and sentence structures while preserving its original meaning. This increases sentence diversity, thereby improving the performance of downstream tasks, such as question‚Äìanswering systems and machine translation. This study proposes a novel paraphrase generation model that combines the Transformer architecture with part-of-speech features, and this model is trained using a Chinese corpus. New features are incorporated to improve the performance of the Transformer architecture, and the pointer generation network is used when the training data contain low-frequency words. This allows the model to focus on input words with important information according to their attention distributions.","2169-3536","","10.1109/ACCESS.2023.3260849","National Science and Technology Council, Taiwan(grant numbers:NSTC 111-2121-M-035-001,NSTC 112-2420-H-006-002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10078879","Multi-encoder;paraphrase generation;pointer generation network;transformer","Transformers;Decoding;Network architecture;Measurement;Task analysis;Data preprocessing;Data models","language translation;natural language processing;text analysis","downstream tasks;hardware advancements;increases sentence diversity;intense computing requirements;natural language processing tasks;novel paraphrase generation model;original meaning;paraphrase generation model integrating Transformer architecture;part-of-speech features;pointer generation network;pointer generator network;question-answering systems","","","","28","CCBYNCND","23 Mar 2023","","","IEEE","IEEE Journals"
"A Vision-based Smart Human Computer Interaction System for Hand-gestures Recognition","Y. Manoharan; S. Saxena; D. R","Department of CSE, KCG College of' Techmnology, Chennai, India; Department of CSE, KCG College of' Techmnology, Chennai, India; Department of CSE, KCG College of' Techmnology, Chennai, India","2022 1st International Conference on Computational Science and Technology (ICCST)","14 Feb 2023","2022","","","321","324","A very important category of computational intelligence with respect to social applicability is the ability to transform any linguistic sign gesture into corresponding language text. This is formally known as Sign Language translation (SLT). Quite recently, SLT has been employed by making use of Convolution Neural Networks Models (CNN models) and are decently successful in achieving the target goal. SLT focuses on population who are hearing or speech-impaired. The technique, in focus, takes users' hand gestures as input images or videos and captures the relevant features to convert the same into an appropriate linguistic symbol. The entire process makes it convenient for users to interact with the world and vice versa. In an attempt to make life easier for a certain focused section of the society, SLT is widely studied and a large section of research is invested in this area. SLT gives out the results in the form of physical carriages, signal sets and/or countenances to deliver a particular message across. It is also helpful in delivering the correct emotional state of differently-abled people so that normal people can understand them better. This paper proposes a machine learning model which successfully and efficiently classifies hand gestures into spellings and sentences. Our survey study includes object detection and classification stages. In the preliminary stage, 10 english language letters are repeatedly fed into the system using varied signal images owing to background changes, lighting effects and positions variations. Training and testing data spaces are kept different to yield better result confidence. CNN algorithms are applied aggressively to improve the conversion accuracy.","","978-1-6654-7655-3","10.1109/ICCST55948.2022.10040464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040464","CNN;sign language;speech conversion;ISL;Natural language","Training;Computational modeling;Sociology;Symbols;Transforms;Linguistics;Assistive technologies","computer vision;convolutional neural nets;deep learning (artificial intelligence);feature extraction;gesture recognition;handicapped aids;human computer interaction;image classification;interactive systems;learning (artificial intelligence);natural language processing;neural nets;object detection;pattern classification;sign language recognition","10 english language letters;appropriate linguistic symbol;CNN models;computational intelligence;Convolution Neural Networks Models;corresponding language text;focused section;hand gestures;hand-gestures recognition;important category;input images;linguistic sign gesture;machine learning model;Sign Language translation;SLT;social applicability;users;vision-based smart human computer interaction system","","","","16","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Suffix sequences based morphological segmentation for Afaan Oromo","G. M. Wegari; M. Melucci; S. Teferra","IT Doctoral Program, Addis Ababa University, Addis Ababa, Ethiopia; Department of Information Engineering, University of Padua, Padua, Italy; Department of Information Science, Addis Ababa University, Addis Ababa, Ethiopia","AFRICON 2015","19 Nov 2015","2015","","","1","6","This paper reports on a morphological segmentation model for Afaan Oromo based on suffix sequences approach. Understanding and identifying the suffix sequences of a language allow us to detect morpheme boundaries of many words of Afaan Oromo. Morphological segmentation models can be used in many Natural Language Processing applications such as machine translation, speech recognition, information retrieval and part-of-speech tagging. A divisive hierarchical clustering and frequency distribution were used to build a tree of candidate stems from which segmented suffix sequences can be modeled. The proposed morphological segmentation model was evaluated with test word-lists. The accuracy obtained by our morphological segmentation model is encouraging.","2153-0033","978-1-4799-7498-6","10.1109/AFRCON.2015.7331956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331956","morphological segmentation;suffix sequences;Afaan Oromo","Computational modeling;Training;Natural language processing;Algorithm design and analysis;Clustering algorithms;Pragmatics","natural language processing","suffix sequences based morphological segmentation;Afaan Oromo;suffix sequences approach;morphological segmentation models;natural language processing;machine translation;speech recognition;information retrieval;part-of-speech tagging;divisive hierarchical clustering;frequency distribution","","","","26","IEEE","19 Nov 2015","","","IEEE","IEEE Conferences"
"A brief survey on corpus uses","H. Al-Ibrahim; H. S. Al-Khalifa; A. Al-Salman","Computer Science Department; Information Technology Department, King Saud University, Riyadh, Saudi Arabia; Computer Science Department","2013 International Conference on Current Trends in Information Technology (CTIT)","27 Feb 2014","2013","","","66","70","Corpus opened up many new areas of research in the domain of linguistic, which would never been possible without it. Recently, corpus linguistic became a hot topic in the research community. However, entering this area for new computer researchers may take lots of effort and time. In this paper we provide new researchers in this domain with the basic terms, measures, and tools used in this area.","2377-5335","978-1-4799-2425-7","10.1109/CTIT.2013.6749479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749479","corpus;machine translation;bilingual lexicons;speech recognition;corpus annotation;language learning","Physics","natural language processing","corpus linguistic","","","","42","IEEE","27 Feb 2014","","","IEEE","IEEE Conferences"
"End-to-end trajectory transportation mode classification using Bi-LSTM recurrent neural network","H. Liu; I. Lee","Information Technology Academy, James Cook University Cairns, Queensland, Australia; Information Technology Academy, James Cook University Cairns, Queensland, Australia","2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","15 Jan 2018","2017","","","1","5","Transportation mode classification is a key task in trajectory data mining. It adds human behaviour semantics to raw trajectories for trip recommendation, traffic management and transport planning. Previous approaches require heavy pre-processing and feature extraction processes in order to build a classifier, which is complicated and time-consuming. Recurrent neural network has demonstrated its capacity in sequence modelling tasks ranging from machine translation, speech recognition to image captioning. In this paper, we pro¬≠pose a trajectory transportation mode classification framework that is based on an end-to-end bidirectional LSTM classifier. The proposed classification process does not require any feature extraction process, but automatically learns features from trajectories, and use them for classification. We further improve this framework by feeding the time interval as an external feature by embedding. Our experiments on real GPS datasets demonstrate that our approach outperforms existing methods with regard to AUC.","","978-1-5386-1829-5","10.1109/ISKE.2017.8258799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258799","Transportation mode;Trajectory;LSTM;Recurrent Neural Network","Trajectory;Feature extraction;Transportation;Global Positioning System;Recurrent neural networks;Support vector machines;Machine learning","data mining;feature extraction;image classification;learning (artificial intelligence);pattern classification;recurrent neural nets;speech recognition;traffic engineering computing","Bi-LSTM recurrent neural network;key task;trajectory data mining;human behaviour semantics;raw trajectories;trip recommendation;traffic management;transport planning;feature extraction processes;sequence modelling tasks;trajectory transportation mode classification framework;end-to-end bidirectional LSTM classifier;classification process;feature extraction process;external feature;end-to-end trajectory transportation mode classification","","27","","23","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"T2K2: A Type II KASER","S. H. Rubin; S. -c. Chen; J. B. Law","SSC, San Diego, CA, USA; School of Computer Science, Florida International University, Miami, FL, USA; SSC, San Diego, CA, USA","2006 IEEE International Conference on Information Reuse & Integration","4 Dec 2006","2006","","","147","153","The transformational methodology described in this paper induces new knowledge, which may be open under any deductive process. The method of transposition is used to maintain a maximum size for the application as well as meta-rule bases. The ""move to head"" method is used by both the application and metarule bases for hypotheses formation. Whenever an application rule is fired, it is transposed on the transposition list and also moved to the head on the other list. If any meta-rule on a solution path individually leads to a contradiction on the application rule base, then the offending meta-rule is expunged. Then, when the system is idle enter dream mode, whereby rule i rArr rule j is generated by the 3-2-1 skewed twister as a candidate most-specific meta-rule. Candidate most-specific meta-rules are ""cored"" to create one generalization per candidate. These candidate meta-rules are tested for application to each rule in the application domain rule base. In order to be saved in the meta base, they may not map any existing rule in the application domain rule base to one having the same antecedent as another in this base, but a different consequent (as found by hashing). In addition, all candidate meta-rules must map at least one rule in the application base to another distinct one there, or be symmetrically induced from meta-rules that so map","","0-7803-9788-6","10.1109/IRI.2006.252404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4018481","Computing with Words;Machine Learning;Natural Language;Randomization;Translation","Natural languages;Testing;Cognition;Closed loop systems;Speech synthesis;Synthesizers;Calculus;Clustering algorithms;Machine learning algorithms;Computer applications","knowledge based systems;learning (artificial intelligence);natural languages;randomised algorithms","T2K2;type II KASER;transformational method;move to head method;metarule bases;application rule base;machine learning;natural language;randomization","","","2","11","IEEE","4 Dec 2006","","","IEEE","IEEE Conferences"
"Algorithms for estimation of comic speakers considering reading order of frames and texts","Y. Omori; K. Nagamizo; D. Ikeda","Graduate School of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; ForeVision Inc., Tokyo, Japan; Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan","2022 12th International Congress on Advanced Applied Informatics (IIAI-AAI)","23 Sep 2022","2022","","","367","372","Machine learning methods in recent years have focused on multimodal input and cross-modal tasks, and they are used as approaches to problems in various domains. Associating comic texts and characters using these approaches is informative for commercial activities such as speech synthesis and automatic translation of texts. In this study, we address the task of associating a text with a speaker in comics. It is challenging to correspond between them because these are not self-evidently attached, and few studies have attempted. These previous studies have less considered the continuity of comics such as narrative flow or contextual information. We assume that considering the continuity of comics is effective for speaker estimation. This paper proposes algorithms for estimating the reading order of frames or texts, and it also proposes methods for estimating speakers based on these orders. As a result, our proposed method improves accuracy compared to previous methods. Consideration of the frame order is an effective clue to the comic speaker estimation.","2472-0070","978-1-6654-9755-8","10.1109/IIAIAAI55812.2022.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894649","Speaker estimation;Comic;Multimodal","Correlation coefficient;Estimation;Machine learning;Linguistics;Speech synthesis;Task analysis;Informatics","interactive systems;learning (artificial intelligence);speaker recognition;speech synthesis;word processing","contextual information;comics;reading order;estimating speakers;frame order;comic speaker estimation;comic speakers;multimodal input;cross-modal tasks;comic texts;commercial activities;speech synthesis","","","","11","IEEE","23 Sep 2022","","","IEEE","IEEE Conferences"
"Smoothing of ngram language models of human chats","J. Dumoulin","Next IT Corporation, USA","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","22 Apr 2013","2012","","","1","4","Ngram language models are ubiquitous in speech applications and many other natural language systems. One issue with n-gram language models is that the language is not completely represented in the model. When words appear that are not in the model, we may need to provide a smoothing method to distribute the model probabilities over the unknown values. Many techniques exist for language model smoothing with many different performance characteristics. Often the performance of smoothing algorithms may depend on the application of the language model (so, for example, unigram models with interpolation smoothing may perform better with information retrieval applications, but trigram models with backoff smoothing might perform better for speech). This paper examines the relative performance of some selected smoothing methods with bigram language models created using chat data. The language models are used for machine translation of chat data and for creating text classification models.","","978-1-4673-2743-5","10.1109/SCIS-ISIS.2012.6505411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505411","","","language translation;natural language processing;pattern classification;smoothing methods;text analysis","ngram language models;human chats;speech applications;natural language systems;model probabilities;language model smoothing;smoothing algorithms;unigram models;interpolation smoothing;information retrieval applications;trigram models;backoff smoothing;bigram language models;chat data;text classification models","","3","","5","IEEE","22 Apr 2013","","","IEEE","IEEE Conferences"
"Massive Exploration of Pseudo Data for Grammatical Error Correction","S. Kiyono; J. Suzuki; T. Mizumoto; K. Inui","RIKEN Center for Advanced Intelligence Project, Sendai, Japan; Center for Data-driven Science and Artificial Intelligence, Tohoku University, Sendai, Japan; Future Corporation, Shinagawa, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","31 Jul 2020","2020","28","","2134","2145","Collecting a large amount of training data for grammatical error correction (GEC) models has been an ongoing challenge in the field of GEC. Recently, it has become common to use data demanding deep neural models such as an encoder-decoder for GEC; thus, tackling the problem of data collection has become increasingly important. The incorporation of pseudo data in the training of GEC models is one of the main approaches for mitigating the problem of data scarcity. However, a consensus is lacking on experimental configurations, namely, (i) the methods for generating pseudo data, (ii) the seed corpora used as the source of the pseudo data, and (iii) the means of optimizing the model. In this study, these configurations are thoroughly explored through massive amount of experiments, with the aim of providing an improved understanding of pseudo data. Our main experimental finding is that pretraining a model with pseudo data generated by back-translation-based method is the most effective approach. Our findings are supported by the achievement of state-of-the-art performance on multiple benchmark test sets (the CoNLL-2014 test set and the official test set of the BEA-2019 shared task) without requiring any modifications to the model architecture. We also perform an in-depth analysis of our model with respect to the grammatical error type and proficiency level of the text. Finally, we suggest future directions for further improving model performance.","2329-9304","","10.1109/TASLP.2020.3007753","JSPS KAKENHI(grant numbers:19H04162); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134890","Natural language processing;language generation;grammars and other rewriting systems;machine translation","Task analysis;Data models;Noise measurement;Training data;Optimization;Error correction;Encyclopedias","learning (artificial intelligence);neural nets;text analysis","model architecture;CoNLL-2014 test set;BEA-2019 shared task;back-translation-based method;seed corpora;seed corpora;deep neural models;data scarcity;GEC models;data collection;grammatical error correction models;training data;pseudodata","","8","","65","CCBY","7 Jul 2020","","","IEEE","IEEE Journals"
"A number theoretic transform approach to image rotation in parallel array processors","T. Kriz; D. Bachman","Federal Systems Division, International Business Machines Corporation, Owego, NY, USA; Federal Systems Division, International Business Machines Corporation, Owego, NY, USA","ICASSP '80. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1980","5","","430","433","This paper describes a spectral method for rotating matrix form digital image data in a cartesian-coordinate organized array processor structure. The major elements of the method include: 1) an initial three step number-theoretic-transform (NTT) procedure to implement appropriate column and row data translations, and 2) a final non-spectral translation step within adjacent pixel groups. Digital array image data may be rotated about a selected pivot point through an arbitrary angle within the range ¬±45¬∞. A comparison of the spectral method execution time with a non-spectral implementation indicates that the new method can provide a performance improvement factor of up to 5 depending upon the magnitude of the rotation angle and pixel word size.","","","10.1109/ICASSP.1980.1170935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1170935","","Pixel;Digital images;Array signal processing;Throughput;Filtering;Data analysis;Image analysis;Data compression;Technological innovation;Filters","","","","1","","9","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Myanmar optical character recognition using block definition and featured approach","Z. Z. Aung; C. M. M. Maung","Information and Communication Technology Department, University of Technology, Pyin Oo Lwin, Myanmar; Information and Communication Technology Department, University of Technology, ), Pyin Oo Lwin, Myanmar","2017 3rd International Conference on Science in Information Technology (ICSITech)","15 Jan 2018","2017","","","313","318","Optical Character Recognition (OCR) can be used in many applications such as machine translation, postal processing, script recognition, text-to-speech, reading aid for blind, etc. Myanmar OCR system is essential to convert numerous published books, newspapers and journals of Myanmar into editable computer text files. It is a challenge for recognizing Myanmar old printed document in case of bad quality, absence of standard alphabets, absence of known fonts, ink through page, uneven background, broken characters, overlapped scripts and mixed scripts. This paper presents a new proposed block definition method for isolation printed Myanmar historical text. The proposed Myanmar optical character recognition (MOCR) system consists of local adaptive thresholding method for binarization and skew-slant correction, thinning algorithm is applied to obtain separation lines and words. For isolation of characters, block definition method is applied and adaptive neuro-fuzzy inference system (ANFIS) is matched the features in the trained database as machine readable text. Myanmar alphabets include consonants, vowels, medials and digits. By using block definition method, consonants and vowels are isolated easily and we obtained more accuracy rate of the OCR. The efficient experimental results are presented by using different Myanmar old documents in our proposed algorithms.","","978-1-5090-5866-2","10.1109/ICSITech.2017.8257131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257131","MOCR;skew-slant correction;thinning;block definition;ANFIS","Character recognition;Optical character recognition software;Feature extraction;Bifurcation;Image segmentation;Information technology;Image recognition","document image processing;feature extraction;image matching;optical character recognition;text analysis","adaptive neuro-fuzzy inference system;machine readable text;Myanmar alphabets;block definition method;machine translation;postal processing;script recognition;text-to-speech;Myanmar OCR system;editable computer text files;Myanmar old printed document;Myanmar historical text;Myanmar optical character recognition system;local adaptive thresholding method;Myanmar old documents;published books;MOCR system;skew-slant correction;ANFIS","","","","8","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"Soft sensor modeling of mill level based on convolutional neural network","J. Wei; L. Guo; X. Xu; G. Yan","College of Information Engineering, Taiyuan University of Technology, Taiyuan; College of Information Engineering, Taiyuan University of Technology, Taiyuan; College of Information Engineering, Taiyuan University of Technology, Taiyuan; College of Information Engineering, Taiyuan University of Technology, Taiyuan","The 27th Chinese Control and Decision Conference (2015 CCDC)","20 Jul 2015","2015","","","4738","4743","A soft sensor model based on Convolutional Neural Network (CNN) is proposed for the measurement of fill level in highly complex environment inside ball mill. CNN has achieved success in the field of image and speech recognition due to the use of local filtering and max-pooling, which is applied to frequency domain in our method to acquire high invariance to signal translation, scaling and distortion. A pair of convolution layer and max-pooling layer is added at the lowest end of neural network as a method to extract the high level abstraction from the vibration spectral features of the mill bearing. Then, the learned features are transferred to the Extreme Learning Machine (ELM) to model the mapping between extracted features and mill level. Experimental results show that the proposed CNN-ELM method can get more accurate and efficient measurement.","1948-9447","978-1-4799-7017-9","10.1109/CCDC.2015.7162762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162762","Convolutional Neural Network;mill level;feature extraction","Feature extraction;Convolution;Vibrations;Principal component analysis;Neural networks;Neurons;Indexes","ball milling;convolution;learning (artificial intelligence);machine bearings;neural nets;production engineering computing;spectral analysis;vibrations","soft sensor modeling;mill level;convolutional neural network;fill level measurement;highly complex environment;ball mill;local filtering;frequency domain;signal translation invariance;signal scaling;signal distortion;convolution layer;max-pooling layer;high level abstraction extraction;vibration spectral feature;mill bearing;feature learning;extreme learning machine;feature extracttion;CNN-ELM method","","7","","16","IEEE","20 Jul 2015","","","IEEE","IEEE Conferences"
"MAPGN: Masked Pointer-Generator Network for Sequence-to-Sequence Pre-Training","M. Ihori; N. Makishima; T. Tanaka; A. Takashima; S. Orihashi; R. Masumura","NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7563","7567","This paper presents a self-supervised learning method for pointer-generator networks to improve spoken-text normalization. Spoken-text normalization that converts spoken-style text into style normalized text is becoming an important technology for improving subsequent processing such as machine translation and summarization. The most successful spoken-text normalization method to date is sequence-to-sequence (seq2seq) mapping using pointer-generator networks that possess a copy mechanism from an input sequence. However, these models require a large amount of paired data of spoken-style text and style normalized text, and it is difficult to prepare such a volume of data. In order to construct spoken-text normalization model from the limited paired data, we focus on self-supervised learning which can utilize unpaired text data to improve seq2seq models. Unfortunately, conventional self-supervised learning methods do not assume that pointer-generator networks are utilized. Therefore, we propose a novel self-supervised learning method, MAsked Pointer-Generator Network (MAPGN). The proposed method can effectively pre-train the pointer-generator net-work by learning to fill masked tokens using the copy mechanism. Our experiments demonstrate that MAPGN is more effective for pointer-generator networks than the conventional self-supervised learning methods in two spoken-text normalization tasks.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414738","sequence-to-sequence pre-training;pointer-generator networks;self-supervised learning;spoken-text normalization","Learning systems;Solid modeling;Conferences;Training data;Signal processing;Data models;Acoustics","language translation;learning (artificial intelligence);natural language processing;speech recognition;text analysis","sequence-to-sequence mapping;pointer-generator networks;paired data;spoken-style text;style normalized text;spoken-text normalization model;unpaired text data;seq2seq models;conventional self-supervised learning methods;novel self-supervised learning method;MAsked Pointer-Generator Network;pointer-generator net-work;spoken-text normalization tasks;masked Pointer-Generator Network;sequence-to-sequence pre-training;spoken-text normalization method","","","","32","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"POS-based word alignment for small corpus","J. Srivastava; S. Sanyal","Information Technology, Indian Institute of Information Technology, Allahabad, India; Information Technology, Indian Institute of Information Technology, Allahabad, India","2015 International Conference on Asian Language Processing (IALP)","14 Apr 2016","2015","","","37","40","A good quality word alignment system needs a high quality, domain specific and large size of the parallel corpus for training. Finding a high quality parallel corpus for a particular language pair in a specific domain is expensive and hard to find. Conversely, using a small corpus has some advantages like less training time and low memory requirement. It can be created or corrected manually. So this paper is an effort to achieve good quality word alignment with a small size of parallel corpus. This paper describes use of part of speech (POS) tag to improve the performance of statistical word alignment. This approach works well with small size of the corpus. Experiments were conducted on TDIL sample tourism corpus of 1000 sentences for English-Hindi language pair. Out of these 1000 sentences 950 sentences are used for training and 50 sentences are used for testing. F-measure is increased by approximately 4% and Alignment Error Rate (AER) decreased by approximately 4% in comparison to baseline system for word alignment GIZA++.","","978-1-4673-9596-0","10.1109/IALP.2015.7451526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451526","Word alignment;Statistical Machine Translation;POS tagger;Small Corpus","Training;Gold","natural language processing;statistical analysis;text analysis;travel industry","POS-based word alignment system;training;high quality parallel corpus;part of speech tag;POS tag;TDIL sample tourism corpus;English-Hindi language pair;F-measure;alignment error rate;AER","","3","","13","IEEE","14 Apr 2016","","","IEEE","IEEE Conferences"
"A Hybrid Algorithm for Text Classification Based on CNN-BLSTM with Attention","L. Fu; Z. Yin; X. Wang; Y. Liu","Key Laboratory of Intelligent Computing & Signal Processing, Anhui University, Hefei, P.R. China; Key Laboratory of Intelligent Computing & Signal Processing, Anhui University, Hefei, P.R. China; PKU Shenzhen Institute, China; PKU Shenzhen Institute, China","2018 International Conference on Asian Language Processing (IALP)","31 Jan 2019","2018","","","31","34","We propose an effective text classification framework, which is the hybrid of different weights of character-level and word-level features through concatenation based on Convolutional Neural Network-bidirectional long short-term memory with attention (BACNN). The first step is word segmentation or character segmentation in the process of Chinese natural language processing. However, due to the different semantic relations in Chinese, Chinese sentences usually have several ways of word segmentation, which leads to the problem of word segmentation ambiguity. Although Chinese character segmentation is not ambiguity, its meaning is not accurate and rich enough. And in different situations, the character and word are different in importance. Therefore, to overcome the above problems, we propose the method of hybrid different weights of word-level and character-level features to let them make up the respective shortcomings. The experiment results indicate that our proposed method is better than the simple word or character level feature in classification performance.","","978-1-7281-1175-9","10.1109/IALP.2018.8629219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629219","Text classification;Convolutional Neural Network;Bidirectional long short-term memory;Attention mechanism","Text categorization;Convolution;Deep learning;Task analysis;Logic gates;Support vector machines;Convolutional neural networks","learning (artificial intelligence);natural language processing;neural nets;pattern classification;text analysis","natural language processing;text analysis;learning (artificial intelligence);neural nets;pattern classification;word processing;speech recognition;language translation;support vector machines;recurrent neural nets","","2","","13","IEEE","31 Jan 2019","","","IEEE","IEEE Conferences"
"Analyzing Performance of Pre-trained Models in Detecting Sentiments of Code Mixed Manglish Text Reviews from Social Media","S. S. Bhagya; B. S. Nadera","Department of Computer Applications, TKM College of Engineering, Kollam, Kerala, India; Department of Computer Applications, TKM College of Engineering, Kollam, Kerala, India","2022 International Conference on Computing, Communication, Security and Intelligent Systems (IC3SIS)","15 Sep 2022","2022","","","1","6","In the context of social media and multilingual community, code mixing is a common sociolinguistic occurrence. Analyzing code mixed texts from social media is a vital language processing task for applications such as part-of-speech tagging, named entity recognition, sentiment analysis, conversational systems, and machine translation. Very limited code mixed datasets are available for low-resourced languages like Malayalam, Tamil, Telugu etc. The corpora available in the area of code switching contributed significantly to the advancement of research in this field. This work focuses on processing code mixed Mangish text reviews, which is a mixture of Malayalam and English languages, represented in Roman script. Also it evaluates the performance of different pre-trained models like mBERT, DistilBERT(multilingual), RoBERTa, XLM and XLNET to determine the sentiment class of Manglish text reviews from the Dravidian code mix repository and identify which performs better in the context.","","978-1-6654-6883-1","10.1109/IC3SIS54991.2022.9885285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885285","code mixing;natural language processing;sentiment analysis;pre-trained models.","Analytical models;Sentiment analysis;Codes;Social networking (online);Text recognition;Computational modeling;Switches","language translation;natural language processing;text analysis","analyzing performance;code mixed Manglish text reviews;social media;multilingual community;code mixing;common sociolinguistic occurrence;code mixed texts;vital language processing task;part-of-speech tagging;named entity recognition;sentiment analysis;code mixed datasets;low-resourced languages;Malayalam;processing code mixed Mangish text reviews;English languages;different pre-trained models;sentiment class;Dravidian code mix repository","","","","17","IEEE","15 Sep 2022","","","IEEE","IEEE Conferences"
"Design of Memory System for Recursive Neural Network Hardware Accelerator","Y. Liu; X. Liu; K. Zhou; Q. Shi","School of Electronic Engineering, Xi‚Äôan University of Posts and Telecommunication, Xi‚Äôan, China; School of Electronic Engineering, Xi‚Äôan University of Posts and Telecommunication, Xi‚Äôan, China; School of Electronic Engineering, Xi‚Äôan University of Posts and Telecommunication, Xi‚Äôan, China; School of Electronic Engineering, Xi‚Äôan University of Posts and Telecommunication, Xi‚Äôan, China","2023 5th International Conference on Natural Language Processing (ICNLP)","6 Sep 2023","2023","","","440","444","With the remarkable effectiveness of recurrent neural network (RNN) in speech recognition, machine translation and other fields, more and more scholars at home and abroad have begun to pay attention to the research of cyclic neural network acceleration. In recent years, due to the increase of the scale of the recurrent neural network, the software can speed up the network through the weight pruning network model compression technology. The acceleration of the cyclic neural network does not only stay in the aspect of software acceleration, but also in the aspect of hardware, the acceleration strategy includes the design of RNN accelerator based on GPU, FPGA and special ASIC circuit. The storage system almost determines the upper limit of the working efficiency of the accelerator. When the input data cannot be provided to the computing unit in time, the computing unit has to enter the idle state frequently, resulting in low working efficiency. Therefore, storage systems with continuous data feeds are very important for accelerators. This paper proposes a mapping mechanism of MVM operations on hardware operation units, and proposes a storage system with continuous data feeds.","","979-8-3503-0221-9","10.1109/ICNLP58431.2023.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10236742","Cyclic neural network;Efficient storage system;Data mapping mechanism","Neural network hardware;Recurrent neural networks;Graphics processing units;Speech recognition;Software;Hardware;Feeds","AI chips;application specific integrated circuits;field programmable gate arrays;neural nets;recurrent neural nets;speech recognition","acceleration strategy;computing unit;cyclic neural network acceleration;hardware operation units;memory system;recurrent neural network;recursive neural network hardware accelerator;RNN accelerator;software acceleration;storage system;weight pruning network model compression technology","","","","21","IEEE","6 Sep 2023","","","IEEE","IEEE Conferences"
"Boosting LSTM Performance Through Dynamic Precision Selection","F. Silfa; J. M. Arnau; A. Gonz√°lez","Universitat Polit√®cnica de Catalunya, Barcelona, Spain; Universitat Polit√®cnica de Catalunya, Barcelona, Spain; Universitat Polit√®cnica de Catalunya, Barcelona, Spain","2020 IEEE 27th International Conference on High Performance Computing, Data, and Analytics (HiPC)","28 Apr 2021","2020","","","323","333","The use of low numerical precision is a fundamental optimization included in modern accelerators for Deep Neural Networks (DNNs). The number of bits of the numerical representation is set to the minimum precision that is able to retain accuracy based on an offline profiling, and it is kept constant for DNN inference. In this work, we explore the use of dynamic precision selection during DNN inference. We focus on Long Short Term Memory (LSTM) networks, which represent the state-of-the-art networks for applications such as machine translation and speech recognition. Unlike conventional DNNs, LSTM networks remember information from previous evaluations by storing data in the LSTM cell state. Our key observation is that the cell state determines the amount of precision required: time-steps where the cell state changes significantly require higher precision, whereas time-steps where the cell state is stable can be computed with lower precision without any loss in accuracy. We propose a novel hardware scheme that tracks the evolution of the elements in the LSTM cell state and dynamically selects the appropriate precision on each time-step. For a set of popular LSTM networks, it chooses the lowest precision for 57% of the time, outperforming systems that fix the precision statically. We evaluate our proposal on top of a modern highly-optimized LSTM accelerator, and show that it provides 1.46x speedup and 19.2% energy savings on average without degrading the model accuracy. Our scheme has an overhead of less than 8%.","2640-0316","978-1-6654-2292-5","10.1109/HiPC50609.2020.00046","EU(grant numbers:833057); Spanish State Research Agency(grant numbers:TIN2016-75344-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406683","RNNs;Long Short Term Memory;Accelerators;Quantization","High performance computing;Conferences;Neural networks;Speech recognition;Hardware;Proposals;Machine translation","deep learning (artificial intelligence);inference mechanisms;recurrent neural nets","LSTM performance;dynamic precision selection;deep neural networks;numerical representation;DNN inference;long short term memory networks;LSTM cell state;LSTM networks","","1","","25","IEEE","28 Apr 2021","","","IEEE","IEEE Conferences"
"Lexical Ambiguity in Natural Language Processing Applications","N. S. Harsha; C. N. Kumar; V. K. Sonthi; K. Amarendra","Department of Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department of Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department of Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Department of Computer Science & Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India","2022 International Conference on Electronics and Renewable Systems (ICEARS)","13 Apr 2022","2022","","","1550","1555","Natural language processing (NLP) has long been regarded as one of the most essential areas of AI research. However, in comparison to other fields, development in natural language processing has been modest. The goal of this research is to undertake a comprehensive literature evaluation in order to identify the most popular NLP applications, approaches, and tough difficulties. This article gives a broad overview as well as a discussion of lexical ambiguity in NLP applications such as Machine Translation, Speech Recognition, Text Mining, and Document Retrieval. The goal of this work is to explain several NLP applications, as well as lexical ambiguity and important approaches for dealing with it.This article is to discuss about one of the algorithm Simple 1-nearest-neighbour which deals with Lexical Ambiguity.","","978-1-6654-8425-1","10.1109/ICEARS53579.2022.9752297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752297","Natural Language Processing;Information;NLP Applications;Lexical Ambiguity","Text mining;Renewable energy sources;Text categorization;Speech recognition;Classification algorithms;Machine translation;Artificial intelligence","natural language processing;nearest neighbour methods","natural language processing;AI research;comprehensive literature evaluation;NLP applications;Simple 1-nearest-neighbour;Lexical Ambiguity","","","","36","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"Effective Distributed Representation of Code-Mixed Text","A. Malte; S. Sonawane","Department of Computer Engineering, Pune Institute of Computer Technology, Pune, Maharashtra, India; Department of Computer Engineering, Pune Institute of Computer Technology, Pune, Maharashtra, India","2019 IEEE 16th India Council International Conference (INDICON)","12 Mar 2020","2019","","","1","4","As an increasing number of people embrace social media, mining data generated from the same has become an important task. Possible applications range from opinion mining, sentiment analysis to hate speech detection. More importantly, analyzing code-mixed multilingual text has gained popularity due to the reason that it holds important socio-cultural clues that may be lost in translation. Methods to effectively analyse code-mixed Hindi/English(Hinglish) text have been explored in this paper. Firstly, we generate a large scale code-mixed corpus that would aid in further research of code mixed text on social media. High-quality word embeddings are trained on this code-mixed text. Finally, we demonstrate the efficacy of our proposed method by training machine learning models that improve upon the previous state-of-the-art using a much lighter and explainable architecture. Our main intention behind training the classifier model was not only high performance but also good model explainability and speed.","2325-9418","978-1-7281-2327-1","10.1109/INDICON47234.2019.9028960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028960","Machine Learning;NLP;code-mixing;word embeddings;explainable AI","Social network services;Task analysis;Training;Computer architecture;Machine learning;Computational modeling;Data models","data mining;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis","code-mixed multilingual text;social media;effective distributed representation;socio-cultural clues;data mining;code-mixed Hindi-English text;large scale code-mixed corpus;high-quality word embeddings;machine learning models;classifier model","","1","","14","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Approximate Computing for Long Short Term Memory (LSTM) Neural Networks","S. Sen; A. Raghunathan","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","18 Oct 2018","2018","37","11","2266","2276","Long Short Term Memory (LSTM) networks are a class of recurrent neural networks that are widely used for machine learning tasks involving sequences, including machine translation, text generation, and speech recognition. Large-scale LSTMs, which are deployed in many real-world applications, are highly compute intensive. To address this challenge, we propose AxLSTM, an application of approximate computing to improve the execution efficiency of LSTMs. An LSTM is composed of cells, each of which contains a cell state along with multiple gating units that control the addition and removal of information from the state. The LSTM execution proceeds in timesteps, with a new symbol of the input sequence processed at each timestep. AxLSTM consists of two techniques‚ÄîDynamic Timestep Skipping (DTS) and Dynamic State Reduction (DSR). DTS identifies, at runtime, input symbols that are likely to have little or no impact on the cell state and skips evaluating the corresponding timesteps. In contrast, DSR reduces the size of the cell state in accordance with the complexity of the input sequence, leading to a reduced number of computations per timestep. We describe how AxLSTM can be applied to the most common application of LSTMs, viz., sequence-to-sequence learning. We implement AxLSTM within the TensorFlow deep learning framework and evaluate it on 3 state-of-the-art sequence-to-sequence models. On a 2.7 GHz Intel Xeon server with 128 GB memory and 32 processor cores, AxLSTM achieves  $ {1.08\times -1.31 \times }$  speedups with minimal loss in quality, and  $ {1.12 \times -1.37 \times }$  speedups when moderate reductions in quality are acceptable.","1937-4151","","10.1109/TCAD.2018.2858362","Semiconductor Research Corporation; Defense Advanced Research Projects Agency; National Science Foundation(grant numbers:1423290); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417946","Approximate computing;long short term memory (LSTM) networks;sequence-to-sequence learning","Approximate computing;Logic gates;Recurrent neural networks;Machine learning;Computational modeling;Embedded systems;Approximate computing;Long short term memory;Sequences","learning (artificial intelligence);recurrent neural nets","approximate computing;long Short Term Memory neural networks;recurrent neural networks;machine learning tasks;machine translation;text generation;speech recognition;AxLSTM;cell state;input sequence;DTS;DSR;sequence-to-sequence learning;TensorFlow deep learning framework;LSTM execution;dynamic state reduction;dynamic timestep skipping;Intel Xeon server","","31","","42","IEEE","23 Jul 2018","","","IEEE","IEEE Journals"
"A practical approach for representing context and for performing word sense disambiguation using neural networks","S. I. Gallant","ICANN, Cambridge, MA, USA","IJCNN-91-Seattle International Joint Conference on Neural Networks","6 Aug 2002","1991","ii","","1007 vol.2","","Summary form only given. The author proposes a method for representing context information so that the correct meaning for a word in a sentence can be selected. The approach is primarily based upon work by Waltz and Pollack, who emphasized neurally plausible systems. By contrast the author focuses upon computationally feasible methods applicable to full-scale natural language processing systems. There are two key elements: a collection of context vectors defined for every word used by a natural language processing system, and a context algorithm that computes a dynamic context vector at any position in a body of text. Once the dynamic context vector has been computed it is easy to choose among competing meanings for a word. This choice of definitions is essentially a neural network computation, and neural network learning algorithms should be able to improve the system's choices. Good candidates for full-scale context vector implementations are machine translation systems and Japanese word processors.<>","","0-7803-0164-1","10.1109/IJCNN.1991.155584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=155584","","Neural networks;Speech synthesis;Natural language processing;Testing;Synthesizers;Computer networks;Large-scale systems;Natural languages;Application software;Automatic control","learning systems;natural languages;neural nets","context representation;word sense disambiguation;neural networks;natural language processing systems;context vectors;learning algorithms;machine translation systems;Japanese word processors","","2","1","","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Attention-Based Models for Text-Dependent Speaker Verification","F. A. Rezaur rahman Chowdhury; Q. Wang; I. L. Moreno; L. Wan","Washington State University; Google Inc., USA; Google Inc., USA; Google Inc., USA","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","5359","5363","Attention-based models have recently shown great performance on a range of tasks, such as speech recognition, machine translation, and image captioning due to their ability to summarize relevant information that expands through the entire length of an input sequence. In this paper, we analyze the usage of attention mechanisms to the problem of sequence summarization in our end-to-end text-dependent speaker recognition system. We explore different topologies and their variants of the attention layer, and compare different pooling methods on the attention weights. Ultimately, we show that attention-based models can improves the Equal Error Rate (EER) of our speaker verification system by relatively 14% compared to our non-attention LSTM baseline model.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461587","‚îÄ Attention-based model;sequence summarization;speaker recognition;pooling;LSTM","Google;Training;Microsoft Windows;Speaker recognition;Error analysis;Password;Machine learning","neural nets;speaker recognition","equal error rate;pooling methods;text-dependent speaker recognition system;sequence summarization;attention mechanisms;speaker verification system;attention-based models;attention weights;attention layer","","31","","17","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Applications of Recurrent Neural Network Language Model in Offline Handwriting Recognition and Word Spotting","N. Li; J. Chen; H. Cao; B. Zhang; P. Natarajan","Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Raytheon BBN Technologies, Cambridge, MA, USA; Information Sciences Institute, University of Southern California, Marina del Rey, California, USA","2014 14th International Conference on Frontiers in Handwriting Recognition","15 Dec 2014","2014","","","134","139","The recurrent neural network language model (RNNLM) is a discriminative, non-Markovian model that can capture long-span word history in natural language. It has been proved to be successful in automatic speech recognition and machine translation. In this work, we applied RNNLM to the n-best rescoring stage of the state-of-the-art BBN Byblos OCR (optical character recognition) system for handwriting recognition.1 With RNNLM scores as additional features, our system achieved significant improvement (p <; 0.001), a 3.5% relative reduction on OCR word error rate, compared with a high baseline that uses n-gram language model for rescoring. We have also developed a novel method to integrate the OCR n-best RNNLM scores into the word posterior probabilities in OCR confusion networks, which resulted in consistent observable improvements in word spotting for OCR'ed handwritten documents, as measured by both mean average precision (MAP) and detection-error tradeoff (DET) curves.","2167-6445","978-1-4799-4334-0","10.1109/ICFHR.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6981009","optical character recognition;keyword search;recurrent neural networks;information retrieval","Hidden Markov models;Optical character recognition software;Handwriting recognition;Recurrent neural networks;Training;Lattices;Character recognition","document image processing;handwriting recognition;language translation;optical character recognition;recurrent neural nets;word processing","recurrent neural network language model;offline handwriting recognition;word spotting;nonMarkovian model;long-span word history;automatic speech recognition;machine translation;BBN Byblos OCR;optical character recognition system;RNNLM scores;OCR word error rate;word posterior probabilities;OCR confusion networks;OCR handwritten documents;mean average precision;MAP;detection-error tradeoff curves;DET curves","","7","","36","IEEE","15 Dec 2014","","","IEEE","IEEE Conferences"
"Retrieval-Based Natural 3D Human Motion Generation","Z. Tan; W. Yang; S. Wu","School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","It is challenging to generate 3D human motions automatically from text. In an ideal scenario, the generated motions should explore the text-grounded motion space while accurately depicting the content in the prescribed text descriptions. Text2length and text2motion training have been used to address this problem in previous research. There is, however, a lack of knowledge about the relationship between motion length and text. In this work, context-aware retrieval-based approaches are proposed for predicting motion lengths and generating proper 3D motions (C-MO). Specifically, we train a context-aware encoder-decoder model that uses the previous output of the decoder or the embedding vector of a ground truth motion as context so that the model becomes increasingly aware of previous alignments. Retrieving the most similar motions from the training set is based on the trained model given a text. Finally, we use the retrieval motion to guide the probability distribution for the final generated motions. Our method combines the advantages of both information retrieval and neural machine translation. C-MO is evaluated on a large-scale dataset, KIT, and its experimental results show that it achieves great improvements over the state-of-the-art.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10096666","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096666","Retrieval;3D human motion;text-3D","Training;Measurement;Three-dimensional displays;Speech coding;Natural languages;Signal processing;Probability distribution","","","","","","18","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Beyond Error Propagation: Language Branching Also Affects the Accuracy of Sequence Generation","L. Wu; X. Tan; T. Qin; J. Lai; T. -Y. Liu","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; Microsoft Research Asia, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","30 Aug 2019","2019","27","12","1868","1879","Sequence generation tasks, such as neural machine translation (NMT) and abstractive summarization, usually suffer from exposure bias as well as the error propagation problem due to the autoregressive training and generation. Many previous works have discussed the relationship between error propagation and the accuracy drop problem (i.e., the right part of the generated sentence is often worse than its left part in left-to-right decoding models). In this paper, taking NMT as a typical sequence generation task, we measure the accuracy of the generated sentence with various metrics and conduct a series of analyses to deeply understand the accuracy drop problem. We obtain several interesting findings. First, The role of error propagation on accuracy drop is overstated in the literature, although it is indeed a cause to the accuracy drop problem. Second, Characteristics of a language play a more important role in causing the accuracy drop problem: the left part of the generated sentence in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are also confirmed on other generation tasks (e.g., image captioning, abstractive summarization and language modeling) with multiple left/right-branching languages, as well as in various model structures.","2329-9304","","10.1109/TASLP.2019.2933727","National Natural Science Foundation of China(grant numbers:61573387); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790778","Sequence generation;error propagation;accuracy drop;language characteristic","Task analysis;Decoding;Training;Measurement;Linguistics;Speech processing;Maximum likelihood estimation","natural language processing;text analysis","sequence generation tasks;abstractive summarization;error propagation problem;autoregressive training;accuracy drop problem;left-branching language;language modeling;sequence generation task;neural machine translation","","1","","50","IEEE","7 Aug 2019","","","IEEE","IEEE Journals"
"A Combination of Statistical and Rule-Based Approach for Mongolian Lexical Analysis","L. Zhao; J. Men; C. Zhang; Q. Liu; W. Jiang; J. Wu; Q. Chang","College of Computer and Information Technology, Henan Normal University, Xinxiang, China; College of Computer Software, Henan Polytechnic Institute, Nanyang, China; College of Computer and Information Technology, Henan Normal University, Xinxiang, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Mongolian Studies College, Inner Mongolia University for the Nationalities, Hohhot, China; Mongolian Studies College, Inner Mongolia University for the Nationalities, Hohhot, China","2010 International Conference on Asian Language Processing","6 Jan 2011","2010","","","7","10","Mongolian lexical analysis is the first step in Mongolian information processing such as Chinese-Mongolian machine translation. In this paper, we introduce a statistic and rule based approach to solving the Mongolian word segmentation & POS tagging all at once. In this method, we use tree frame as basic statistical model. And then we combine the model with some rules to improve the lexical analysis system accuracy. The experiment results show that the word-level accuracy of joint segmentation and POS tagging is 95.2%, stem / postfix-level accuracy is 94.6%.","","978-1-4244-9063-9","10.1109/IALP.2010.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5681555","Mongolian information processing;Mongolian Word Segment;Mongolian Part of Speech Tagging;Joint segmentation and POS tagging","Tagging;Dictionaries;Joints;Accuracy;Pragmatics;Analytical models;Probability","identification technology;knowledge based systems;natural language processing;statistical analysis;word processing","Mongolian lexical analysis;Mongolian information processing;Chinese-Mongolian machine translation;statistical approach;rule based approach;Mongolian word segmentation;POS tagging;tree frame;word level accuracy","","1","","12","IEEE","6 Jan 2011","","","IEEE","IEEE Conferences"
"Hybrid CNNs-LSTM Deep Analyzer for Arabic Opinion Mining","M. Al Omari; M. Al-Hajj; A. Sabra; N. Hammami","Centre for Language Sciences and Communication, Lebanese University, Beirut, Lebanon; Centre for Language Sciences and Communication, Lebanese University, Beirut, Lebanon; Centre for Language Sciences and Communication, Lebanese University, Beirut, Lebanon; College of Computer and Information Sciences, Jouf University, Aljouf, KSA","2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS)","16 Dec 2019","2019","","","364","368","Deep learning models have showed great capabilities in data modelling on natural language processing various applications, including sentiment analysis, part-of-speech tagging, machine translation, and many others. In particular, convolutional neural network (CNNs) and long-term short memory (LSTM) have proved to be effective in capturing longterm dependencies in sequential data that result in state-of-the-art performance in comparison to traditional machine learning algorithms. This research paper, therefore, structures an enhanced model of both CNNs and LSTM for the feature resourcefulness of Arabic text data on freely available benchmark datasets, with word2vec representation model for each corpus. The model is projected for Arabic sentiment analysis (ASA) in highlight. The proposed architecture has achieved better performance on three datasets out of five in comparison to previous studies. In research conduct, the model achieved a total accuracy of 0.881 for Main-AHS, 0.968 for Sub-AHS, 0.842 for Ar-Twitter, 0.7918 for ASTD, 0.903 for OCLAR.","","978-1-7281-2946-4","10.1109/SNAMS.2019.8931819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931819","Natural language processing (NLP);Arabic NLP;Arabic sentiment analysis (ASA);Deep sequential learning","Sentiment analysis;Feature extraction;Computer architecture;Twitter;Deep learning;Data models","convolutional neural nets;data mining;learning (artificial intelligence);natural language processing;recurrent neural nets;sentiment analysis;social networking (online);text analysis","hybrid CNNs-LSTM deep analyzer;Arabic opinion mining;deep learning models;part-of-speech tagging;machine translation;convolutional neural network;sequential data;feature resourcefulness;Arabic text data;word2vec representation model;Arabic sentiment analysis;long-term short memory","","6","","26","IEEE","16 Dec 2019","","","IEEE","IEEE Conferences"
"Traffic Signs Classification using Convolutional Neural Networks: A review","Palak; A. L. Sangal","Dept. of Computer Science and Engineering, Dr B R Ambedkar National Institute of Technology, Jalandhar, India; Dept. of Computer Science and Engineering, Dr B R Ambedkar National Institute of Technology, Jalandhar, India","2021 2nd International Conference on Secure Cyber Computing and Communications (ICSCCC)","13 Jul 2021","2021","","","450","455","With advancement in artificial intelligence (AI), Deep learning models are used to imitate the actions of human beings. These activities of personage are controlled by their brain and similar to that, machines are capable of data processing, decision making, speech recognition and language translations just like human beings. One of the applications of deep learning includes Autonomous Vehicles design i.e., driver less cars. To implement this, we need an automatic traffic sign recognition (TSR) model. These models are designed with the use of convolutional neural networks (CNN). The main task of this model is to extract the various features of the different traffic sign images and classify according to unique categories. This paper includes a comprehensive review of various models that can be used for classifying traffic signs. Researchers have applied various CNN models to predict the class of traffic sign and these are proven to be better than machine learning algorithms. CNN works as a feed forward neural network which has been stimulated from animal visual cortex.","","978-1-6654-4415-6","10.1109/ICSCCC51823.2021.9478172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478172","Artificial Neural Network (ANN);Convolutional Neural Network (CNN);Classification;Deep learning;TrafficSigns","Deep learning;Visualization;Graphics processing units;Computer architecture;Brain modeling;Feature extraction;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;feedforward neural nets;image classification","language translations;human beings;autonomous vehicles design;automatic traffic sign recognition model;convolutional neural networks;traffic sign images;CNN models;machine learning algorithms;feed forward neural network;artificial intelligence;deep learning models;data processing;decision making;speech recognition;feature extraction;animal visual cortex","","3","","22","IEEE","13 Jul 2021","","","IEEE","IEEE Conferences"
"Recognition Of Handwritten English Character Using Convolutional Neural Network","S. Katoch; M. Rakhra; D. Singh","Department of Computer Science, Lovely Professional University, Phagwara, India; Department of Computer Science, Lovely Professional University, Phagwara, India; Department of Computer Science, Lovely Professional University, Phagwara, India","2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)","17 Mar 2023","2022","","","1","6","In the domain of computer vision and image processing, one of the most active and difficult study fields is handwritten character recognition. It may be used as a reading tool for bank checks, for identifying characters on forms, and for a slew of other purposes. The optical character recognition of the papers is similar to documents produced by hand by a human. This OCR is put to use to improve the simplification of the process of character translation, which may be obtained from a broad range of file types, such as image and word document files. Researchers have made tremendous progress in HCR by making use of vast amounts of raw data and new breakthroughs in Deep Learning and Machine Learning algorithms. The fundamental purpose of this research paper is to give a solution for several techniques of handwriting recognition. These methods include the usage of touch input through a mobile screen as well as the use of an image file. CNN is used to identify characters in a test dataset in this work. Work on CNNs' capacity to detect characters from a picture dataset and their accuracy of recognition will be examined. Characters are recognized by CNN by comparing and contrasting their shapes and distinguishing characteristics. The dataset A_Z Handwritten was used to test our CNN implementation's handwriting accuracy and model gives the 100% result to recognize the character.","","978-1-6654-9902-6","10.1109/AIST55798.2022.10064860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064860","Handwritten character recognition;A_Z Dataset;Deep Learning;CNN (Convolutional neural network);Feature extraction;Classification","Training;Handwriting recognition;Shape;Optical character recognition;Speech recognition;Production;Convolutional neural networks","computer vision;convolutional neural nets;deep learning (artificial intelligence);feature extraction;handwriting recognition;handwritten character recognition;learning (artificial intelligence);neural nets;optical character recognition","active study fields;character translation;convolutional neural network;dataset A_Z Handwritten;difficult study fields;file types;handwriting recognition;Handwritten english character;image file;image processing;Machine Learning algorithms;optical character recognition;word document files","","","","27","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Automatic Pilot Report Extraction from Radio Communications","S. Chen; H. Kopald; B. Avjian; M. Fronzak","Proactive Safety Monitoring, The MITRE Corporation, McLean, USA; Operational Safety Mitigations, The MITRE Corporation, McLean, USA; NAS Automation Evolution, The MITRE Corporation, McLean, USA; NAS Future Vision & Research, The MITRE Corporation, McLean, USA","2022 IEEE/AIAA 41st Digital Avionics Systems Conference (DASC)","31 Oct 2022","2022","","","1","8","Pilot reports (PIREPs) provide weather information critical to the safety and efficiency of the National Airspace System (NAS), but existing PIREP submission and dissemination mechanisms have persistent deficiencies. To help the Federal Aviation Administration (FAA) modernize the PIREP system and increase the number of PIREPs available to the National Airspace System (NAS), the MITRE Corporation Center for Advanced Aviation System Development (MITRE CAASD) developed a prototype to evaluate the feasibility of automatically extracting PIREPs directly from pilot radio communications using automatic speech recognition, neural classifiers, and rules-based translation. The prototype detection system was better than 90 percent accurate and is estimated to produce an order of magnitude more PIREPs than are captured through traditional means in the NAS today. This automated process could ease the burden on controllers and increase the consistency and geographical density of pilot reports to better inform both tactical and strategic traffic planning.","2155-7209","978-1-6654-8607-1","10.1109/DASC55683.2022.9925803","Federal Aviation Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925803","automatic speech recognition;machine learning;pilot reports;pireps;radio communications","Prototypes;Process control;FAA;Aerospace electronics;Safety;Planning;Autopilot","air traffic control;aircraft communication;speech recognition","weather information;safety;National Airspace System;dissemination mechanisms;persistent deficiencies;Federal Aviation Administration;PIREP system;PIREPs;MITRE Corporation Center;Advanced Aviation System Development;MITRE CAASD;pilot radio communications;automatic speech recognition;prototype detection system;NAS today;pilot reports;automatic pilot report extraction;efficiency 90.0 percent","","2","","24","IEEE","31 Oct 2022","","","IEEE","IEEE Conferences"
"Large marginwavelet-based dictionary for signal classification","F. Yger; A. Rakotomamonjy","LITIS EA 4108, Universit√© de Rouen, Saint Etienne du Rouvray, France; LITIS EA 4108, Universit√© de Rouen, Saint Etienne du Rouvray, France","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","2182","2185","This paper addresses the problem of automatic wavelet feature extraction for signal classication. We propose to jointly learn wavelet-based features (including scale and translation of the wavelet as well as its shape) and a decision function by casting the problem as a Multi-Kernel Learning problem. A novel active constraints algorithm is then proposed. Our method has been tested on a toy dataset and compared to classical methods with competitive results.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495675","SVM;parametrized waveform;Multi-Kernel Learning","Dictionaries;Pattern classification;Kernel;Feature extraction;Shape;Testing;Support vector machines;Support vector machine classification;Pattern recognition;Wavelet coefficients","feature extraction;learning (artificial intelligence);signal classification;wavelet transforms","automatic wavelet feature extraction;signal classication;large margin wavelet-based dictionary;multikernel learning;decision function;active constraints algorithm;toy dataset","","","","10","IEEE","28 Jun 2010","","","IEEE","IEEE Conferences"
"An Accurate Speech Emotion Analysis using Gradient Boosting and BiLSTM Techniques","I. S. Raina; N. Gupta; G. Kumar; G. Rathee","Department of Computer Science Engineering, Netaji Subhas University of Technology Main Campus, New Delhi, India; Department of Computer Science Engineering, Netaji Subhas University of Technology Main Campus, New Delhi, India; Department of Computer Science Engineering, Netaji Subhas University of Technology Main Campus, New Delhi, India; Department of Computer Science Engineering, Netaji Subhas University of Technology Main Campus, New Delhi, India","2023 IEEE International Conference on Contemporary Computing and Communications (InC4)","29 Sep 2023","2023","1","","1","6","In the field of speech emotion recognition, machine learning (ML) algorithms face a challenging problem in recognizing emotional states. Speech emotion recognition (SER) is crucial in many real-time solutions, including human-robot interaction, assessing human behavior, and analyzing the emotional state of speakers. Due to its intricacy, SER is a task that is still difficult to complete but one that is becoming more and more crucial in emotional computing. Although there are many datasets available for SER, we are selecting RADVESS since we believe it to be the most ap-propriate. For many applications, such as a speech-to-speech translation system, feature selection can increase the accuracy of speech emotion recognition. Although tens of thousands of features can be retrieved from speech signals, it can be challenging to determine which elements are most closely associated to speaker emotional state. Most characteristics that are connected to emotional states are yet to be discovered. Mel Spectrogram and MFCC feature selection are the major topics of this research. Then passing these combined features through a modified CNN. In this paper we have used various neural network techniques (NN) for SER. The models we used include CNN, MLP, CNN-Attention-BiLSTM, Gradient Boosting. CNN-Attention- BiLSTM gives the best results among these models with accuracy of about 86 percent, precision of 0.834, recall of 0.802, F1-Score of 0.802.","","979-8-3503-3577-4","10.1109/InC457730.2023.10263225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10263225","Speech Recognition;Emotions;LSTM;CNN","","","","","","","10","IEEE","29 Sep 2023","","","IEEE","IEEE Conferences"
"A Review on Various Facial Expression Recognition Techniques","P. Pranathi; C. Lakshmi; M. Suneetha","Department of CSE, SRMIST Kattankulathur, Chennai, India; Department of Computational Intelligence, SRMIST Kattankulathur, Chennai, India; Department of Information Technology, V R Siddhartha Engineering College, Kanuru, India","2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)","20 Dec 2021","2021","","","1246","1254","In present days, Facial Expression Acknowledgment (FER) has now become an unbelievable issue in contemporary science study. Facial expression, a vital mode of communication of human emotions, has been studied throughout the world in the fields of Driver Protection, human computer interaction (HCI), deception detection, health care, monitoring etc. Generative Adverse Networks (GANs), with various gestures, can create more one-to-one faces that can be used to improve data base. Facial expression recognition is one of the most effective naturalizing and instant means for people to convey their feelings and intentions. FER is a common research subject which has brought a range of computational vision tasks, such as image generation, video generation, super resolution reconstruction and image translation. Happiness, sorrow, surprise, disgust, fear and rage all constitute six universal facial expressions. Other things such as eye change, breathing and combing the variance in expression will underline all the emotion above. A face recognition device is a behavior request for the identification or authentication of an individual automatically from a digital picture or from a video source. This analysis paper helps to explain methods, strategies and challenges in the real time FER climate, which discuss and examine problems and challenges. This paper finally concludes the latest developments and addresses the problems facing the FER process and the potential of future growth.","2768-0673","978-1-6654-2642-8","10.1109/I-SMAC52330.2021.9640733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9640733","Facial Expression Recognition;Image;Machine learning;deep learning;optimization","Human computer interaction;Support vector machines;Face recognition;Speech recognition;Streaming media;Speech enhancement;Feature extraction","computer vision;emotion recognition;face recognition;gesture recognition;video signal processing","video source;digital picture;GANs;human computer interaction;human emotion communication;real-time FER climate;universal facial expressions;computational vision tasks;database;Generative Adverse Networks;health care;deception detection;Driver Protection;contemporary science study;Facial Expression Acknowledgment;facial expression recognition techniques;FER process;face recognition device","","","","66","IEEE","20 Dec 2021","","","IEEE","IEEE Conferences"
"A Quantitative Analysis Framework for Recurrent Neural Network","X. Du; X. Xie; Y. Li; L. Ma; Y. Liu; J. Zhao","Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Kyushu University, Japan; Nanyang Technological University, Singapore; Kyushu University, Japan","2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)","9 Jan 2020","2019","","","1062","1065","Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.","2643-1572","978-1-7281-2508-4","10.1109/ASE.2019.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952565","recurrent neural netwrod;model abstraction;quantitative analysis;similarity metrics;coverage criteria","Testing;Tools;Training data;Statistical analysis;Recurrent neural networks;Security;Computer architecture","image classification;learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition;video signal processing","image classification;defect-triggering tests;adversarial attacks;quantitative analysis framework;recurrent neural network;automatic speech recognition;natural language processing;machine translation;reliability issues;security analysis;software systems;RNN architectures;industrial-grade RNN models;test generation tools;LSTM;GRU;adversarial sample detectors;deepstellar;video demonstration","","4","","18","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Parsing modern standard Arabic using Treebank resources","M. Al-Emran; S. Zaza; K. Shaalan","Al Buraimi University College, Al Buraimi, Oman; The British University in Dubai, Dubai, UAE; The British University in Dubai, Dubai, UAE","2015 International Conference on Information and Communication Technology Research (ICTRC)","16 Jul 2015","2015","","","80","83","A Treebank is a linguistic resource that is composed of a large collection of manually annotated and verified syntactically analyzed sentences. Statistical Natural Language Processing (NLP) approaches have been successful in using these annotations for developing basic NLP tasks such as tokenization, diacritization, part-of-speech tagging, parsing, among others. In this paper, we address the problem of exploiting Treebank resources for statistical parsing of Modern Standard Arabic (MSA) sentences. Statistical parsing is significant for NLP tasks that use parsed text as an input such as Information Retrieval, and Machine Translation. We conducted an experiment on Pen Arabic Treebank (PATB) and the parsing performance obtained in terms of Precision, Recall, and F-measure was 82.4%, 86.6%, 84.4%, respectively.","","978-1-4799-8966-9","10.1109/ICTRC.2015.7156426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156426","Statistical Parsing;Treebank;Arabic","Natural language processing;Standards;Training;Pragmatics;Syntactics;Gold","computational linguistics;grammars;natural language processing;statistical analysis","modern standard Arabic parsing;Treebank resources;linguistic resource;manually annotated sentences;verified syntactically analyzed sentences;statistical natural language processing;NLP tasks;tokenization;diacritization;part-of-speech tagging;MSA sentence statistical parsing;information retrieval;machine translation;Pen Arabic Treebank;PATB","","17","","19","IEEE","16 Jul 2015","","","IEEE","IEEE Conferences"
"CRF based Name Entity Recognition (NER) in Manipuri: A highly agglutinative Indian Language","K. Nongmeikapam; T. Shangkhunem; N. M. Chanu; L. N. Singh; B. Salam; S. Bandyopadhyay","Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India; Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India; Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India; Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India; Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India; Department of Computer Sc. & Engineering Manipur Institute of Technology, Manipur University, Imphal, India","2011 2nd National Conference on Emerging Trends and Applications in Computer Science","15 Apr 2011","2011","","","1","6","This paper deals about the Name Entity Recognition (NER) of Manipuri, a highly agglutinative Indian Language. This language is an Eight Scheduled Language of Indian Constitution. NER plays an important role in the applications of Natural Language Processing like Machine Translation, Part of Speech tagging, Information Retrieval, Question Answering etc. Feature selection is an important factor in recognition of Manipuri Name Entity using Conditional Random Field (CRF). This model proved to have a Recall of 81.12%, Precision of 85.67% and F-Score of 83.33%.","","978-1-4244-9581-8","10.1109/NCETACS.2011.5751390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751390","NER;CRF;feature;Manipuri;stem","Training;Testing;Hidden Markov models;Tagging;Artificial neural networks;Object recognition;Organizations","natural language processing","name entity recognition;Manipuri language;Indian language;natural language processing;machine translation;part-of-speech tagging;information retrieval;question answering;feature selection factor;conditional random field;recall model;precision model;F-score model","","8","","17","IEEE","15 Apr 2011","","","IEEE","IEEE Conferences"
"A rule-based approach for building an artificial English-ASL corpus","Z. Tmar; A. Othman; M. Jemni","Research Laboratory LaTICE, University of Tunis, Tunisia; Research Laboratory LaTICE, University of Tunis, Tunisia; Research Laboratory LaTICE, University of Tunis, Tunisia","2013 International Conference on Electrical Engineering and Software Applications","15 Aug 2013","2013","","","1","4","A serious problem facing the Community for researchers in the field of sign language is the absence of a large parallel corpus for signs language. The ASLG-PC12 project, proposes a rule-based approach for building big parallel corpus between English written texts and American Sign Language Gloss. We present a novel algorithm which transforms an English part-of-speech sentence to ASL gloss. This project was started in the beginning of 2010, a part of the project WebSign, and it offers today a corpus containing more than one hundred million pairs of sentences between English and ASL gloss. It is available online for free in order to develop and design new algorithms and theories for American Sign Language processing, for example statistical machine translation and any related fields. In this paper, we present tasks for generating ASL sentences from the corpus Gutenberg Project that contains only English written, texts.","","978-1-4673-6301-3","10.1109/ICEESA.2013.6578458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578458","Natural Language Processing;Sign Language;Parallel Corpora","Assistive technology;Gesture recognition;Transforms;Buildings;Educational institutions;Pragmatics;Communities","knowledge based systems;natural language processing;sign language recognition;text analysis","rule-based approach;artificial English-ASL corpus;parallel corpus;ASLG-PC12 project;English written text;American sign language gloss;English part-of-speech sentence;ASL gloss;WebSign;American sign language processing;statistical machine translation;ASL sentences;Gutenberg project","","4","","19","IEEE","15 Aug 2013","","","IEEE","IEEE Conferences"
"FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-Gram Language Model","D. Yuret","Department of Computer Engineering, Ko√ß University, Istanbul, Turkey","IEEE Signal Processing Letters","10 Sep 2012","2012","19","11","725","728","Lexical substitutes have found use in areas such as paraphrasing, text simplification, machine translation, word sense disambiguation, and part of speech induction. However the computational complexity of accurately identifying the most likely substitutes for a word has made large scale experiments difficult. In this letter we introduce a new search algorithm, FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for a given word in a sentence based on an n-gram language model. The computation is sublinear in both K and the vocabulary size V. An implementation of the algorithm and a dataset with the top 100 substitutes of each token in the WSJ section of the Penn Treebank are available at http://goo.gl/jzKH0.","1558-2361","","10.1109/LSP.2012.2215587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6287552","Lexical substitutes;statistical language modeling","Upper bound;Mathematical model;Signal processing algorithms;Equations;Vocabulary;Context;Probability","computational complexity;natural language processing;search problems;trees (mathematics);vocabulary","FASTSUBS;lexical substitutes;N-gram language model;paraphrasing;text simplification;machine translation;word sense disambiguation;part of speech induction;computational complexity;large scale experiments;search algorithm;n-gram language model;vocabulary size;WSJ section;Penn treebank","","4","1","9","IEEE","27 Aug 2012","","","IEEE","IEEE Journals"
"Target Word Sense Disambiguation system for Kannada language","S. Parameswarappa; V. N. Narayana","Department of Computer Science & Engineering, Government Polytechnic Ramanagara, India; Depatiment of Computer Science & Engineering, Malnad College of Engineering, Hassan, India","3rd International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom 2011)","3 May 2012","2011","","","269","273","The process of identifying the correct sense of a word in a specific context is called as Word Sense Disambiguation (WSD). It is essential for communication in a natural language. It is motivated by its use in many crucial applications such as Information retrieval, Information extraction, Machine Translation, Part-of-Speech tagging etc. The aim of our research is to develop a WSD system for target words in Kannada language. This paper presents our preliminary work towards building target word sense disambiguation system for Kannada language. To the best of our knowledge, this is the first attempt towards building WSD system for Kannada. Our work is a mile stone for Kannada language processing activities. In the present work, we exploited the compound words clue and syntactic features in a local context for target word sense disambiguation. It is noticed that, the use of syntax will improve the performance of the WSD system. The Kannada Shallow parser has been used for syntactic analysis. The ambiguous target word is disambiguated using supervised learning techniques. The experiments are conducted using Naive Bayes classifier. We created Kannada Corpora for evaluating the experiments and the results are encouraging.","","978-8-19106-918-1","10.1049/ic.2011.0097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6193586","Compound words;Sense inventories;Target word;Parser;Syntax;Wordnet","","learning (artificial intelligence);natural language processing;pattern classification","target word sense disambiguation system;Kannada language;natural language;information retrieval;information extraction;machine translation;part-of-speech tagging;WSD system;compound words clue feature;compound words syntactic feature;Kannada shallow parser;naive Bayes classifier;Kannada corpora","","2","","","","3 May 2012","","","IET","IET Conferences"
"A Semi-Supervised method for Persian homograph Disambiguation","N. Riahi; F. Sedghi","Computer Engineering Department, Alzahra University, Tehran, Iran; Computer Engineering Department, Alzahra University, Tehran, Iran","20th Iranian Conference on Electrical Engineering (ICEE2012)","3 Sep 2012","2012","","","748","751","One of the major challenges in the most natural languages processing (NLP) tasks such as machine translation, text to speech and text mining is Word Sense Disambiguation (WSD). Supervised methods are the most common solutions for WSD. However, they need large tagged corpuses which are not available in some languages such as Persian. The Semi-Supervised methods can solve this problem by using small tagged corpus and large untagged corpus. This paper presents a coarse-grained work in WSD that uses tri-training as the semi-supervised method and decision list as supervised classifier for training. The proposed method was evaluated on a corpus. The results show that the proposed method is more precise than the conventional Decision list when the tagged corpus is small.","2164-7054","978-1-4673-1148-9","10.1109/IranianCEE.2012.6292453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292453","homograph disambiguation;semi-supervised;tritraining;decision list","Accuracy","natural language processing;pattern classification","semisupervised method;Persian homograph disambiguation;natural languages processing task;NLP tasks;machine translation;text to speech;text mining;word sense disambiguation;WSD;small tagged corpus;large untagged corpus;tri-training;decision list;supervised classifier","","2","","10","IEEE","3 Sep 2012","","","IEEE","IEEE Conferences"
"GT-SGD: A Novel Gradient Synchronization Algorithm in Training Distributed Recurrent Neural Network Language Models","X. Zhang; N. Gu; R. Yasrab; H. Ye","Department of Computer Science, University of Science and Technology of China, Hefei, China; Department of Computer Science, University of Science and Technology of China, Hefei, China; Department of Computer Science, University of Science and Technology of China, Hefei, China; Department of Computer Science, University of Science and Technology of China, Hefei, China","2017 International Conference on Networking and Network Applications (NaNA)","8 Jan 2018","2017","","","274","278","The emergence of Recurrent Neural Networks (RNNs) has resulted in the state-of-the-art efficiency and performance in many fields including language modeling, machine translation and speech recognition. RNNs are difficult to apply distributed training because of the communication bottleneck problem, which is caused by the large number of trainable weights. In this paper, a Gradient Truncated Stochastic Gradient Descent (GT-SGD) algorithm is proposed for distributed RNN training. This algorithm can aggressively reduce the communication workload across compute nodes, by truncating gradients that are below a certain threshold. We implement the GT-SGD algorithm on a cluster with multiple Graphic Processing Units (GPUs) for training a large-scale language modeling task. Experimental results demonstrate that the GT-SGD algorithm significantly improves the training efficiency with no performance loss in the obtained RNN model.","","978-1-5386-0604-9","10.1109/NaNA.2017.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8247150","","Training;Computational modeling;Recurrent neural networks;Synchronization;Stochastic processes;Parallel processing","gradient methods;graphics processing units;learning (artificial intelligence);natural language processing;recurrent neural nets;stochastic processes;synchronisation","novel gradient synchronization algorithm;recurrent neural networks;gradient truncated stochastic gradient descent algorithm;graphic processing units;large-scale language modeling;speech recognition;machine translation;training efficiency;GT-SGD algorithm;distributed RNN training;communication bottleneck problem","","1","","20","IEEE","8 Jan 2018","","","IEEE","IEEE Conferences"
"Answer Fast: Accelerating BERT on the Tensor Streaming Processor","I. Ahmed; S. Parmar; M. Boyd; M. Beidler; K. Kang; B. Liu; K. Roach; J. Kim; D. Abts",Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.; Groq Inc.,"2022 IEEE 33rd International Conference on Application-specific Systems, Architectures and Processors (ASAP)","17 Oct 2022","2022","","","80","87","Transformers have become a predominant machine learning workload, they are not only the de-facto standard for natural language processing tasks, but they are also being deployed in other domains such as vision and speech recognition. Many of the transformer-based applications are real-time systems such as machine translation and web search. These real time systems often come with strict end-to-end inference latency requirements. Unfortunately, while the majority of the transformer computation comes from matrix multiplications, transformers also include several non-linear components that tend to become the bottleneck during an inference. In this work, we accelerate the inference of BERT models on the tensor streaming processor. By carefully fusing all the nonlinear components with the matrix multiplication components, we are able to efficiently utilize the on-chip matrix multiplication units resulting in a deterministic tail latency of 130 Œºs for a batch-1 inference through BERT-base, which is 6√ó faster than the current state-of-the-art.","2160-052X","978-1-6654-8308-7","10.1109/ASAP54787.2022.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912001","ML acceleration","Tensors;Bit error rate;Systems architecture;Tail;Transformers;Real-time systems;System-on-chip","field programmable gate arrays;learning (artificial intelligence);mathematics computing;matrix multiplication;natural language processing;power aware computing;real-time systems;speech recognition","on-chip matrix multiplication units;batch-1 inference;BERT-base;natural language processing tasks;speech recognition;transformer-based applications;real-time systems;machine translation;Web search;strict end-to-end inference latency requirements;transformer computation;nonlinear components;BERT models;tensor streaming processor;matrix multiplication components","","1","","30","IEEE","17 Oct 2022","","","IEEE","IEEE Conferences"
"Memristor-Based Circuit Demonstration of Gated Recurrent Unit for Predictable Neural Network","Z. Zhang; Q. Chen; T. Han; C. Li; Y. Liu; G. Liu","School of Microelectronics, Hefei University of Technology, Hefei, China; School of Materials, Sun Yat-sen University, Guangzhou, China; School of Microelectronics, Hefei University of Technology, Hefei, China; School of Microelectronics, Hefei University of Technology, Hefei, China; Department of Micro/Nano Electronics, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Electron Devices","1 Dec 2022","2022","69","12","6763","6768","Analysis of time-series data can be used to recognize long-term trends and make correct forecasts. Compared with artificial neural network (ANN), gated recurrent unit (GRU) can process time-series signals and has a wide range of applications in natural language processing, speech recognition, machine translation, and so on. However, GRU models suffer from bottlenecks in hardware implementation due to the large number of parameters and circuit complexity. Here, we build a memristor-based GRU unit with full circuit function yet fewer input‚Äìoutput parameters. Inclusion of the as-designed GRU unit into predictable neural network allows the recognition and prediction of handwritten characters with the accuracies of 93% and 92%, respectively. The implementation of GRU network with memristor circuit extends its capability in machine learning and artificial intelligence.","1557-9646","","10.1109/TED.2022.3217116","National Natural Science Foundation of China(grant numbers:62111540271,62104267,61974090,U19A2053); Natural Science Foundation of Shanghai(grant numbers:19ZR1474500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940265","Gated recurrent unit (GRU);handwritten character;memristor;recognition and prediction;recurrent neural network (RNN)","Logic gates;Integrated circuit modeling;Memristors;Voltage;Computer architecture;Iterative methods;Hardware","artificial intelligence;learning (artificial intelligence);memristors;natural language processing;recurrent neural nets;speech recognition;time series","artificial neural network;circuit complexity;circuit function;correct forecasts;gated recurrent unit;hardware implementation;input-output parameters;long-term trends;machine translation;memristor circuit;memristor-based circuit demonstration;memristor-based GRU unit;natural language processing;predictable neural network;speech recognition;time-series data;time-series signals","","","","11","IEEE","4 Nov 2022","","","IEEE","IEEE Journals"
"Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks","A. Boroumand; S. Ghose; B. Akin; R. Narayanaswami; G. F. Oliveira; X. Ma; E. Shiu; O. Mutlu",Carnegie Mellon Univ; Univ. of Illinois Urbana-Champaign; Google; Google; ETH Z√ºrich; Google; Google; ETH Z√ºrich,"2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)","18 Oct 2021","2021","","","159","172","Emerging edge computing platforms often contain machine learning (ML) accelerators that can accelerate inference for a wide range of neural network (NN) models. These models are designed to fit within the limited area and energy constraints of the edge computing platforms, each targeting various applications (e.g., face detection, speech recognition, translation, image captioning, video analytics). To understand how edge ML accelerators perform, we characterize the performance of a commercial Google Edge TPU, using 24 Google edge NN models (which span a wide range of NN model types) and analyzing each NN layer within each model. We find that the Edge TPU suffers from three major shortcomings: (1) it operates significantly below peak computational throughput, (2) it operates significantly below its theoretical energy efficiency, and (3) its memory system is a large energy and performance bottleneck. Our characterization reveals that the one-size-fits-all, monolithic design of the Edge TPU ignores the high degree of heterogeneity both across different NN models and across different NN layers within the same NN model, leading to the shortcomings we observe. We propose a new acceleration framework called Mensa. Mensa incorporates multiple heterogeneous edge ML accelerators (including both on-chip and near-data accelerators), each of which caters to the characteristics of a particular subset of NN models and layers. During NN inference, for each NN layer, Mensa decides which accelerator to schedule the layer on, taking into account both the optimality of each accelerator for the layer and layer-to-layer communication costs. Our comprehensive analysis of the Google edge NN models shows that all of the layers naturally group into a small number of clusters, which allows us to design an efficient implementation of Mensa for these models with only three specialized accelerators. Averaged across all 24 Google edge NN models, Mensa improves energy efficiency and throughput by 3.0x and 3.1x over the Edge TPU, and by 2.4x and 4.3x over Eyeriss v2, a state-of-the-art accelerator.","","978-1-6654-4278-7","10.1109/PACT52795.2021.00019","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563028","","Analytical models;Computational modeling;Image edge detection;Visual analytics;Artificial neural networks;Machine learning;Throughput","inference mechanisms;Internet;learning (artificial intelligence);neural nets","machine learning accelerators;edge computing platforms;NN model types;NN layers;multiple heterogeneous edge ML accelerators;NN inference;Google neural network models;edge devices;emerging edge;machine learning inference bottlenecks;Google edge NN models;commercial Google edge TPU","","22","","103","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Automating Transliteration of Cuneiform from Parallel Lines with Sparse Data","B. Bogacz; M. Klingmann; H. Mara","Forensic Computational Geometry Laboratory (FCGL), University Heidelberg, Germany; Forensic Computational Geometry Laboratory (FCGL), University Heidelberg, Germany; Forensic Computational Geometry Laboratory (FCGL), University Heidelberg, Germany","2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)","29 Jan 2018","2017","01","","615","620","Cuneiform tablets appertain to the oldest textual artifacts and are in extent comparable to texts written in Latin or ancient Greek. The Cuneiform Commentaries Project (CPP) from Yale University provides tracings of cuneiform tablets with annotated transliterations and translations. As a part of our work analyzing cuneiform script computationally with 3D-acquisition and word-spotting, we present a first approach for automatized learning of transliterations of cuneiform tablets based on a corpus of parallel lines. These consist of manually drawn cuneiform characters and their transliteration into an alphanumeric code. Since the Cuneiform script is only available as raster-data, we segment lines with a projection profile, extract Histogram of oriented Gradients (HoG) features, detect outliers caused by tablet damage, and align those features with the transliteration. We apply methods from part-of-speech tagging to learn a correspondence between features and transliteration tokens. We evaluate point-wise classification with K-Nearest Neighbors (KNN) and a Support Vector Machine (SVM); sequence classification with a Hidden Markov Model (HMM) and a Structured Support Vector Machine (SVM-HMM). Analyzing our findings, we reach the conclusion that the sparsity of data, inconsistent labeling and the variety of tracing styles do currently not allow for fully automatized transliterations with the presented approach. However, the pursuit of automated learning of transliterations is of great relevance as manual annotation in larger quantities is not viable, given the few experts capable of transcribing cuneiform tablets.","2379-2140","978-1-5386-3586-5","10.1109/ICDAR.2017.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270037","Cuneiform;Transliteration;HMM;Machine Learning;Computer Linguistics;Online Databases;Handwriting Recognition;Language Modeling","Hidden Markov models;Feature extraction;Support vector machines;Task analysis;Image segmentation;Microsoft Windows;Histograms","feature extraction;hidden Markov models;image classification;learning (artificial intelligence);natural language processing;notebook computers;support vector machines;text analysis","parallel lines;cuneiform tablets;Cuneiform Commentaries Project;annotated transliterations;cuneiform script;cuneiform characters;transliteration tokens;fully automatized transliterations;CPP;3D-acquisition;3D-word-spotting;Yale University;structured support vector machine;hidden Markov model;SVM-HMM;k-nearest neighbors;KNN;histogram-of-oriented gradient feature extraction;HoG feature extraction","","4","","25","IEEE","29 Jan 2018","","","IEEE","IEEE Conferences"
"1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design","J. Dean","Google Research, Mountain View, CA","2020 IEEE International Solid- State Circuits Conference - (ISSCC)","13 Apr 2020","2020","","","8","14","The past decade has seen a remarkable series of advances in machine learning, and in particular deeplearning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore's Lawera. It also discusses some of the ways that machine learning may be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, exampleand task-based routing than the machine learning models of today.","2376-8606","978-1-7281-3205-1","10.1109/ISSCC19947.2020.9063049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063049","","Machine learning;Neural networks;Computational modeling;Task analysis;Training;Computer vision;Data models","computer architecture;learning (artificial intelligence);neural nets","machine learning;task-based routing;circuit design process;natural language understanding tasks;language translation;speech recognition;computer vision;artificial neural networks;chip design;computer architecture;deep learning revolution","","22","","71","IEEE","13 Apr 2020","","","IEEE","IEEE Conferences"
"Hybrid Multi Purpose Voice Assistant","D. Lahiri; P. C. P. Kandimalla; A. Jeysekar","CTech SRMIST, SRM Institute of Science and Technology, Chennai, India; CTech SRMIST, SRM Institute of Science and Technology, Chennai, India; CTech SRMIST, SRM Institute of Science and Technology, Chennai, India","2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","8 Jun 2023","2023","","","816","822","In this modern world everyone is busy in their works and there are many people around us who feel it very hard to organize their tasks, so everyone is looking for an intelligent assistant which can organize their tasks and perform some simple tasks with just a single command using voice assistants like Siri, Alexa, Google Assistant, and Bixby. Similarly, a chatbot is a computer program that uses natural language processing and machine learning to simulate or mimic a conversation with human users. Chatbots can also be used for a variety of purposes, including customer services, marketing, and providing information. They can also be programmed to perform specific tasks such as making appointments, placing orders, or providing recommendations. The aim of the paper is to propose a hybrid system which combines the traditional voice assistance system with a chatbot. This is not only combining both the advantages of chatbots and voice assistants but also gives us the option of customize the software to users based on their needs. This Hybrid Multipurpose Voice Assistant uses Natural Language Processing (NLP) to convert voice commands to instructions that will be sent to the voice assistant based on which it will perform the tasks. It uses Long Short-term Memory to process the instructions. Long short-term memory (LSTM) is a type of recurrent neural network (RNN) which is capable of learning and maintaining a record of information for long periods of time. Unlike traditional RNNs, which have a tendency to lose information over longer sequences, LSTMs use a special type of memory cell called a ‚Äúmemory gate‚Äù to control the flow of information into and out of the network. This allows LSTMs to selectively choose which information to keep and which to discard, making them well-suited for tasks such as language translation and speech recognition. The proposed Hybrid Multi-Purpose voice assistant will be a cost effective and efficient system which will understand and respond to a user's requests better and the interactions will be more human like. This project will also aim at helping people with a variety of disabilities and will be a step forward in the direction of making people, who suffer from different types of disabilities, lives easier. This voice assistant can be run both locally on a person's computer or externally using servers based on the requirement as the program is scalable and can work with existing technology and architecture.","","978-1-6654-5630-2","10.1109/ICAAIC56838.2023.10140365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10140365","voice assistant;chatbot;speech recognition;users;Buddy;natural language processing","Recurrent neural networks;Virtual assistants;Speech recognition;Computer architecture;Oral communication;Chatbots;Software","chatbots;customer services;handicapped aids;learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition","chatbot;Google Assistant;Hybrid Multipurpose Voice Assistant;Hybrid MultiPurpose voice assistant;Hybrid MultiPurpose Voice Assistant;intelligent assistant;natural language processing;short-term memory;simple tasks;traditional voice assistance system;voice commands","","","","16","IEEE","8 Jun 2023","","","IEEE","IEEE Conferences"
"Arabic Diacritization Using Bidirectional Long Short-Term Memory Neural Networks With Conditional Random Fields","A. Al-Thubaity; A. Alkhalifa; A. Almuhareb; W. Alsanie","National Center for Data Analytics and Artificial Intelligence, KACST, Riyadh, Saudi Arabia; National Center for Data Analytics and Artificial Intelligence, KACST, Riyadh, Saudi Arabia; National Center for Data Analytics and Artificial Intelligence, KACST, Riyadh, Saudi Arabia; National Center for Data Analytics and Artificial Intelligence, KACST, Riyadh, Saudi Arabia","IEEE Access","31 Aug 2020","2020","8","","154984","154996","Arabic diacritics play a significant role in distinguishing words with the same orthography but different meanings, pronunciations, and syntactic functions. The presence of Arabic diacritics can be useful in many natural language processing applications, such as text-to-speech tasks, machine translation, and part-of-speech tagging. This article discusses the use of bidirectional long short-term memory neural networks with conditional random fields for Arabic diacritization. This approach requires no morphological analyzers, dictionary, or feature engineering, but rather uses a sequence-to-sequence schema. The input is a sequence of characters that constitute the sentence, and the output consists of the corresponding diacritic(s) for each character in that sentence. The performance of the proposed approach was examined using four datasets with different sizes and genres, namely, the King Abdulaziz City for Science and Technology text-to-speech (KACST TTS) dataset, the Holy Quran, Sahih Al-Bukhary, and the Penn Arabic Treebank (ATB). For training, 60% of the sentences were randomly selected from each dataset, 20% were selected for validation, and 20% were selected for testing. The trained models achieved diacritic error rates of 3.41%, 1.34%, 1.57%, and 2.13% and word error rates of 14.46%, 4.92%, 5.65%, and 8.43% on the KACST TTS, Holy Quran, Sahih Al-Bukhary, and ATB datasets, respectively. Comparison of the proposed method with those used in other studies and existing systems revealed that its results are comparable to or better than those of the state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2020.3018885","King Abdulaziz City for Science and Technology (KACST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174712","Arabic diacritic restoration;bi-directional long short-term memory;computational linguistics;conditional random fields;deep learning;neural network","Hidden Markov models;Neural networks;Training;Syntactics;Task analysis;Tagging;Testing","maximum entropy methods;natural language processing;recurrent neural nets;speech processing;text analysis;word processing","Arabic diacritization;bidirectional long short-term memory neural networks;conditional random fields;Arabic diacritics;natural language processing applications;text-to-speech tasks;part-of-speech tagging;sequence-to-sequence schema;corresponding diacritic;Penn Arabic Treebank;diacritic error rates;King Abdulaziz City for Science and Technology text-to-speech dataset;KACST TTS;Holy Quran;ATB datasets;Sahih Al-Bukhary datasets","","7","","49","CCBY","24 Aug 2020","","","IEEE","IEEE Journals"
"Generating Music from Literature Using Topic Extraction and Sentiment Analysis","J. Salas","Cognitive science and computational modeling, University of California, Berkeley","IEEE Potentials","10 Jan 2018","2018","37","1","15","18","This article presents Tambr, a new software for translating literature into sound using multiple synthesized voices selected for the way in which their timbre relates to the meaning and sentiment of the topics conveyed in the story. It achieves this by leveraging a large lexical semantic database to implement a machine-learning-based synthesizer search engine used to select the synthesizers whose meaning best reflects the ideas of the novel. Tambr uses sentiment analysis to generate the pitches, durations, and intervals of the output melodies in a way corresponding to the sentiment of the novel-implementing algorithmic composition of literature-based music at a level of musicality not previously explored.","1558-1772","","10.1109/MPOT.2016.2550015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253722","","Speech processing;Synthesizers;Timbre;Software development;Semantics;Databases;Search engines;Sentiment analysis","information analysis;information retrieval;learning (artificial intelligence);music;search engines","music generation;topic extraction;sentiment analysis;Tambr software;synthesized voices;literature translation;lexical semantic database;machine-learning-based synthesizer search engine;literature-based music","","7","","3","IEEE","10 Jan 2018","","","IEEE","IEEE Magazines"
"Learning matrices and their applications","K. Steinbuch; U. A. W. Piske","Technische Hochschule Karlsruhe, Institute for Information Processing and Communication, Karlsruhe, Germany; Technische Hochschule Karlsruhe, Institute for Information Processing and Communication, Karlsruhe, Germany","IEEE Transactions on Electronic Computers","26 Dec 2006","1963","EC-12","6","846","862","The paper gives a survey of the learning circuits which became known as learning matrices and some of their possible technological applications. The first section describes the principle of learning matrices. So-called conditioned connections between the characteristics of an object and the meaning of an object are formed in the learning phase. During the operation of connecting the characteristics of an object with its meaning (EB operation of the knowing phase) upon presenting the object characteristics, the associated most similar meaning is realized in the form of a signal by maximum likelihood decoding. Conversely, in operation from the meaning of an object to its characteristics (BE operation) the associated object characteristics are obtained as signals by parallel reading upon application of an object meaning. According to the characteristic signals processed (binary or analog signals) discrimination must be made between binary and nonbinary learning matrices. In the case of the binary learning matrix the conditioned connections are a statistical measure for the frequency of the coordination of object characteristics and object meaning, in the case of the nonbinary learning matrix they are a measure for an analog value proportional to a characteristic. Both types of matrices allow for the characteristic sets applied during EB operation to be unsystematically disturbed within limits. Moreover, the nonbinary learning matrix is invariant to systematic deviations between presented and learned characteristic sets (invariance to affine transformation, translation and rotated skewness).","0367-7508","","10.1109/PGEC.1963.263588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4038032","","Circuits;Speech processing;Machine learning;Automatic control;Speech analysis;Signal processing;Frequency measurement;Filters;Speech recognition;National electric code","","","","78","2","34","IEEE","26 Dec 2006","","","IEEE","IEEE Journals"
"Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading","X. Zhang; F. Cheng; W. Shilin","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","713","722","Current state-of-the-art approaches for lip reading are based on sequence-to-sequence architectures that are designed for natural machine translation and audio speech recognition. Hence, these methods do not fully exploit the characteristics of the lip dynamics, causing two main drawbacks. First, the short-range temporal dependencies, which are critical to the mapping from lip images to visemes, receives no extra attention. Second, local spatial information is discarded in the existing sequence models due to the use of global average pooling (GAP). To well solve these drawbacks, we propose a Temporal Focal block to sufficiently describe short-range dependencies and a Spatio-Temporal Fusion Module (STFM) to maintain the local spatial information and to reduce the feature dimensions as well. From the experiment results, it is demonstrated that our method achieves comparable performance with the state-of-the-art approach using much less training data and much lighter Convolutional Feature Extractor. The training time is reduced by 12 days due to the convolutional structure and the local self-attention mechanism.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009091","","Lips;Feature extraction;Hidden Markov models;Training;Task analysis;Convolution;Decoding","convolutional neural nets;feature extraction;image segmentation;learning (artificial intelligence);speech recognition","convolutional sequence learning;temporal focal block;local self-attention mechanism;convolutional structure;convolutional feature extractor;Spatio-Temporal Fusion Module;short-range dependencies;global average pooling;sequence models;local spatial information;lip images;short-range temporal dependencies;lip dynamics;audio speech recognition;natural machine translation;sequence-to-sequence architectures;lip reading;time 12.0 d","","31","","42","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Memory Trojan Attack on Neural Network Accelerators","Y. Zhao; X. Hu; S. Li; J. Ye; L. Deng; Y. Ji; J. Xu; D. Wu; Y. Xie","Department of Electrical and Computer Engineering, University of California, Santa Barbara; Department of Electrical and Computer Engineering, University of California, Santa Barbara; Department of Electrical and Computer Engineering, University of California, Santa Barbara; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; Department of Electrical and Computer Engineering, University of California, Santa Barbara; Tsinghua University; Tsinghua University; Tsinghua University; Department of Electrical and Computer Engineering, University of California, Santa Barbara","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","1415","1420","Neural network accelerators are widely deployed in application systems for computer vision, speech recognition, and machine translation. Due to ubiquitous deployment of these systems, a strong incentive rises for adversaries to attack such artificial intelligence (AI) systems. Trojan is one of the most important attack models in hardware security domain. Hardware Trojans are malicious modifications to original ICs inserted by adversaries, which lead the system to malfunction after being triggered. The globalization of the semiconductor gives a chance for the adversary to conduct the hardware Trojan attacks.Previous works design Neural Network (NN) Trojans with access to the model, toolchain, and hardware platform. However, the threat model is impractical which hinders their real adoption. In this work, we propose a memory Trojan methodology without the help of toolchain manipulation and model parameter information. We first leverage the memory access patterns to identify the input image data. Then we propose a Trojan triggering method based on the dedicated input image other than the circuit events, which has better controllability. The triggering mechanism works well even with environment noise and preprocessing towards the original images. In the end, we implement and verify the effectiveness of accuracy degradation attack.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8715027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715027","","Trojan horses;Hardware;Artificial neural networks;Integrated circuit modeling;Payloads;Biological system modeling","artificial intelligence;computer vision;cryptography;fault diagnosis;integrated circuit design;invasive software;neural nets;speech recognition","memory Trojan attack;Neural network accelerators;application systems;computer vision;speech recognition;machine translation;ubiquitous deployment;artificial intelligence systems;hardware security domain;Hardware Trojans;malicious modifications;original ICs;hardware Trojan attacks;hardware platform;threat model;memory Trojan methodology;memory access patterns;Trojan triggering method;accuracy degradation attack;Neural Network Trojans","","24","","27","","16 May 2019","","","IEEE","IEEE Conferences"
"FPGA-based accelerator for long short-term memory recurrent neural networks","Y. Guan; Z. Yuan; G. Sun; J. Cong","Center for Energy-Efficient Computing and Applications, Peking University, China; Center for Energy-Efficient Computing and Applications, Peking University, China; Center for Energy-Efficient Computing and Applications, Peking University, China; Center for Energy-Efficient Computing and Applications, Peking University, China","2017 22nd Asia and South Pacific Design Automation Conference (ASP-DAC)","20 Feb 2017","2017","","","629","634","Long Short-Term Memory Recurrent neural networks (LSTM-RNNs) have been widely used for speech recognition, machine translation, scene analysis, etc. Unfortunately, general-purpose processors like CPUs and GPGPUs can not implement LSTM-RNNs efficiently due to the recurrent nature of LSTM-RNNs. FPGA-based accelerators have attracted attention of researchers because of good performance, high energy-efficiency and great flexibility. In this work, we present an FPGA-based accelerator for LSTM-RNNs that optimizes both computation performance and communication requirements. The peak performance of our accelerator achieves 7.26 GFLOP/S, which significantly outperforms previous approaches.","2153-697X","978-1-5090-1558-0","10.1109/ASPDAC.2017.7858394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858394","","Computer architecture;Logic gates;Recurrent neural networks;Standards;Microprocessors;Speech recognition;Computational modeling","field programmable gate arrays;recurrent neural nets","FPGA-based accelerator;long short-term memory recurrent neural networks;LSTM-RNN;speech recognition;machine translation;scene analysis;computation performance;communication requirements","","118","2","19","IEEE","20 Feb 2017","","","IEEE","IEEE Conferences"
"Achieving Full Parallelism in LSTM via a Unified Accelerator Design","X. Zhang; W. Jiang; J. Hu","Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA","2020 IEEE 38th International Conference on Computer Design (ICCD)","21 Dec 2020","2020","","","469","477","Recently, Long Short-Term Memory (LSTM), a type of recurrent neural network, has been widely employed in realtime applications, such as speech recognition, word segmentation, machine translation, etc. While existing works demonstrate that LSTM can be efficiently deployed in cloud platforms, the high communication latency between cloud and edge will drastically reduce its efficiency. Therefore, efficient LSTM accelerators at the edge are highly demanded. The limited resource in edge devices and the heterogeneous operations in LSTM (e.g., LSTM gates) bring challenges for the LSTM accelerator design. It seems straightforward to implement each operation as a specific hardware kernel. However, the data dependency among gates leads to significant running stalls in the existing heterogeneous-kernel accelerator, resulting in low parallelism and low resource utilization. To overcome the above challenges, this work proposes a novel generic LSTM accelerator design for Field-programmable Gate Array (FPGA) and Application-specific Integrated Circuit (ASIC) platforms, where two fundamental computing patterns (i.e., element-wise multiplication and addition) are incorporated in a unified computing kernel to execute operations in all LSTM gates simultaneously. Thus, the running stalls caused by heterogeneous kernels can be eliminated, achieving full parallelism in LSTM. The proposed technique and architecture are validated on Xilinx PYNQ-Z1 FPGA which can fully utilize the available resource, achieving 10x faster in inference time and 15.2x improvement in computing power efficiency compared with the state-of-the-art LSTM accelerator.","2576-6996","978-1-7281-9710-4","10.1109/ICCD50377.2020.00086","National Science Foundation(grant numbers:NSF CCF-1820537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283568","Long Short-Term Memory;Edge computing;Accelerator;FPGA;ASIC","Schedules;Speech recognition;Parallel processing;Logic gates;Resource management;Kernel;Field programmable gate arrays","application specific integrated circuits;cloud computing;field programmable gate arrays;parallel processing;power aware computing;reconfigurable architectures;recurrent neural nets","long short-term memory;LSTM gates;unified computing kernel;full parallelism;unified accelerator design;generic LSTM accelerator design;speech recognition;word segmentation;machine translation;cloud platforms;data dependency;heterogeneous-kernel accelerator;low parallelism;low resource utilization;field-programmable gate array;application-specific integrated circuit platforms;Xilinx PYNQ-Z1 FPGA","","6","","23","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Application of Computer Virtual Reality Technology and Artificial Intelligence in Remote Network Course","Q. Hui","Fundamental Department, Engineering University of PAP, Xi'an, China","2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)","6 Apr 2022","2022","","","490","494","With its unique characteristics of full immersion and interactivity, virtual reality technology provides powerful support for the creation of teaching situations and greatly changes the way students learn. The article uses design and development research methods to build a deep learning field model based on virtual reality technology. The author designs an immersive English learning system based on Kinect somatosensory equipment, introduces somatosensory recognition and speech recognition technology into English teaching, and supports virtual scenes with machine dialogue, pronunciation correction, real-time translation, reading training, English games and other functions. It has the advantages of humanized design, easy operation, customizable and strong scalability, and provides comprehensive help for English learners.","","978-1-6654-1606-1","10.1109/EEBDA53927.2022.9744966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744966","computer;virtual reality technology;college English;scene teaching;virtual reality system","Courseware;Training;Solid modeling;Scalability;Education;Psychology;Virtual reality","computer aided instruction;computer science education;distance learning;human computer interaction;learning (artificial intelligence);speech recognition;teaching;virtual reality","immersive English learning system;Kinect somatosensory equipment;somatosensory recognition;speech recognition technology;English teaching;virtual scenes;computer virtual reality technology;artificial intelligence;remote network course;powerful support;development research methods;deep learning field model","","","","8","IEEE","6 Apr 2022","","","IEEE","IEEE Conferences"
"Morphological Analysis of Malay Words for Resolving Ambiguity","M. F. Yahaya; N. A. Rahman; Z. A. Bakar","Fakulti Sains Komputer dan Matematik, Universiti Teknologi MARA, Shah Alam, Selangor; Fakulti Sains Komputer dan Matematik, Universiti Teknologi MARA, Shah Alam, Selangor; Fakulti Sains Komputer dan IT, Universiti Al-Madinah, Shah Alam, Selangor","2018 Fourth International Conference on Information Retrieval and Knowledge Management (CAMP)","16 Sep 2018","2018","","","1","5","The issue of morphological uncertainty is broadly tended to in the cutting edge in Natural Language Processing (NLP). For the most part, vagueness is settled with the utilization of substantial physically explained corpora and machine learning. Be that as it may, such strategies do not generally accessible, as great preparing information is not available for all dialects. In this paper, we introduce a technique for disambiguation without highest quality level corpora utilizing a few factual models, to be specific, Braille Translation Algorithms and unambiguous N-grams from the naturally explained corpus. Every one of the strategies was tried on the Corpus of Glosbe and on the Corpus of Dewan Bahasa Pustaka (DBP). Therefore, more than a half of words with uncertain examinations were disambiguated in the two corpora, exhibiting high exactness. Our technique for morphological disambiguation shows that it is conceivable to dispose of a portion of the uncertain examinations in the corpus without particular phonetic assets, just with the utilization of crude information, where all conceivable morphological investigations for each word are shown.","","978-1-5386-3812-5","10.1109/INFRKM.2018.8464773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8464773","morphological analysis;part of speech tagging;disambiguation techniques;malay corpus;malay ambiguity words","Uncertainty;Writing;Information retrieval;Knowledge management;Law enforcement;Testing;Machine learning","handicapped aids;learning (artificial intelligence);natural language processing","morphological analysis;morphological uncertainty;Natural Language Processing;NLP;machine learning;Braille Translation Algorithms;unambiguous N-grams;Dewan Bahasa Pustaka;morphological disambiguation;Malay words","","","","44","IEEE","16 Sep 2018","","","IEEE","IEEE Conferences"
"Object identification and orientation determination in 3-space with no point correspondence information","D. Cyganski; J. Orr","Department of Electrical Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Electrical Engineering, Worcester Polytechnic Institute, Worcester, MA, USA","ICASSP '84. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1984","9","","279","282","The problem of identification of rigid planar-patch objects from images projected from random orientations in 3-D space is approached using tensor representation of image measures. This allows standardization of object orientation without knowledge of object identity, and hence permits a simple one-step comparison operation between the standardized unknown and each library image. Geometric transformations due to oblique viewing angles, as well as translation, rotation, and scale change are handled. Experimental results using a library of camera-acquired images demonstrate correct performance of the algorithm.","","","10.1109/ICASSP.1984.1172543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1172543","","Position measurement;Tensile stress;Libraries;Extraterrestrial measurements;Electric variables measurement;Standardization;Machine vision;Testing;Prototypes;Measurement standards","","","","8","","9","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Object classification and registration by Radon transform based invariants","H. Dohse; J. Sanz; A. Jain","Computer Vision Research Laboratory, University of California, Davis, Davis, CA, USA; IBM Almaden Research Center, San Jose, CA, USA; Computer Vision Research Laboratory, University of California, Davis, Davis, CA, USA","ICASSP '87. IEEE International Conference on Acoustics, Speech, and Signal Processing","29 Jan 2003","1987","12","","225","228","An algorithm for object recognition which is based on translation and rotation invariant signatures is presented. The Radon transform is used as a tool for efficient implementation of the algorithm. Good classification results where obtained even with objects having similar shapes.","","","10.1109/ICASSP.1987.1169670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1169670","","Fourier transforms;Object recognition;Computer vision;Laboratories;Shape;Machine vision;Hardware;Image recognition;Oceans;Aircraft","","","","4","2","6","IEEE","29 Jan 2003","","","IEEE","IEEE Conferences"
"Neural network shape: Organ shape representation with radial basis function neural networks","G. Lu; L. Ren; A. Kolagunda; C. Kambhamettu","Department of Computer and Information Science, University of Delaware, USA; Department of Computer and Information Science, University of Delaware, USA; Department of Computer and Information Science, University of Delaware, USA; Department of Computer and Information Science, University of Delaware, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","932","936","We propose to represent the shape of an organ using a neural network classifier. The shape is represented by a function learned by a neural network. Radial Basis Function (RBF) is used as the activation function for each perceptron. The learned implicit function is a combination of radial basis functions, which can represent complex shapes. The organ shape representation is learned using classification methods. Our testing results show that the neural network shape provides the best representation accuracy. The use of RBF provides a rotation, translation and scaling invariant feature to represent the shape. Experiments show that our method can accurately represent the organ shape.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7471812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471812","Shape Presentation;RBF Kernel;Artificial Neural Network;3D Reconstruction","Shape;Liver;Three-dimensional displays;Neural networks;Training;Kernel;Support vector machines","biological organs;image classification;medical image processing;neural nets;radial basis function networks;transfer functions","neural network shape;organ shape representation;radial basis function neural networks;neural network classifier;activation function;classification methods","","4","","21","IEEE","19 May 2016","","","IEEE","IEEE Conferences"
"Relative and Absolute Errors in Sensor Network Localization","J. N. Ash; R. L. Moses","Department of Electrical and Computer Engineering, Ohio State Uinversity, Columbus, OH, USA; Department of Electrical and Computer Engineering, Ohio State Uinversity, Columbus, OH, USA","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","2","","II-1033","II-1036","This paper considers the accuracy of sensor node location estimates from self-calibration in sensor networks. The location parameters are shown to have a natural decomposition into relative configuration and centroid transformation components based on the influence of measurements and prior information in the problem. A linear representation of the transformation parameter space, which includes rotations and translations, is shown to coincide with the nullspace of the unconstrained Fisher information matrix (FIM). To regularize the absolute localization problem, we consider constraints on the coordinate locations and the impact of these constraints on relative and absolute location error. A geometric interpretation of the constrained Cramer-Rao bound (CRB) is provided based on the principal angles between the measurement subspace and the constraint subspace. Examples illustrate the utility of this error decomposition.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.366415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4217588","Sensor networks;Localization;Constrained estimation;Cram√©r-Rao bound;Fisher information","Subspace constraints;Wireless sensor networks;Coordinate measuring machines;Position measurement;Shape measurement;Matrix decomposition;Computer errors;Intelligent networks;Ash;State estimation","matrix algebra;transforms;wireless sensor networks","relative location error;absolute location error;sensor network localization;sensor node location estimation;sensor network self-calibration;relative configuration component;centroid transformation component;linear representation;transformation parameter space;unconstrained Fisher information matrix;absolute localization problem;coordinate locations;constrained Cramer-Rao bound","","3","","8","IEEE","4 Jun 2007","","","IEEE","IEEE Conferences"
"A Comparative Study for Language Recognition using Learning-based Approaches","C. M. Chew; K. Ming Lim; C. P. Lee; X. Yang Chan; C. H. Lew; V. W. Ru Song","Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia; Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia; Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia; Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia; Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia; Faculty of Information Science & Technology, Multimedia University, Melaka, Malaysia","2023 11th International Conference on Information and Communication Technology (ICoICT)","29 Sep 2023","2023","","","528","532","Language recognition is helpful for determining the natural language in a given document or part of text. Language recognition has attracted more attention in recent times due to its wide-ranging applications, including speech translation, multilingual speech recognition and more. Indeed, language recognition should be effective to ensure practical implementation. Therefore, learning-based approaches are introduced to enhance the effectiveness of language recognition. In this paper, a total of six learning-based approaches have been implemented for solving the language recognition problem. Experiments and evaluations are conducted to study the effectiveness of these learning-based approaches on identifying 5 different languages which are English, German, Czech, French, and Swedish. The experimental results show that the 1D-CNN model achieves the highest accuracy score of 65.99%.","2162-1241","979-8-3503-2198-2","10.1109/ICoICT58202.2023.10262698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262698","Language Recognition;Machine Learning;Deep Learning;Convolutional Neural Network","Deep learning;Training;Text recognition;Transfer learning;Neural networks;Speech recognition;Transformers","","","","","","17","IEEE","29 Sep 2023","","","IEEE","IEEE Conferences"
"Hierarchical Similarity Transformations Between Gaussian Mixtures","G. Rigas; C. Nikou; Y. Goletsis; D. I. Fotiadis","Department of Computer Science, University of Ioannina, Ioannina, Greece; Department of Computer Science, University of Ioannina, Ioannina, Greece; Department of Economics, University of Ioannina, Ioannina, Greece; Unit of Medical Technology and Intelligent Information Systems, Department of Materials Science and Engineering, University of Ioannina, Ioannina, Greece","IEEE Transactions on Neural Networks and Learning Systems","17 Oct 2013","2013","24","11","1824","1835","In this paper, we propose a method to estimate the density of a data space represented by a geometric transformation of an initial Gaussian mixture model. The geometric transformation is hierarchical, and it is decomposed into two steps. At first, the initial model is assumed to undergo a global similarity transformation modeled by translation, rotation, and scaling of the model components. Then, to increase the degrees of freedom of the model and allow it to capture fine data structures, each individual mixture component may be transformed by another, local similarity transformation, whose parameters are distinct for each component of the mixture. In addition, to constrain the order of magnitude of the local transformation (LT) with respect to the global transformation (GT), zero-mean Gaussian priors are imposed onto the local parameters. The estimation of both GT and LT parameters is obtained through the expectation maximization framework. Experiments on artificial data are conducted to evaluate the proposed model, with varying data dimensionality, number of model components, and transformation parameters. In addition, the method is evaluated using real data from a speech recognition task. The obtained results show a high model accuracy and demonstrate the potential application of the proposed method to similar classification problems.","2162-2388","","10.1109/TNNLS.2013.2267803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549112","Expectation maximization (EM) algorithm;Gaussian mixture model;registration of point sets;similarity transformation","Estimation;Covariance matrices;Data models;Mixture models;Learning systems;Image registration","expectation-maximisation algorithm;Gaussian processes;learning (artificial intelligence);signal classification;speech recognition","hierarchical similarity transformation;data space density estimation;Gaussian mixture model;hierarchical geometric transformation;global similarity transformation;model component translation;model component rotation;model component scaling;fine data structure capture;local similarity transformation;zero-mean Gaussian priors;expectation maximization framework;data dimensionality;transformation parameters;speech recognition task;classification problem;machine learning","","4","","34","IEEE","28 Jun 2013","","","IEEE","IEEE Journals"
"Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools","S. Schl√∂gl; G. Doherty; S. Luz",NA; NA; NA,"Interacting with Computers","18 Jan 2018","2015","27","6","592","615","Wizard of Oz (WOZ) is a well-established method for simulating the functionality and user experience of future systems. Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with speech and language technologies, but advances in sensor technology and pattern recognition as well as new application areas such as human‚Äìrobot interaction have made it increasingly relevant to the design of a wider range of interactive systems. In such cases, achieving acceptable performance at the user interface level often hinges on resource-intensive improvements such as domain tuning, which are better done once the overall design is relatively stable. Although WOZ is recognized as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view. Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for speech recognition and synthesis as well as for machine translation. This architecture is instantiated in WebWOZ‚Äîa new web-based open-source WOZ prototyping platform. The viability of generic support is explored empirically through a series of evaluations. Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ. The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in wizard performance.","1873-7951","","10.1093/iwc/iwu016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8155004","natural language interfaces;HCI design and evaluation methods;speech based (interaction techniques);interface design prototyping","","","","","1","","","","18 Jan 2018","","","OUP","OUP Journals"
"The effectiveness of surrogate functions in improving the accuracy of PSO-type algorithms in an NLP task","G. Tambouratzis","Dept. of Machine Translation, Institute for Language and Speech Processing/Athena Research Centre, Greece","2017 IEEE Symposium Series on Computational Intelligence (SSCI)","8 Feb 2018","2017","","","1","8","The present article investigates a strategy for improving the performance of PSO-type algorithms when optimizing a set of parameter values, by utilising information regarding the relative values of parameters. For this study, a specific Natural Language Processing task is revisited, related to splitting an arbitrary sentence into phrases. In solving this task here, the central idea is to determine if domain-dependent information for expected near-symmetric parameter values can be exploited to reach a better solution in a given number of iterations. If there a known task-specific relation between parameters (for instance the order of magnitude of certain parameters is likely to be similar, as they correspond to similar inputs), the research question is whether such knowledge may be exploited to initially solve a simpler problem in a lower dimensionality. Thus, a hybrid strategy is proposed, that splits the PSO evolution into two phases, (i) the first one optimizing over a reduced set of grouped parameters in order to establish the relative magnitude of each group of related parameters and (ii) the second one optimizing over the entire set of parameters to fine-tune each parameter independently. Two PSO variants are evaluated on this strategy and for each case the results are compared to baselines, to investigate the quality of the solutions achieved. Experimental results indicate that the two variants behave differently with respect to this hybrid strategy, and that one of the PSO variants benefits to generate substantially better optimization solutions.","","978-1-5386-2726-6","10.1109/SSCI.2017.8285353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285353","parsing of natural language;syntactically-derived phrasing;particle swarm optimization;parameter optimization;surrogate models","Task analysis;Optimization;Natural language processing;Force;Training data;Training","natural language processing;particle swarm optimisation;text analysis","optimization solutions;Natural Language Processing task;PSO evolution;known task-specific relation;domain-dependent information;NLP task;PSO-type algorithms;surrogate functions;hybrid strategy;PSO variants;grouped parameters;reduced set","","2","","16","IEEE","8 Feb 2018","","","IEEE","IEEE Conferences"
"Natural language processing tools and environments: the field in perspective","B. Z. Manaris","Computer Science Department, University of Southwestern Louisiana, Lafayette, LA, USA","Proceedings Sixth International Conference on Tools with Artificial Intelligence. TAI 94","6 Aug 2002","1994","","","228","","Tools and environments for natural language processing (NLP) originated approximately four decades ago with dictionary-based machine translation systems. When examining the evolution of the field, one observes a transition from these ""embryonic"" systems of the fifties to the more adaptable, robust, and user-friendly environments of the nineties. Currently, the state-of-the-art in such systems is based on a wide variety of linguistic theories, cognitive models, and engineering approaches. The paper looks briefly at the tools and environments available today.<>","","0-8186-6785-0","10.1109/TAI.1994.346491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=346491","","Natural language processing;Natural languages;Machine intelligence;Speech processing;Intelligent systems;Research and development;Computer science;Robustness;Art;Databases","natural languages;natural language interfaces","natural language processing tools;natural language processing environments;user-friendly environments;state-of-the-art;linguistic theories;cognitive models","","1","","5","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"Ancient Character Recognition: A Novel Image Dataset of Shui Manuscript Characters and Classification Model","M. Tang; S. Xie; X. Liu","Department of Computer Science and Technology, School of Informatics, Xiamen University, Xiamen, China; Department of Computer Science and Technology, School of Informatics, Xiamen University, Xiamen, China; Department of Computer Science and Technology, School of Informatics, Xiamen University, Xiamen, China","Chinese Journal of Electronics","7 Feb 2023","2023","32","1","64","75","Shui manuscripts are part of the national intangible cultural heritage of China. Owing to the particularity of text reading, the level of informatization and intelligence in the protection of Shui manuscript culture is not adequate. To address this issue, this study created Shuishu_C, the largest image dataset of Shui manuscript characters that has been reported. Furthermore, after extensive experimental validation, we proposed ShuiNet-A, a lightweight artificial neural network model based on the attention mechanism, which combines channel and spatial dimensions to extract key features and finally recognize Shui manuscript characters. The effectiveness and stability of ShuiNet-A were verified through multiple sets of experiments. Our results showed that, on the Shui manuscript dataset with 113categories, the accuracy of ShuiNet-A was 99.8%, which is 1.5% higher than those of similar studies. The proposed model could contribute to the classification accuracy and protection of ancient Shui manuscript characters.","2075-5597","","10.23919/cje.2022.00.077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038801","Shui manuscript characters;ShuiNet-A;Artificial neural network;Handwritten character recognition","Training;Speech recognition;Artificial neural networks;Feature extraction;Stability analysis;Character recognition;Machine translation","character recognition;feature extraction;history;image classification;neural nets","ancient character recognition;ancient Shui manuscript characters;classification model;image dataset;lightweight artificial neural network model;Shui manuscript dataset;Shui manuscripts","","1","","42","","7 Feb 2023","","","CIE","CIE Journals"
"Ancient Tibetan Word Segmentation based on Deep Learning","B. An; C. Long","CASS Research Center for Ethnic Minority languages, Institute of Ethnology and Anthropology Chinese Academy of Social Sciences, Beijing, China; CASS Research Center for Ethnic Minority languages, Institute of Ethnology and Anthropology Chinese Academy of Social Sciences, Beijing, China","2021 International Conference on Asian Language Processing (IALP)","19 Jan 2022","2021","","","292","297","Tibetan ancient literature is an important literature material for the study of ancient Tibetan culture, history and language, which has important academic value for the study of the development of Sino Tibetan language family. However, the lack of research on ancient Tibetan word segmentation seriously restricts the research of ancient Tibetan literature. In view of this situation, this paper utilizes ancient Tibetan interlaced contrast tagging data to extract the ancient Tibetan word segmentation data set. Based on this dataset, this paper conduct the research on dictionary based word segmentation, statistics based word segmentation and deep learning based ancient Tibetan word segmentation. Experimental results show that BiLSTM + CRF word segmentation algorithm can achieve the best performance by a single model, and the effect of ancient Tibetan word segmentation can be further improved through model ensemble. And the results show that the unknown words, insufficient training data and word ambiguity also restrict the performance of ancient Tibetan word segmentation.","","978-1-6654-8311-7","10.1109/IALP54817.2021.9675149","National Natural Science Foundation of China(grant numbers:62076233); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675149","Word Segmentation;Ancient Tibetan;Deep Learning","Deep learning;Dictionaries;Speech analysis;Statistical learning;Training data;Tagging;Machine translation","deep learning (artificial intelligence);linguistics;natural language processing;recurrent neural nets;statistical analysis;word processing","ancient Tibetan culture;ancient Tibetan literature;ancient Tibetan word segmentation data;dictionary based word segmentation;statistics based word segmentation;word ambiguity;BiLSTM CRF word segmentation algorithm;ancient Tibetan interlaced contrast tagging data;deep learning;Sino Tibetan language family","","1","","55","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"Accelerating Deep Learning Inference in Constrained Embedded Devices Using Hardware Loops and a Dot Product Unit","J. Vreƒça; K. J. X. Sturm; E. Gungl; F. Merchant; P. Bientinesi; R. Leupers; Z. Brezoƒçnik","Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany; Department of Computing Science, Ume√• University, Ume√•, Sweden; Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia","IEEE Access","18 Sep 2020","2020","8","","165913","165926","Deep learning algorithms have seen success in a wide variety of applications, such as machine translation, image and speech recognition, and self-driving cars. However, these algorithms have only recently gained a foothold in the embedded systems domain. Most embedded systems are based on cheap microcontrollers with limited memory capacity, and, thus, are typically seen as not capable of running deep learning algorithms. Nevertheless, we consider that advancements in compression of neural networks and neural network architecture, coupled with an optimized instruction set architecture, could make microcontroller-grade processors suitable for specific low-intensity deep learning applications. We propose a simple instruction set extension with two main components-hardware loops and dot product instructions. To evaluate the effectiveness of the extension, we developed optimized assembly functions for the fully connected and convolutional neural network layers. When using the extensions and the optimized assembly functions, we achieve an average clock cycle count decrease of 73% for a small scale convolutional neural network. On a per layer base, our optimizations decrease the clock cycle count for fully connected layers and convolutional layers by 72% and 78%, respectively. The average energy consumption per inference decreases by 73%. We have shown that adding just hardware loops and dot product instructions has a significant positive effect on processor efficiency in computing neural network functions.","2169-3536","","10.1109/ACCESS.2020.3022824","Slovenian Research Agency through the Research Program Advanced Methods of Interaction in Telecommunication(grant numbers:P2-0069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187807","Deep learning;embedded systems;instruction set optimization;RISC-V","Hardware;Machine learning;Biological neural networks;Instruction sets;Microcontrollers;Embedded systems","embedded systems;instruction sets;learning (artificial intelligence);microcontrollers;neural net architecture","deep learning inference;constrained embedded devices;dot product unit;machine translation;speech recognition;embedded systems domain;cheap microcontrollers;neural network architecture;optimized instruction set architecture;microcontroller-grade processors;simple instruction;optimized assembly functions;convolutional neural network layers;neural network functions;average clock cycle count;components-hardware loops;connected network layers;deep learning applications","","8","","36","CCBY","8 Sep 2020","","","IEEE","IEEE Journals"
"Chinese R&D in Natural Language Technology","C. Zong; Q. Gao","Chinese Academy and Sciences, China; University of Science and Technology, Beijing, China","IEEE Intelligent Systems","9 Jan 2009","2008","23","6","42","48","In the past 50 years, China's R&D in natural language technology has made fruitful advances but still faces many challenges. National programs continue to contribute to these achievements.","1941-1294","","10.1109/MIS.2008.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4747608","natural language processing;natural language technology;human language technology;computational linguistics;Chinese language processing","Natural languages;Humans;Speech synthesis;Pervasive computing;Speech processing;Computers;Educational institutions;Research and development;Ear;Globalization","history;natural language processing;research and development","Chinese R&D;natural language technology;machine translation;history","","","","38","IEEE","9 Jan 2009","","","IEEE","IEEE Magazines"
"Research on face recognition based on deep learning","X. Han; Q. Du","College of Software, Shenyang Normal University, Shenyang, China; College of Software, Shenyang Normal University, Shenyang, China","2018 Sixth International Conference on Digital Information, Networking, and Wireless Communications (DINWC)","10 May 2018","2018","","","53","58","With the deep learning in different areas of success, beyond the other methods, set off a new wave of neural network development. The concept of deep learning originated from the artificial neural network, in essence, refers to a class of neural networks with deep structure of the effective training methods[1]. As a powerful technology to realize artificial intelligence, deep learning has been widely used in handwriting digital recognition, dimension simplification, speech recognition, image comprehension, machine translation, protein structure prediction and emotion recognition. In this paper, we focus on the research hotspots of face recognition based on depth learning in the field of biometrics, Combined with the relevant theory and methods of depth learning, face recognition technology, along the order of depth learning, based on the depth of learning face recognition, face recognition application to start research.","","978-1-5386-3903-0","10.1109/DINWC.2018.8356995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356995","Deep Learning;Face Recognition;Biometrics;Neural Network","Face recognition;Face;Machine learning;Hidden Markov models;Feature extraction;Biological neural networks;Convolution","face recognition;learning (artificial intelligence)","deep learning;neural network development;artificial neural network;handwriting digital recognition;speech recognition;protein structure prediction;emotion recognition;depth learning;face recognition technology;face recognition application","","23","","19","IEEE","10 May 2018","","","IEEE","IEEE Conferences"
"Scattering Statistics of Generalized Spatial Poisson Point Processes","M. Perlmutter; J. He; M. Hirn","Department of Mathematics, University of California, Los Angeles; Department of Computational Mathematics, Science & Engineering, Michigan State University; Department of Computational Mathematics, Science & Engineering, Michigan State University","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","5528","5532","We present a machine learning model for the analysis of randomly generated discrete signals, modeled as the points of an inhomogeneous, compound Poisson point process. Like the wavelet scattering transform introduced by Mallat, our construction is naturally invariant to translations and reflections, but it decouples the roles of scale and frequency, replacing wavelets with Gabor-type measurements. We show that, with suitable nonlinearities, our measurements distinguish Poisson point processes from common self-similar processes, and separate different types of Poisson point processes.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746382","Scattering transform;Poisson point process;convolutional neural network","Wavelet transforms;Analytical models;Neural networks;Scattering;Signal processing;Nonhomogeneous media;Reflection","convolutional neural nets;Gabor filters;learning (artificial intelligence);signal processing;statistical analysis;stochastic processes;wavelet transforms","scattering statistics;generalized spatial Poisson point processes;machine learning model;inhomogeneous Poisson point process;wavelet scattering;self-similar processes;randomly generated discrete signal analysis;wavelet scattering transform;inhomogeneous compound Poisson point process;Gabor-type measurements;convolutional neural networks","","","","15","USGov","27 Apr 2022","","","IEEE","IEEE Conferences"
"LSMCore: A 69k-Synapse/mm2 Single-Core Digital Neuromorphic Processor for Liquid State Machine","L. Wang; Z. Yang; S. Guo; L. Qu; X. Zhang; Z. Kang; W. Xu","College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Circuits and Systems I: Regular Papers","27 Apr 2022","2022","69","5","1976","1989","Neuromorphic processors have gained momentum recently due to their high energy efficiency in artificial intelligence applications compared to DNN accelerators. Most neuromorphic processors are executing SNNs (Spiking Neural Networks). Liquid State Machine (LSM), as the spiking version of reservoir computing, shows advantages and great potential in image classification, speech recognition, language translation, etc.. Comparing with other SNN models, LSM has the characteristics of easy to train and low resource utilization, which is suitable for low-power and resource-constrained edge computing scenarios. In this paper, we propose a novel design of a neuromorphic processor, LSMCore, aiming at LSM acceleration. LSMCore supports both training and inference of LSM. It consists of 256 input neurons, 1024 liquid neurons, and 1.31M synapses. Besides, multiple optimization techniques, including weight quantization for reducing storage, zero-skipping for decreasing dynamic sparsity, and mini-batch training are adopted in this processor. The experimental results show that the frequency of LSMCore achieves 400 MHz, the power is 4.9W and the area is 18.49 mm2 with a 40nm library. Comparing with the baseline, LSMCore achieves up to  $80.7\times $ ( $49.6\times $ ),  $91.3\times $ ( $56.3\times $ ), and  $83.1\times $ ( $56.8\times $ ) speedup on MNIST, N-MNIST, and Free Spoken Digital Dataset (FSDD) respectively for training (inference), while the accuracy of LSMCore on these three datasets are 96.8%, 97.6%, and 90% respectively.","1558-0806","","10.1109/TCSI.2022.3147380","National Key Research and Development Program of China(grant numbers:2018YFB2202603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9704873","Neuromorphic processor;spiking neural network;liquid state machine;microarchitecture","Neurons;Liquids;Neuromorphics;Training;Hardware;Computer architecture;Real-time systems","energy conservation;low-power electronics;neural chips;neuromorphic engineering","LSMCore;single-core digital neuromorphic processor;liquid state machine;energy efficiency;artificial intelligence;reservoir computing;resource-constrained edge computing;LSM acceleration;low-power edge computing;N-MNIST dataset;free spoken digital dataset;FSDD;MNIST dataset;spiking neural networks;SNN;weight quantization","","10","","54","IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"Investigating Deep Learning for Predicting Multi-linguistic Interactions with a Chatterbot","R. Kulesza; Y. Kumar; R. Ruiz; A. Torres; E. Weinman; J. J. Li; P. Morreale","Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA; Computer Science, Kean University, Union NJ, USA","2020 IEEE Conference on Big Data and Analytics (ICBDA)","15 Dec 2020","2020","","","20","25","Deep Learning (DL) becomes a mainstream technique for Artificial Intelligence (AI) machine learning because of its success in performing many tasks, such as image recognition, speech interpretation, language prediction and translation. We are investigating the underlying principles of DL Neural Networks (NN) to design optimal DL NN for predicting human multi-linguistic conversations with a chatterbot. This research attempts to tackle the well-known open problem of finding optimal NN designs for data of various characteristics. We are in particular focusing on Recurrent Neural Networks (RNN) models with time progression, which takes into consideration the results from the previous steps plus the current input to predict the next step, i.e. it ‚Äòremembers‚Äô what it has previously learnt. Through the experiments of tuning an RNN to achieve an optimal performance in terms of accuracy and training time, we found that characteristics such as word counts and layers of neurons could affect the training performance. We applied the tuned optimal model to a game implementation, inspired by IBM Watson, where users can guess the words to be generated by a computer, called ‚ÄúBeat AI‚Äù to have human predict the machine prediction.","","978-1-7281-9246-8","10.1109/ICBDA50157.2020.9289710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289710","Machine Learning;Deep Learning;Neural Networks;RNN;TensorFlow;Keras;Yandex Predictor","Training;Deep learning;Recurrent neural networks;Artificial neural networks;Predictive models;Task analysis;Tuning","deep learning (artificial intelligence);linguistics;recurrent neural nets;software agents","speech interpretation;language prediction;DL neural networks;human multilinguistic conversations;chatterbot;optimal NN designs;recurrent neural networks models;RNN;time progression;Beat AI;machine prediction;artificial intelligence;image recognition;machine learning;multilinguistic interaction","","2","","18","IEEE","15 Dec 2020","","","IEEE","IEEE Conferences"
"An Approach to Dynamic Model Combination on Solving Decision-Making Problem","C. Wu; M. J. Xin; W. H. Li","Northwestern Polytechnical University, China; Shanghai University, China; Northwestern Polytechnical University, China","2006 IEEE Asia-Pacific Conference on Services Computing (APSCC'06)","26 Dec 2006","2006","","","629","634","This paper discusses an approach to distributed parallel model combination for solving complicated decision-making problem. Firstly, we classify those existed models by model logic types, implement a model combination strategy to realize automatic combined model dynamic reconstruction and obtain the model series of decision-making problem solving by using and/or graph. Furthermore, during the process on model combination, we develop an index table based fast parameter matching algorithm to implement parameter match among those related models and also to apply blackboard dynamic storage technique so as to implement parameter transfer among related models. Finally, we concentrate our deep study on combined model dynamic reconstruction, fast parameter dynamic matching and transferring algorithm. Algorithm analysis listed in this paper shows that the fast parameter matching algorithm outperforms other translation methods","","0-7695-2751-5","10.1109/APSCC.2006.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4041303","","Decision making;Educational institutions;Computer science;Problem-solving;Machine learning algorithms;Chaos;Automatic logic units;Speech analysis;Algorithm design and analysis;Statistical analysis","decision making;decision theory;distributed algorithms;formal logic","dynamic model combination;decision-making problem;distributed parallel model combination;model logic type;model dynamic reconstruction;and/or graph;index table;blackboard dynamic storage;fast parameter dynamic matching;transferring algorithm","","","","11","IEEE","26 Dec 2006","","","IEEE","IEEE Conferences"
"An Internet of Things humanoid robot teleoperated by an open source Android application","G. Angelopoulos; G. T. Kalampokis; M. Dasygenis","Department of Informatics and Telecommunications Engineering, University of Western Macedonia, Kozani, Greece; Department of Informatics and Telecommunications Engineering, University of Western Macedonia, Kozani, Greece; Department of Informatics and Telecommunications Engineering, University of Western Macedonia, Kozani, Greece","2017 Panhellenic Conference on Electronics and Telecommunications (PACET)","18 Jan 2018","2017","","","1","4","The Internet of Things (IoT) is a system of interrelated computing devices, mechanical and digital machines, objects, animals or people that are provided with unique identifiers and the ability to transfer data over a network without requiring human-to-human interaction. Nowadays the use of IoT in industrial robots is very popular contrary to humanoid robots. There have been a few attempts that combine IoT with humanoid robots but they have limited features. This paper proposes a novel open source platform Android application for controlling a humanoid robot, with more features than other research projects, in remote areas either behind NAT (Network Address Translation) or without it. The user can view what the robot is seeing through the Android application and can also move it. Additionally, our system has the ability to deliver real-time audio through our application. We envision that our proposed system is not only an attractive solution for many telerobotic applications but also an extensive educational tool in special needs education.","","978-1-5386-2287-2","10.1109/PACET.2017.8259978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259978","Android Application;Cyber-Physical Systems;Humanoid Robot;Internet of Thing (IoT);Real Time Control System;Telerobotics","Humanoid robots;Androids;Smart phones;Speech;Servers;Bridges","Android (operating system);humanoid robots;Internet of Things;mobile computing;public domain software;telerobotics","IoT;telerobotic applications;open source Android application;interrelated computing devices;mechanical machines;digital machines;Internet of Things humanoid robot;open source Android application","","2","","14","IEEE","18 Jan 2018","","","IEEE","IEEE Conferences"
"A Therapeutic Dialogue Agent for Polish Language","A. Zygadlo","Institute of Telecommunications, Warsaw University of Technology, Warsaw, Poland","2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)","10 Jan 2022","2021","","","1","5","Mental disorders affect large numbers of people worldwide. Computer-aided therapies have been designed and successfully applied in the context of mental health, including solutions based on artificial intelligence (AI). Application of AI in mental disorder therapies frequently has the form of dialogue systems. The long-term goal of this research is to develop a therapeutic chatbot capable of having a conversation in Polish. The initial work was focused on literature review and choosing the software framework for the dialogue system. As a next step, we have prepared a dataset of emotionally grounded dialogue turns in Polish, by applying machine translation to an existing resource for English. The resulting dataset was used for training an emotion recognition model which will be incorporated into the dialogue system. Next, the chatbot will be exposed to a group of test users, and its language understanding capabilities will be further developed. In future, the system will be equipped with a speech interface to also enable verbal communication. We plan to conduct experiments with the help of therapists and patients, followed by thorough analyses of their interactions with the chatbot.","","978-1-6654-0021-3","10.1109/ACIIW52867.2021.9666281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666281","chatbot;dialogue system;affective computing;emotion recognition;natural language processing;mental health","Training;Emotion recognition;Mental disorders;Conferences;Bibliographies;Medical treatment;Mental health","artificial intelligence;behavioural sciences computing;emotion recognition;interactive systems;language translation;medical disorders;natural language interfaces;natural language processing;patient treatment;speech-based user interfaces","dialogue system;therapeutic chatbot;Polish language;emotionally grounded dialogue;emotion recognition model;language understanding capabilities;therapeutic dialogue agent;mental disorder;computer-aided therapies;mental health;artificial intelligence;AI","","","","35","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Chinese Mathematical Text Analysis","G. L. Walker; S. Kuno; B. N. Smith; R. B. Holt","American Mathematical Society (AMS), Providence, RI, USA; Harvard University, Cambridge, MA, USA; American Mathematical Society (AMS), Providence, RI, USA; Device Development Corporation, Waltham, MA, USA","IEEE Transactions on Engineering Writing and Speech","12 Nov 2007","1968","11","2","118","128","Various machine methods have been developed to assist editors in quality control of Chinese-English translations and to prepare improved glossaries for mathematical work. Included are techniques for encoding Chinese characters by a display scope and by a plotting table, and for using these devices as a ten-thousand character ""typewriter"" keyboard. Also, vector sets describing several thousand Chinese characters have been developed and are being used for printout employing a Stromberg-Carlson 4020 recorder. The system configuration is shown in Fig. 2 of the text. Methods of preparing concordances for past translations of Chinese mathematical terms, resolving ambiguities resulting from multiple meanings of characters through context analysis, and automatically locating such troublesome phrases by machine comparison of parallel English and Chinese versions of a complete volume of Acta Mathematica Sinica have been studied and carried out on a computer. Also, a major revision of a standard glossary of mathematical terms is being made using computer-generated output for printing.","2331-3706","","10.1109/TEWS.1968.4322342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4322342","","Text analysis;Encoding;Writing;Terminology;Quality control;Displays;Keyboards;Printing;Loudspeakers;Speech","","","","3","","10","IEEE","12 Nov 2007","","","IEEE","IEEE Journals"
"Natural Language Processing Models: A Comparative Perspective","B. S. Sangma; V. Sharma","Amity Institute of Information Technology, Amity University, Noida, India; Amity Institute of Information Technology, Amity University, Noida, India","2023 2nd International Conference on Edge Computing and Applications (ICECAA)","16 Aug 2023","2023","","","698","703","Natural Language Processing is a thriving branch of artificial intelligence with diverse applications across multiple domains. In recent years, advances in machine learning models for NLP tasks have resulted in a parallel development in NLP methodologies. These models are capable of performing complicated NLP tasks such language translation, sentiment analysis, text categorization, and text production. This study reviews the NLP models by analyzing the traditional models, such as rule-based systems and statistical models, and then move on to the recent neural network and deep learning models. Natural Language Processing (NLP) is a branch of artificial intelligence with diverse applications across multiple domains. In recent years, advances in machine learning models for NLP tasks have resulted in a parallel development of NLP methodologies. These models are capable of performing complicated NLP tasks such as language translation, sentiment analysis, text categorization, and text production.","","979-8-3503-4757-9","10.1109/ICECAA58104.2023.10212389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10212389","natural language processing;machine learning;neural network;artificial intelligence","Analytical models;Sentiment analysis;Computational modeling;Text categorization;Neural networks;Speech recognition;Production","deep learning (artificial intelligence);natural language processing","artificial intelligence;deep learning;machine learning;natural language processing;neural network;NLP","","","","20","IEEE","16 Aug 2023","","","IEEE","IEEE Conferences"
"Improved Transformer With Multi-Head Dense Collaboration","H. Wang; X. Shen; M. Tu; Y. Zhuang; Z. Liu","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of System Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Samsung Research China-Beijing (SRC-B), Beijing, China; Samsung Research China-Beijing (SRC-B), Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","30 Aug 2022","2022","30","","2754","2767","Recently, the attention mechanism boosts the performance of many neural network models in Natural Language Processing (NLP). Among the various attention mechanisms, Multi-Head Attention (MHA) is a powerful and popular variant. MHA helps the model to attend to different feature subspaces independently which is an essential component of Transformer. Despite its success, we conjecture that the different heads of the existing MHA may not collaborate properly. To validate this assumption and further improve the performance of Transformer, we study the collaboration problem for MHA in this paper. First, we propose the Single-Layer Collaboration (SLC) mechanism to help each attention head improve its attention distribution based on the feedback of other heads. Furthermore, we extend SLC to the cross-layer Multi-Head Dense Collaboration (MHDC) mechanism. MHDC helps each MHA layer learn the attention distributions considering the knowledge from the other MHA layers. Both SLC and MHDC are implemented as lightweight modules with very few additional parameters. When equipped with these modules, our new framework, i.e., Collaborative TransFormer (CollFormer), significantly outperforms the vanilla Transformer on a range of NLP tasks, including machine translation, sentence semantic relatedness, natural language inference, sentence classification, and reading comprehension. Besides, we also carry out extensive quantitative experiments to analyze the properties of the MHDC in different settings. The experimental results validate the effectiveness and universality of MHDC as well as CollFormer.","2329-9304","","10.1109/TASLP.2022.3199648","National Key Research and Development Program of China(grant numbers:2020AAA0106500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870037","Multi-head attention;multi-head collaboration;Transformer;attention model","Computational modeling;Semantics;Neural networks;Memory management;Collaboration;Transformers;Magnetic heads","language translation;learning (artificial intelligence);natural language processing;neural nets;pattern classification","NLP tasks;single-layer collaboration mechanism;cross-layer multihead dense collaboration mechanism;natural language inference;vanilla transformer;collaborative transformer;MHA layer;attention distribution;attention head;SLC;collaboration problem;feature subspaces;popular variant;powerful variant;multihead attention;natural language processing;neural network models;attention mechanism;MHDC","","","","61","IEEE","30 Aug 2022","","","IEEE","IEEE Journals"
"Sentimental Analysis on voice using AWS Comprehend","G. Satyanarayana; J. Bhuvana; M. Balamurugan","Department of MCA (SCT), Jain University, Bangalore, India; Department of MCA, Jain University, Bangalore, India; Department of CSE, Christ University, Bangalore, India","2020 International Conference on Computer Communication and Informatics (ICCCI)","1 Jun 2020","2020","","","1","4","Sentimental analysis plays an important role in these days because many start-ups have started with user-driven content [1]. Sentiment analysis is an important research area in natural language processing. Natural language processing has a wide range of applications like voice recognition, machine translation, product review, aspect-oriented product analysis, sentiment analysis and text classification etc [2]. This process will improve the business by analyse the emotions of the conversation. In this project author going to perform sentimental analysis using Amazon Comprehend. Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to extract the content of the document. By using this service can extract the unstructured data like images, voice etc. Thus, will identify the emotions of the conversation and give the output whether the conversation is Positive, Negative, Neutral, or Mixed. To perform this author going to use some services from Aws like s3 which is used for the data store, Transcribe which is used for converting the audio to text, Aws Glue is used to generate the metadata from the comprehend file, Aws Comprehend is used to generate the sentiment file from the audio, Lambda is used to trigger from the data store s3, Aws Athena is used to convert text into structured data and finally there is quick sight where he can visualize the data from the given file.","2329-7190","978-1-7281-4514-3","10.1109/ICCCI48352.2020.9104105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104105","Sentimental analysis;NLP;S3;Transcribe;Aws Glue;Aws Comprehend;Lambda;Aws Athena;Quick sight","","audio signal processing;learning (artificial intelligence);meta data;sentiment analysis;speech processing","AWS Comprehend;sentiment analysis;Amazon Comprehend;natural language processing service;sentiment file;Aws Athena;meta data;Aws Glue;machine learning;NLP;audio-to-text;Transcribe","","7","","10","IEEE","1 Jun 2020","","","IEEE","IEEE Conferences"
"Dynamic Graph Construction Framework for Multimodal Named Entity Recognition in Social Media","W. Mai; Z. Zhang; K. Li; Y. Xue; F. Li","School of Electronics and Information Engineering, South China Normal University, Foshan, China; School of Electronics and Information Engineering, South China Normal University, Foshan, China; School of Electronics and Information Engineering, South China Normal University, Foshan, China; School of Electronics and Information Engineering, South China Normal University, Foshan, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China","IEEE Transactions on Computational Social Systems","","2023","PP","99","1","10","Multimodal named entity recognition (MNER) aims to detect named entities and identify the entity types based on texts and attached images, which also generates inputs for other comprehensive tasks, such as multimodal machine translation, visual dialog, and multimodal sentiment analysis. Existing studies have limitations in text-image matching and multimodal semantic disparity reduction. For one thing, current methods fail to resolve both overall and local text-image matching issues in a self-guided way. For another, the static graphs constructed in MNER models are challenging in bridging the semantic gap between different modalities. In this work, a dynamic graph construction framework (DGCF) is proposed to solve the above-mentioned limitations. A similarity vector-based text-image matching inferring strategy is designed to obtain the overall and local matching relation between text and image while the overall matching determines the retained proportion of visual information. Then, a multimodal dynamic graph interaction module is developed. Within each layer of the module, the local matching relations and part of speech (POS)-based multihead attention are integrated to construct a dynamic cross-modal graph and a semantic graph. Lastly, a CRF layer is used to predict entity label. Extensive experiments are performed on two benchmark datasets. The experimental results reveal that our model is a competitive alternative and achieves state-of-the-art performance.","2329-924X","","10.1109/TCSS.2023.3303027","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515011370); Characteristic Innovation Projects of Guangdong Colleges and Universities(grant numbers:2018KTSCX049); Science and Technology Plan Project of Guangzhou(grant numbers:202102080258); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515012290); Guangdong Provincial Key Laboratory of Cyber-Physical Systems(grant numbers:2020B1212060069); National and Local Joint Engineering Research Center of Intelligent Manufacturing Cyber-Physical Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223602","Dynamic cross-modal graph;dynamic graph construction;multimodal named entity recognition (MNER);text-image matching","Visualization;Semantics;Task analysis;Social networking (online);Machine translation;Computational modeling;Representation learning","","","","","","","IEEE","17 Aug 2023","","","IEEE","IEEE Early Access Articles"
"Assistive Technology for the Visually Impaired Using Computer Vision","M. P. Arakeri; N. S. Keerthana; M. Madhura; A. Sankar; T. Munnavar","Information Science and Engineering, Ramaiah Institute of Technology, Bangalore, India; Information Science and Engineering, Ramaiah Institute of Technology, Bangalore, India; Information Science and Engineering, Ramaiah Institute of Technology, Bangalore, India; Information Science and Engineering, Ramaiah Institute of Technology, Bangalore, India; Information Science and Engineering, Ramaiah Institute of Technology, Bangalore, India","2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","2 Dec 2018","2018","","","1725","1730","India has more than a quarter of the world's 36 million blind people. Educating the blind to avoid unemployment among their population is one of the most challenging problems faced by blind schools today. Although many schools resort to the use of Braille to eradicate illiteracy among them, its steep learning curve, insufficient availability and high cost makes it quite unapproachable. Braille Literacy Statistics in India indicate that out of the 12 million blind people in the country, less than 10 percent learn Braille. It is evident that one of the most crucial issues faced by the blind and the visually impaired is the inability to read and learn without the use of Braille. In order to overcome this issue, there is a need to develop a system that can assist the visually impaired in reading. Hence, the proposed solution is to design an inexpensive wearable device that uses computer vision to read out any form of text around the user in various alignments and lighting conditions. The system makes use of a Raspberry Pi with a compatible camera to capture the content around the visually impaired or blind person and reads it out to them in a regional language. A sensor is also incorporated to notify the user of the distance to the nearest object at his eye level and the device enumerates various objects in its sight. The system is based on the combination of image processing, machine learning and speech synthesis techniques. The observed accuracy combining both the optical character recognition and the object recognition algorithms was found to be 84%.","","978-1-5386-5314-2","10.1109/ICACCI.2018.8554625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554625","Raspberry Pi;Optical Character Recognition (OCR);Object Recognition;Text to Speech;Language Translation","Optical character recognition software;Cameras;Object recognition;Google;Blindness;Text recognition;Instruction sets","computer vision;handicapped aids;learning (artificial intelligence);optical character recognition","blind person;machine learning;blind schools;lighting conditions;inexpensive wearable device;crucial issues;Braille Literacy Statistics;insufficient availability;steep learning curve;India;computer vision;visually impaired;assistive technology","","14","","14","IEEE","2 Dec 2018","","","IEEE","IEEE Conferences"
"Adversarial Audio Attacks that Evade Temporal Dependency","H. Liu; G. Ditzler","Department of Electrical & Computer Engineering, The University of Arizona, Tucson, AZ; Department of Electrical & Computer Engineering, The University of Arizona, Tucson, AZ","2020 IEEE Symposium Series on Computational Intelligence (SSCI)","5 Jan 2021","2020","","","639","646","As the real-world applications (image segmentation, speech recognition, machine translation, etc.) are increasingly adopting Deep Neural Networks (DNNs), DNN‚Äôs vulnerabilities in a malicious environment have become an increasingly important research topic in adversarial machine learning. Adversarial machine learning (AML) focuses on exploring vulnerabilities and defensive techniques for machine learning models. Recent work has shown that most adversarial audio generation methods fail to consider audios‚Äô temporal dependency (TD) (i.e., adversarial audios exhibit weaker TD than benign audios). As a result, the adversarial audios are easily detectable by examining their TD. Therefore, one area of interest in the audio AML community is to develop a novel attack that evades a TD-based detection model. In this contribution, we revisit the LSTM model for audio transcription and propose a new audio attack algorithm that evades the TD-based detection by explicitly controlling the TD in generated adversarial audios. The experimental results show that the detectability of our adversarial audio is significantly reduced compared to the state-of-the-art audio attack algorithms. Furthermore, experiments also show that our adversarial audios remain nearly indistinguishable from benign audios with only negligible perturbation magnitude.","","978-1-7281-2547-3","10.1109/SSCI47803.2020.9308597","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308597","","Logic gates;Perturbation methods;Task analysis;Neural networks;Speech recognition;Security;Quantization (signal)","audio signal processing;learning (artificial intelligence);neural nets","state-of-the-art audio attack algorithms;benign audios;adversarial audio attacks;evade temporal dependency;adversarial machine learning;machine learning models;adversarial audio generation methods;audio AML community;audio transcription;audio attack algorithm;generated adversarial audios","","","","26","IEEE","5 Jan 2021","","","IEEE","IEEE Conferences"
"Face as mouse through visual face tracking","Jilin Tu; T. Huang; Hai Tao","Electrical and Computer Engineering Department, University of Illinois, Urbana-Champaign, IL, USA; Electrical and Computer Engineering Department, University of Illinois, Urbana-Champaign, IL, USA; Electrical Engineering Department, University of California, Santa Cruz, CA, USA","The 2nd Canadian Conference on Computer and Robot Vision (CRV'05)","20 Jun 2005","2005","","","339","346","This paper introduces a novel camera mouse driven by 3D model based visual face tracking technique. While camera becomes standard configuration for personal computer (PC) and computer speed becomes faster and faster, achieving human machine interaction through visual face tracking becomes a feasible solution to hand-free control. The human facial movement can be decomposed into rigid movement, e.g. rotation and translation, and non-rigid movement, such as the open/close of mouth, eyes, and facial expressions, etc. We introduce our visual face tracking system that can robustly and accurately retrieve these motion parameters from video at real-time. After calibration, the retrieved head orientation and translation can be employed to navigate the mouse cursor, and the detection of mouth movement can be utilized to trigger mouse events. 3 mouse control modes are investigated and compared. Experiments in Windows XP environment verify the convenience of navigation and operations using our face mouse. This technique can be an alternative input device for people with hand and speech disability and for futuristic vision-based game and interface.","","0-7695-2319-6","10.1109/CRV.2005.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1443150","","Mice;Cameras;Humans;Mouth;Face detection;Navigation;Microcomputers;Eyes;Tracking;Robustness","mouse controllers (computers);gesture recognition;cameras;face recognition;human computer interaction","camera mouse;3D model based visual face tracking;personal computer;human machine interaction;hand-free control;human facial movement;head orientation;head translation;mouse cursor;mouth movement detection;mouse event triggering;mouse control modes;Windows XP;face mouse;vision-based game;vision-based interface","","4","1","22","IEEE","20 Jun 2005","","","IEEE","IEEE Conferences"
"BFGAN: Backward and Forward Generative Adversarial Networks for Lexically Constrained Sentence Generation","D. Liu; J. Fu; Q. Qu; J. Lv","College of Computer Science, State Key Laboratory of Hydraulics and Mountain River Engineering, Sichuan University, Chengdu, China; Quebec Artificial Intelligence Institute (Mila), Polytechnique Montreal, Montreal, Canada; College of Computer Science, State Key Laboratory of Hydraulics and Mountain River Engineering, Sichuan University, Chengdu, China; College of Computer Science, State Key Laboratory of Hydraulics and Mountain River Engineering, Sichuan University, Chengdu, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","28 Nov 2019","2019","27","12","2350","2361","Incorporating prior knowledge like lexical constraints into the model's output to generate meaningful and coherent sentences has many applications in dialogue system, machine translation, image captioning, etc. However, existing auto-regressive models incrementally generate sentences from left to right via beam search, which makes it difficult to directly introduce lexical constraints into the generated sentences. In this paper, we propose a new algorithmic framework, dubbed BFGAN, to address this challenge. Specifically, we employ a backward generator and a forward generator to generate lexically constrained sentences together, and use a discriminator to guide the joint training of two generators by assigning them reward signals. Due to the difficulty of BFGAN training, we propose several training techniques to make the training process more stable and efficient. Our extensive experiments on three large-scale datasets with human evaluation demonstrate that BFGAN has significant improvements over previous methods.","2329-9304","","10.1109/TASLP.2019.2943018","National Key R&D Program of China(grant numbers:2017YFB1002201); National Natural Science Fund for Distinguished Young Scholar(grant numbers:61625204); State Key Program of the National Science Foundation of China(grant numbers:61836006,61432014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846084","Lexically constrained sentence generation;generative adversarial networks;natural language generation","Generators;Training;Task analysis;Gallium nitride;Search problems;Natural languages;Force","feature extraction;learning (artificial intelligence);natural language processing;regression analysis;text analysis","lexically constrained sentences;dubbed BFGAN;generated sentences;image captioning;machine translation;dialogue system;coherent sentences;meaningful sentences;lexical constraints;prior knowledge;lexically constrained sentence generation;forward generative adversarial networks;BFGAN training","","15","","39","IEEE","23 Sep 2019","","","IEEE","IEEE Journals"
"Grow and Prune Compact, Fast, and Accurate LSTMs","X. Dai; H. Yin; N. K. Jha","Department of Electrical Engineering, Princeton University, Princeton, USA; Department of Electrical Engineering, Princeton University, Princeton, USA; Department of Electrical Engineering, Princeton University, Princeton, USA","IEEE Transactions on Computers","10 Feb 2020","2020","69","3","441","452","Long short-term memory (LSTM) has been widely used for sequential data modeling. Researchers have increased LSTM depth by stacking LSTM cells to improve performance. This incurs model redundancy, increases run-time delay, and makes the LSTMs more prone to overfitting. To address these problems, we propose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original one-level nonlinear control gates. H-LSTM increases accuracy while employing fewer external stacked layers, thus reducing the number of parameters and run-time latency significantly. We employ grow-and-prune (GP) training to iteratively adjust the hidden layers through gradient-based growth and magnitude-based pruning of connections. This learns both the weights and the compact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for image captioning, speech recognition, and neural machine translation applications. For the NeuralTalk architecture on the MSCOCO dataset, our three models reduce the number of parameters by 38.7√ó [floating-point operations (FLOPs) by 45.5√ó], run-time latency by 4.5√ó, and improve the CIDEr-D score by 2.8 percent, respectively. For the DeepSpeech2 architecture on the AN4 dataset, the first model we generated reduces the number of parameters by 19.4√ó and run-time latency by 37.4 percent. The second model reduces the word error rate (WER) from 12.9 to 8.7 percent. For the encoder-decoder sequence-to-sequence network on the IWSLT 2014 German-English dataset, the first model we generated reduces the number of parameters by 10.8√ó and run-time latency by 14.2 percent. The second model increases the BLEU score from 30.02 to 30.98. Thus, GP-trained H-LSTMs can be seen to be compact, fast, and accurate.","1557-9956","","10.1109/TC.2019.2954495","National Science Foundation(grant numbers:CNS-1617640); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907435","Deep learning;grow-and-prune training;long short-term memory;neural network","Logic gates;Training;Computer architecture;Stacking;Speech recognition;Neural networks;Computational modeling","recurrent neural nets","long short-term memory;sequential data modeling;hidden-layer LSTM;one-level nonlinear control gates;H-LSTM increases accuracy;grow-and-prune training;gradient-based growth;compact architecture;H-LSTM control gates;GP-trained H-LSTMs;neural machine translation applications;DeepSpeech2 architecture;external stacked layers;AN4 dataset;CIDEr-D score;IWSLT 2014 German-English dataset;BLEU score","","38","","58","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"WaveTransformer: An Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information","A. Tran; K. Drossos; T. Virtanen","Audio Research Group Tampere University, Tampere, Finland; Audio Research Group Tampere University, Tampere, Finland; Audio Research Group Tampere University, Tampere, Finland","2021 29th European Signal Processing Conference (EUSIPCO)","8 Dec 2021","2021","","","576","580","Automated audio captioning (AAC) is a novel task, where a method takes as an input an audio sample and outputs a textual description (i.e. a caption) of its contents. Most AAC methods are adapted from image captioning or machine translation fields. In this work, we present a novel AAC method, explicitly focused on the exploitation of the temporal and time-frequency patterns in audio. We employ three learnable processes for audio encoding, two for extracting the temporal and time-frequency information, and one to merge the output of the previous two processes. To generate the caption, we employ the widely used Transformer decoder. We assess our method utilizing the freely available splits of the Clotho dataset. Our results increase previously reported highest SPIDEr to 17.3, from 16.2 (higher is better).","2076-1465","978-9-0827-9706-0","10.23919/EUSIPCO54536.2021.9616340","European Union's Horizon 2020(grant numbers:957337); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616340","automated audio captioning;wavetransformer;wavenet;transformer","Measurement;Time-frequency analysis;Neural networks;Europe;Transformers;Encoding;Decoding","audio coding;language translation;learning (artificial intelligence);speech processing;time-frequency analysis","temporal information;WaveTransformer;learning temporal;automated audio captioning;audio sample;textual description;AAC methods;image captioning;temporal patterns;learnable processes;audio encoding;time-frequency information;time-frequency patterns;Transformer decoder","","2","","24","","8 Dec 2021","","","IEEE","IEEE Conferences"
"Alignment of spoken narratives for automated neuropsychological assessment","E. T. Prud'hommeaux; B. Roark","Center for Spoken Language Understanding, Oregon Health and Sciences University, Portland, OR, USA; Center for Spoken Language Understanding, Oregon Health and Sciences University, Portland, OR, USA","2011 IEEE Workshop on Automatic Speech Recognition & Understanding","5 Mar 2012","2011","","","484","489","Narrative recall tasks are commonly included in neurological examinations, as deficits in narrative memory are associated with disorders such as Alzheimer's dementia. We explore methods for automatically scoring narrative retellings via alignment to a source narrative. Standard alignment methods, designed for large bilingual corpora for machine translation, yield high alignment error rates (AER) on our small monolingual corpora. We present modifications to these methods that obtain a decrease in AER, an increase in scoring accuracy, and diagnostic classification performance comparable to that of manual methods, thus demonstrating the utility of these techniques for this task and other tasks relying on monolingual alignments.","","978-1-4673-0367-5","10.1109/ASRU.2011.6163979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6163979","","Hidden Markov models;Training data;Accuracy;Data models;Training;Manuals;Dementia","natural language processing;neurophysiology;psychology","spoken narratives;automated neuropsychological assessment;neurological examinations;narrative memory;Alzheimer dementia;narrative retellings;standard alignment method;bilingual corpora;machine translation;alignment error rates;diagnostic classification;monolingual alignments","","5","","30","IEEE","5 Mar 2012","","","IEEE","IEEE Conferences"
"The Future Is Not What It Used to Be","R. Pieraccini; L. Rabiner",NA; NA,"The Voice in the Machine: Building Computers That Understand Speech","","2012","","","263","283","This chapter contains sections titled: Solving Problems, Rich Phone Applications, No Data like ... Even More Data, Learning to Discriminate, Emotions, Distillations and Translations, Voice Search, Put That There!, Making Friends, What We Must Do, Invisible Speech","","9780262301534","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278127.pdf&bkn=6267547&pdfType=chapter","","","","","","","","","","4 Sep 2012","","","MIT Press","MIT Press eBook Chapters"
"Vision 360: Image Caption Generation Using Encoder-Decoder Model","A. Kumari; A. Chauhan; A. Singhal","Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, India; Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, India; Amity School of Engineering and Technology, Amity University Uttar Pradesh, Noida, India","2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence)","21 Mar 2022","2022","","","312","317","Vision360 incorporates three features in itself Image elaboration, speech to text, text to speech. The Main feature is Image Caption Generation, i.e., not only it is responsible for Image Segmentation, Object classification but it also establishes a relation between the objects classified that too with a logical relation that somehow gives the human vibe. Encoder-decoder model has been used. CNN has been used for Image and LSTM has been used for text. The paper also demonstrates the integration InceptionV3 model. Vision360 is a way of providing aid to blind people or partially blind people. It‚Äôs a way to bring convenience in their proximity in a single touch. It tries to bridge the gap that they have been feeling all along while walking on the same path with different people. A task to describe an Image is not very hard but if we want to automate this task of depicting something from an image and make the machine do it, it‚Äôll be nearly impossible, even if the new researches have been made and feature extraction is attainable. Logically establishing semantically and syntactically correct sentences is still a hard task to accomplish. We used encoder-decoder model for parallel training of Image and text data, and used InceptionV3 for extracting feature vector. We evaluated our result on BLEU score metric and the model achieved BLEU score in-range of 0.70 to 0.78 for various images in the validation set.","","978-1-6654-3701-1","10.1109/Confluence52989.2022.9734167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9734167","CNN;LSTM;Feature extraction","Training;Measurement;Legged locomotion;Image segmentation;Image coding;Blindness;Feature extraction","computational linguistics;feature extraction;grammars;handicapped aids;image classification;image segmentation;language translation;learning (artificial intelligence);natural language processing;text analysis","Image Caption Generation;encoder-decoder model;Image Segmentation;Object classification;logical relation;integration InceptionV3 model;partially blind people;feature extraction;text data;feature vector","","","","20","IEEE","21 Mar 2022","","","IEEE","IEEE Conferences"
"UBERT22: Unsupervised Pre-training of BERT for Low Resource Urdu Language","B. Tahir; M. A. Mehmood","Al- Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan; Al- Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan","2022 16th International Conference on Open Source Systems and Technologies (ICOSST)","18 Jan 2023","2022","","","1","6","Natural Language Understanding (NLU) tools have enabled the development of sophisticated and powerful Natural Language Processing (NLP) models. However, this progress is limited to English and European languages and low resource languages lack such tools due paucity of resources. In this paper, we develop UBERT22- an unsupervised pre-trained BERT model for Urdu language. For this purpose, first, we develop a dataset of ‚ÄòZakheera‚Äô containing high-quality content of 1.16 million Urdu language news and blog articles posted on top 37 Urdu websites. Next, we pre-process the text and tokenize content in 21.8 million Urdu sentences. Finally, we extract the word-piece vocabulary of 30,000 tokens and pre-train the BERT model for Masked Language Modelling (MLM) and Next Sentence Prediction (NSP) tasks. We compare the performance of UBERT22 with existing multilingual and small-Urdu BERT for various downstream tasks. We notice that UBERT22 outperforms multilingual and small-Urdu BERT for fake news identification, propaganda classification, topic categorization, and sentiment analysis tasks. Overall, UBERT22 achieves 2-19% higher accu-racy compared to baseline results and competitive BERT models. We believe that the public availability of our pre-trained model and upstream dataset will enable the development of state-of-the-art NLP models of Urdu language such as chatbots, question answering systems, sentiment analyzers, virtual assistants, speech recognizers, and machine translators.","2770-8225","978-1-6654-6477-2","10.1109/ICOSST57195.2022.10016821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016821","Urdu BERT;text classification;NLU model","Analytical models;Vocabulary;Sentiment analysis;Bit error rate;Blogs;Speech recognition;Predictive models","language translation;natural language processing;natural languages;text analysis;Web sites","37 Urdu websites;competitive BERT models;European languages;high-quality content;low resource languages;low resource Urdu;Masked Language Modelling;Natural Language Understanding tools;Next Sentence Prediction tasks;small-Urdu BERT;state-of-the-art NLP models;tokenize content;UBERT22 outperforms;unsupervised pre-trained BERT model;Urdu language","","","","31","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"TagMic: Listening Through RFID Signals","Y. Li; C. Duan; X. Ding; C. Liu",Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University,"2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)","23 Feb 2021","2020","","","1187","1188","RFID is an increasingly ubiquitous technology widely adopted in both the industry and our daily life nowadays. But when it comes to eavesdropping, people usually pay attention to devices like cameras and mobile phones, instead of small-volume and battery-free RFID tags. This work shows the possibility of using prevalence RFIDs to capture and recognize the acoustic signals. To be specific, we attach an RFID tag on an object, which is located in the vicinity of the sound source. Our key innovation lies in the translation between the vibrations induced when the sound wave hits the object surface and the fluctuations in the tag's RF signals. Although the inherent sampling rate of commercial RFID devices is deficient, and the vibrations are very subtle, we still extract characteristic features from imperfect measurements by taking advantage of state-of-the-art machine learning and signal processing algorithms. We have implemented our system with commercial RFID and loudspeaker equipment and evaluated it intensively in our lab environment. Experimental results show that the average success rate in detecting single tone sounds can reach as high as 93.10%. We believe our work would raise the attention of RFID in the concern of surveillance and security.","2575-8411","978-1-7281-7002-2","10.1109/ICDCS47774.2020.00136","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355596","TagMic;speech recognition;RFID;privacy","Vibrations;Technological innovation;Surveillance;Vibration measurement;Feature extraction;RFID tags;Surface treatment","acoustic signal processing;learning (artificial intelligence);loudspeakers;radiofrequency identification","ubiquitous technology;mobile phones;battery-free RFID tags;acoustic signals;sound source;sound wave;object surface;commercial RFID devices;machine learning;signal processing algorithms;TagMic;RFID signals;prevalence RFID;characteristic features extraction;lab environment","","","","6","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"A country report ‚Äì OCOCOSDA activities in China","A. Li; D. Wang","Chinese Academy of Social Sciences Research Institute of Information Technology, Tsinghua University, Cebu City, Philippines; Institute of Linguistics, Chinese Academy of Social Sciences Research Institute of Information Technology, Tsinghua University, Cebu City, Philippine","2019 22nd Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)","23 Apr 2020","2019","","","1","6","Till now, there are 109 corpora, including speech synthesis/recognition corpora, corpora for machine translation, lexicon and other natural language processing corpora.","2472-7695","978-1-7281-2449-0","10.1109/O-COCOSDA46868.2019.9060828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060828","","","","","","","","","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"Comparing the Modeling Powers of RNN and HMM","A. Sala√ºn; Y. Petetin; F. Desbouvries","Institut Polytechnique de Paris, Telecom SudParis, Evry, France; CNRS, Samovar, Telecom SudParis, Institut Polytechnique de Paris, Evry, France; Institut Polytechnique de Paris, Telecom SudParis, Evry, France","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","1496","1499","Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM) are popular models for processing sequential data and have found many applications such as speech recognition, time series prediction or machine translation. Although both models have been extended in several ways (eg. Long Short Term Memory and Gated Recurrent Unit architectures, Variational RNN, partially observed Markov models...), their theoretical understanding remains partially open. In this context, our approach consists in classifying both models from an information geometry point of view. More precisely, both models can be used for modeling the distribution of a sequence of random observations from a set of latent variables; however, in RNN, the latent variable is deterministically deduced from the current observation and the previous latent variable, while, in HMM, the set of (random) latent variables is a Markov chain. In this paper, we first embed these two generative models into a generative unified model (GUM). We next consider the subclass of GUM models which yield a stationary Gaussian observations probability distribution function (pdf). Such pdf are characterized by their covariance sequence; we show that the GUM model can produce any stationary Gaussian distribution with geometrical covariance structure. We finally discuss about the modeling power of the HMM and RNN submodels, via their associated observations pdf: some observations pdf can be modeled by a RNN, but not by an HMM, and vice versa; some can be produced by both structures, up to a re-parameterization.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999058","Recurrent Neural Networks;Hidden Markov Models","Hidden Markov models;Computational modeling;Biological system modeling;Recurrent neural networks;Covariance matrices;Probability density function;Gaussian distribution","Gaussian distribution;hidden Markov models;probability;recurrent neural nets","HMM;latent variable;Markov chain;generative unified model;GUM model;stationary Gaussian observations probability distribution function;stationary Gaussian distribution;RNN submodels;recurrent neural networks;Hidden Markov Models;sequential data;long short term memory;gated recurrent unit architectures;variational RNN","","7","","14","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Learning Approach to Trademark International Class Identification","G. Showkatramani; N. Khatri; A. Landicho; D. Layog","United States Patent and Trademark Office, Alexandria, VA, USA; Arktix Solutions, Inc., Dumfries, VA, United States; Arktix Solutions, Inc., Dumfries, VA, United States; Arktix Solutions, Inc., Dumfries, VA, United States","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","608","612","A trademark may be a word, phrase, symbol, sound, color, scent or design, or combination of these, that identifies and distinguishes the goods or services of a particular source from those of others. Obtaining a trademark is a complex, time intensive and costly process that involves varied steps before the trademark can be registered including searching prior trademarks, filing of the trademark application, review of the trademark application and final publication for opposition by the public. One of the crucial aspect in the review of the trademark application is determining the international classes of the filed marks based on their goods and services description. Currently, the process of identifying the international classes of a filed mark is performed manually in the United States Patent and Trademark Office (USPTO) and takes significant amount of time. Recently, word embeddings and deep neural networks (DNNs) have revolutionized the field of computer vision and demonstrated excellent performance in various natural language processing (NLP) tasks such as machine translation, speech recognition, sentence and document classification etc. to name a few. In this study, we explored fastText and different neural networks such as Convolution Neural Networks (CNN), Long Short Term Memory (LSTM), bidirectional versions of both LSTM and Gated Recurrent Unit (GRU) and Recurrent Convolutional Neural Network (RCNN) to automate the international class identification for the filed marks based on their goods and services description. Overall, it was found that the trademark word embeddings with RCNN model outperformed the other models. Our study thereby seeks to provide a solution towards the manual, time intensive and laborious process of identifying international classes for the trademarks.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999317","Convolutional Neural Networks, Gated Recurrent Unit, International Trademark Classes, Long Short Term Memory, Nice Classification, Recurrent Neural Networks, Trademark Text Classification, Word Embeddings.","Trademarks;Logic gates;Computer architecture;Recurrent neural networks;Natural language processing;Training","convolutional neural nets;document handling;learning (artificial intelligence);patents;pattern classification;recurrent neural nets;trademarks","trademark application;international classes;filed mark;United States Patent and Trademark Office;deep neural networks;natural language processing tasks;trademark word embeddings;recurrent convolutional neural network;trademark international class identification;gated recurrent unit;LSTM;long short term memory","","1","","23","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Fact-checking with explanations","A. Groza; √Å. Katona","Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","2022 24th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","25 May 2023","2022","","","150","157","We present an approach for automated fact-checking, given a trusted knowledge base and a natural language text. The FACE (FAct Checker with Explanations) system is capable of extracting the knowledge behind the sentences, and decide what is entailed in the trusted sources and what is in conflict with them, providing also explanations and counter speeches in English. The system also specifies the provenance of each of its argument, thus it can be traced back to the source of the information.Description logic representation of the input is obtained using the FRED machine reader, which is further improved by detecting and handling translation patterns. The obtained ontology is aligned to the knowledge base using the WordNet database in a custom algorithm, then entailment and conflict detection is performed with the Hermit reasoner, through which we obtain the explanations and counter speeches which are verbalized to Attempto Controlled English.The fact checker is demonstrated on Covid-19 related sentences, however it is domain independent, and can be used with other knowledge bases as well.","2470-881X","978-1-6654-6545-8","10.1109/SYNASC57785.2022.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130971","fake news;natural language understanding;description logics;automated fact-checking;explainable AI","COVID-19;Scientific computing;Databases;Knowledge based systems;Natural languages;Ontologies;Fake news","diseases;epidemics;inference mechanisms;natural language processing;natural languages;ontologies (artificial intelligence);text analysis","automated fact-checking;conflict detection;Covid-19 related sentences;description logic representation;detecting handling translation patterns;FAct Checker with Explanations;FRED machine reader;natural language text;speeches;trusted knowledge base;trusted sources","","","","14","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"Trademark Design Code Identification Using Deep Neural Networks","G. J. Showkatramani; N. Khatri; A. Landicho; D. Layog","United States Patent and Trademark Office, Alexandria, VA, United States; Arktix Solutions, Inc., Dumfries, VA, United States; Arktix Solutions, Inc., Dumfries, VA, United States; Arktix Solutions, Inc., Dumfries, VA, United States","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","61","65","Trademark review and approval is a complex process that involves thorough analysis and review of the design components of the marks including the visual characteristics as well as the textual mark description data specifying the significant aspects of the mark. One of the crucial aspect in review of the trademark application is determining the design codes of the trademarks based on their mark description. Currently, the process of identifying the design codes for a trademark is performed manually in the United States Patent and Trademark Office (USPTO) and takes substantial amount of time. Recently, word embeddings and deep neural networks (DNNs) have demonstrated excellent performance in computer vision and various natural language processing (NLP) tasks such as machine translation, speech recognition, sentence and document classification etc. to name a few. In this study, we explored fastText and different neural networks such as Convolution Neural Networks (CNN), Long Short Term Memory (LSTM), bidirectional versions of both LSTM and Gated Recurrent Unit (GRU) and Recurrent Convolutional Neural Network (RCNN) to automate trademark design code classification based on their mark description. Overall, it was found that the trademark word embeddings with RCNN model outperformed other models. Our study thereby seeks to provide a solution towards the time intensive and laborious process of identifying design codes of the trademarks.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614042","Convolutional Neural Networks, Gated Recurrent Unit, Long Short Term Memory, Recurrent Neural Networks, Trademark design code, Trademark Text Classification, Word Embeddings.","Trademarks;Logic gates;Natural language processing;Text categorization;Computer architecture;Convolutional neural networks","computer vision;convolutional neural nets;image classification;patents;recurrent neural nets;text analysis;trademarks","deep neural networks;textual mark description data;natural language processing tasks;trademark design code classification;trademark word embeddings;United States Patent and Trademark Office;convolution neural networks;trademark design code identification;recurrent convolutional neural network;computer vision;long short term memory network;gated recurrent unit","","","","23","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Supplement to a bibliography of information theory (communication theory - cybernetics)","F. Stumpers","Philips Research Laboratories, Eindhoven, Netherlands","IRE Transactions on Information Theory","6 Jan 2003","1955","1","2","31","47","","2168-2712","","10.1109/TIT.1955.1055128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1055128","","Bibliographies;Information theory;Network address translation;TV;Instruments;Servomechanisms;Mathematics;Cybernetics","","Bibliographies;Coding;Cybernetics;Image processing;Information theory;Man-machine systems;Multiplexing;Pulse modulation;Radar;Servosystems;Speech processing;TV","","3","","","IEEE","6 Jan 2003","","","IEEE","IEEE Journals"
"Text Generator using Natural Language Processing Methods","R. Kaur; A. Kaur","Computer Science and Engineering, Chandigarh University, Punjab, India; Computer Science and Engineering, Chandigarh University, Punjab, India","2023 International Conference on Computational Intelligence, Communication Technology and Networking (CICTN)","7 Jun 2023","2023","","","421","425","The field of NLP (Natural Language Processing) deals with the interaction between computers and human languages, focusing on the automated analysis of human language to extract useful information. For organisations that place a premium on efficiency, automation is the ultimate aim, and natural language processing is key to getting there. A machine can only completely comprehend humans when it talks with them in human language; it‚Äôs like speaking to someone in your mother tongue as opposed to someone you hardly understand or can barely speak. It is closed using natural language processing. . This paper will examine how a machine can truly comprehend human language while also delving into the specific algorithms that are employed. This essay will examine text conversions in-depth and go over how natural language processing proceeds. For those who are curious about the fundamentals of comprehending and mastering natural language processing, this review essay may be helpful. Natural language processing has a wide range of applications, including text processing, summarization, translation, voice and speech recognition, AI-powered user interfaces, expert systems, and more. However, Despite advances, NLP still faces challenges in accurately detecting emotions and expressions conveyed through non-verbal cues such as voice tone and body language. There are far too many difficulties in processing the complex linguistic system of humans. It will soon be necessary for everyone to communicate with a machine for both domestic and space-related tasks, making it crucial to comprehend how artificial intelligence works and how crucial NLP is to it.","","979-8-3503-3802-7","10.1109/CICTN57981.2023.10140271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10140271","Natural language processing (NLP) encompasses techniques such as machine learning;stemming;lemmatization;TF-IDF;artificial intelligence (AI);bag of words;and vectorization to process and understand human language","Computers;Automation;Tongue;Text analysis;User interfaces;Natural language processing;Grammar","artificial intelligence;computational linguistics;expert systems;natural language processing;speech recognition;text analysis;user interfaces","body language;comprehending;human language;mastering natural language processing;Natural Language Processing methods;natural language processing proceeds;voice tone","","","","19","IEEE","7 Jun 2023","","","IEEE","IEEE Conferences"
"Current communication technologies in language processing","J. Mizera-Pietraszko","Institute of Mathematics and Computer Science, Opole University, Opole, Poland","2015 Fourth International Conference on Future Generation Communication Technology (FGCT)","26 Oct 2015","2015","","","1","6","Even the most cutting-edge communication-mediated technology like satellite navigation for orbit positioning, pedestrian movement recognition systems based on inertial sensors, 5G systems, let alone medical devices for coordination of human organs functionality would not be invented without technologies for language processing as an information source between humans and communication systems. Regardless of the way we communicate that is via emails, website short tweets, video conferencing systems, social networking, blogs, instant messaging through websites or mobile applications, or texting only, we use a language that is processed by computer system. Thus, the keynote paper discusses language processing technologies as the core of any other communication between human beings or human-machine. From the user‚Äôs perspective communication technologies simply integrate computer science and linguistics, but that is a very broad scope of knowledge. Technologies of a particular language rely on computational linguistics, natural language processing including also speech processing, information retrieval, knowledge acquisition and machine translation tools.","2377-2638","978-1-4799-8267-7","10.1109/FGCT.2015.7300260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300260","Computer-Mediated Communication;Language Processing;Intelligent Information Systems;Computational Linguistics;Machine Translation;Information Retrieval","Syntactics;Pragmatics;Semantics;Grammar;Communication systems;Natural language processing","","","","","","23","IEEE","26 Oct 2015","","","IEEE","IEEE Conferences"
"Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation","R. Goel; S. Susan; S. Vashisht; A. Dhanda","Department of Information Technology, Delhi Technological University, New Delhi, India; Department of Information Technology, Delhi Technological University, New Delhi, India; Department of Information Technology, Delhi Technological University, New Delhi, India; Department of Information Technology, Delhi Technological University, New Delhi, India","2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)","10 Jan 2022","2021","","","1","6","Modern day conversational agents are trained to emulate the manner in which humans communicate. To emotionally bond with the user, these virtual agents need to be aware of the affective state of the user. Transformers are the recent state of the art in sequence-to-sequence learning that involves training an encoder-decoder model with word embeddings from utterance-response pairs. We propose an emotion-aware transformer encoder for capturing the emotional quotient in the user utterance in order to generate human-like empathetic responses. The contributions of our paper are as follows: 1) An emotion detector module trained on the input utterances determines the affective state of the user in the initial phase 2) A novel transformer encoder is proposed that adds and normalizes the word embedding with emotion embedding thereby integrating the semantic and affective aspects of the input utterance 3) The encoder and decoder stacks belong to the Transformer-XL architecture which is the recent state of the art in language modeling. Experimentation on the benchmark Facebook AI empathetic dialogue dataset confirms the efficacy of our model from the higher BLEU-4 scores achieved for the generated responses as compared to existing methods. Emotionally intelligent virtual agents are now a reality and inclusion of affect as a modality in all human-machine interfaces is foreseen in the immediate future.","","978-1-6654-0021-3","10.1109/ACIIW52867.2021.9666315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666315","Transformer-XL;Empathetic dialogue generation;Affective state;Encoder-decoder model","Training;Social networking (online);Conferences;Semantics;Computer architecture;Detectors;Transformers","decoding;emotion recognition;human computer interaction;interactive systems;language translation;learning (artificial intelligence);natural language processing;social networking (online);speech recognition;virtual reality","emotion-aware transformer encoder;empathetic dialogue generation;modern day conversational agents;emotionally bond;affective state;transformers;recent state;sequence-to-sequence learning;encoder-decoder model;word embeddings;utterance-response pairs;emotional quotient;user utterance;empathetic responses;emotion detector module;input utterance;novel transformer encoder;decoder stacks;Transformer-XL architecture;benchmark Facebook AI empathetic dialogue dataset;generated responses;intelligent virtual agents","","2","","31","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Neural Attention Model for Abstractive Text Summarization Using Linguistic Feature Space","A. Dilawari; M. U. G. Khan; S. Saleem; Zahoor-Ur-Rehman; F. S. Shaikh","Department of Computer Science and Information Technology, University of Home Economics, Lahore, Pakistan; Department of Computer Science, University of Engineering and Technology, Lahore (UET Lahore), Lahore, Pakistan; Rheinland-Pf√§lzische Technische Universit√§t, Kaiserslautern, Kaiserslautern, Germany; Department of Computer Science, COMSATS University Islamabad, Attock Campus, Attock, Pakistan; Computer Information Systems Department, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, Dammam, Saudi Arabia","IEEE Access","13 Mar 2023","2023","11","","23557","23564","Summarization generates a brief and concise summary which portrays the main idea of the source text. There are two forms of summarization: abstractive and extractive. Extractive summarization chooses important sentences from the text to form a summary whereas abstractive summarization paraphrase using advanced and nearer-to human explanation by adding novel words or phrases. For a human annotator, producing summary of a document is time consuming and expensive because it requires going through the long document and composing a short summary. An automatic feature-rich model for text summarization is proposed that can reduce the amount of labor and produce a quick summary by using both extractive and abstractive approach. A feature-rich extractor highlights the important sentences in the text and linguistic characteristics are used to enhance results. The extracted summary is then fed to an abstracter to further provide information using features such as named entity tags, part of speech tags and term weights. Furthermore, a loss function is introduced to normalize the inconsistency between word-level and sentence-level attentions. The proposed two-staged network achieved a ROUGE score of 37.76% on the benchmark CNN/DailyMail dataset, outperforming the earlier work. Human evaluation is also conducted to measure the comprehensiveness, conciseness and informativeness of the generated summary.","2169-3536","","10.1109/ACCESS.2023.3249783","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054384","Abstractive summarization;encoder-decoder;extractive summarization;feature rich model;linguistic features;summarization evaluation","Feature extraction;Linguistics;Computational modeling;Encoding;Mathematical models;Computer science;Machine translation","feature extraction;natural language processing;text analysis","abstracter;abstractive approach;abstractive summarization paraphrase;abstractive text summarization;automatic feature-rich model;brief concise summary;conciseness;extracted summary;extractive approach;extractive summarization;feature-rich extractor highlights;generated summary;human annotator;human evaluation;human explanation;important sentences;linguistic characteristics;linguistic feature space;long document;neural attention model;quick summary;sentence-level attentions;short summary;source text;word-level","","1","","38","CCBYNCND","27 Feb 2023","","","IEEE","IEEE Journals"
"A Tibetan Sentence Boundary Disambiguation Model Considering the Components on Information on Both Sides of Shad","F. Li; H. Lv; Y. Gao; Dolha; Y. Li; Q. Zhou","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Key Laboratory of China's National Linguistic Information Technology, Northwest Minzu University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China","Tsinghua Science and Technology","28 Jul 2023","2023","28","6","1085","1100","Sentence Boundary Disambiguation (SBD) is a preprocessing step for natural language processing. Segmenting text into sentences is essential for Deep Learning (DL) and pretraining language models. Tibetan punctuation marks may involve ambiguity about the sentences' beginnings and endings. Hence, the ambiguous punctuation marks must be distinguished, and the sentence structure must be correctly encoded in language models. This study proposed a component-level Tibetan SBD approach based on the DL model. The models can reduce the error amplification caused by word segmentation and part-of-speech tagging. Although most SBD methods have only considered text on the left side of punctuation marks, this study considers the text on both sides. In this study, 465 669 Tibetan sentences are adopted, and a Bidirectional Long Short-Term Memory (Bi-LSTM) model is used to perform SBD. The experimental results show that the F1-score of the Bi-LSTM model reached 96%, the most efficient among the six models. Experiments are performed on low-resource languages such as Turkish and Romanian, and high-resource languages such as English and German, to verify the models' generalization.","1007-0214","","10.26599/TST.2022.9010055","National Key R&D Program of China(grant numbers:2020YFC0832500); Fundamental Research Funds for the Central Universities(grant numbers:lzujbky-2022-kb12,lzujbky-2021-sp43,lzujbky-2020-sp02,lzujbky-2019-kb51,lzujbky-2018-k12); National Natural Science Foundation of China(grant numbers:61402210); NVIDIA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197207","Sentence Boundary Disambiguation (SBD);punctuation marks;ambiguity;Bidirectional Long Short-Term Memory (Bi-LSTM) model","Deep learning;Text categorization;Tagging;Market research;Data models;Machine translation;Labeling","deep learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","465 669 Tibetan sentences;ambiguous punctuation marks;Bi-LSTM model;component-level Tibetan SBD approach;DL model;high-resource languages;language models;low-resource languages;natural language processing;preprocessing step;SBD methods;sentence structure;Tibetan punctuation marks;Tibetan sentence Boundary Disambiguation model considering","","","","53","","28 Jul 2023","","","TUP","TUP Journals"
"Custom Hardware Architectures for Deep Learning on Portable Devices: A Review","K. S. Zaman; M. B. I. Reaz; S. H. Md Ali; A. A. A. Bakar; M. E. H. Chowdhury","Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical Engineering, Qatar University, Doha, Qatar","IEEE Transactions on Neural Networks and Learning Systems","27 Oct 2022","2022","33","11","6068","6088","The staggering innovations and emergence of numerous deep learning (DL) applications have forced researchers to reconsider hardware architecture to accommodate fast and efficient application-specific computations. Applications, such as object detection, image recognition, speech translation, as well as music synthesis and image generation, can be performed with high accuracy at the expense of substantial computational resources using DL. Furthermore, the desire to adopt Industry 4.0 and smart technologies within the Internet of Things infrastructure has initiated several studies to enable on-chip DL capabilities for resource-constrained devices. Specialized DL processors reduce dependence on cloud servers, improve privacy, lessen latency, and mitigate bandwidth congestion. As we reach the limits of shrinking transistors, researchers are exploring various application-specific hardware architectures to meet the performance and efficiency requirements for DL tasks. Over the past few years, several software optimizations and hardware innovations have been proposed to efficiently perform these computations. In this article, we review several DL accelerators, as well as technologies with emerging devices, to highlight their architectural features in application-specific integrated circuit (IC) and field-programmable gate array (FPGA) platforms. Finally, the design considerations for DL hardware in portable applications have been discussed, along with some deductions about the future trends and potential research directions to innovate DL accelerator architectures further. By compiling this review, we expect to help aspiring researchers widen their knowledge in custom hardware architectures for DL.","2162-2388","","10.1109/TNNLS.2021.3082304","Research University Grant, Universiti Kebangsaan Malaysia(grant numbers:DPK-2021-001,DIP-2020-004,MI-2020-002); Qatar National Research Foundation (QNRF)(grant numbers:NPRP12s-0227-190164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447019","Application-specific integrated circuit (ASIC);deep learning (DL);deep neural network (DNN);energy-efficient architectures;field-programmable gate array (FPGA);hardware accelerator;machine learning (ML);neural network hardware;review","Computer architecture;Hardware;Computational modeling;Artificial neural networks;Optimization;Memory management;Convolution","application specific integrated circuits;field programmable gate arrays;image recognition;object detection","application-specific hardware architectures;architectural features;aspiring researchers;custom hardware architectures;DL hardware;efficiency requirements;efficient application-specific computations;emerging devices;fast application-specific computations;field-programmable gate array;hardware architecture;hardware innovations;image generation;image recognition;innovate DL accelerator architectures;mitigate bandwidth congestion;music synthesis;numerous deep learning applications;object detection;portable applications;portable devices;potential research directions;resource-constrained devices;smart technologies;software optimizations;speech translation;staggering innovations;substantial computational resources","Neural Networks, Computer;Deep Learning;Computers;Software","14","","146","IEEE","4 Jun 2021","","","IEEE","IEEE Journals"
"Estimating the Joint Statistics of Images Using Nonparametric Windows with Application to Registration Using Mutual Information","N. Dowson; T. Kadir; R. Bowden","Siemens Molecular Imaging, Oxford, UK; Siemens Molecular Imaging, Oxford, UK; Centre for Vision Speech and Signal Processing, University of Surrey, Guildford, UK","IEEE Transactions on Pattern Analysis and Machine Intelligence","19 Aug 2008","2008","30","10","1841","1857","Recently, the Non-Parametric (NP) Windows has been proposed to estimate the statistics of real 1D and 2D signals. NP Windows is accurate, because it is equivalent to sampling images at a high (infinite) resolution for an assumed interpolation model. This paper extends the proposed approach to consider joint distributions of image-pairs. Secondly, Green's Theorem is used to simplify the previous NP Windows algorithm. Finally, a resolution aware NP Windows algorithm is proposed, to improve robustness to relative scaling between an image-pair. Comparative testing of 2D image registration was performed using translation-only and affine transformations. Although more expensive than other methods, NP Windows frequently demonstrated superior performance for bias (distance between ground truth and global maximum) and frequency of convergence. Unlike other methods, the number of samples and histogram bin-size has little effect on NP Windows, and the prior selection of a kernel is not required.","1939-3539","","10.1109/TPAMI.2007.70832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407723","Interpolation;Optimization;Distribution functions;Nonparametric statistics;Antialiasing;Image-based rendering;Image Processing and Computer Vision;Sampling;Signal processing;Interpolation;Optimization;Distribution functions;Nonparametric statistics;Antialiasing;Image-based rendering;Image Processing and Computer Vision;Sampling;Signal processing","Statistics;Mutual information;Image resolution;Signal resolution;Statistical distributions;Image sampling;Interpolation;Robustness;Testing;Image registration","affine transforms;Green's function methods;image registration;interpolation","joint statistics;nonparametric windows;image registration;mutual information;interpolation model;Green's theorem;affine transformations","Algorithms;Artificial Intelligence;Computer Simulation;Data Interpretation, Statistical;Image Enhancement;Image Interpretation, Computer-Assisted;Models, Theoretical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique","23","","30","IEEE","19 Aug 2008","","","IEEE","IEEE Journals"
"Decoding Music in the Human Brain Using EEG Data","C. Foster; D. Dharmaretnam; H. Xu; A. Fyshe; G. Tzanetakis","Department of Computer Science, University of Victoria, Victoria, Canada; Department of Computer Science, University of Victoria, Victoria, Canada; Department of Computer Science, University of Victoria, Victoria, Canada; Department of Computer Science, University of Victoria, Victoria, Canada; Department of Computer Science, University of Victoria, Victoria, Canada","2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)","29 Nov 2018","2018","","","1","6","Semantic vectors, or language embeddings, are used in computational linguistics to represent language for a variety of machine related tasks including translation, speech to text, and natural language understanding. These semantic vectors have also been extensively studied in correlation with human brain data, showing evidence that the representation of language in the human brain can be modeled through these vectors with high correlation. Further, various attempts have been made to study how the human brain represents and understands music. For example, it has been shown that EEG data of subjects listening to music can be used for tempo detection and singer gender recognition. We propose studying the relationship between the EEG data of subjects listening to audio and the audio feature vectors modeled after the semantic vectors in computational linguistics. This could provide new insight into how the brain processes and understands music.","2473-3628","978-1-5386-6070-6","10.1109/MMSP.2018.8547051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8547051","","Electroencephalography;Brain modeling;Correlation;Predictive models;Semantics;Feature extraction;Data models","audio signal processing;brain;computational linguistics;electroencephalography;feature extraction;language translation;medical signal processing;music;natural language processing;vectors","language embeddings;computational linguistics;natural language understanding;semantic vectors;human brain data;high correlation;music;EEG data;audio feature vectors;machine related tasks","","6","","20","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks","K. Cho; A. Courville; Y. Bengio","Universit√© de Montr√©al, Information and Operational Research, Montr√©al, Canada; Universit√© de Montr√©al, Information and Operational Research, Montr√©al, Canada; Universit√© de Montr√©al, Information and Operational Research, Montr√©al, Canada","IEEE Transactions on Multimedia","26 Oct 2015","2015","17","11","1875","1886","Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","1941-0077","","10.1109/TMM.2015.2477044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243334","Attention mechanism;deep learning;recurrent neural networks","Context;Decoding;Context modeling;Computational modeling;Recurrent neural networks;Mathematical model","learning (artificial intelligence);multimedia systems;recurrent neural nets","multimedia content;attention-based encoder-decoder networks;deep neural networks;machine translation;image caption generation;video clip description;speech recognition;gated recurrent neural networks;convolutional neural networks","","247","4","75","IEEE","4 Sep 2015","","","IEEE","IEEE Journals"
"Evaluating Sequence-to-Sequence Models for Handwritten Text Recognition","J. Michael; R. Labahn; T. Gr√ºning; J. Z√∂llner","Computational Intelligence Technology Lab, University of Rostock, Rostock, Germany; Computational Intelligence Technology Lab, University of Rostock, Rostock, Germany; PLANET artificial intelligence GmbH, Rostock, Germany; PLANET artificial intelligence GmbH, Rostock, Germany","2019 International Conference on Document Analysis and Recognition (ICDAR)","3 Feb 2020","2019","","","1286","1293","Encoder-decoder models have become an effective approach for sequence learning tasks like machine translation, image captioning and speech recognition, but have yet to show competitive results for handwritten text recognition. To this end, we propose an attention-based sequence-to-sequence model. It combines a convolutional neural network as a generic feature extractor with a recurrent neural network to encode both the visual information, as well as the temporal context between characters in the input image, and uses a separate recurrent neural network to decode the actual character sequence. We make experimental comparisons between various attention mechanisms and positional encodings, in order to find an appropriate alignment between the input and output sequence. The model can be trained end-to-end and the optional integration of a hybrid loss allows the encoder to retain an interpretable and usable output, if desired. We achieve competitive results on the IAM and ICFHR2016 READ data sets compared to the state-of-the-art without the use of a language model, and we significantly improve over any recent sequence-to-sequence approaches.","2379-2140","978-1-7281-3014-9","10.1109/ICDAR.2019.00208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978104","sequence-to-sequence;Seq2Seq;encoder-decoder;attention;handwritten text recognition;HTR","Decoding;Feature extraction;Task analysis;Text recognition;Computer architecture;Computational modeling;Visualization","convolutional neural nets;feature extraction;handwritten character recognition;learning (artificial intelligence);text analysis","convolutional neural network;feature extractor;recurrent neural network;character sequence;attention mechanisms;language model;handwritten text recognition;encoder-decoder models;machine translation;image captioning;speech recognition;attention-based sequence-to-sequence model;visual information","","59","","42","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"HolyLight: A Nanophotonic Accelerator for Deep Learning in Data Centers","W. Liu; W. Liu; Y. Ye; Q. Lou; Y. Xie; L. Jiang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Computer Science, Chongqing University, Chongqing, China; School of Electronics and Information Engineering, Southwest University, Chongqing, China; School of Informatics, Computing and Engineering, Indiana University, Bloomington, USA; School of Electronics and Information Engineering, Southwest University, Chongqing, China; School of Informatics, Computing and Engineering, Indiana University, Bloomington, USA","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)","16 May 2019","2019","","","1483","1488","Convolutional Neural Networks (CNNs) are widely adopted in object recognition, speech processing and machine translation, due to their extremely high inference accuracy. However, it is challenging to compute massive computationally expensive convolutions of deep CNNs on traditional CPUs and GPUs. Emerging Nanophotonic technology has been employed for on-chip data communication, because of its CMOS compatibility, high bandwidth and low power consumption. In this paper, we propose a nanophotonic accelerator, HolyLight, to boost the CNN inference throughput in datacenters. Instead of an all-photonic design, HolyLight performs convolutions by photonic integrated circuits, and process the other operations in CNNs by CMOS circuits for high inference accuracy. We first build HolyLight-M by microdisk-based matrix-vector multipliers. We find analog-to-digital converters (ADCs) seriously limit its inference throughput per Watt. We further use microdisk-based adders and shifters to architect HolyLight-A without ADCs. Compared to the state-of-the-art ReRAM-based accelerator, HolyLight-A improves the CNN inference throughput per Watt by 13√ó with trivial accuracy degradation.","1558-1101","978-3-9819263-2-3","10.23919/DATE.2019.8715195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715195","Nanophotonic Computing;Convolutional Neural Network;Accelerator","Photonics;Adders;Optical resonators;Convolutional codes;Data centers;Throughput;Optical switches","adders;analogue-digital conversion;CMOS integrated circuits;computer centres;convolutional neural nets;integrated circuit design;integrated optoelectronics;learning (artificial intelligence);nanophotonics;neural chips","nanophotonic accelerator;deep learning;data centers;object recognition;speech processing;machine translation;on-chip data communication;CMOS compatibility;low power consumption;CNN inference throughput;all-photonic design;photonic integrated circuits;CMOS circuits;HolyLight-M;microdisk-based matrix-vector multipliers;analog-to-digital converters;convolutional neural networks;deep CNN;traditional CPU;nanophotonic technology;microdisk-based adders;HolyLight-A","","30","","31","","16 May 2019","","","IEEE","IEEE Conferences"
"Practical Attacks on Deep Neural Networks by Memory Trojaning","X. Hu; Y. Zhao; L. Deng; L. Liang; P. Zuo; J. Ye; Y. Lin; Y. Xie","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20 May 2021","2021","40","6","1230","1243","Deep neural network (DNN) accelerators are widely deployed in computer vision, speech recognition, and machine translation applications, in which attacks on DNNs have become a growing concern. This article focuses on exploring the implications of hardware Trojan attacks on DNNs. Trojans are one of the most challenging threat models in hardware security where adversaries insert malicious modifications to the original integrated circuits (ICs), leading to malfunction once being triggered. Such attacks can be conducted by adversaries because modern ICs commonly include third-party intellectual property (IP) blocks. Previous studies design hardware Trojans to attack DNNs with the assumption that adversaries have full knowledge or manipulation of the DNN systems' victim model and toolchain in addition to the hardware platforms, yet such a threat model is strict, limiting their practical adoption. In this article, we propose a memory Trojan methodology that implants the malicious logics merely into the memory controllers of DNN systems without the necessity of toolchain manipulation or accessing to the victim model and thus is feasible for practical uses. Specifically, we locate the input image data among the massive volume of memory traffics based on memory access patterns and propose a Trojan trigger mechanism based on detecting the geometric feature in input images. Extensive experiments show that the proposed trigger mechanism is effective even in the presence of environmental noises and preprocessing operations. Furthermore, we design and implement the payload and verify that the proposed Trojan technique can effectively conduct both untargeted and targeted attacks on DNNs.","1937-4151","","10.1109/TCAD.2020.2995347","National Science Foundation(grant numbers:1725447,1730309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096397","Convolutional neural networks (CNNs);deep learning accelerator;deep learning attack;hardware Trojan","Trojan horses;Hardware;Integrated circuit modeling;Computational modeling;Security;Payloads","deep learning (artificial intelligence);security of data","memory access patterns;Trojan trigger mechanism;untargeted attacks;practical attacks;memory trojaning;deep neural network accelerators;computer vision;speech recognition;machine translation applications;hardware Trojan attacks;hardware security;adversaries insert malicious modifications;original integrated circuits;modern ICs;third-party intellectual property blocks;DNN systems;hardware platforms;threat model;practical adoption;malicious logics;memory controllers;toolchain manipulation;victim model;practical uses;input image data;memory traffics;memory Trojan;geometric feature detection","","11","","74","IEEE","19 May 2020","","","IEEE","IEEE Journals"
"Long Short-Term Memory for Radio Frequency Spectral Prediction and its Real-Time FPGA Implementation","Siddhartha; Y. H. Lee; D. J. M. Moss; J. Faraone; P. Blackmore; D. Salmond; D. Boland; P. H. W. Leong","The University of Sydney, Australia; The University of Sydney, Australia; The University of Sydney, Australia; The University of Sydney, Australia; Defence Science and Technology Group, Canberra, Australia; Defence Science and Technology Group, Edinburgh, Australia; The University of Sydney, Australia; The University of Sydney, Australia","MILCOM 2018 - 2018 IEEE Military Communications Conference (MILCOM)","3 Jan 2019","2018","","","1","9","Reactive communication waveforms hosted in current generation tactical radios often fail to achieve good performance and resilience in highly dynamic and complex environments. Arguably, novel waveforms that can proactively adapt to anticipated channel conditions may better meet the challenges of the tactical environment. This motivates the ability to accurately predict spectral behaviour in real-time. A Long Short- Term Memory (LSTM) network is a type of recurrent neural network which has been extremely successful in dealing with time-dependent signal processing problems such as speech recognition and machine translation. In this paper, we apply it to the task of spectral prediction and present a module generator for a latency-optimised Field-Programmable Gate Array (FPGA) implementation. We show that our implementation obtains superior results to other time series prediction techniques including a na√Øve predictor, moving average and ARIMA for the problem of radio frequency spectral prediction. For a single LSTM layer plus a fully- connected output layer with 32 inputs and 32 outputs, we demonstrate that a prediction latency of 4.3 Œº s on a Xilinx XC7K410T Kintex-7 FPGA is achievable.","2155-7586","978-1-5386-7185-6","10.1109/MILCOM.2018.8599833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599833","","Field programmable gate arrays;Radio frequency;Real-time systems;Predictive models;Training;Standards;Big Data","field programmable gate arrays;military communication;moving average processes;recurrent neural nets;signal processing;telecommunication computing;time series;wireless channels","radio frequency spectral prediction;real-time FPGA implementation;reactive communication waveforms;anticipated channel conditions;tactical environment;recurrent neural network;time-dependent signal processing problems;speech recognition;machine translation;module generator;time series prediction techniques;single LSTM layer;Xilinx XC7K410T Kintex-7 FPGA;ARIMA;moving average;na√Øve predictor;latency-optimised field-programmable gate array;spectral behaviour prediction;generation tactical radios;long short-term memory network","","4","","19","Crown","3 Jan 2019","","","IEEE","IEEE Conferences"
"A Survey on Natural Language Processing and its Applications","P. J. Beslin Pajila; K. Sudha; D. M. Kalai Selvi; V. N. Kumar; S. Gayathri; R. S. Subramanian","Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology; Dept of CSBS, R.M.D Engineering College; Dept of CSE, R.M.D Engineering College; Dept of ECE, Jawahar Engineering College; Dept of CSE, S A Engineering College; Dept of CSE, R.M.K.College of Engg and Tech","2023 4th International Conference on Electronics and Sustainable Communication Systems (ICESC)","1 Aug 2023","2023","","","996","1001","Advancements in technologies and the internet help to pave the way to collect and store unstructured data which includes voice, etc. The use of unstructured data in an efficient manner helps to understand the insight of the data and make a decision based on the data. To analyse the unstructured data, natural language processing (NLP) is applied. NLP referred to a subcategory of AI that mainly focus on computers understanding, and interpreting human being language. NLP is developed by combing different fields like linguistics, and computer science which helps the computer to perform as a human does in process in speech and text data. NLP is mostly applied in the application areas like content categorization, contextual extraction, sentiment analysis, document summarization, Machine translation, etc. The objective of the research is to perform an in-depth study about the NLP, its methodology, and applications area with pros and cons. Further, this survey helps new researchers to easily understand the underlying concepts of NLP and its terms.","","979-8-3503-0009-3","10.1109/ICESC57686.2023.10193469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193469","Natural Language Processing;Natural Language Components;Artificial Intelligence;Text;Data","Surveys;Computers;Computer science;Sentiment analysis;Text analysis;Communication systems;Linguistics","Internet;natural language processing;text analysis","applications area;computer science;content categorization;contextual extraction;document summarization;human being language interpretation;Internet;linguistics;machine translation;natural language processing;NLP;sentiment analysis;speech data;text data;unstructured data","","","","15","IEEE","1 Aug 2023","","","IEEE","IEEE Conferences"
"An Ambiguity Discovery Algorithm on Chinese Word Segmentation Based Dictionary","T. Sun; Y. Liu; L. Yang; Z. Li; Z. Liu","Department of Computer Science, School of Computer, Northeast Normal University, Changchun, China; Department of Computer Science, School of Computer, Northeast Normal University, Changchun, China; Department of Computer Science, School of Computer, Northeast Normal University, Changchun, China; Department of Computer Science, School of Computer, Northeast Normal University, Changchun, China; Department of Computer Science, School of Computer, Northeast Normal University, Changchun, China","2009 Second Pacific-Asia Conference on Web Mining and Web-based Application","4 Sep 2009","2009","","","39","42","Chinese word segmentation is a basic research issue on Chinese information processing tasks such as information extraction; information retrieval; machine translation; text classification; automatic text summarization; speech recognition, natural language understanding and So on. And the ambiguity problem is one of the most difficult problems in Chinese word segmentation .The paper introduced several normal ambiguity discovery algorithm, and proposed a new method of ambiguity discovery that can discover many crossing ambiguity fields and covering ambiguity fields at the same time.","","978-0-7695-3646-0","10.1109/WMWA.2009.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5232462","Chinese word segmentation;Chinese information processing;crossing ambiguity;covering ambiguity;ambiguity discovery algorithm","Dictionaries;Natural languages;Information processing;Data mining;Information retrieval;Vocabulary;Computer science;Electronic mail;Text categorization;Speech recognition","dictionaries;natural language processing;word processing","Chinese word segmentation;ambiguity discovery algorithm;Chinese information processing;dictionary","","2","","20","IEEE","4 Sep 2009","","","IEEE","IEEE Conferences"
"Affix-augmented stem-based language model for persian","H. Faili; H. Ravanbakhsh","Department ECE, University of Tehran, Tehran, Iran; Department ECE, University of Tehran, Tehran, Iran","Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)","30 Sep 2010","2010","","","1","4","Language modeling is used in many NLP applications like machine translation, POS tagging, speech recognition and information retrieval. It assigns a probability to a sequence of words. This task becomes a challenging problem for high inflectional languages. In this paper we investigate standard statistical language models on the Persian as an inflectional language. We propose two variations of morphological language models that rely on a morphological analyzer to manipulate the dataset before modeling. Then we discuss shortcoming of these models, and introduce a novel approach that exploits the structure of the language and produces more accurate. Experimental results are encouraging especially when we use n-gram models with small training dataset.","","978-1-4244-6899-7","10.1109/NLPKE.2010.5587823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5587823","Tracking;language model;n-gram;morphological;Persian","Training;Computational modeling;Probability;Mathematical model;Data models;Vocabulary;Speech recognition","natural language processing;statistical analysis","affix-augmented stem-based language;Persian language;natural language processing;language modeling;statistical language models;inflectional language;morphological language models;morphological analyzer","","1","","9","IEEE","30 Sep 2010","","","IEEE","IEEE Conferences"
"A Survey of Current Datasets for Code-Switching Research","N. Jose; B. R. Chakravarthi; S. Suryawanshi; E. Sherly; J. P. McCrae","Machine Intelligence, Indian Institute of Information Technology and Management-Kerala, Trivandrum, India; Data Science Institute, National University of Ireland, Galway, Ireland; Data Science Institute, National University of Ireland, Galway, Ireland; Machine Intelligence, Indian Institute of Information Technology and Management-Kerala, Trivandrum, India; Data Science Institute, National University of Ireland, Galway, Ireland","2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)","23 Apr 2020","2020","","","136","141","Code switching is a prevalent phenomenon in the multilingual community and social media interaction. In the past ten years, we have witnessed an explosion of code switched data in the social media that brings together languages from low resourced languages to high resourced languages in the same text, sometimes written in a non-native script. This increases the demand for processing code-switched data to assist users in various natural language processing tasks such as part-of-speech tagging, named entity recognition, sentiment analysis, conversational systems, and machine translation, etc. The available corpora for code switching research played a major role in advancing this area of research. In this paper, we propose a set of quality metrics to evaluate the dataset and categorize them accordingly.","2575-7288","978-1-7281-5197-7","10.1109/ICACCS48705.2020.9074205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074205","code switching;natural language processing;dataset","Switches;Task analysis;Social network services;Natural language processing;Vocabulary;Tagging;Measurement","natural language processing;social networking (online)","low resourced languages;high resourced languages;nonnative script;natural language processing tasks;multilingual community;social media interaction;code-switched data processing","","27","","51","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"Building high-level features using large scale unsupervised learning","Q. V. Le","Google, Inc., USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8595","8598","We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200√ó200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639343","","Face;Neurons;Detectors;Training;Accuracy;Buildings;Visualization","face recognition;image coding;object detection;unsupervised learning","unsupervised learning;high-level class-specific feature detectors;face detector;deep sparse autoencoder;model parallelism;asynchronous SGD;cat faces;human bodies;ImageNet;picture size 200 pixel","","621","5","31","IEEE","21 Oct 2013","","","IEEE","IEEE Conferences"
"Being Low-Rank in the Time-Frequency Plane","V. Emiya; R. Hamon; C. Chaux","CNRS, Universit√© de Toulon, Marseille, France; CNRS, Universit√© de Toulon, Marseille, France; CNRS, Aix Marseille Univ, Marseille, France","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","4659","4663","When using optimization methods with matrix variables in signal processing and machine learning, it is customary to assume some low-rank prior on the targeted solution. Nonnegative matrix factorization of spectrograms is a case in point in audio signal processing. However, this low-rank prior is not straightforwardly related to complex matrices obtained from a short-time Fourier - or discrete Gabor - transform (STFT), which is generally defined from and studied based on a modulation operator and a translation operator applied to a so-called window. This paper is a first study of the low-rankness property of time-frequency matrices. We characterize the set of signals with a rank- r (complex) STFT matrix in the case of a unit hop size and frequency step with few assumptions on the transform parameters. We discuss the scope of this result and its implications on low-rank approximations of STFT matrices.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462423","Short-Time Fourier Transform;low-rankness;approximation","Time-frequency analysis;Spectrogram;Discrete Fourier transforms;Frequency modulation","audio signal processing;Fourier transforms;matrix decomposition;optimisation;spectral analysis;time-frequency analysis","Gabor transform;short-time Fourier transform matrix;signal processing;nonnegative matrix factorization;low-rank approximations;time-frequency matrices;low-rankness property;complex matrices;matrix variables;optimization methods;time-frequency plane","","2","","10","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Improving Audio Captioning Using Semantic Similarity Metrics","R. Mahfuz; Y. Guo; E. Visser","Qualcomm Technologies, Inc.; Qualcomm Technologies, Inc.; Qualcomm Technologies, Inc.","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Audio captioning quality metrics which are typically borrowed from the machine translation and image captioning areas measure the degree of overlap between predicted tokens and gold reference tokens. In this work, we consider a metric measuring semantic similarities between predicted and reference captions instead of measuring exact word overlap. We first evaluate its ability to capture similarities among captions corresponding to the same audio file and compare it to other established metrics. We then propose a fine-tuning method to directly optimize the metric by backpropagating through a sentence embedding extractor and audio captioning network. Such fine-tuning results in an improvement in predicted captions as measured by both traditional metrics and the proposed semantic similarity captioning metric.","","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10096522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096522","audio captioning;semantic similarity;sentence embedding","Measurement;Gold;Computational modeling;Semantics;Area measurement;Signal processing;Acoustics","","","","","","21","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Knowledge Weightage Calculation: an AI & ML based smart modelling for text- content summarization and quantification","A. Kumar; D. R. Rizvi","Dept. of Computer Engineering, Faculty of Engineering, Jamia Millia Islamia, New Delhi, India; Dept. of Computer Engineering, Faculty of Engineering, Jamia Millia Islamia, New Delhi, India","2023 1st International Conference on Intelligent Computing and Research Trends (ICRT)","12 Jun 2023","2023","","","1","7","The need for naturalistic interaction and communication between humans and machines is essential for high-level artificial intelligence. Matured Natural Language Processing is the first step towards attaining artificial intelligence by automation equal to human beings. It has been a hot and active area of research among researchers for a long. In the advanced forms of natural language processing-based intelligent application building must-have capabilities of generating summaries, understanding context, representing and combining multiple information, and knowledge extraction. Artificial Intelligence and various computing ways (Statistical Reasoning, Machine Learning, and Deep Learning) for achieving it, have been proven the next big weapon for attaining a true sense of intelligent automation and smart system building. Many tasks related to natural language processing have shown consistent and comparable high performances since the introduction of deep learning modelling. Using sequence-to-sequence models has been advantageous and demonstrated promising outcomes in many other areas, including machine translation, speech recognition, picture captioning, etc. This paper aims to propose a novel framework to extract important features from an unstructured text by removing the noisy, irrelevant, and redundant components, and to provide accountability for the information delivered by retrieving image-based text, deep text summarization by knowledge weight calculation, and representation- based on the extracted features.","","979-8-3503-3677-1","10.1109/ICRT57042.2023.10146669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146669","nlp;deep text summarization;deep learning;knowledge weight calculation;natural language processing","Deep learning;Intelligent automation;Weapons;Speech recognition;Feature extraction;Market research;Data mining","deep learning (artificial intelligence);feature extraction;image retrieval;information retrieval;natural language processing;text analysis","AI and ML;deep learning;deep learning modelling;deep text summarization;high-level artificial intelligence;image-based text;intelligent automation;knowledge extraction;knowledge weight calculation;machine learning;matured natural language processing;natural language processing-based intelligent application;sequence-to-sequence models;smart modelling;smart system building;statistical reasoning;text- content summarization","","","","16","IEEE","12 Jun 2023","","","IEEE","IEEE Conferences"
"DropDim: A Regularization Method for Transformer Networks","H. Zhang; D. Qu; K. Shao; X. Yang","School of the Information Engineering University, Zhengzhou, China; School of the Information Engineering University, Zhengzhou, China; Jiangnan Institute of Computing Technology, Wuxi, China; School of the Information Engineering University, Zhengzhou, China","IEEE Signal Processing Letters","1 Feb 2022","2022","29","","474","478","We introduce DropDim, a structured dropout method designed for regularizing the self-attention mechanism, which is a key component of the transformer. In contrast to the general dropout method, which randomly drops neurons, DropDim drops part of the embedding dimensions. In this way, the semantic information can be completely discarded. Thus, the excessive co-adapting between different embedding dimensions can be broken, and the self-attention is forced to encode meaningful features with a certain number of embedding dimensions erased. Experiments on a wide range of tasks executed on the MUST-C English-Germany dataset show that DropDim can effectively improve model performance, reduce over-fitting, and show complementary effects with other regularization methods. When combined with label smoothing, the WER can be reduced from 19.1% to 15.1% on the ASR task, and the BLEU value can be increased from 26.90 to 28.38 on the MT task. On the ST task, the model can reach a BLEU score of 22.99, an increase by 1.86 BLEU points compared to the strong baseline.","1558-2361","","10.1109/LSP.2022.3140693","National Natural Science Foundation of China(grant numbers:61673395,62171470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670702","End-to-end;transformer;regularization;dropout","Task analysis;Transformers;Smoothing methods;Semantics;Training;Neurons;Decoding","language translation;natural language processing","regularization method;transformer networks;DropDim;structured dropout method;self-attention mechanism;general dropout method;semantic information;embedding dimensions","","3","","33","IEEE","5 Jan 2022","","","IEEE","IEEE Journals"
"Implementing Spatio-Temporal Graph Convolutional Networks on Graphcore IPUs","J. Moe; K. Pogorelov; D. T. Schroeder; J. Langguth","Simula Research Laboratory, University of Oslo, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, University of Bergen, Oslo, Norway","2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","1 Aug 2022","2022","","","45","54","Artificial neural networks have been used for a multitude of regression tasks, and their descendants have expanded the domain to many applications such as image and speech recognition, filtering of social networks, and machine translation. While conventional and recurrent neural networks work well on data represented in Euclidean space, they struggle with data in non-Euclidean space. Graph Neural Networks (GNN) expand recurrent neural networks to directly process sparse representations of graphs, but they are computationally expensive, which invites the use of powerful hardware accelerators. In this paper, we investigate the viability of the Graphcore Intelligence Processing Unit (IPU) for efficient implementation of Spatio-Temporal Graph Convolutional Networks. The results show that IPUs are well suited for this task.","","978-1-6654-9747-3","10.1109/IPDPSW55747.2022.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835385","Graph Neural Networks;STGCN;IPU;AI accelerator;Performance","Recurrent neural networks;Tensors;Software algorithms;Graphics processing units;Computer architecture;Speech recognition;Software","artificial intelligence;graph theory;recurrent neural nets","spatio-temporal graph convolutional networks;graphcore IPU;artificial neural networks;regression tasks;speech recognition;social networks;recurrent neural networks;Euclidean space;nonEuclidean space;graph neural networks;GNN;sparse representations;graphcore intelligence processing unit","","","","25","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"Deep Stock Representation Learning: From Candlestick Charts to Investment Decisions","G. Hu; Y. Hu; K. Yang; Z. Yu; F. Sung; Z. Zhang; F. Xie; J. Liu; N. Robertson; T. Hospedales; Q. Miemie","Queen's University, Belfast; Shanghai University of Finance and Economics; University of Shanghai for Science and Technology; Xiamen University; Independent Researcher; Xiamen University; Shanghai University of Finance and Economics; Shanghai University of Finance and Economics; Queen's University, Belfast; The University of Edinburgh; The University of Edinburgh","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","2706","2710","We propose a novel investment decision strategy (IDS) based on deep learning. The performance of many IDSs is affected by stock similarity. Most existing stock similarity measurements have the problems: (a) The linear nature of many measurements cannot capture nonlinear stock dynamics; (b) The estimation of many similarity metrics (e.g. covariance) needs very long period historic data (e.g. 3K days) which cannot represent current market effectively; (c) They cannot capture translation-invariance. To solve these problems, we apply Convolutional AutoEncoder to learn a stock representation, based on which we propose a novel portfolio construction strategy by: (i) using the deeply learned representation and modularity optimisation to cluster stocks and identify diverse sectors, (ii) picking stocks within each cluster according to their Sharpe ratio (Sharpe 1994). Overall this strategy provides low-risk high-return portfolios. We use the Financial Times Stock Exchange 100 Index (FTSE 100) data for evaluation. Results show our portfolio outperforms FTSE 100 index and many well known funds in terms of total return in 2000 trading days.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462215","","Portfolios;Investment;Visualization;Time series analysis;Clustering methods;Machine learning;Optimization","convolution;decision making;feedforward neural nets;investment;learning (artificial intelligence);optimisation;pattern clustering;stock markets","candlestick charts;investment decisions;convolutional autoencoder;IDS;FTSE 100 index;financial times stock exchange 100 index data;portfolio construction strategy;stock similarity measurements;deep stock representation learning;low-risk high-return portfolios;cluster stocks;modularity optimisation;current market;nonlinear stock dynamics;linear nature;deep learning","","15","1","21","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"An Analysis of Myanmar Inflectional Morphology Using Finite-state Method","T. M. Latt; A. Thida","Faculty of Computer Science, University of Computer Studies, Mandalay, Myanmar; Faculty of Computer Science, University of Computer Studies, Mandalay, Myanmar","2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS)","20 Sep 2018","2018","","","297","302","This Morphological analysis are essential steps in any Natural Language Processing (NLP) Application. Morphological analysis means taking a word as input and identifying their roots/stems and affixes. It can help in automatic spelling and grammar checking, machine translation, part of speech tagging, and parsing applications. Myanmar language is morphologically rich and it is an inflected language because Myanmar words can be combined to create a new word. This paper presents an analysis of inflection for morphology of Myanmar words. Finite State Automaton (FSA) is used to model for morphological analysis which are based on a set of rules and a monolingual lexicon that contains root words or stem words. It uses the inflectional morphemes to indicate if a word is singular or plural on noun whether it is singular or plural on pronoun, besides it is a positive or comparative or superlative form on adjective and present tense or past tense or future tense on verb.","","978-1-5386-5892-5","10.1109/ICIS.2018.8466526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466526","Morphological analysis;Finite-state automaton;inflectional Morphology","Morphology;Automata;Grammar;Computer science;Analytical models;Cats","finite state machines;grammars;natural language processing","Myanmar words;root words;stem words;inflectional morphemes;Myanmar inflectional morphology;finite-state method;Myanmar language;finite state automaton;natural language processing application;monolingual lexicon","","","","10","IEEE","20 Sep 2018","","","IEEE","IEEE Conferences"
"Automated audio captioning with recurrent neural networks","K. Drossos; S. Adavanne; T. Virtanen","Tampere University of Technology, Tampere, Finland; Tampere University of Technology, Tampere, Finland; Tampere University of Technology, Tampere, Finland","2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","11 Dec 2017","2017","","","374","378","We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered.","1947-1629","978-1-5386-1632-1","10.1109/WASPAA.2017.8170058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170058","audio captioning;recurrent neural networks;RNN;gated recurrent unit;GRU;attention mechanism","Decoding;Libraries;Training;Measurement;Feature extraction;Recurrent neural networks","audio signal processing;decoding;pattern classification;recurrent neural nets;speech coding;word processing","automated audio captioning;recurrent neural networks;encoder-decoder scheme;alignment model;log mel-band energies;audio file;bi-directional gated recurrent unit;GRU;classification layer;image captioning fields;sound effects library;ProSound Effects;word prediction","","33","1","23","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"Attribute value-range detection in identification of paraphrase sentence pairs","S. Kumova; B. Karaoƒülan; T. Kƒ±≈üla","Izmir Ekonomi Universitesi, Izmir, Izmir, TR; Ege Universitesi, Izmir, Izmir, TR; Ege Universitesi, Izmir, Izmir, TR","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","1393","1396","Identification of paraphrase sentence pairs becomes increasingly prominent in natural language processing area (e.g plagiarism detection, summarization, machine translation). In this study, it is proposed to employ information gain measure in determining the value-ranges of the paraphrase classification features on the renown paraphrase corpus of Microsoft Research (MSRP). The classification performances of value-ranges that are determined by information gain measure and an alternative heuristic method are compared by the use of Bayes classifier. The results show that the proposed method performs better than the heuristic method.","","978-1-5090-1679-2","10.1109/SIU.2016.7496009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496009","paraphrase;paraphrase sentence pairs;information gain;features;Bayesian classification","Gain measurement;Bayes methods;Chebyshev approximation;Feature extraction;Conferences;Natural language processing;Plagiarism","Bayes methods;natural language processing;speech processing","paraphrase sentence pairs;natural language processing area;information gain measure;paraphrase corpus;Microsoft Research;MSRP;value-ranges;alternative heuristic method;Bayes classifier","","","","","IEEE","23 Jun 2016","","","IEEE","IEEE Conferences"
"Dzongkha Word Segmentation using Deep Learning","Y. Jamtsho; P. Muneesawang","Department of Electrical and Computer Engineering, Faculty of Engineering, Naresuan University, Phitsanulok, Thailand; Department of Electrical and Computer Engineering, Faculty of Engineering, Naresuan University, Phitsanulok, Thailand","2020 12th International Conference on Knowledge and Smart Technology (KST)","9 Apr 2020","2020","","","1","5","Natural Language Processing (NLP) has been applied to machine translation, chatbots, speech recognition, question and answer systems, document summarization and so on. The Dzongkha language of Bhutan, however, has not been considered in NLP systems, due, presumably, to the fact that the language is complex and written as a string of syllables without proper word boundaries. Thus, Dzongkha word segmentation is the essential first step in building the NLP applications. The novelty of our research is in applying Deep Learning to the task of Dzongkha word segmentation, avoiding the need for manual feature engineering. The segmentation problem is formulated as a syllable tagging task. We also incorporate the windows approach where the tag of a syllable depends on its surrounding syllables. Two sets of experiments were designed, with four models of varying context sizes in each set. We evaluated our models using the syllable-tagged-corpus prepared by Dzongkha Development Commission. The model with context size 2 achieved the highest F-score of 94.40% with 94.47% Precision and 94.35% Recall.","2374-314X","978-1-7281-4466-5","10.1109/KST48564.2020.9059451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9059451","Deep Learning;Deep Neural Network;syllable tagging;Dzongkha Word Segmentation;window approach;syllable embedding;Natural Language Processing","Natural language processing;Machine learning;Tagging;Context modeling;Task analysis;Feature extraction;Neural networks","learning (artificial intelligence);natural language processing;text analysis","Dzongkha word segmentation;deep learning;natural language processing;Dzongkha language;NLP systems;word boundaries;NLP applications;syllable tagging task;syllable-tagged-corpus;Dzongkha Development Commission","","1","","20","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Face Video Generation from a Single Image and Landmarks","K. Songsri-in; S. Zafeiriou","Department of Computing, Imperial College, London, UK; Department of Computing, Imperial College, London, UK","2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)","18 Jan 2021","2020","","","69","76","In this paper, we are concerned with the challenging problem of producing a full image sequence of a deformable face given only an image and generic facial motions encoded by a set of sparse landmarks. To this end, we build upon recent breakthroughs in image-to-image translation such as pix2pix, CycleGAN and StarGAN which learn Deep Convolutional Neural Networks (DCNNs) that learn to map aligned pairs of images between different domains (i.e., having different labels) and propose a new architecture which is not driven any more by labels but by spatial maps, facial landmarks. In particular, we propose the MotionGAN which transforms an input face image into a new one according to a heatmap of target landmarks. We show that it is possible to create very realistic face videos using a single image and a set of target landmarks. Furthermore, our method can be used to edit a facial image with arbitrary motions according to landmarks (e.g., expression, speech, etc.). This provides much more flexibility to face editing, expression transfer, facial video creation, etc. than models based on discrete expressions, audio or action units.","","978-1-7281-3079-8","10.1109/FG47880.2020.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320273","facial video generation;generative adversarial networks","Generators;Face recognition;Training;Generative adversarial networks;Faces;Computer architecture;Convolution","face recognition;image representation;image sequences;learning (artificial intelligence);neural nets","single image;image sequence;deformable face;generic facial motions;sparse landmarks;image-to-image translation;pix2pix;deep convolutional neural networks;spatial maps;facial landmarks;input face image;target landmarks;realistic face videos;facial image;facial video creation;face video generation","","2","","37","IEEE","18 Jan 2021","","","IEEE","IEEE Conferences"
"Clustering word roots syntactically","M. B. √ñzt√ºrk; B. Can","Hacettepe Universitesi, Ankara, Ankara, TR; Hacettepe Universitesi, Ankara, Ankara, TR","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","1461","1464","Distributional representation of words is used for both syntactic and semantic tasks. In this paper two different methods are presented for clustering word roots. In the first method, the distributional model word2vec [1] is used for clustering word roots, whereas distributional approaches are generally used for words. For this purpose, the distributional similarities of roots are modeled and the roots are divided into syntactic categories (noun, verb etc.). In the other method, two different models are proposed: an information theoretical model and a probabilistic model. With a metric [8] based on mutual information and with another metric based on Jensen-Shannon divergence, similarities of word roots are calculated and clustering is performed using these metrics. Clustering word roots has a significant role in other natural language processing applications such as machine translation and question answering, and in other applications that include language generation. We obtained a purity of 0.92 from the obtained clusters.","","978-1-5090-1679-2","10.1109/SIU.2016.7496026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496026","Turkish syntax;clustering;morphology","Mutual information;Syntactics;Measurement;Computational modeling;Knowledge discovery;Morphology;Speech","natural language processing;pattern clustering;word processing","word distributional representation;syntactic task;semantic task;word root clustering;distributional model word2vec;root distributional similarity;probabilistic model;information theoretical model;Jensen-Shannon divergence;natural language processing application;language generation","","","","","IEEE","23 Jun 2016","","","IEEE","IEEE Conferences"
"Model-Aided Wireless Artificial Intelligence: Embedding Expert Knowledge in Deep Neural Networks for Wireless System Optimization","A. Zappone; M. Di Renzo; M. Debbah; T. T. Lam; X. Qian","Research associate, CentraleSupelec, Gif-sur-Yvette, France; Laboratory of Signals and Systems, Paris-Saclay University‚ÄôCNRS, CentraleSupelec, France; Huawei France research and development center, Mathematical and Algorithmic Sciences Lab, Paris, France; Laboratory of Signals and Systems, Paris-Saclay University‚ÄôCNRS, CentraleSupelec, France; Laboratory of Signals and Systems, Paris-Saclay University, France","IEEE Vehicular Technology Magazine","19 Aug 2019","2019","14","3","60","69","Deep learning based on artificial neural networks (ANNs) is a powerful machine-learning method that, in recent years, has been successfully used to realize tasks such as image classification, speech recognition, and language translation, among others, that are usually simple for human beings but extremely difficult for machines. This is one reason deep learning is considered one of the main enablers for realizing artificial intelligence (AI). The current methodology in deep learning consists of employing a data-driven approach to identify the best architecture of an ANN that allows input-output data pairs to be fitted. Once the ANN is trained, it is capable of responding to never-observed inputs by providing the optimum output based on past acquired knowledge. In this context, a recent trend in the deep-learning community complements purely data-driven approaches with prior information based on expert knowledge. In this article, we describe two methods that implement this strategy to optimize wireless communication networks. In addition, we provide numerical results to assess the performance of the proposed approaches compared with purely data-driven implementations.","1556-6080","","10.1109/MVT.2019.2921627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765703","","Training data;Deep learning;Machine learning;Wireless communication;Mathematical model;Biological system modeling;Complexity theory","data analysis;expert systems;learning (artificial intelligence);neural net architecture;optimisation;telecommunication computing;wireless sensor networks","model-aided wireless artificial intelligence;deep neural networks;wireless system optimization;artificial neural networks;data-driven approach;wireless communication networks;expert knowledge;machine learning;deep learning;ANN architecture;ANN training","","110","","15","IEEE","18 Jul 2019","","","IEEE","IEEE Magazines"
"A deep learning approach to flight delay prediction","Y. J. Kim; S. Choi; S. Briceno; D. Mavris","Aerospace Systems Design Laboratory, Georgia Institute of Technology, Atlanta, Georgia; Aerospace Systems Design Laboratory, Georgia Institute of Technology, Atlanta, Georgia; Aerospace Systems Design Laboratory, Georgia Institute of Technology, Atlanta, Georgia; Aerospace Systems Design Laboratory, Georgia Institute of Technology, Atlanta, Georgia","2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC)","12 Dec 2016","2016","","","1","6","Deep learning has achieved significant improvement in various machine learning tasks including image recognition, speech recognition, machine translation and etc. Inspired by the huge success of the paradigm, there have been lots of tries to apply deep learning algorithms to data analytics problems with big data including traffic flow prediction. However, there has been no attempt to apply the deep learning algorithms to the analysis of air traffic data. This paper investigates the effectiveness of the deep learning models in the air traffic delay prediction tasks. By combining multiple models based on the deep learning paradigm, an accurate and robust prediction model has been built which enables an elaborate analysis of the patterns in air traffic delays. In particular, Recurrent Neural Networks (RNN) has shown its great accuracy in modeling sequential data. Day-to-day sequences of the departure and arrival flight delays of an individual airport have been modeled by the Long Short-Term Memory RNN architecture. It has been shown that the accuracy of RNN improves with deeper architectures. In this study, four different ways of building deep RNN architecture are also discussed. Finally, the accuracy of the proposed prediction model was measured, analyzed and compared with previous prediction methods. It shows best accuracy compared with all other methods.","2155-7209","978-1-5090-2523-7","10.1109/DASC.2016.7778092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778092","","Delays;Computer architecture;Atmospheric modeling;Computational modeling;Machine learning;Meteorology;Airports","air traffic;Big Data;data analysis;learning (artificial intelligence);recurrent neural nets","deep learning;flight delay prediction;machine learning;data analytics;Big Data;traffic flow prediction;air traffic data analysis;air traffic delay prediction tasks;robust prediction model;recurrent neural networks;sequential data modeling;day-to-day sequences;long short-term memory RNN architecture;deep RNN architecture","","86","","22","IEEE","12 Dec 2016","","","IEEE","IEEE Conferences"
"Efficient Deep Neural Network Serving: Fast and Furious","F. Yan; Y. He; O. Ruwase; E. Smirni","Department of Computer Science and Engineering, University of Nevada at Reno, Reno, NV, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Department of Computer Science, College of William and Mary, Williamsburg, VA, USA","IEEE Transactions on Network and Service Management","9 Mar 2018","2018","15","1","112","126","The emergence of deep neural networks (DNNs) as a state-of-the-art machine learning technique has enabled a variety of artificial intelligence applications for image recognition, speech recognition and translation, drug discovery, and machine vision. These applications are backed by large DNN models running in serving mode on a cloud computing infrastructure to process client inputs such as images, speech segments, and text segments. Given the compute-intensive nature of large DNN models, a key challenge for DNN serving systems is to minimize the request response latencies. This paper characterizes the behavior of different parallelism techniques for supporting scalable and responsive serving systems for large DNNs. We identify and model two important properties of DNN workloads: 1) homogeneous request service demand and 2) interference among requests running concurrently due to cache/memory contention. These properties motivate the design of serving deep learning systems fast (SERF), a dynamic scheduling framework that is powered by an interference-aware queueing-based analytical model. To minimize response latency for DNN serving, SERF quickly identifies and switches to the optimal parallel configuration of the serving system by using both empirical and analytical methods. Our evaluation of SERF using several well-known benchmarks demonstrates its good latency prediction accuracy, its ability to correctly identify optimal parallel configurations for each benchmark, its ability to adapt to changing load conditions, and its efficiency advantage (by at least three orders of magnitude faster) over exhaustive profiling. We also demonstrate that SERF supports other scheduling objectives and can be extended to any general machine learning serving system with the similar parallelism properties as above.","1932-4537","","10.1109/TNSM.2018.2808352","NSF(grant numbers:CCF-1218758,CCF-1649087,CCF-1756013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8298516","Deep learning;DNN serving;scheduling;parallelism;performance;analytical model;interference-aware","Parallel processing;Computational modeling;Neurons;Servers;Load modeling;Neural networks;Benchmark testing","cloud computing;image recognition;learning (artificial intelligence);neural nets;scheduling","SERF;good latency prediction accuracy;optimal parallel configuration;general machine learning;efficient deep neural network serving;deep neural networks;artificial intelligence applications;image recognition;drug discovery;machine vision;DNN models;compute-intensive nature;key challenge;DNN serving systems;request response latencies;different parallelism techniques;scalable serving systems;responsive serving systems;DNN workloads;deep learning systems;dynamic scheduling framework;interference-aware queueing;DNN;cache-memory contention","","11","","61","IEEE","21 Feb 2018","","","IEEE","IEEE Journals"
"A Brief Review of the Recent Trends in Sign Language Recognition","K. Nimisha; A. Jacob","Department of Electronics and Communication Engineering, Government College of Engineering Kannur, Kerala; Faculty of Department of Electronics and Communication Engineering, Government College of Engineering Kannur, Kerala","2020 International Conference on Communication and Signal Processing (ICCSP)","1 Sep 2020","2020","","","186","190","Sign language (SL) is an art of conveying our ideas and emotions non-verbally. SL is the main communication means for the deaf / dumb community. They usually communicate through a multitude of hand gestures. The correct interpretation of SL is all the more important, since the deaf and dumb constitute nearly 90 million of the world population. Therefore sign language recognition (SLR) and translation is a fertile area of research. The two main approaches for SLR are (i) image based and (ii) sensor based. Image based approach involves one or more cameras to capture an image sequence of the signer performing the sign, and then uses image processing to recognize the sign. The sensor based method uses instrumental gloves assembled with sensors to track the hand articulates. This paper mainly describes various image or vision based SLR systems comprising feature extraction and classification. Translation of SL to speech is also described briefly. Overall, this paper is expected to be a complete introduction to automatic hand gesture recognition and sign language interpretation (SLI).","","978-1-7281-4988-2","10.1109/ICCSP48568.2020.9182351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182351","American Sign Language (ASL);Feature extraction algorithm;You Only Look Once (YOLO);Visual Geometry Group 16 (VGG16)","Feature extraction;Gesture recognition;Assistive technology;Cameras;Machine learning;Principal component analysis;Auditory system","data gloves;feature extraction;gesture recognition;handicapped aids;image sequences;sign language recognition","correct interpretation;image based approach;image sequence;image processing;sensor based method;hand articulates;SLR systems;automatic hand gesture recognition;sign language interpretation;sign language recognition;main communication;hand gestures","","14","","22","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Anthropomatics - the science of building smart artifacts for humans","R. Dillmann","Institute of Anthropmatics, Humanoids and Intelligence Systems Laboratory, Karlsruhe Institute of Technology, Germany","2011 RO-MAN","29 Aug 2011","2011","","","xviii","xix","Anthropomatics addresses the symbiosis between humans and machines, focusing on a deeper understanding of the cooperation, interaction and coexistence between humans and machines stimulating and strengthen advanced and deep research in response to the challenges of increasingly smart environments and multimodal access to various complex technical systems. At KIT the Focus Anthropomatics and Robotics - APR has been set up by a number of research groups focusing on the research field of Anthropomatics and Robotics with more than 250 researchers. Modelling humans and their capabilities requires a deep understanding of the principle of biomechanics and kinematics, as well as the underlaying neural control principles and the perceptive and actuatoric system. Modelling and understanding of the sensomotoric mechanisms, learning and developement of skills and cognititve capabilities to enable humans to interact with the world is of high importance to design technical systems operating closely and interactively with humans via various modalities like speech, haptics, vision, grasping and locomotion. Typical research fields are related to active vision, interpretation of scenes and human activities, recognition and tracking technologies multimodal & perceptual user interfaces, understanding and translation of speech. Complementary research needed is related to the retrieval & access and summarization of multimedia data sources, translation of spoken text, context aware learning computers, implicit services and many more. The robotics application field ranges from interactive industrial robotics, service robotic companions, humanoids and medical robotics. In all domains the integrating aspects are focusing on algorithms processing real word data as well as open self-organizing architectures which allow autonomy, skill and task learning as well as interaction with humans. Our research emphasizes the critical path from basic understanding of cognitive processes and robotics foundations to applications in various domains. The research includes new approaches to sensor and actuator methodologies, foundations of machine perception, motion and action planning algorithms, simulation and computer graphics, robot machine learning, speech recognition and understanding, multimodal man-machine interaction, and many others. APR addresses the needs of humans in smart living environments and robotics focusing on both, basic foundations and applications. The focus has an intellectual center point on Anthropomatics and a commitment to Robotics as a new human centered discipline. The research is to explore new ideas and to build systematically systems which reflect human daily needs and a basic understanding of intelligent robots and adaptive safe system behaviour in general. Such cognitive robots are able to do tasks skillfully and efficiently together with humans, and they should be able to operate in dangerous or inaccessible areas. Furthermore, future robots should discover new things and learn new capabilities and knowledge about their environment and they should be able to reason about the effects of their actions and interventions. Thus, the basic capabilities to perform intelligent actions in the real world are perception, cognition, locomotion and skillful manipulation. The humanoid robot series ARMAR is used to illustrate recent research and achieved results in the field.","1944-9437","978-1-4577-1573-0","10.1109/ROMAN.2011.6005213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6005213","","","","","","","1","","IEEE","29 Aug 2011","","","IEEE","IEEE Conferences"
"A Sketch of a Deep Learning Approach for Discovering UML Class Diagrams from System‚Äôs Textual Specification","Y. Rigou; D. Lamontagne; I. Khriss","D√©partement de math√©matiques, d‚Äôinformatique et de g√©nie, UQAR, Rimouski, Canada; D√©partement de math√©matiques, d‚Äôinformatique et de g√©nie, UQAR, Rimouski, Canada; D√©partement de math√©matiques, d‚Äôinformatique et de g√©nie, UQAR, Rimouski, Canada","2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)","14 May 2020","2020","","","1","6","Drafting a formal or semi-formal model describing the functional requirements of a system from a textual specification is a prerequisite in the context of a model-driven engineering approach, such as the model-driven architecture initiative proposed by OMG. This model, called a platform-independent model (PIM), is used to derive automatically or semi-automatically the source code of a system. Different knowledge-based approaches have been proposed to extract a PIM from a textual specification automatically. These approaches use a predefined set of rules to perform this discovery. These approaches impose several restrictions on the way a specification is written. The emergence of machine learning techniques and more specifically of deep learning and their obvious success among others in several tasks in automatic language processing, such as speech recognition and translation, suggests the possibility of using these techniques to reach our objective. In this paper, we review state of the art in the domain and we sketch a rough deep learning approach to achieve our objective of extracting a PIM from the textual specification of a system.","","978-1-7281-4979-0","10.1109/IRASET48871.2020.9092144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092144","Natural language processing;deep learning;recurrent neural networks;Platform independent model extraction;UML class diagram;textual specification","Unified modeling language;Natural languages;Machine learning;Semantics;Syntactics;Task analysis","formal specification;learning (artificial intelligence);natural language processing;Unified Modeling Language","functional requirements;textual specification;model-driven engineering;model-driven architecture;platform-independent model;PIM;knowledge-based approach;machine learning;automatic language processing;rough deep learning approach;UML class diagrams","","1","","33","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Indian Language Resources--Evaluation Subcommittee Report","",,"Indian Language Resources--02","14 Sep 2023","2023","","","1","44","This report summarizes the findings and recommendations of the evaluation subcommittee of the IEEE prestandardization effort on standards for Indian language resources and evaluation for speech and language technology. Evaluation is a crucial aspect of any system. There are several generic challenges for the evaluation of speech and language technology, especially due to the ambiguous nature of human languages, as well as the interactive nature of many of these systems. Furthermore, there are additional challenges specific to Indian languages and the India context. This report summarizes the current metrics and datasets used for the evaluation of various technologies and identifies gaps that need to be addressed. Since there were dedicated subcommittees on speech, text, script, and accessibility, therefore, this report does not go into the details of various evaluation issues specific to these subareas. Instead, here the more generic issues that cut across these subdomains are discussed. A case study is also presented on machine translation (MT).","","979-8-8557-0065-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251750","accessibility;evaluation;Indian language;information communication technology;language;speech;standards development;whitepaper","","","","","","","","","14 Sep 2023","","","IEEE","IEEE Standards"
"DOA-Based Three-Dimensional Node Geometry Calibration in Acoustic Sensor Networks and Its Cram√©r‚ÄìRao Bound and Sensitivity Analysis","R. Wang; Z. Chen; F. Yin","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","24 Jun 2019","2019","27","9","1455","1468","Acoustic sensor networks (ASNs) are widely applied in scenarios like teleconference, teaching, and theatre. ASNs can be used in tracking speakers, enhancing the speaker's speech and human-machine interactions, etc., but the geometric structure of the ASN has to be calibrated. ASN geometry calibration is a challenging task due to the irregular geometric structures of ASNs. A three-dimensional (3D) node geometry calibration approach based on direction of arrival (DOA) measurements and artificial bee colony (ABC) algorithm is proposed in this paper. The theoretical DOAs of sound sources relative to nodes are first derived based on 3D rotation matrices and translation vectors, and the corresponding measured DOAs are estimated by the time-difference-of-arrival. Then, the node geometry calibration problem is formulated as the minimization of a cost function measuring the mismatch between theoretical and measured DOAs, and such non-convex minimization is effectively solved by the ABC algorithm. Next, Cram√©r-Rao bound is presented to provide a theoretical lower bound for DOA-based node geometry calibration. Finally, the sensitivity of the proposed method to the sound source position error is discussed. The proposed method can calibrate node geometry positions successfully in both 2D plane and 3D space and requires no information transmission among nodes when the positions of few sound sources and the relative geometry of microphones in each node are known. Experimental results reveal the validity of the proposed node geometry calibration method.","2329-9304","","10.1109/TASLP.2019.2921892","National Natural Science Foundation of China(grant numbers:61771091,61871066); National High Technology Research and Development Program (863 Program) of China(grant numbers:2015AA016306); Natural Science Foundation of Liaoning Province(grant numbers:20170540159); Fundamental Research Fund for the Central Universities of China(grant numbers:DUT17LAB04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735787","Acoustic sensor network;node geometry calibration;direction of arrival;artificial bee colony algorithm;Cram√©r‚ÄìRao bound","Geometry;Calibration;Direction-of-arrival estimation;Microphones;Three-dimensional displays;Estimation;Cost function","artificial bee colony algorithm;calibration;concave programming;direction-of-arrival estimation;geometry;matrix algebra;microphones;minimisation;wireless sensor networks","acoustic sensor networks;Cram√©r-Rao bound;sensitivity analysis;tracking speakers;geometric structure;ASN geometry calibration;irregular geometric structures;three-dimensional node geometry calibration approach;artificial bee colony algorithm;theoretical DOAs;3D rotation matrices;time-difference-of-arrival;node geometry calibration problem;ABC algorithm;DOA-based node geometry calibration;sound source position error;node geometry positions;node geometry calibration method;non-convex minimization;directional of arrival measurements;DOA-based three-dimensional node geometry calibration","","6","","37","IEEE","12 Jun 2019","","","IEEE","IEEE Journals"
"Deep Learning Model for House Price Prediction Using Heterogeneous Data Analysis Along With Joint Self-Attention Mechanism","P. -Y. Wang; C. -T. Chen; J. -W. Su; T. -Y. Wang; S. -H. Huang","Institute of Information Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Institute of Information Management, National Chiao Tung University, Hsinchu, Taiwan; Institute of Information Management, National Chiao Tung University, Hsinchu, Taiwan; Department of Information Management and Finance, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","IEEE Access","13 Apr 2021","2021","9","","55244","55259","House price prediction is a popular topic, and research teams are increasingly performing related studies by using deep learning or machine learning models. However, because some studies have not considered comprehensive information that affects house prices, prediction results are not always sufficiently precise. Therefore, we propose an end to end joint self-attention model for house prediction. In this model, we import data on public facilities such as parks, schools, and mass rapid transit stations to represent the availability of amenities, and we use satellite maps to analyze the environment surrounding houses. We adopt attention mechanisms, which are widely used in image, speech, and translation tasks, to identify crucial features that are considered by prospective house buyers. The model can automatically assign weights when given transaction data. Our proposed model differs from self-attention models because it considers the interaction between two different features to learn the complicated relationship between features in order to increase prediction precision. We conduct experiments to demonstrate the performance of the model. Experimental data include actual selling prices in real estate transaction data for the period from 2017 to 2018, public facility data acquired from the Taipei and New Taipei governments, and satellite maps crawled using the Google Maps application programming interface. We utilize these datasets to train our proposed and compare its performance with that of other machine learning-based models such as Extreme Gradient Boosting and Light Gradient Boosted Machine, deep learning, and several attention models. The experimental results indicate that the proposed model achieves a low prediction error and outperforms the other models. To the best of our knowledge, we are the first research to incorporate attention mechanism and STN network to conduct house price prediction.","2169-3536","","10.1109/ACCESS.2021.3071306","Ministry of Science and Technology, Taiwan(grant numbers:MOST 110-2622-8-009-014-TM1,MOST 109-2221-E-009-139,MOST 109-2622-E-009-002-CC2,MOST 109-2218-E-009-015); Financial Technology (FinTech) Innovation Research Center, National Yang Ming Chiao Tung University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395585","House price prediction;heterogeneous data;Google satellite map;spatial transformer network;joint self-attention mechanism","Hidden Markov models;Predictive models;Autoregressive processes;Biological system modeling;Satellites;Data models;Feature extraction","application program interfaces;data analysis;learning (artificial intelligence);pricing;property market","public facility data;estate transaction data;actual selling prices;prediction precision;self-attention models;transaction data;prospective house buyers;attention mechanisms;environment surrounding houses;satellite maps;house prediction;end joint self-attention model;house prices;machine learning models;joint self-attention mechanism;heterogeneous data analysis;deep learning model;house price prediction;incorporate attention mechanism;low prediction error;machine learning-based models","","29","","34","CCBYNCND","6 Apr 2021","","","IEEE","IEEE Journals"
"Exploring Creative Frontiers of AI for M&E Production and Distribution","G. Olson; R. Singer","GHO Group LLC; Singer Media Engineering, LLC","SMPTE 2018","13 Jan 2019","2018","","","1","10","In recent years, Artificial Intelligence (AI) tools are beginning to revolutionize Media and Entertainment production and distribution industry. As with other industries, such as mobile, business, transportation, gaming, robotics, security, education. Much like non-intelligent automation, artificial intelligence applications can be trained to perform human-like tasks. Some of the common skills currently imitated by AI include visual perception, speech recognition, decision-making, and adaptability ‚Äî AI tools can be applied to perform tasks that were impossible to accomplish. AI performs tasks faster and with greater precision than their experience human counterparts. AI applications are beginning to augment the creative decision making skills of the production and distribution team, enabling energy, talent and enthusiasm to be focused on what only humans can do. At least for now. ‚Äî During the M&E creative production process images, sound, and metadata are collected and stored, analyzed, identified and selected. Recovery of content from the media libraries and archives is critical to the creative process. This includes production generated multiple media formats of video, audio, graphics, and other types of data including text, scripts, and playlists. ‚Äî It takes multiple tools to organize, collate and curate media and data from different formats and types. Metadata creation technologies help create the metadata necessary for content search and retrieval. While several logging and recognition technologies can generate some metadata automatically, the results are often incomplete inaccurate and far from comprehensive. ‚Äî AI can be applied to myriad applications enhancing, accelerating and increasing the capacity of the media supply chain: AI services such transcription, language and dialect translation. face and object recognition, social media sentiment analysis. geolocation and fingerprinting are just a few examples. ‚Äî Newly developed artificial intelligence for media asset management fuses the ability to understand all media and data types and perform discovery, synthesize and curate. ‚Äî Across the media supply chain employing automated technologies that can create the metadata, then index and catalog it during the creation or acquisition process would help the creative process downstream. There is a still a requirement for the creative team to identify relationaships and the preferred clip selection. It needs the legal team to add permissions, control access, contractual distribution and expiration criteria. In news production using ML techniques with pattern matching, additional content can be identified.","","978-1-61482-960-7","10.5594/M001817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8609876","AI;Artificial Intelligence;machine learning;Deep learning;inference;semantic;metadata;MAM;DAM;PAM;asset management;media management","","","","","","","2","","13 Jan 2019","","","SMPTE","SMPTE Conferences"
"Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more","D. Rothman",NA,"Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more","","2021","","","","","Publisher's Note: A new edition of this book is out now that includes working with GPT-3 and comparing the results with other models. It includes even more use cases, such as casual language analysis and computer vision tasks, as well as an introduction to OpenAI's Codex.Key FeaturesBuild and implement state-of-the-art language models, such as the original Transformer, BERT, T5, and GPT-2, using concepts that outperform classical deep learning modelsGo through hands-on applications in Python using Google Colaboratory Notebooks with nothing to install on a local machineTest transformer models on advanced use casesBook DescriptionThe transformer architecture has proved to be revolutionary in outperforming the classical RNN and CNN models in use today. With an apply-as-you-learn approach, Transformers for Natural Language Processing investigates in vast detail the deep learning for machine translations, speech-to-text, text-to-speech, language modeling, question answering, and many more NLP domains with transformers. The book takes you through NLP with Python and examines various eminent models and datasets within the transformer architecture created by pioneers such as Google, Facebook, Microsoft, OpenAI, and Hugging Face. The book trains you in three stages. The first stage introduces you to transformer architectures, starting with the original transformer, before moving on to RoBERTa, BERT, and DistilBERT models. You will discover training methods for smaller transformers that can outperform GPT-3 in some cases. In the second stage, you will apply transformers for Natural Language Understanding (NLU) and Natural Language Generation (NLG). Finally, the third stage will help you grasp advanced language understanding techniques such as optimizing social network datasets and fake news identification. By the end of this NLP book, you will understand transformers from a cognitive science perspective and be proficient in applying pretrained transformer models by tech giants to various datasets.What you will learnUse the latest pretrained transformer modelsGrasp the workings of the original Transformer, GPT-2, BERT, T5, and other transformer modelsCreate language understanding Python programs using concepts that outperform classical deep learning modelsUse a variety of NLP platforms, including Hugging Face, Trax, and AllenNLPApply Python, TensorFlow, and Keras programs to sentiment analysis, text summarization, speech recognition, machine translations, and moreMeasure the productivity of key transformers to define their scope, potential, and limits in productionWho this book is forSince the book does not teach basic programming, you must be familiar with neural networks, Python, PyTorch, and TensorFlow in order to learn their implementation with Transformers. Readers who can benefit the most from this book include experienced deep learning & NLP practitioners and data analysts & data scientists who want to process the increasing amounts of language-driven data.","","9781800568631","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10163499.pdf&bkn=10163498&pdfType=book","","","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, Hugging Face, and OpenAI's GPT-3, ChatGPT, and GPT-4","D. Rothman; A. Gulli",NA; NA,"Transformers for Natural Language Processing: Build, train, and fine-tune deep neural network architectures for NLP with Python, Hugging Face, and OpenAI's GPT-3, ChatGPT, and GPT-4","","2022","","","","","OpenAI‚Äôs GPT-3, ChatGPT, GPT-4 and Hugging Face transformers for language tasks in one book. Get a taste of the future of transformers, including computer vision tasks and code writing and assistance. Purchase of the print or Kindle book includes a free eBook in PDF formatKey FeaturesImprove your productivity with OpenAI‚Äôs ChatGPT and GPT-4 from prompt engineering to creating and analyzing machine learning modelsPretrain a BERT-based model from scratch using Hugging FaceFine-tune powerful transformer models, including OpenAI's GPT-3, to learn the logic of your dataBook DescriptionTransformers are...well...transforming the world of AI. There are many platforms and models out there, but which ones best suit your needs? Transformers for Natural Language Processing, 2nd Edition, guides you through the world of transformers, highlighting the strengths of different models and platforms, while teaching you the problem-solving skills you need to tackle model weaknesses. You'll use Hugging Face to pretrain a RoBERTa model from scratch, from building the dataset to defining the data collator to training the model. If you're looking to fine-tune a pretrained model, including GPT-3, then Transformers for Natural Language Processing, 2nd Edition, shows you how with step-by-step guides. The book investigates machine translations, speech-to-text, text-to-speech, question-answering, and many more NLP tasks. It provides techniques to solve hard language problems and may even help with fake news anxiety (read chapter 13 for more details). You'll see how cutting-edge platforms, such as OpenAI, have taken transformers beyond language into computer vision tasks and code creation using DALL-E 2, ChatGPT, and GPT-4. By the end of this book, you'll know how transformers work and how to implement them and resolve issues like an AI detective.What you will learnDiscover new techniques to investigate complex language problemsCompare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based transformersCarry out sentiment analysis, text summarization, casual speech analysis, machine translations, and more using TensorFlow, PyTorch, and GPT-3Find out how ViT and CLIP label images (including blurry ones!) and create images from a sentence using DALL-ELearn the mechanics of advanced prompt engineering for ChatGPT and GPT-4Who this book is forIf you want to learn about and apply transformers to your natural language (and image) data, this book is for you. You'll need a good understanding of Python and deep learning and a basic understanding of NLP to benefit most from this book. Many platforms covered in this book provide interactive user interfaces, which allow readers with a general interest in NLP and AI to follow several chapters. And don't worry if you get stuck or have questions; this book gives you direct access to our AI/ML community to help guide you on your transformers journey!","","9781803243481","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10162341.pdf&bkn=10162340&pdfType=book","","","","","","","","","","27 Jun 2023","","","Packt Publishing","Packt Publishing eBooks"
"Anomaly Detection in Lexical Definitions via One-Class Classification Techniques","S. Jumpathong; K. Kriengket; P. Boonkwan; T. Supnithi","Language and Semantic Technology Research Team, National Electronics and Computer Technology Center, Pathumthani, Thailand; Language and Semantic Technology Research Team, National Electronics and Computer Technology Center, Pathumthani, Thailand; Language and Semantic Technology Research Team, National Electronics and Computer Technology Center, Pathumthani, Thailand; Language and Semantic Technology Research Team, National Electronics and Computer Technology Center, Pathumthani, Thailand","2021 16th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)","18 Jan 2022","2021","","","1","6","It takes a long time to build vocabularies and their definitions because they must be approved only by the experts in the meeting of building vocabularies and the definitions are also unstructured. To save time, we applied three techniques of classification to the experiments that are one-class SVMs, isolation forests, and local outlier factors, and also observed how well the method can suggest word definition status via the accuracy. As a result, the local outlier factors obtained the highest accuracy when they used vectors that were produced by USE. They can recognize the boundary of the approved class better and there are several approved clusters and outliers are scattered among them. Also, it is found that the detected status of definitions is both identical and opposite to the reference one. For the patterns of definition writing, the approved definitions are always written in the logical order, and start with wide or general information, then is followed by specific details, examples, and references of English terms or examples. In case of the rejected definitions, they are not always written in the logical order, and their definition patterns are also various - only Thai translation, Thai translation with related entries, parts of speech (POS), Thai translation, related entries, and English term references followed by definitions, etc.","","978-1-6654-0947-6","10.1109/iSAI-NLP54397.2021.9678166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678166","One-class classification;lexical definition;dictionary-making process;vocabularies","Vocabulary;Buildings;Forestry;Writing;Natural language processing;Artificial intelligence;Anomaly detection","data mining;natural language processing;pattern classification;pattern clustering;support vector machines","local outlier factors;approved class;approved clusters;detected status;definition writing;approved definitions;rejected definitions;definition patterns;anomaly detection;lexical definitions;one-class classification techniques;building vocabularies;one-class SVMs;word definition status","","","","9","IEEE","18 Jan 2022","","","IEEE","IEEE Conferences"
"Neural Architecture Search Based on a Multi-Objective Evolutionary Algorithm With Probability Stack","Y. Xue; C. Chen; A. S≈Çowik","School of Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Electronics and Computer Science, Koszalin University of Technology, Koszalin, Poland","IEEE Transactions on Evolutionary Computation","1 Aug 2023","2023","27","4","778","786","With the emergence of deep neural networks, many research fields, such as image classification, object detection, speech recognition, natural language processing, machine translation, and automatic driving, have made major breakthroughs in technology and the research achievements have been successfully applied in many real-life applications. Combining evolutionary computation and neural architecture search (NAS) is an important approach to improve the performance of deep neural networks. Usually, the related researchers only focus on precision. Thus, the searched neural architectures always perform poorly in the other indexes such as time cost. In this article, a multi-objective evolutionary algorithm with a probability stack (MOEA-PS) is proposed for NAS, which considers the two objects of precision and time consumption. MOEA-PS uses an adjacency list to represent the internal structure of deep neural networks. Besides, a unique mechanism is introduced into the multi-objective genetic algorithm to guide the process of crossover and mutation when generating offspring. Furthermore, the structure blocks are stacked using a proxy model to generate deep neural networks. The results of the experiments on Cifar-10 and Cifar-100 demonstrate that the proposed algorithm has a similar error rate compared with the most advanced NAS algorithms, but the time cost is lower. Finally, the network structure searched on Cifar-10 is transferred directly to the ImageNet dataset, which can achieve 73.6% classification accuracy.","1941-0026","","10.1109/TEVC.2023.3252612","National Natural Science Foundation of China(grant numbers:61876089,61876185,61902281); Natural Science Foundation of Jiangsu Province(grant numbers:BK20141005); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:14KJB520025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059145","Deep learning;evolutionary computation;multi-objective optimization;neural architecture search (NAS)","Computer architecture;Classification algorithms;Statistics;Sociology;Graphics processing units;Neural networks;Evolutionary computation","deep learning (artificial intelligence);evolutionary computation;genetic algorithms;image classification;natural language processing;neural net architecture;object detection;search problems;speech recognition","deep neural networks;MOEA-PS;multiobjective evolutionary algorithm;multiobjective genetic algorithm;neural architecture search;object detection;probability stack;searched neural architectures","","3","","51","IEEE","6 Mar 2023","","","IEEE","IEEE Journals"
"Pattern Recognition","E. Alpaydin",Boƒüazi√ßi University,"Machine Learning: The New AI","","2016","","","55","84","This chapter contains sections titled: Learning to Read, Matching Model Granularity, Generative Models, Face Recognition, Speech Recognition, Natural Language Processing and Translation, Combining Multiple Models, Outlier Detection, Dimensionality Reduction, Decision Trees, Active Learning, Learning to Rank, Bayesian Methods","","9780262337595","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7845239.pdf&bkn=7845169&pdfType=chapter","","","","","","","","","","8 Feb 2017","","","MIT Press","MIT Press eBook Chapters"
"Deep Learning","S. Tuffery","University of Rennes 1, France","Deep Learning: From Big Data to Artificial Intelligence with R","","2023","","","269","346","Deep Learning is a branch of machine learning largely based on neural networks that is particularly well suited to learning complex data in order to create advanced supervised or unsupervised models. Deep learning seeks to model complex phenomena using mechanisms inspired by neuroscience. The main deep learning methods are based on neural networks, whose structure differs, however, from that of classical neural networks, such as the perceptron, and is adapted to the realization of complex images, videos and written and spoken natural language recognition tasks. Convolutional neural networks are the most widespread in deep learning, especially in computer vision. In a convolutional neural network, convolution is performed by applying a sliding window to an input matrix, multiplying the input data and the kernel data term by term, and then adding them. Recurrent neural networks have proven to be among the best suited for natural language recognition, translation, speech recognition and generation, and even music generation.","","9781119845027","10.1002/9781119845041.ch7","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9968175.pdf&bkn=9968098&pdfType=chapter","","Speech recognition;Deep learning;Feature extraction;Reinforcement learning;Convolutional neural networks;Visualization;Training","","","","","","","","1 Dec 2022","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Unsupervised Melody Style Conversion","E. Nakamura; K. Shibata; R. Nishikimi; K. Yoshii","Kyoto University, Kyoto, Japan; Kyoto University, Kyoto, Japan; Kyoto University, Kyoto, Japan; Kyoto University, Kyoto, Japan","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","196","200","We study a method for converting the music style of a given melody to a target style (e.g. from classical music style to pop music style) based on unsupervised statistical learning. Following the analogy with machine translation, we propose a statistical formulation of style conversion based on integration of a music language model of the target style and an edit model representing the similarity between the original and arranged melodies. In supervised-learning approaches for constructing style-specific language models, it has been crucial to use data that properly specify a music style. To reduce reliance on manual data selection and annotation, we propose a novel statistical model that can spontaneously discover styles in pitch and rhythm organization. We also point out the importance of an edit model that incorporates syntactic functions of notes such as tonic and build a model that can infer such functions unsupervisedly. We confirm that the proposed method improves the quality of arrangement by examining the results and by subjective evaluation.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682331","Symbolic music processing;music arrangement;style conversion;statistical music language models;unsupervised grammar induction","Manganese;Hidden Markov models;Data models;Rhythm;C# languages;Syntactics;Bars","learning (artificial intelligence);music;statistical analysis","unsupervised melody style conversion;classical music style;unsupervised statistical learning;music language model;style-specific language models;statistical model;rhythm organization;pitch organization","","6","","39","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"AVSS2011 demo session: Real-time human detection using fast contour template matching for visual surveillance","C. Beleznai; M. Rauter; D. Shao","AIT Austrian Institute of Technology GmbH, Donau-City-Stra√üe 1, Vienna, Austria; AIT Austrian Institute of Technology GmbH, Donau-City-Stra√üe 1, Vienna, Austria; AIT Austrian Institute of Technology GmbH, Donau-City-Stra√üe 1, Vienna, Austria","2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)","26 Sep 2011","2011","","","514","514","Summary form only given. Anthropomatics addresses the symbiosis between humans and machines, focusing on a deeper understanding of the cooperation, interaction and coexistence between humans and machines stimulating and strengthen advanced and deep research in response to the challenges of increasingly smart environments and multimodal access to various complex technical systems. At KIT the Focus Anthropomatics and Robotics - APR has been set up by a number of research groups focusing on the research field of Anthropomatics and Robotics with more than 250 researchers. Modelling humans and their capabilities requires a deep understanding of the principle of biomechanics and kinematics, as well as the underlaying neural control principles and the perceptive and actuatoric system. Modelling and understanding of the sensomotoric mechanisms, learning and developement of skills and cognititve capabilities to enable humans to interact with the world is of high importance to design technical systems operating closely and interactively with humans via various modalities like speech, haptics, vision, grasping and locomotion. Typical research fields are related to active vision, interpretation of scenes and human activities, recognition and tracking technologies multimodal & perceptual user interfaces, understanding and translation of speech. Complementary research needed is related to the retrieval & access and summarization of multimedia data sources, translation of spoken text, context aware learning computers, implicit services and many more. The robotics application field ranges from interactive industrial robotics, service robotic companions, humanoids and medical robotics. In all domains the integrating aspects are focusing on algorithms processing real word data as well as open self-organizing architectures which allow autonomy, skill an","","978-1-4577-0845-9","10.1109/AVSS.2011.6027393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027393","","","","","","","","","IEEE","26 Sep 2011","","","IEEE","IEEE Conferences"
"Deep Learning Transformer Architecture for Named-Entity Recognition on Low-Resourced Languages: State of the art results","R. Hanslo","University of Pretoria, Gauteng, South Africa","2022 17th Conference on Computer Science and Intelligence Systems (FedCSIS)","10 Oct 2022","2022","","","53","60","This paper reports on the evaluation of Deep Learning (DL) transformer architecture models for Named-Entity Recognition (NER) on ten low-resourced South African (SA) languages. In addition, these DL transformer models were com-pared to other Neural Network and Machine Learning (ML) NER models. The findings show that transformer models substantially improve performance when applying discrete fine-tuning parameters per language. Furthermore, fine-tuned transformer models outperform other neural network and ma-chine learning models on NER with the low-resourced SA languages. For example, the transformer models obtained the highest F-scores for six of the ten SA languages and the highest average F-score surpassing the Conditional Random Fields ML model. Practical implications include developing high-performance NER capability with less effort and resource costs, potentially improving downstream NLP tasks such as Machine Translation (MT). Therefore, the application of DL trans-former architecture models for NLP NER sequence tagging tasks on low-resourced SA languages is viable. Additional re-search could evaluate the more recent transformer architecture models on other Natural Language Processing tasks and applications, such as Phrase chunking, MT, and Part-of-Speech tagging.","","978-83-962423-9-6","10.15439/2022F53","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9909310","4Named-Entity Recognition;Natural Language Processing;Neural Networks;Sequence Tagging;XLM-R;Machine Learning;Transformer Models","Deep learning;Costs;Computational modeling;Government;Artificial neural networks;Tagging;Transformers","learning (artificial intelligence);natural language processing;neural nets;text analysis","SA languages;highest average F-score;Conditional Random Fields ML model;high-performance NER capability;resource costs;DL trans-former architecture models;NLP NER sequence;recent transformer architecture models;Natural Language Processing tasks;named-entity recognition;low-resourced languages;South African languages;DL transformer models;fine-tuning parameters;fine-tuned transformer models;neural network;ma-chine learning models","","","","21","","10 Oct 2022","","","IEEE","IEEE Conferences"
"Sign Language Recognition using Sensor and Vision Based Approach","A. S. Antony; K. B. V. Santhosh; N. Salimath; S. H. Tanmaya; Y. Ramyapriya; M. Suchith","Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India; Department of Electronics and Communication Engineering, New Horizon College of Engineering, Bengaluru, India","2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)","15 Apr 2022","2022","","","1","8","Sign language is one of mode for Communication which conveys the ideas non-verbally. It is gift for speech and hear impaired community. A Sign Language Recognition system has made this community close to regular communication. This system convert's the signs or gestures to a form which is easily understood. System is trained by using images of letters. As technology improved many different methods have evolved to create this system. In this study, two main methods for implementing sign language recognition are i) Sensor-based approach ii) Vision-based approach are studied. In image-based, cameras are used to capture the images of signs and use image processing to identify the sign in image and then algorithm that interpret the meaning of that sign. The sensor-based approach a glove is constructed using sensors that will track the sign made by hand. In this paper translation of sign to speech, methods to SLR, sign language interpretation, techniques to map features and popular methods are studied.","","978-1-6654-9529-5","10.1109/ACCAI53970.2022.9752580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752580","Sign language;Sensor;Vision;Cameras","Training;Support vector machines;Image processing;Gesture recognition;Artificial neural networks;Assistive technologies;Sensors","data gloves;gesture recognition;handicapped aids;sign language recognition","ideas nonverbally;impaired community;Sign Language Recognition system;regular communication;Sensor-based approach ii;Vision-based approach;image processing;sign language interpretation","","9","","24","IEEE","15 Apr 2022","","","IEEE","IEEE Conferences"
"Measures of Syntactic Complexity and their Change over Time (the Case of Russian)","T. Sherstinova; E. Ushakova; A. Melnik","National Research University Higher School of Economics, St. Petersburg, Saint Petersburg, Russia; National Research University Higher School of Economics, St. Petersburg, Saint Petersburg, Russia; Speech Technology Center, Saint Petersburg, Russia","2020 27th Conference of Open Innovations Association (FRUCT)","2 Oct 2020","2020","","","221","229","Syntactic complexity is an important feature of any text, both written and oral. The information about syntactic complexity is crucial for successful solution of many practical NLP tasks starting from intellectual understanding of texts and ending with automatic machine translation. Because of this, syntactic complexity and its measures are in the center of attention of NLP developers. Thus far, quite a series of different measures of syntactic complexity have been developed; in this paper, it is proposed to consider 10 syntactic measures that have been proposed for syntactic stylometric analysis. The pilot experiment described in this paper was made on automatic syntactic text annotation made by UDPipe syntactic parser, which was manually corrected. In our approach, particular attention is paid to the analysis of stability of certain measures of syntactic complexity and the analysis of their variation. Thus, we try to evaluate, which syntactic properties of Russian texts may be considered as inherent for the language as a whole, and which of them undergo some changes. To achieve this task, we analyze the corpus of Russian literary texts for three decades. Due to their high stylistic variability, texts of fiction may be considered as excellent data for assessing different levels of complexity. The obtained results show the effectiveness of different measures for estimating text syntactic complexity and revealing their correlation.","2305-7254","978-952-69244-3-4","10.23919/FRUCT49677.2020.9211027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211027","","Syntactics;Complexity theory;Task analysis;Linguistics;Time measurement;Frequency measurement;Market research","computational linguistics;grammars;natural language processing;text analysis","syntactic measures;syntactic stylometric analysis;automatic syntactic text annotation;UDPipe syntactic parser;syntactic properties;text syntactic complexity;Russian texts;stylistic variability","","5","","45","","2 Oct 2020","","","IEEE","IEEE Conferences"
"Kannada spell checker with sandhi splitter","S. R. Murthy; A. N. Akshatha; C. G. Upadhyaya; P. R. Kumar","Department of ISE, RVCE, Bangalore Bangalore, India; Department of ISE, RVCE, Bangalore Bangalore, India; Department of ISE, RVCE, Bangalore Bangalore, India; RVCE, Bangalore Bangalore, India","2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","4 Dec 2017","2017","","","950","956","Spelling errors are introduced in text either during typing, or when the user does not know the correct phoneme or grapheme. If a language contains complex words like sandhi where two or more morphemes join based on some rules, spell checking becomes very tedious. In such situations, having a spell checker with sandhi splitter which alerts the user by flagging the errors and providing suggestions is very useful. A novel algorithm of sandhi splitting is proposed in this paper. The sandhi splitter can split about 7000 most common sandhi words in Kannada language used as test samples. The sandhi splitter was integrated with a Kannada spell checker and a mechanism for generating suggestions was added. A comprehensive, platform independent, standalone spell checker with sandhi splitter application software was thus developed and tested extensively for its efficiency and correctness. A comparative analysis of this spell checker with sandhi splitter was made and results concluded that the Kannada spell checker with sandhi splitter has an improved performance. It is twice as fast, 200 times more space efficient, and it is 90% accurate in case of complex nouns and 80% accurate for complex verbs. Such a spell checker with sandhi splitter will be of foremost significance in machine translation systems, voice processing, etc. This is the first sandhi splitter in Kannada and the advantage of the novel algorithm is that, it can be extended to all Indian languages.","","978-1-5090-6367-3","10.1109/ICACCI.2017.8125964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8125964","Natural language processing;Morphology;Computational linguistics;Sandhi splitter;Spell checker","Dictionaries;Compounds;Morphology;Grammar;Redundancy;Mood;Software","natural language processing;speech processing;text analysis","Kannada spell checker;sandhi splitter application software;sandhi words;spelling errors","","2","","","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Big data language model of contemporary polish","K. Wo≈Çk; A. Wo≈Çk; K. Marasek","Polish-Japanese Academy of Information Technology, Warszawa, Poland; Polish-Japanese Academy of Information Technology, Warszawa, Poland; Polish-Japanese Academy of Information Technology, Warszawa, Poland","2017 Federated Conference on Computer Science and Information Systems (FedCSIS)","13 Nov 2017","2017","","","389","395","Based on big data training we provide 5-gram language models of contemporary Polish which are based on the Common Crawl corpus (which is a compilation of more than 9,000,000,000 pages from across the web) and other resources. We prove that our model is better than the Google WEB1T n-gram counts and assures better quality in terms of perplexity and machine translation. The model includes lower-counting entries and also de-duplication in order to lessen boilerplate. We also provide POS tagged version of raw corpus and raw corpus itself. We also provide dictionary of contemporary Polish. By maintaining singletons, Kneser-Ney smoothing in SRILM toolkit was used in order to construct big data language models. In this research, it is detailed exactly how the corpus was obtained and pre-processed, with a prominence on issues which surface when working with information on this scale. We train the language model and finally present advances of BLEU score in MT and perplexity values, through the utilization of our model.","","978-83-946253-7-5","10.15439/2017F432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104570","","Data models;Speech;Google;Dictionaries;Big Data;Electronic mail","Big Data;natural language processing","5-gram language models;Common Crawl corpus;Big Data language model;Big Data training;SRILM toolkit;Kneser-Ney smoothing;BLEU score;MT;Google WEB1T n-gram counts;contemporary Polish","","","","32","","13 Nov 2017","","","IEEE","IEEE Conferences"
"A Clustering Based Adaptive Sequence-to-Sequence Model for Dialogue Systems","D. Ren; Y. Cai; W. H. Chan; Z. Li","School of Software Engineer, South China University of Technology, Guangzhou, China; School of Software Engineer, South China University of Technology, Guangzhou, China; Department of Mathematics and Information Technology, The Education University of Hong Kong, Hong Kong, China; Department of Mathematics and Information Technology, The Education University of Hong Kong, Hong Kong, China","2018 IEEE International Conference on Big Data and Smart Computing (BigComp)","28 May 2018","2018","","","775","781","Dialogue systems which can communicate with people in natural language is popularly used in entertainments and language learning tools. As the development of deep neural networks, Sequence-to-Sequence models become the main stream models of conversation generation tasks which are the key part of dialogue systems, because Sequence-to-Sequence models is good at dealing with the tasks like machine translation and conversation generation whose input's length and output's length is unknown previously. However, recent works find that Sequence-to-Sequence models tend to respond in dull sentences. We propose a clustering based adaptive Sequence-to-Sequence model to improve the performance of dialogue systems. Different with previous models who treat all the dialogue data as input of a single model, we cluster the dialogue data and use several Sequence-to-Sequence models to train different cluster of data to catch different characteristic in different cluster. Our experiments show that our models can improve the performance of dialogue systems.","2375-9356","978-1-5386-3649-7","10.1109/BigComp.2018.00148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367228","dialogue systems;conversation generation;a clustering based adaptive Sequence-to-Sequence model","Data models;Task analysis;Adaptation models;Logic gates;Recurrent neural networks;Speech recognition","interactive systems;natural language processing;neural nets;pattern clustering","dialogue systems;clustering based adaptive sequence-to-sequence model;natural language;conversation generation tasks;deep neural networks","","","","16","IEEE","28 May 2018","","","IEEE","IEEE Conferences"
"Hardware architecture of Bidirectional Long Short-Term Memory Neural Network for Optical Character Recognition","V. Rybalkin; N. Wehn; M. R. Yousefi; D. Stricker","Microelectronic Systems Design Research Group, University of Kaiserslautern, Germany; Microelectronic Systems Design Research Group, University of Kaiserslautern, Germany; Augmented Vision Department, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Augmented Vision Department, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017","15 May 2017","2017","","","1390","1395","Optical Character Recognition is conversion of printed or handwritten text images into machine-encoded text. It is a building block of many processes such as machine translation, text-to-speech conversion and text mining. Bidirectional Long Short-Term Memory Neural Networks have shown a superior performance in character recognition with respect to other types of neural networks. In this paper, to the best of our knowledge, we propose the first hardware architecture of Bidirectional Long Short-Term Memory Neural Network with Connectionist Temporal Classification for Optical Character Recognition. Based on the new architecture, we present an FPGA hardware accelerator that achieves 459 times higher throughput than state-of-the-art. Visual recognition is a typical task on mobile platforms that usually use two scenarios either the task runs locally on embedded processor or offloaded to a cloud to be run on high performance machine. We show that computationally intensive visual recognition task benefits from being migrated to our dedicated hardware accelerator and outperforms high-performance CPU in terms of runtime, while consuming less energy than low power systems with negligible loss of recognition accuracy.","1558-1101","978-3-9815370-8-6","10.23919/DATE.2017.7927210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927210","","Hardware;Memory management;Character recognition;Logic gates;Field programmable gate arrays;Bandwidth","embedded systems;field programmable gate arrays;graphics processing units;handwritten character recognition;neural net architecture;optical character recognition","hardware architecture;bidirectional long short-term memory neural network;optical character recognition;handwritten text images;printed text images;machine-encoded text;connectionist temporal classification;FPGA hardware accelerator;visual recognition;mobile platforms;embedded processor;hardware accelerator;high-performance CPU","","30","","14","","15 May 2017","","","IEEE","IEEE Conferences"
"Deep learning systems as complex networks","A. Testolin; M. Piccolini; S. Suweis; T. Gross","Department of General Psychology, University of Padova, Via Venezia 12, 35131 Padova, Italy; Department of Physics and Astronomy, University of Padova, Via Marzolo 8, 35128 Padova, Italy; Department of Physics and Astronomy, University of Padova, Via Marzolo 8, 35128 Padova, Italy and Padova Neuroscience Center, University of Padova, Via Giuseppe Orus 2b, Padua, Veneto, Italy; samir.suweis@unipd.it; NA","Journal of Complex Networks","10 Jun 2020","2020","8","1","1","21","Thanks to the availability of large scale digital datasets and massive amounts of computational power, deep learning algorithms can learn representations of data by exploiting multiple levels of abstraction. These machine-learning methods have greatly improved the state-of-the-art in many challenging cognitive tasks, such as visual object recognition, speech processing, natural language understanding and automatic translation. In particular, one class of deep learning models, known as deep belief networks (DBNs), can discover intricate statistical structure in large datasets in a completely unsupervised fashion, by learning a generative model of the data using Hebbian-like learning mechanisms. Although these self-organizing systems can be conveniently formalized within the framework of statistical mechanics, their internal functioning remains opaque, because their emergent dynamics cannot be solved analytically. In this article, we propose to study DBNs using techniques commonly employed in the study of complex networks, in order to gain some insights into the structural and functional properties of the computational graph resulting from the learning process.","2051-1329","","10.1093/comnet/cnz018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113840","networks theory;artificial neural networks;deep belief networks;hierarchical generative models;machine learning;graph analysis","","","","","2","","","","10 Jun 2020","","","OUP","OUP Journals"
"Long Short-Term Memory Implementation Exploiting Passive RRAM Crossbar Array","H. Nikam; S. Satyam; S. Sahay","Department of Mechanical Engineering, IIT Kanpur, Kanpur, India; Department of Mechanical Engineering, IIT Kanpur, Kanpur, India; Department of Electrical Engineering, IIT Kanpur, Kanpur, India","IEEE Transactions on Electron Devices","24 Mar 2022","2022","69","4","1743","1751","The ever-increasing demand to extract temporal correlations across sequential data and perform context-based learning in this era of big data has led to the development of long short-term memory (LSTM) networks. Furthermore, there is an urgent need to perform these time-series data-dependent applications, including speech/video processing and recognition, and language modeling and translation, on compact Internet-of-Things (IoT) edge devices with limited energy. To this end, in this work, for the first time, we propose an extremely area- and energy-efficient LSTM network implementation exploiting the passive resistive random access memory (RRAM) crossbar array. We developed a hardware-aware LSTM network simulation framework and performed an extensive analysis of the proposed LSTM implementation considering the nonideal hardware artifacts such as spatial (device-to-device) and temporal variations, nonlinearity, and noise, utilizing an experimentally calibrated comprehensive phenomenological model for passive RRAM crossbar array. Our results indicate that the proposed passive RRAM crossbar-based LSTM network implementation not only outperforms the prior digital and active 1Transistor-1RRAM (1T-1R) crossbar-based LSTM implementations by more than three orders of magnitude in terms of area and two orders of magnitude in terms of training energy for identical network accuracy but also exhibits robustness against spatial and temporal variations and noise, and a faster convergence rate. Our work may provide the incentive for experimental realization of LSTM networks on passive RRAM crossbar arrays.","1557-9646","","10.1109/TED.2021.3133197","initiation grant(grant numbers:IITK/EE/2020058); from IIT Kanpur; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651522","In situ training;long short-term memory (LSTM);passive resistive random access memory (RRAM) crossbar;recurrent neural networks (RNNs)","Logic gates;Virtual machine monitors;Performance evaluation;Transistors;Training;Recurrent neural networks;Atmospheric modeling","Internet;Internet of Things;learning (artificial intelligence);random-access storage;recurrent neural nets;time series;video signal processing","long short-term memory implementation;passive RRAM crossbar array;sequential data;big data;short-term memory networks;time-series data-dependent applications;Internet-of-Things edge devices;energy-efficient LSTM network implementation;passive resistive random access memory crossbar array;hardware-aware LSTM network simulation framework;LSTM implementation;passive RRAM crossbar-based LSTM network implementation;prior digital Transistor-1RRAM;active 1Transistor-1RRAM;identical network accuracy;LSTM networks","","6","","42","IEEE","15 Dec 2021","","","IEEE","IEEE Journals"
"Application of sentence parsing for determining keywords in Ukrainian texts","L. Vasyl; V. Victoria; D. Dmytro; H. Roman; R. Zoriana","Information Systems and Network Department, Lviv Polytechnic National University, Lviv, Ukraine; Information Systems and Network Department, Lviv Polytechnic National University, Lviv, Ukraine; Information Systems and Network Department, Lviv Polytechnic National University, Lviv, Ukraine; Social Communication Department, Lviv Polytechnic National University, Lviv, Ukraine; Information Systems and Network Department, Lviv Polytechnic National University, Lviv, Ukraine","2017 12th International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT)","9 Nov 2017","2017","1","","326","331","The article presents the use of generative grammars in linguistic modeling. A description of sentence syntax modeling is used to automate the process of analysis and synthesis of natural language texts. The article reveals the features of synthesizing sentences of different languages with the use of generative grammars. The article examines influence of norms and rules of a language on the process of constructing grammars. The use of generative grammars has great potential in the development and creation of automated systems for text content processing, linguistic support for linguistic computer systems etc. In natural languages there are situations where notions, which are dependent on the context, are described as independent of context, i.e. in terms of context-free grammars. This description is complicated by the formation of new categories and rules. The article features the process of introducing new restrictions on these grammar classes through the introduction of new rules. Uncut grammars were received if the number of characters in the right part of the rules were not less than the number of characters in the left one. Then by replacing the only character a context-sensitive grammar was received. A grammar with only one character in the left part of the rule is called a context-free grammar. No further natural restrictions may be applied to the left part of a rule. Based on the importance of automatic processing of text content in modern information media (e.g., information retrieval systems, machine translation, semantic, statistical, optical and acoustic analysis and speech synthesis, automated editing, extracting knowledge from text content, abstracting and annotating text content, indexing text content, teaching and didactic, management of linguistic corpora, various tools for lexicography, etc.), specialists are actively looking for new models, ways of their description and methods of automatic processing of text content. One of such methods lies in developing general principles of syntactic lexicographical systems formation and developing mentioned systems of processing text content for specific languages based in these principles. Any parsing tools consist of two parts: a knowledge base of concrete natural language and parsing algorithm, i.e. a set of standard operators of text content processing based on this knowledge. The source of grammatical knowledge is data of morphological analysis and various tables filled with concepts and linguistic units. They are the result of an empirical study of the text content in natural language by experts aiming at highlighting the basic laws for parsing.","","978-1-5386-1639-0","10.1109/STC-CSIT.2017.8098797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8098797","text;Ukrainian language;algorithm;content monitoring;keywords;linguistic analysis;parsing;generative grammar;sentence structure scheme;linguistic information system","Grammar;Pragmatics;Natural languages;Information systems;Syntactics;Semantics;Analytical models","computational linguistics;context-free grammars;context-sensitive grammars;natural language processing;text analysis","Ukrainian texts;generative grammars;linguistic modeling;sentence syntax modeling;natural language texts;automated systems;text content processing;linguistic computer systems;context-free grammar;grammar classes;uncut grammars;context-sensitive grammar;automatic processing;acoustic analysis;speech synthesis;syntactic lexicographical systems formation;concrete natural language;sentence parsing;keyword determination;modern information media;grammatical knowledge;morphological analysis;linguistic units;text content abstraction;text content annotation;text content indexing","","7","","40","IEEE","9 Nov 2017","","","IEEE","IEEE Conferences"
"Sensory aids for the blind: A challenging problem with lessons for the future","P. W. Nye; J. C. Bliss","California Institute of Technology, Pasadena, CA, USA; Stanford Research Institute, University of Stanford, Stanford, CA, USA","Proceedings of the IEEE","28 Jun 2005","1970","58","12","1878","1898","The two major objectives of sensory aids for the blind are to permit access to printed matter and to permit safe travel through the environment. The difficulties of designing technological means to achieve these objectives are in many respects unrelated to the concerns of the engineering laboratory. Social, economic, political, and logistic considerations all play a role. The ""blind population"" in the United States includes both the totally blind and those with a wide range of visual impairment. This population totals about 400 000 people in which the aged, the multiply handicapped, and those with significant residual vision predominate. Singly handicapped, working-aged people are the initial targets of the current sensory aids. Expansion of their range of usefulness to larger fractions of the blind population is expected to come later. About 800 agencies serve the blind population in the United States, and in 1967 they were responsible for an annual expenditure of $500 million. In contrast the sensory-aids research and development budget was less than $1 million. Nevertheless, several potentially useful prototype devices have been developed and are about to be evaluated in this country; at least one is of foreign origin. But if these devices are ever to have the opportunity of reaching the blind public, then mechanisms for evaluation, field trials, manufacture, and deployment must be set up. The field of currently active sensory-aids research programs is reviewed. Several programs are concerned with increasing the convenience and accessibility of braille by the application of computer technology. Nevertheless, despite the unquestionable value of these developments, the usefulness of braille is limited by its bulk, its cost, and the transcription time. To provide direct access to printed documents several devices are being developed that transform optical images from a printed page into auditory or tactile displays requiring motivation and training for effective use. These machines are termed ""direct-translation"" units and are designed for simplicity and low cost. Other systems utilize print recognition techniques to create a reading machine providing braille or speech as an output. These machines offer potentially faster reading rates and their use promises to be easier to learn than direct-translation machines, but at the penalty of complexity and high cost. Several mobility aids designed to augment the cane or guide dog have recently been developed. These are also described. The prospects of achieving direct input to the visual cortex are discussed. It is apparent that the cost of this research is likely to be extremely high in relation to the size of the blind population which might ultimately benefit. Somewhat more easily realizable is a visual substitution system involving stimulation of an area of the skin. Several systems are being developed but all suffer from limitations in image resolution. Finally, an examination of the organization of research and funding reveals that the U.S. program is small, poorly coordinated, and contains some seemingly unnecessary duplication of effort. Several obvious lessons emerge which, if heeded, could greatly improve the effectiveness of sensory-aids research by providing development, manufacture, evaluation, and deployment services within an integrated program.","1558-2256","","10.1109/PROC.1970.8061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1449991","","Sensory aids;Costs;Manufacturing;Design engineering;Laboratories;Environmental economics;Logistics;Aging;Research and development;Prototypes","","","","39","2","107","IEEE","28 Jun 2005","","","IEEE","IEEE Journals"
"Keynote speakers: Benefits and drawbacks of the BigData era","G. Barzdins",University of Latvia,"2017 Advances in Wireless and Optical Communications (RTUWO)","21 Dec 2017","2017","","","xiii","xiv","We have voluntarily surrendered our private data to BigData companies like Google and FaceBook in hope that our data there will be safe and will be used only for ethical machine learning purposes to further advance artificial intelligence capabilities we already use daily: smart search, machine translation, speech recognition, guessing our interests etc. But alongside these positive BigData uses, unexpectedly the world was recently astounded by the success of the DataScience killer-application: microtargeting, discussed in this presentation.","","978-1-5386-0585-1","10.1109/RTUWO.2017.8228494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8228494","","Optical noise;Optical crosstalk;Jacobian matrices;Optical fiber networks;Optical transmitters;Integrated optics;Energy efficiency","","","","","","","IEEE","21 Dec 2017","","","IEEE","IEEE Conferences"
"The personality of humanoid robots","P. K√°d√°r","Dept. of Power Systems Alternative Energy Research Knowledge Center, √ìbuda University, Budapest, HUNGARY","2019 IEEE 23rd International Conference on Intelligent Engineering Systems (INES)","5 Jun 2020","2019","","","000213","000218","Humanoid Robots that can be our chat partners have recently appeared on the scene. In the first section of the paper we overview the existing techniques that bestow the robot many automatic smart skills, like image and speech recognition, speech synthesis, autonomous control functions, translation capability, sense of locality, follow up rule base.The next part discusses the simulation of the peccable human individual by inaccuracy, probability, playing by the game theory. The traditional machine learning uses preprogrammed structures and the database is broadened continuously. A simple reproduction algorithm can also be programmed. The notions of beauty and art are still only statistical concepts.In the third part the truly in grain innovation capability is treated, the generation of new structures that have not been solved, yet. We also make distinction between the sensors, senses and sensuality. We cannot talk about self-consciousness yet. The notions of transcendence, love, faith without previously stored algorithms have not emerged either.As a conclusion we can say that robots look perfectly human, but without human inference they do not possess natural self-awareness.","1543-9259","978-1-7281-1213-8","10.1109/INES46365.2019.9109505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109505","humanoid robots;algorithmic features;personality","","game theory;humanoid robots;learning (artificial intelligence);probability","game theory;reproduction algorithm;grain innovation capability;sensuality;humanoid robots;chat partners;rule base;peccable human individual;automatic smart skills;probability","","2","","10","IEEE","5 Jun 2020","","","IEEE","IEEE Conferences"
"Design and Implementation of LSTM Accelerator Based on FPGA","W. Zhang; F. Ge; C. Cui; Y. Yang; F. Zhou; N. Wu","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2020 IEEE 20th International Conference on Communication Technology (ICCT)","24 Dec 2020","2020","","","1675","1679","Long Short-Term Memory (LSTM) shows excellent performance in the fields of speech recognition, sentiment analysis, machine translation, etc. In this paper, we designed and implemented a LSTM accelerator based on FPGA. First, in order to reduce data movement and power consumption, a multi-level storage strategy is proposed. Next, we arrange multiple Processing Engine (PE) to realize the parallel operation of the matrix-vector multiplication module, and use the pipeline structure to improve performance. In the Element-wise module, time-division multiplexing is proposed to reduce hardware consumption. Finally, we use the Internet Movie Data Base (IMDB) dataset to train a small-scale LSTM neural network, and implemented our accelerator on Virtex-7 VC707 FPGA at 200MHz. Compared with existing works, the peak performance of accelerator can reach 10.9 GOP/s, which means our accelerator performance is better than existing work.","2576-7828","978-1-7281-8141-7","10.1109/ICCT50939.2020.9295665","Aeronautical Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9295665","LSTM;FPGA;accelerator","Field programmable gate arrays;Logic gates;Hardware;Acceleration;Biological neural networks;Adders;Recurrent neural networks","data reduction;field programmable gate arrays;learning (artificial intelligence);matrix multiplication;parallel processing;recurrent neural nets;storage management","LSTM accelerator;long short term memory;data movement reduction;power consumption reduction;multilevel storage strategy;parallel operation;matrix vector multiplication;time division multiplexing;Virtex-7 VC707 FPGA;LSTM neural network training;IMDB dataset;Internet Movie Data Base dataset;element wise module;multiple processing engine","","8","","16","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"A LSTM Neural Network applied to Mobile Robots Path Planning","F. Nicola; Y. Fujimoto; R. Oboe","Department of Electrical and Computer Engineering, Yokohama National University, Yokohama, Japan; Department of Electrical and Computer Engineering, Yokohama National University, Yokohama, Japan; Department of Engineering and Management, University of Padova, Vicenza, Italy","2018 IEEE 16th International Conference on Industrial Informatics (INDIN)","27 Sep 2018","2018","","","349","354","Mobile robots path planning is a central problem in every situation where human intervention is not desired or not possible to accept: full automated industrial warehouses or general stocking areas and every domestic application that involves a mobile robot and special cases where environment is prohibited for human accessing like toxic wastes and bombs defusing [1]. Currently, neural networks are applied to problems related to mobile robot navigation. However, they are not as popular as in applications like image processing, speech recognition or machine translation, where they are commercially relevant. In this paper we propose a Long Short-Term Memory (LSTM) neural network as an online search agent to tackle the problem of mobile robots path planning in unknown environments, meaning that the agent relies only on local map awareness realized with a LRF sensor and relative information between robot and goal position. Specifically, a final structure of LSTM network is analyzed and its performance is compared with the A* algorithm, a widely known method that follows the best-first search approach. Subsequently, an analysis of the method developed on a real robot is described.","2378-363X","978-1-5386-4829-2","10.1109/INDIN.2018.8472028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472028","Artificial Intelligence;LSTM;Path-Planning;Mobile Robots","Logic gates;Training;Path planning;Recurrent neural networks;Mobile robots;Analytical models","mobile robots;navigation;neurocontrollers;path planning;recurrent neural nets","mobile robots path planning;LSTM neural network;mobile robot navigation;Long Short-Term Memory neural network;local map awareness;LRF sensor","","8","1","16","IEEE","27 Sep 2018","","","IEEE","IEEE Conferences"
"Artificial intelligence research at IBM","J. Fan; M. Campbell; B. Kingsbury","IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA","IBM Journal of Research and Development","3 Oct 2011","2011","55","5","16:1","16:4","IBM has a history of achieving seminal results in artificial intelligence, such as the first machine translation program, the first competitive game-playing computer, and the first speech recognition system that addressed the general dictation problem. This paper describes the unique characteristics of IBM that enabled researchers to continuously achieve breakthroughs in artificial intelligence.","0018-8646","","10.1147/JRD.2011.2163282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032771","","Artificial intelligence;Research and development;Learning systems","","","","4","","","IBM","3 Oct 2011","","","IBM","IBM Journals"
"A Vietnamese language model based on Recurrent Neural Network","V. -T. Tran; K. -H. Nguyen; D. -H. Bui",Hanoi University of Science and Technology; Hanoi University of Science and Technology; Hanoi University of Science and Technology,"2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","1 Dec 2016","2016","","","274","278","Language modeling plays a critical role in many natural language processing (NLP) tasks such as text prediction, machine translation and speech recognition. Traditional statistical language models (e.g. n-gram models) can only offer words that have been seen before and can not capture long word context. Neural language model provides a promising solution to surpass this shortcoming of statistical language model. This paper investigates Recurrent Neural Networks (RNNs) language model for Vietnamese, at character and syllable-levels. Experiments were conducted on a large dataset of 24M syllables, constructed from 1,500 movie subtitles. The experimental results show that our RNN-based language models yield reasonable performance on the movie subtitle dataset. Concretely, our models outperform n-gram language models in term of perplexity score.","","978-1-4673-8929-7","10.1109/KSE.2016.7758066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758066","","Context;Motion pictures;Recurrent neural networks;Training;Context modeling;Predictive models","natural language processing;recurrent neural nets","Vietnamese language model;natural language processing tasks;NLP;statistical language models;natural language model;RNN-based language models;recurrent neural network language model;movie subtitle dataset","","3","","21","IEEE","1 Dec 2016","","","IEEE","IEEE Conferences"
"N-gram based text classification for Persian newspaper corpus","M. Farhoodi; A. Yari; A. Sayah","Iran Telecommunication Research Center, Iran; Iran Telecommunication Research Center, Iran; Iran Telecommunication Research Center, Iran","The 7th International Conference on Digital Content, Multimedia Technology and its Applications","12 Sep 2011","2011","","","55","59","Statistical n-gram language modeling is applied in many domains like speech recognition, language identification, machine translation, character recognition and topic classification. Most language modeling approaches work on n-grams of words. In this paper, we employ language models classifier based on word level n-grams for Persian text classification. The presented approach computes the occurrence probability on word sequence in training data. Then by extracting the word sequence in test data, it can predict the highest probability for related class to given news text. We show that statistical language modeling can significantly cause high classification performance. The experimental results on Hamshahri corpus show satisfactory results and n-grams of length 3 are the most useful for Persian text classification.","","978-89-88678-47-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016631","Persian text classification;N-gram;language modeling;Hamshahri courpus;Smoothing methods","Text categorization;Smoothing methods;Mathematical model;Training;Accuracy;Equations;Computational modeling","computational linguistics;pattern classification;probability;publishing;text analysis","n-gram based text classification;Persian newspaper corpus;statistical n-gram language modeling;language models classifier;Persian text classification;occurrence probability;word sequence extraction;Hamshahri corpus","","2","","23","","12 Sep 2011","","","IEEE","IEEE Conferences"
"MOSAIC: A Platform for Monitoring and Security Analytics in Public Clouds","A. Oprea; A. Turk; C. Nita-Rotaru; O. Krieger","CCIS, Northeastern University; ECE Department, Boston University; CCIS, Northeastern University; ECE Department, Boston University","2016 IEEE Cybersecurity Development (SecDev)","6 Feb 2017","2016","","","69","70","Public clouds have enabled a number of new computing-intensive applications (e.g., personalized medicine, real-time speech recognition and machine translation) that positively impact our daily lives. Compared to traditional computing environments, public clouds offer many economical advantages to both users and service providers. However, the shared, large-scale infrastructure of public clouds amplifies well-known security risks and introduces new security threats compared to traditional organizational networks or private clouds.","","978-1-5090-5589-0","10.1109/SecDev.2016.025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839794","","Cloud computing;Monitoring;Electronic mail;Engines;Computer security;Intrusion detection","cloud computing;data analysis;monitoring;risk management;security of data","MOSAIC;monitoring platform;security analytics;public cloud;security risk;security threat","","1","","16","IEEE","6 Feb 2017","","","IEEE","IEEE Conferences"
"Word Sense Disambiguation: A modified maximum entropy approach","G. Dhopavkar; M. Kshirsagar; L. Bopche","Yeshwantrao Chavan College of Engineering, Nagpur, Maharashtra, IN; G. H. Raisoni College of Engg. And Research, Nagpur, Maharashtra, India; Yeshwantrao Chavan College of Engineering, Nagpur, Maharashtra, IN","Confluence 2013: The Next Generation Information Technology Summit (4th International Conference)","16 Jun 2014","2013","","","110","114","Word Sense Disambiguation (WSD) is defined as the task of selecting a precise sense for a word in a sentence of a text from a set of predefined possibilities. WSD is very essential for applications like Information Retrieval, Machine Translation, Speech Related Problems. In this paper, we present new method called as modified maximum entropy approach for performing Word Sense Disambiguation of Natural Language text.","","978-1-84919-846-2","10.1049/cp.2013.2302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832317","WSD;Modified Maximum Entropy;Corpus","","natural language processing;text analysis","natural language text;WSD;modified maximum entropy approach;word sense disambiguation","","1","","","","16 Jun 2014","","","IET","IET Conferences"
"Analyzing various topic modeling approaches for Domain-Specific language model","D. K. Phull; G. B. Kumar","School of computing Science and Engineering, VIT University, Chennai, Tamil Nadu, India; VIT University, Vellore, Tamil Nadu, IN","2017 International Conference on Networks & Advances in Computational Technologies (NetACT)","23 Oct 2017","2017","","","69","73","In recent times, topic modeling approaches for adaptive language modeling have been extensively explored for Natural Language Processing applications such as machine translation, speech recognition etc. Language model is extremely fragile in adapting towards the required domain, so it needs to be channeled towards an area or a topic for producing optimal results. This paves the need to investigate various topic modeling approaches which are used to infer knowledge from a large corpora. In this paper, we mileage various topic modeling techniques which include Latent Semantic Indexing, Latent Dirichlet Allocation and Hierarchical Dirichlet Process. In this process, the baseline language model is dynamically adapted to different topics and the results are analyzed for these three topic modeling approaches.","","978-1-5090-6590-5","10.1109/NETACT.2017.8076743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076743","Domain Modeling;Topic Modeling;Language Modeling;Dynamically adapted","Adaptation models;Computational modeling;Mathematical model;Semantics;Analytical models;Indexing;Large scale integration","indexing;inference mechanisms;natural language processing;specification languages;statistical analysis;text analysis","topic modeling approaches;domain-specific language model;adaptive language modeling;baseline language model;natural language processing;knowledge inference;latent semantic indexing;latent Dirichlet allocation;hierarchical Dirichlet process","","","","24","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Depth Attention Net","H. -J. S. Kim; S. -M. Choi; S. Chi","ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea; ETRI (Electronics and Telecommunications Research Institute), Korea","2019 International Conference on Information and Communication Technology Convergence (ICTC)","27 Dec 2019","2019","","","1110","1112","Depth estimation has been achieved much attention in recent five years for visual SLAM, AR, VR, autonomous vehicle, 3D object understanding and so on. Similar to stereo matching analysis, stereo image based depth estimation methods using deep learning surprisingly obtain great improvement in performance but still cannot understand why learning depth can estimate depth in a new image and also require accuracy improvement. Therefore, we apply attention mechanism into depth learning. Attention mechanism achieve great improvement in various applications such as visual identification of objects, speech recognition, reasoning, image captioning, summarization, segmentation, machine translation(or NLP) and image classification etc. but yet depth estimation. This is because depth estimation is considered as geometric area. We apply attention mechanism in the PSMNet [1] method. Attention mechanism usually apply to channel attention. In our experiments, we apply our method into two benchmark dataset: KITTI [2] and SceneFlow [3]. In our experiments, we found the attention net can improve the quality of depth.","2162-1233","978-1-7281-0893-3","10.1109/ICTC46691.2019.8939688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939688","Depth;Attention;Deep Learning","Three-dimensional displays;Estimation;Mathematical model;Training;Visualization;Deep learning;Autonomous vehicles","image matching;learning (artificial intelligence);robot vision;SLAM (robots);stereo image processing","attention mechanism;depth attention net;3D object understanding;stereo matching analysis;stereo image based depth estimation methods;deep learning;learning depth;depth learning;image captioning","","","","7","IEEE","27 Dec 2019","","","IEEE","IEEE Conferences"
"IEEE Draft Standard for Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","",,"IEEE P3300/D2, July 2022","28 Jul 2022","2022","","","1","107","","","978-1-5044-8892-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844311","","Standards;IEEE Standards;Oral communication;Encoding;Artificial intelligence;Regulation;Warranties","","","","","","","","28 Jul 2022","","","IEEE","IEEE Standards"
"FPGA Acceleration of LSTM Based on Data for Test Flight","Z. Sun; Y. Zhu; Y. Zheng; H. Wu; Z. Cao; P. Xiong; J. Hou; T. Huang; Z. Que","School of Microelectronics, Shanghai Jiao Tong University; Chinese Academy of Sciences, Shanghai Advanced Research Institute; School of Microelectronics, Shanghai Jiao Tong University; School of Microelectronics, Shanghai Jiao Tong University; School of Microelectronics, Shanghai Jiao Tong University; School of Microelectronics, Shanghai Jiao Tong University; School of Microelectronics, Shanghai Jiao Tong University; Department of Physics, University of Cambridge; Department of Computing, Imperial College","2018 IEEE International Conference on Smart Cloud (SmartCloud)","28 Oct 2018","2018","","","1","6","Long Short-Term Memory Recurrent neural networks are generally used in speech recognition, machine translation and other fields. And LSTM-RNN also performs well in data anomaly detection. However, Due to the repeatability of LSTM-RNN, general-purpose processors such as CPU and GPGPU cannot efficiently implement LSTM-RNN, most of the existing model optimizations on FPGA are aimed at LSTM cells or large-scale model accelerations that do not require high accuracy (such as speech recognition). For the model of aircraft anomaly detection, which models with short data sampling intervals, high speed requirements and high precision requirements, the accuracy and speed of existing models are insufficient. Therefore, we proposed an FPGA-based LSTM-RNN accelerator to optimize the accuracy and speed of existing models. We achieve the optimization in the computation speed without sacrificing the accuracy, and balance performance and resources utilized in FPGA. The peak performance of our accelerator reaches 13.45 GOP/s, which is superior to other existing methods.","","978-1-5386-8000-1","10.1109/SmartCloud.2018.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513707","LSTM, RNN, FPGA, Deep Learning","Logic gates;Computational modeling;Field programmable gate arrays;Electronic mail;Recurrent neural networks;Microelectronics","aircraft;feature extraction;field programmable gate arrays;recurrent neural nets","Long Short-Term Memory Recurrent neural networks;data anomaly detection;general-purpose processors;aircraft anomaly detection;LSTM-RNN accelerator;FPGA acceleration;aircraft test flight data","","15","","17","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"Are Robots and AI the Future of the Media?","D. Augey; M. Alcaraz",NA; NA,"Digital Information Ecosystems: Smart Press","","2019","","","161","177","In the fall of 2016, Komoroid and Otonaroid, two presenters in Japan, surprised their audience. Seemingly human and feminine, the two ""journalists"" were in fact robots who came to read the day's headlines at a conference at the National Museum of Science and Technology in Tokyo. Artificial intelligence (AI) will play an important role in the future. AI consists of using complex computer algorithms that mix large amounts of data to produce reasoning or learning. Elisabeth Blankespoor and Christina Zhu, Stanford researchers, and Ed Dehaan, from the Foster School of Business at the University of Washington, offer an economic analysis of automatic journalism. Natural language processing uses computational techniques to learn, understand and produce content in human language. The first computational approaches to language research focused on automating the analysis of the linguistic structure of language, and developing basic technologies such as machine translation, speech recognition and speech synthesis.","","9781119579731","10.1002/9781119579717.ch8","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9822779.pdf&bkn=9820832&pdfType=chapter","","Robots;Artificial intelligence;Media;Journalism;Presses;Ecosystems;Databases","","","","","","","","12 Jul 2022","","","Wiley","Wiley Data and Cybersecurity eBook Chapters"
"Privacy-Preserving Collaborative Deep Learning With Unreliable Participants","L. Zhao; Q. Wang; Q. Zou; Y. Zhang; Y. Chen","School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Information Forensics and Security","20 Dec 2019","2020","15","","1486","1500","With powerful parallel computing GPUs and massive user data, neural-network-based deep learning can well exert its strong power in problem modeling and solving, and has archived great success in many applications such as image classification, speech recognition and machine translation etc. While deep learning has been increasingly popular, the problem of privacy leakage becomes more and more urgent. Given the fact that the training data may contain highly sensitive information, e.g., personal medical records, directly sharing them among the users (i.e., participants) or centrally storing them in one single location may pose a considerable threat to user privacy. In this paper, we present a practical privacy-preserving collaborative deep learning system that allows users to cooperatively build a collective deep learning model with data of all participants, without direct data sharing and central data storage. In our system, each participant trains a local model with their own data and only shares model parameters with the others. To further avoid potential privacy leakage from sharing model parameters, we use functional mechanism to perturb the objective function of the neural network in the training process to achieve  $\epsilon $ -differential privacy. In particular, for the first time, we consider the existence of unreliable participants, i.e., the participants with low-quality data, and propose a solution to reduce the impact of these participants while protecting their privacy. We evaluate the performance of our system on two well-known real-world datasets for regression and classification tasks. The results demonstrate that the proposed system is robust against unreliable participants, and achieves high accuracy close to the model trained in a traditional centralized manner while ensuring rigorous privacy protection.","1556-6021","","10.1109/TIFS.2019.2939713","National Natural Science Foundation of China(grant numbers:61822207,U1636219); Equipment Pre-Research Joint Fund of Ministry of Education of China (Youth Talent)(grant numbers:6141A02033327); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); National Natural Science Foundation of China(grant numbers:61872277,41571437); Natural Science Foundation of Hubei Province(grant numbers:2018CFB482); National Natural Science Foundation of China(grant numbers:61702380); Natural Science Foundation of Hubei Province(grant numbers:2017CFB134); Hubei Provincial Technological Innovation Special Funding Major Projects(grant numbers:2017AAA125); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825829","Collaborative learning;deep learning;privacy","Deep learning;Data models;Privacy;Differential privacy;Collaboration;Neural networks","data protection;learning (artificial intelligence);neural nets","classification tasks;regression tasks;parallel computing GPU;privacy protection;low-quality data;Œµ-differential privacy;training process;neural network;potential privacy leakage;collective deep learning model;privacy-preserving collaborative deep learning system;user privacy;personal medical records;neural-network-based deep learning;massive user data;unreliable participants","","91","","50","IEEE","5 Sep 2019","","","IEEE","IEEE Journals"
"Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures","E. Georganas; S. Avancha; K. Banerjee; D. Kalamkar; G. Henry; H. Pabst; A. Heinecke",Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation,"SC18: International Conference for High Performance Computing, Networking, Storage and Analysis","14 Mar 2019","2018","","","830","841","Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation and speech recognition. The computationally expensive nature of a convolution operation has led to the proliferation of implementations including matrix-matrix multiplication formulation, and direct convolution primarily targeting GPUs. In this paper, we introduce direct convolution kernels for √ó86 architectures, in particular for Xeon and Xeon Phi systems, which are implemented via a dynamic compilation approach. Our JIT-based implementation shows close to theoretical peak performance, depending on the setting and the CPU architecture at hand. We additionally demonstrate how these JIT-optimized kernels can be integrated into a light-weight multi-node graph execution model. This illustrates that single- and multi-node runs yield high efficiencies and high image-throughputs when executing state-of-the-art image recognition tasks on CPUs.","","978-1-5386-8384-2","10.1109/SC.2018.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665811","","Convolutional codes;Registers;Kernel;Computer architecture;Optimization;Lenses","convolutional neural nets;graph theory;learning (artificial intelligence);matrix multiplication;parallel processing;program compilers","direct convolution kernels;Xeon Phi systems;JIT-optimized kernels;high-performance deep learning convolutions;SIMD architectures;deep neural networks;matrix-matrix multiplication formulation;multinode graph execution model;convolutional neural networks;√ó86 architectures;dynamic compilation","","55","4","25","IEEE","14 Mar 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Sequence-to-Sequence Models","Y. Keneshloo; T. Shi; N. Ramakrishnan; C. K. Reddy","Department of Computer Science, Discovery Analytics Center, Virginia Tech, Arlington, USA; Department of Computer Science, Discovery Analytics Center, Virginia Tech, Arlington, USA; Department of Computer Science, Discovery Analytics Center, Virginia Tech, Arlington, USA; Department of Computer Science, Discovery Analytics Center, Virginia Tech, Arlington, USA","IEEE Transactions on Neural Networks and Learning Systems","6 Jul 2020","2020","31","7","2469","2489","In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks, such as machine translation, headline generation, text summarization, speech-to-text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these seq2seq models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with seq2seq models that enable remembering long-term memories. We present some of the most recent frameworks that combine the concepts from RL and deep neural networks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization and provide some targeted experiments for these RL models, both in terms of performance and training time.","2162-2388","","10.1109/TNNLS.2019.2929141","U.S. National Science Foundation(grant numbers:IIS-1619028,IIS-1707498,IIS-1838730,DGE-1545362,IIS-1633363); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8801910","Actor‚Äìcritic (AC) methods;deep learning;policy gradients (PGs);Q-learning;reinforcement learning (RL);sequence-to-sequence (seq2seq) learning","Training;Analytical models;Maximum likelihood decoding;Computational modeling;Learning systems;Reinforcement learning","decision making;learning (artificial intelligence);neural nets;text analysis","seq2seq models;RL models;sequence-to-sequence models;encoder-decoder models;pointer-generation models;text summarization;source code;decision making;deep reinforcement learning;deep neural network","","52","","183","IEEE","15 Aug 2019","","","IEEE","IEEE Journals"
"E-LSTM: An Efficient Hardware Architecture for Long Short-Term Memory","M. Wang; Z. Wang; J. Lu; J. Lin; Z. Wang","Nanjing University, Nanjing, Jiangsu, CN; Nanjing University, Nanjing, Jiangsu, CN; Nanjing University, Nanjing, Jiangsu, CN; Nanjing University, Nanjing, Jiangsu, CN; Nanjing University, Nanjing, Jiangsu, CN","IEEE Journal on Emerging and Selected Topics in Circuits and Systems","11 Jun 2019","2019","9","2","280","291","Long Short-Term Memory (LSTM) and its variants have been widely adopted in many sequential learning tasks, such as speech recognition and machine translation. Significant accuracy improvements can be achieved using complex LSTM model with a large memory requirement and high computational complexity, which is time-consuming and energy demanding. The low-latency and energy-efficiency requirements of the real-world applications make model compression and hardware acceleration for LSTM an urgent need. In this paper, several hardware-efficient network compression schemes are introduced first, including structured top- $k$  pruning, clipped gating, and multiplication-free quantization, to reduce the model size and the number of matrix operations by 32  $\times $  and 21.6  $\times $ , respectively, with negligible accuracy loss. Furthermore, efficient hardware architectures for accelerating the compressed LSTM are proposed, which support the inference of multi-layer and multiple time steps. The computation process is judiciously reorganized and the memory access pattern is well optimized, which alleviate the limited memory bandwidth bottleneck and enable higher throughput. Moreover, the parallel processing strategy is carefully designed to make full use of the sparsity introduced by pruning and clipped gating with high hardware utilization efficiency. Implemented on Intel Arria10 S $\times $ 660 FPGA running at 200MHz, the proposed design is able to achieve 1.4‚Äì2.2  $\times $  energy efficiency and requires significantly less hardware resources compared with the state-of-the-art LSTM implementations.","2156-3365","","10.1109/JETCAS.2019.2911739","National Natural Science Foundation of China(grant numbers:61774082,61604068); Fundamental Research Funds for the Central Universities(grant numbers:021014380065,021014380087); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693509","Hardware acceleration;long short-term memory (LSTM);model compression;recurrent neural network (RNN);deep learning;FPGA","Hardware;Computational modeling;Random access memory;Quantization (signal);Memory management;Microprocessors","computational complexity;data compression;field programmable gate arrays;learning (artificial intelligence);matrix algebra;optimisation;parallel processing;recurrent neural nets","e-LSTM;sequential learning tasks;complex LSTM model;memory requirement;time-consuming;energy demanding;energy-efficiency requirements;hardware acceleration;hardware-efficient network compression schemes;multiplication-free quantization;compressed LSTM;memory access pattern;memory bandwidth bottleneck;hardware resources;hardware architecture;long short-term memory;computational complexity;hardware utilization efficiency;LSTM implementations;FPGA;matrix operations;structured top-k pruning;clipped gating","","35","","27","IEEE","17 Apr 2019","","","IEEE","IEEE Journals"
"An Energy-Efficient and Noise-Tolerant Recurrent Neural Network Using Stochastic Computing","Y. Liu; L. Liu; F. Lombardi; J. Han","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Institute of Microelectronics Department, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","23 Aug 2019","2019","27","9","2213","2221","Recurrent neural networks (RNNs) are widely used to solve a large class of recognition problems, including prediction, machine translation, and speech recognition. The hardware implementation of RNNs is, however, challenging due to the high area and energy consumption of these networks. Recently, stochastic computing (SC) has been considered for implementing neural networks and reducing the hardware consumption. In this paper, we propose an energy-efficient and noise-tolerant long short-term memory-based RNN using SC. In this SC-RNN, a hybrid structure is developed by utilizing SC designs and binary circuits to improve the hardware efficiency without significant loss of accuracy. The area and energy consumption of the proposed design are between 1.6%-2.3% and 6.5%-11.2%, respectively, of a 32-bit floating-point (FP) implementation. The SC-RNN requires significantly smaller area and lower energy consumption in most cases compared to an 8-bit fixed point implementation. The proposed design achieves a higher noise tolerance compared to binary implementations. The inference accuracy is from 10% to 13% higher than an FP design when the noise level is high in the computation process.","1557-9999","","10.1109/TVLSI.2019.2920152","Natural Sciences and Engineering Research Council of Canada(grant numbers:RES0025211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746640","Energy efficient;long short-term memory (LSTM);noise tolerance;recurrent neural network (RNN);stochastic computing (SC)","Logic gates;Recurrent neural networks;Hardware;Neurons;Kernel;Energy consumption;Very large scale integration","electronic engineering computing;fault tolerant computing;fixed point arithmetic;floating point arithmetic;logic design;power aware computing;recurrent neural nets;stochastic processes","stochastic computing;SC-RNN;binary circuits;32-bit floating-point implementation;short-term memory-based RNN;SC designs;energy-efficiency recurrent neural network;noise-tolerant recurrent neural network;noise-tolerant RNN;energy-efficiency RNN","","26","","33","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"Major‚ÄìMinor Long Short-Term Memory for Word-Level Language Model","K. Shuang; R. Li; M. Gu; J. Loo; S. Su","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computing and Engineering, University of West London, London, U.K.; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","5 Oct 2020","2020","31","10","3932","3946","Language model (LM) plays an important role in natural language processing (NLP) systems, such as machine translation, speech recognition, learning token embeddings, natural language generation, and text classification. Recently, the multilayer long short-term memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling. For each LSTM layer, larger hidden size usually means more diverse semantic features, which enables the LM to perform better. However, we have observed that when a certain LSTM layer reaches a sufficiently large scale, the promotion of overall effect will slow down, as its hidden size increases. In this article, we analyze that an important factor leading to this phenomenon is the high correlation between the newly extended hidden states and the original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a small-scale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major‚ÄìMinor LSTMs (MMLSTMs). In experiments, we demonstrate the LM with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) data sets and outperforms the baseline by 3.3 points in perplexity on WikiText-103 data set without increasing model parameter counts.","2162-2388","","10.1109/TNNLS.2019.2947563","National Key Research and Development Program of China(grant numbers:2017YFB1400603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924934","Language model (LM);long short-term memory (LSTM);natural language processing (NLP);shortcut connections","Feature extraction;Semantics;Correlation;Natural language processing;Data models;Task analysis;Training","learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","major-minor long short-term memory;LM;multilayer long short-term memory models;LSTM hidden states;feature expression;semantic features;natural language processing systems;word-level language model","","9","","49","IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Self-Aware Neural Network Systems: A Survey and New Perspective","B. Z. Du; Q. Guo; Y. Zhao; T. Zhi; Y. Chen; Z. Xu","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","Proceedings of the IEEE","18 Jun 2020","2020","108","7","1047","1067","Neural network (NN) processors are specially designed to handle deep learning tasks by utilizing multilayer artificial NNs. They have been demonstrated to be useful in broad application fields such as image recognition, speech processing, machine translation, and scientific computing. Meanwhile, innovative self-aware techniques, whereby a system can dynamically react based on continuously sensed information from the execution environment, have attracted attention from both academia and industry. Actually, various self-aware techniques have been applied to NN systems to significantly improve the computational speed and energy efficiency. This article surveys state-of-the-art self-aware NN systems (SaNNSs), which can be achieved at different layers, that is, the architectural layer, the physical layer, and the circuit layer. At the architectural layer, SaNNS can be characterized from a data-centric perspective where different data properties (i.e., data value, data precision, dataflow, and data distribution) are exploited. At the physical layer, various parameters of physical implementation are considered. At the circuit layer, different logics and devices can be used for high efficiency. In fact, the self-awareness of existing SaNNS is still in a preliminary form. We propose a comprehensive SaNNS from a new perspective, that is, the model layer, to exploit more opportunities for high efficiency. The proposed system is called as MinMaxNN, which features model switching and elastic sparsity based on monitored information from the execution environment. The model switching mechanism implies that models (i.e., min and max model) dynamically switch given different inputs for both efficiency and accuracy. The elastic sparsity mechanism indicates that the sparsity of NNs can be dynamically adjusted in each layer for efficiency. The experimental results show that compared with traditional SaNNS, MinMaxNN can achieve 5.64√ó and 19.66% performance improvement and energy reduction, respectively, without notable loss of accuracy and negative effects on developers' productivity.","1558-2256","","10.1109/JPROC.2020.2977722","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1003101,2018AAA0103300,2017YFA0700900,2017YFA0700902,2017YFA0700901); NSF of China(grant numbers:61732007,61432016,61532016,61672491,61602441,61602446,61732002,61702478,61732020); Beijing Natural Science Foundation(grant numbers:JQ18013); National Science and Technology Major Project(grant numbers:2018ZX01031102); Transformation and Transfer of Scientific and Technological Achievements of Chinese Academy of Sciences(grant numbers:KFJ-HGZX-013); Key Research Projects in Frontier Science of Chinese Academy of Sciences(grant numbers:QYZDB-SSW-JSC001); Strategic Priority Research Program of Chinese Academy of Science(grant numbers:XDB32050200,XDC01020000); Standardization Research Project of Chinese Academy of Sciences(grant numbers:BZ201800001); Beijing Academy of Artificial Intelligence (BAAI) through the Beijing Nova Program of Science and Technology(grant numbers:Z191100001119093); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9045930","Self-aware neural network (NN) processors","Artificial neural networks;Neural networks;Self-aware;Monitoring;Sensors;Program processors;Logic gates","learning (artificial intelligence);neural nets","architectural layer;physical layer;circuit layer;data-centric perspective;comprehensive SaNNS;model layer;model switching mechanism;elastic sparsity mechanism;neural network processors;deep learning tasks;multilayer artificial NNs;self-aware NN systems;self-aware neural network systems;MinMaxNN","","6","","106","IEEE","24 Mar 2020","","","IEEE","IEEE Journals"
"An Energy Efficient Accelerator for Bidirectional Recurrent Neural Networks (BiRNNs) Using Hybrid-Iterative Compression With Error Sensitivity","G. Nan; Z. Wang; C. Wang; B. Wu; Z. Wang; W. Liu; F. Lombardi","College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronics and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, Northeastern University, Boston, Boston, MA, USA","IEEE Transactions on Circuits and Systems I: Regular Papers","10 Aug 2021","2021","68","9","3707","3718","Recurrent Neural Networks (RNNs) have been widely used in many sequential applications, such as machine translation, speech recognition and sentiment analysis. Long Term Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) are widely used variants of RNN due to their effectiveness in overcoming gradient vanishing and exploding problems; however, compared to conventional RNN, their massive storage and computation requirements hinder their application. In addition, the recurrent structure of RNNs makes them prone to accumulate errors, resulting in a severe loss of accuracy. In this work, we propose a hybrid-iterative compression (HIC) algorithm for LSTM/GRU. By exploiting the error sensitivity of RNN, the gating units are divided into error-sensitive and error-insensitive groups, that are compressed using different algorithms. By using this approach, a  $37.1\times /32.3\times $  compression ratio is achieved with negligible accuracy loss for LSTM/GRU. Further, an energy efficient accelerator for bidirectional RNNs is proposed. In this accelerator, the data flow of the matrix operation unit based on the block structure matrix (MOU-S) is improved through rearranging weights; the utilization of BRAM is improved through a fine-grained parallelism configuration of matrix-vector multiplications (MVMs). Meanwhile, the timing matching strategy alleviates the load-imbalance problem between MOU-S and the matrix operation unit based on top- $k$  pruning (MOU-P). When running at 200MHz on Xilinx ADM-PCIE-7V3 FPGA, the proposed design achieves an improvement in energy efficiency in a range of 5%-237% for LSTM networks, and an improvement of 58% for GRU networks compared with state-of-the-art designs.","1558-0806","","10.1109/TCSI.2021.3091318","National Natural Science Foundation of China(grant numbers:62022041,61871216); Six Talent Peaks Project in Jiangsu Province(grant numbers:2018- XYDXX-009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467293","Recurrent neural network (RNN);neural network compression;FPGAs","Logic gates;Sensitivity;Recurrent neural networks;Hardware;Matrix converters;Field programmable gate arrays;Compression algorithms","data compression;field programmable gate arrays;iterative methods;matrix algebra;power aware computing;recurrent neural nets","bidirectional recurrent neural networks;error sensitivity;sequential applications;long term short term memory;gated recurrent unit;gating units;energy efficient accelerator;bidirectional RNNs;matrix operation unit;block structure matrix;energy efficiency;LSTM networks;GRU networks;hybrid-iterative compression;Xilinx ADM-PCIE-7V3 FPGA;frequency 200.0 MHz","","4","","32","IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"Weakening the Dominant Role of Text: CMOSI Dataset and Multimodal Semantic Enhancement Network","C. Jin; C. Luo; M. Yan; G. Zhao; G. Zhang; S. Zhang","School of Information and Communication Engineering and the State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; School of Information and Communication Engineering and the State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; School of Information and Communication Engineering and the State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; School of Electrical and Information Engineering, Beijing University of Civil Engineering and Architecture, Beijing, China; Institute of Automation (IA), Chinese Academy of Sciences (CAS), Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","Multimodal sentiment analysis (MSA) is important for quickly and accurately understanding people‚Äôs attitudes and opinions about an event. However, existing sentiment analysis methods suffer from the dominant contribution of text modality in the dataset; this is called text dominance. In this context, we emphasize that weakening the dominant role of text modality is important for MSA tasks. To solve the above two problems, from the perspective of datasets, we first propose the Chinese multimodal opinion-level sentiment intensity (CMOSI) dataset. Three different versions of the dataset were constructed: manually proofreading subtitles, generating subtitles using machine speech transcription, and generating subtitles using human cross-language translation. The latter two versions radically weaken the dominant role of the textual model. We randomly collected 144 real videos from the Bilibili video site and manually edited 2557 clips containing emotions from them. From the perspective of network modeling, we propose a multimodal semantic enhancement network (MSEN) based on a multiheaded attention mechanism by taking advantage of the multiple versions of the CMOSI dataset. Experiments with our proposed CMOSI show that the network performs best with the text-unweakened version of the dataset. The loss of performance is minimal on both versions of the text-weakened dataset, indicating that our network can fully exploit the latent semantics in nontext patterns. In addition, we conducted model generalization experiments with MSEN on MOSI, MOSEI, and CH-SIMS datasets, and the results show that our approach is also very competitive and has good cross-language robustness.","2162-2388","","10.1109/TNNLS.2023.3282953","National Natural Science Foundation of China(grant numbers:62207029,62271454,62176018); National Key Research and Development Program of China(grant numbers:2018YFB1403903,2021YFF0900700); Beijing Natural Science Foundation(grant numbers:L223033); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154459","Chinese multimodal opinion-level sentiment intensity (CMOSI) dataset;multimodal multihead attention fusion;multimodal sentiment analysis (MSA);semantic enhancement","Task analysis;Feature extraction;Sentiment analysis;Semantics;Analytical models;Data models;Annotations","","","","1","","","IEEE","16 Jun 2023","","","IEEE","IEEE Early Access Articles"
"39 Managing Data in Sign Language Corpora","S. G. Thomason",NA,"The Open Handbook of Linguistic Data Management","","2021","","","463","470","With the arrival of video on desktop computers at the start of this century, the use of data in the study of signed languages saw substantial changes. Where before, few larger data sets were created (primarily for observational data in first-l anguage acquisition), from 2004 onward many research groups started constructing sign language corpora (Crasborn et al. 2007; Johnston 2009). These corpora were virtually all elicited, rather than harvested from external sources, as such sources simply did not exist in sufficient quantities. Even today, it is debatable whether YouTube and similar online platforms contain a large enough collection of the use of a particular sign language from which a balanced sample could be created (Crasborn & van Winsum 2014). In addition, the available metadata are often limited and legal and ethical issues also make it difficult to actually use such data for research purposes. An exception is the TV-recorded weather reports that were interpreted from spoken German to German Sign Language (Deutsche Geb√§rdensprache, or DGS), and collected in a corpus by Rheinisch-Westf√§lische Technische Hochschule Aachen for the purpose of developing sign language technologies (Bungeroth et al. 2006). Such a limited domain of language use has its advantages for developing automatic sign recognition and machine translation of sign to speech. Because the topic of language use is constrained, the ""type-token ratio"" of signs will be better (fewer signs occurring only once, for instance).","","9780262366076","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9740289.pdf&bkn=9740254&pdfType=chapter","","","","","","","","","","23 Mar 2022","","","MIT Press","MIT Press eBook Chapters"
"4 Indigenous Peoples, Ethics, and Linguistic Data","S. G. Thomason",NA,"The Open Handbook of Linguistic Data Management","","2021","","","49","60","The world is dominated by just a few large languages that mediate mass communication, social media, education, politics, and many other domains. A study by Kornai (2013) found just sixteen of the world's nearly seven thousand languages to be ""digitally thriving,"" with a firmly established online presence and the tools necessary to live and interact in an increasingly digitally connected world. These sixteen languages are spoken natively by some 2.8 billion people, or nearly 40% of the world's population. These are the languages of Big Data, machine translation, automated speech recognition‚Äî the ones that technology companies care most about. For these languages, ethical protocols are largely driven by commercial interests and entail regional legal structures pertaining to data governance.<superscript>1</superscript> But the vast majority of the world's linguistic diversity is found elsewhere: namely, within the thousands of minority languages, many of which belong to small, often politically and economically marginalized Indigenous groups.<superscript>2</superscript> Data from these small and often critically endangered languages are key for understanding linguistic diversity‚Äîa major focus of linguistic science‚Äîbut also for maintaining that diversity through language maintenance and reclamation efforts. Linguistic research on Indigenous minority languages takes place against a backdrop of increasing threats to Indigenous language vitality and pressures to shift away from Indigenous languages toward languages of wider communication‚Äîoften colonial languages (e.g., English, Spanish, Mandarin). We emphasize that language endangerment, along with the response by various stakeholders such as linguists, archivists, and especially the communities these languages come from, is central to the discussion of Indigenous peoples, ethics, and linguistic data. While the causes of language endangerment are many and complex, social and cultural dislocation due to unequal power relations between minority communities and majority populations have played major roles in facilitating language shift (Grenoble 2011). Traditional models of linguistic research often mirror these unequal power relationships (Leonard 2018), with the result that linguists researching Indigenous languages may be seen as agents of social and cultural dislocation as well. Moreover, Indigenous communities may view linguistic research as out of step with the impending threat of language loss. In particular, communities experiencing rapid language shift and consequent language endangerment may take a more holistic view of language research as being embedded within a process of language reclamation (cf. Leonard 2017) and psychological healing (cf. Meek 2010; Jacob 2013) against a backdrop of numerous ethical violations that underlie language shift. Hence, any discussion of ethics in linguistic data requires a discussion of Indigenous data and must adhere to protocols for working with Indigenous data, as well as to the broader sociopolitical contexts in which language work takes place. However, where formal, legal frameworks do exist governing Indigenous and minority language data, these frameworks tend to be modeled on those developed for large languages rather than the cultural values or political concerns of Indigenous populations. We thus focus in this chapter on ethical issues in relation to Indigenous languages and the communities they come from, for it is in this context that the intersection of people, ethics, and data has been least formalized, despite its significant implications.","","9780262366076","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9740256.pdf&bkn=9740254&pdfType=chapter","","","","","","","","","","23 Mar 2022","","","MIT Press","MIT Press eBook Chapters"
"Keynote speech 3: Toward simultaneous, natural and multimodal speech-to-speech translation","S. Nakamura","Nara Sentan Kagaku Gijutsu Daigakuin Daigaku, Ikoma, Nara, JP","2015 International Conference Oriental COCOSDA held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)","17 Dec 2015","2015","","","1","2","Spoken language technologies enable to support natural oral communication. Language barrier between different language speaking people is one of the biggest communication problems for human being. Speech-to-speech translation had been studied in order to break the language barrier. In this talk, I would like to introduce brief history of the speech-to-speech translation research and state-of-the-art speech translation system. Then I would introduce new challenges toward a simultaneous speech-to-speech translation including simultaneous incremental machine translation, joint-optimization of ASR and MT modules, Speech-to-speech translation preserving para/non-linguistic information and multimodal speech translation with a talking face.","","978-1-4673-8279-3","10.1109/ICSDA.2015.7357853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357853","","","language translation;natural language processing;speech processing","speech-to-speech translation;spoken language technology;natural oral communication;language barrier;language speaking people;speech translation system;incremental machine translation;joint-optimization;ASR module;MT module;para/nonlinguistic information;multimodal speech translation","","","","","IEEE","17 Dec 2015","","","IEEE","IEEE Conferences"
"Invited Speech 2 : An Overview Of Machine Translation","A. P. Wibawa","Universitas Negeri Malang, Indonesia","2019 2nd International Conference of Computer and Informatics Engineering (IC2IE)","27 Dec 2019","2019","","","1","1","The global communication can be easily performed due to this linguist assistance. However, driving the translator to stay around is almost impossible. Thus, a technology support is needed to answer the problem. Machine translation (MT) could be the solution. It is a branch of machine learning, focused on computation of translating the source language into target language. The first type, rule-based MT (RBMT) has been evolved to various systems such as example based MT (EBMT), statistical MT (SMT), and Neural MT (NMT). Now, it is widely used as pedagogical tools, especially for who want to study the four foundational skills of foreign language learning. Its translation result is always refined, yet it is never beat the human translator.","","978-1-7281-2384-4","10.1109/IC2IE47452.2019.8940876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940876","","","language translation;learning (artificial intelligence);natural language processing","example based MT;foreign language learning;Neural MT;statistical MT;rule-based MT;machine learning;linguist assistance;machine translation","","","","","IEEE","27 Dec 2019","","","IEEE","IEEE Conferences"
"Keynote speech: FPGA-based machine learning for prognostics and system health management","P. Leong",University of Sydney,"2017 Prognostics and System Health Management Conference (PHM-Harbin)","23 Oct 2017","2017","","","1","1","Machine learning has improved to a point where it can achieve near-human accuracy on difficult tasks such as image recognition, speech recognition, and machine translation. However, efficient implementation of machine learning algorithms, particularly for real-time applications remains a challenge. This presentation will first detail the improved Energy, Parallelism, Interface and Customisation (EPIC) opportunities offered by FPGAs over conventional technologies such as microprocessors and graphics processing units (GPUs). These enable systems with smaller footprint, operating at low power and achieving improved functionality. Next, our recent research on FPGA-based implementations of machine learning algorithms will be described. This work includes highspeed and low-latency implementations of kernel adaptive filters, random projections and binarized convolutional neural networks. The talk will conclude with a discussion of applications of this technology to prognostics and system health management.","2166-5656","978-1-5386-0370-3","10.1109/PHM.2017.8079103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079103","","","fault diagnosis;field programmable gate arrays;learning (artificial intelligence)","FPGA;machine learning algorithms;prognostics;system health management;energy parallelism interface and customisation;EPIC opportunities;kernel adaptive filters;random projections;binarized convolutional neural networks;field programmable gate array","","","","","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Guest Editorial: Special Issue on Affective Speech and Language Synthesis, Generation, and Conversion","S. Amiriparian; B. W. Schuller; N. Asghar; H. Zen; F. Burkhardt","Embedded Intelligence for Health Care & Wellbeing, University of Augsburg, Augsburg, Germany; Embedded Intelligence for Health Care & Wellbeing, University of Augsburg, Augsburg, Germany; Microsoft, Redmond, WA, USA; Google Research, Tokyo, Japan; Speech Communication, Technical University of Berlin, Berlin, Germany","IEEE Transactions on Affective Computing","28 Feb 2023","2023","14","1","3","5","The papers in this special section focus on affective speech and language synthesis, generation, and conversion. As an inseparable and crucial part of spoken language, emotions play a substantial role in human-human and human-technology conversation. They convey information about a person‚Äôs needs, how one feels about the objectives of a conversation, the trustworthiness of one‚Äôs verbal communication, and more. Accordingly, substantial efforts have been made to generate affective text and speech for conversational AI, artificial storytelling, and machine translation. Similarly, there is a push for converting the affect in text and speech, ideally, in real-time and fully preserving intelligibility, e. g., to hide one‚Äôs emotion, for creative applications and in entertainment, or even to augment training data for affect analyzing AI.","1949-3045","","10.1109/TAFFC.2022.3233120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056373","Emotion synthesis;emotion conversion;human-AI interaction;empathic AI;affective computing","Special issues and sections;Affective computing;Speech recognition;Oral communication;Emotion recognition;Real-time systems;Natural language processing;Human computer interaction;Artificial intelligence;Trust computing;Training data","","","","","","3","IEEE","28 Feb 2023","","","IEEE","IEEE Journals"
"Introduction to the Special Issue on Deep Learning for Multi-Modal Intelligence Across Speech, Language, Vision, and Heterogeneous Signals","X. He; L. Deng; R. Rose; M. Huang; I. Trancoso; C. Zhang",NA; NA; NA; NA; NA; NA,"IEEE Journal of Selected Topics in Signal Processing","25 Jun 2020","2020","14","3","474","477","The ten papers included in this special section focus on deep learning for multi-modal intelligence across speech, language, vision, and heterogeneous signals. Thanks to the disruptive advances in deep learning, significant progress has been made in artificial intelligence (AI) applications with single modality, such as speech recognition, speech synthesis, image classification, object detection, as well as machine translation and reading comprehension, etc. However, many AI problems require more than one modality, and techniques developed for different modalities can often be successfully cross-fertilized. Therefore, the studies on the modeling and learning approaches across multiple modalities are of great interest. This special issue brings together a diverse but complementary set of contributions on emerging deep learning methods for problems based on multiple modalities including speech, text, image and video.","1941-0484","","10.1109/JSTSP.2020.2989852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9126279","","Special issues and sections;Deep learning;Speech recognition;Visualization;Speech enhancement;Image processing","","","","2","","0","IEEE","25 Jun 2020","","","IEEE","IEEE Journals"
"Vietnam country report 2016: Updated activities on resources development for Vietnamese Speech and NLP","L. C. Mai","Institute of Information Technology, Vietnam Academy of Science and Technology","2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA)","4 May 2017","2016","","","1","6","‚ñ™ Till now, there are 11 different corpora, including speech synthesis/recognition, corpora for machine translation, lexicon and other NLP Tools. ‚ñ™ New TTS Corpus 2016 ‚Äî Adding 2 speakers from Broadcast News (1 man and 1 woman)to existed TTS corpus ‚Äî Text selection: 6000 text selected sentences to cover around 5000 most frequent used Vietnamese Word ‚Äî Develop new TTS ‚Äî service oriented, easy to adapt with new voice. ‚ñ™ Develop web-based tool to record new voices for further speech corpus development.","2472-7695","978-1-5090-3516-8","10.1109/ICSDA.2016.7919033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919033","","","","","","","","","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Deep Learning and Machine Learning in Robotics [From the Guest Editors]","F. Bonsignorio; D. Hsu; M. Johnson-Roberson; J. Kober","Heron Robots, Genoa, Italy; National University of Singapore; University of Michigan, Ann Arbor, United States; Delft University of Technology, The Netherlands","IEEE Robotics & Automation Magazine","10 Jun 2020","2020","27","2","20","21","Deep learning has gone through massive growth in recent years. In many fields‚Äîcomputer vision, speech recognition, machine translation, game playing, and others‚Äîdeep learning has brought unprecedented progress and become the method of choice. Will the same happen in robotics and automation? In a sense, it is already happening. Today, deep learning is often the most common keyword for work presented at major robotics conferences. At the same time, robots, as physical systems, pose unique challenges for deep learning in terms of sample efficiency and safety in real-world robot applications. With robots, data are abundant, but labels are sparse and expensive to acquire. Reinforcement learning in principle does not require data labeling but does require a significant number of iterations on real robots. Transferring the capabilities learned in simulation to real robots and collecting sufficient data for practical robot applications both present major challenges. Further, mistakes by robot learning systems are often much more costly than those by their counterparts in the virtual world. These mistakes may cause irreversible damage to robot hardware or, even worse, loss of human lives. Safety is thus paramount for robot learning systems. ","1558-223X","","10.1109/MRA.2020.2984470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113363","","Special issues and sections;Deep learning;Machine learning;Robot sensing systems;Safety;Task analysis","","","","1","","0","IEEE","10 Jun 2020","","","IEEE","IEEE Magazines"
"Keynote - AI for the Public Sector and the Case of Legal NLP","M. St√ºrmer","**, **, **, **","2023 Ninth International Conference on eDemocracy & eGovernment (ICEDEG)","15 May 2023","2023","","","1","2","Recent innovations such as ChatGPT have increased public interest in artificial intelligence (AI). The keynote explained why AI is not just a short-term hype but has a long history of spanning several eras. A recent revolution has been in the field of Natural Language Processing (NLP). This interdisciplinary field of research is also known as computational linguistics. It is usually implemented by specific NLP tasks, ranging from simple processing steps such as tokenization, stemming, lemmatization to Part of Speech (PoS) tagging and topic modeling. A second, more complex set of NLP tasks includes Namend Entity Recognition (NER), information retrieval, relationship extraction, sentiment analysis, text similarity, and coreference resolution. Finally, the most challenging NLP tasks are considered Question Answering (QA), text summarization, text simplification, text generation, text translation, and chatbots. NLP has especially great potential in the public sector. For example, a new multilingual legal language model for more than 20 languages, developed for the Swiss Federal Court, offers opportunities to increase accessibility of legal documents for citizens while preserving the digital sovereignty of government institutions. These technical results of the National Research Program (NRP) 77 project ‚ÄúOpen Justice versus Privacy‚Äù are published on Hugging Face, a platform for sharing openly available machine learning models and datasets. Today, it is mostly private companies that build such Large Language Models (LLM), because it requires a large amount of computational resources and highly skilled engineers. For example, to train the new LLaMA model, Meta AI (Facebook) needed more than $30 million worth of graphical processing units (GPU). In addition, 450 MWh of electricity worth about $90,000 was needed to process the data on these GPUs. Negative for innovation and the environment, Meta AI released the LLaMA model only under a non-commercial license. This means that startups and other companies cannot use the model for their own services. This calls for a discussion about how ‚Äúopen‚Äù today's machine learning models should be and what ‚Äúopen‚Äù actually means in the age of AI. The keynote presentation therefore included a proposal of 5 elements of such machine learning models that need to be openly available and licensed under an official open license in order to speak of an Open AI Model. This term is used by the United Nations definition of Digital Public Goods. These five elements include 1) model architecture (detailed scientific publications), 2) hyperparameters (built configuration), 3) training data (labeled and unlabeled datasets), 4) model weights and intermediate checkpoints (parameters), and 5) source code to build the model (programming scripts etc.). A truly openly available AI model is BLOOM, an LLM from the BigScience initiative. It was built by more than 1000 researchers from over 70 countries, trained on an infrastructure that would have cost EUR 3 million. BLOOM was released on July 12th, 2022 on Hugging Face and is licensed under the Responsible AI License (RAIL), a new type of AI license that incorporates ethical aspects while preserving the openness of the machine learning elements described.","2573-1998","979-8-3503-2450-1","10.1109/ICEDEG58167.2023.10122084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122084","","","chatbots;computational linguistics;graphics processing units;law;learning (artificial intelligence);natural language processing;question answering (information retrieval);sentiment analysis;social networking (online);text analysis","AI license;artificial intelligence;chatbots;ChatGPT;computational linguistics;computational resources;digital public goods;graphical processing units;Hugging Face;information retrieval;large language models;legal documents;legal NLP;LLaMA model;machine learning;Meta AI;multilingual legal language model;namend entity recognition;National Research Program 77 project;natural language processing;noncommercial license;official open license;Open AI Model;public sector;question answering;relationship extraction;scientific publications;Swiss Federal Court;text generation;text similarity;text simplification;text summarization;text translation;topic modeling;United Nations definition","","","","","IEEE","15 May 2023","","","IEEE","IEEE Conferences"
"Keynote: The computer science behind the Microsoft Cognitive Toolkit: An open source large-scale deep learning toolkit for Windows and Linux","F. Seide","Microsoft Research, Redmond, WA, US","2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","28 Feb 2017","2017","","","xi","xi","Deep Learning is redefining computing. Deep Neural Networks, or DNNs, have led to breakthrough accuracy improvements for tasks formerly considered AI, like speech recognition, image classification, and translation. Recurrent DNNs are differentiable universal computers. DNNs are layered structures of relatively simple functions with millions to billions of learnable model parameters. The challenge is that these model parameters are obtained by machine learning of sometimes billions of data samples, which often requires harnessing a farm of powerful multi-GPU servers. Microsoft's open source Cognitive Toolkit (CNTK) is used to create the DNNs that power many Microsoft services and products. It enables researchers and data scientists to easily code such neural networks at the right abstraction level, and to efficiently train and test them on production-scale data. This talk will discuss the Cognitive Toolkit and how it takes a functional-style, differentiable user program, compiles it into a computation graph for GPU execution, and distributes execution of its training across a GPU-server farm. This talk will explain how the toolkit's design intersects with several topics of interest to the three co-located conferences.","","978-1-5090-4931-8","10.1109/CGO.2017.7863722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863722","","","computer science;graphics processing units;learning (artificial intelligence);Linux;neural nets;public domain software","computer science;Microsoft cognitive toolkit;open source large-scale deep learning toolkit;windows;Linux;deep neural networks;DNN;image classification;speech recognition;universal computers;learnable model parameters;machine learning;Microsoft open source cognitive toolkit;CNTK;data scientists;production scale data;cognitive toolkit;differentiable user program;GPU-server farm","","10","","","IEEE","28 Feb 2017","","","IEEE","IEEE Conferences"
"Invited Talk #2 Vietnamese Neural Language Model for NLP Tasks With Limited Resources","Q. T. Tho","Vietnam National University, Ho Chi Minh, Vietnam","2018 5th NAFOSTED Conference on Information and Computer Science (NICS)","10 Jan 2019","2018","","","xxviii","xxviii","A statistical language model is a probability distribution over sequences of words. Language modeling is used in various computing tasks such as speech recognition, machine translation, optical character and handwriting recognition and information retrieval and other applications. Whereas n-gram is considered as a traditional language model, neural language model has been emerging recently as a means to approximate the probability of a sentence using neural networks and word embeddings. An advantage of a neural language model is that it can be further applied to other NLP tasks where the training datasets may be limited. In this talk, we realize this idea by introducing the usage of a Vietnamese neural model language trained from a large corpus of social media data. When further applying this neural model language with other NLP tasks including entity recognition, spam detection and topic modeling with relatively small training datasets; we witness improved performance achieved, as compared to other existing approaches using deep learning with typical word embedding techniques.","","978-1-5386-7983-8","10.1109/NICS.2018.8606865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606865","","Computational modeling;Task analysis;Urban areas;Training;Computer science;Probability distribution;Speech recognition","natural language processing;neural nets;statistical distributions","word embeddings;topic modeling;spam detection;NLP tasks;Vietnamese neural language model;entity recognition;neural networks;statistical language model","","","","0","IEEE","10 Jan 2019","","","IEEE","IEEE Conferences"
"IEEE Draft Standard - Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","",,"IEEE Draft Standard - Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","","","","","","","Multimodal Conversation (MPAI-MMC) is an MPAI Standard comprising five Use Cases, all sharing the use of artificial intelligence (AI) to enable a form of human-machine conversation that emulates human-human conversation in completeness and intensity: 1. √¢¬ø¬øConversation with Emotion√¢¬ø¬ù (CWE), supporting audio-visual conversation with a machine impersonated by a synthetic voice and an animated face. 2. √¢¬ø¬øMultimodal Question Answering√¢¬ø¬ù (MQA), supporting request for information about a displayed object. 3. Three Uses Cases supporting conversational translation applications. In each Use Case, users can specify whether speech or text is used as input and, if it is speech, whether their speech features are preserved in the interpreted speech: a. √¢¬ø¬øUnidirectional Speech Translation√¢¬ø¬ù (UST). b. √¢¬ø¬øBidirectional Speech Translation√¢¬ø¬ù (BST). c. √¢¬ø¬øOne-to-Many Speech Translation√¢¬ø¬ù (MST).","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=STDUD25707","","","","","","","","","","17 Oct 2022","","",""," Standards"
"IEEE Draft Standard for Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","",,"IEEE Draft Standard for Adoption of Moving Picture, Audio and Data Coding by Artificial Intelligence (MPAI) Technical Specification Multimodal Conversation Version 1.2","","","","","","","Multimodal Conversation (MPAI-MMC) is an MPAI Standard comprising five Use Cases, all sharing the use of artificial intelligence (AI) to enable a form of human-machine conversation that emulates human-human conversation in completeness and intensity: 1. √¢¬ø¬øConversation with Emotion√¢¬ø¬ù (CWE), supporting audio-visual conversation with a machine impersonated by a synthetic voice and an animated face. 2. √¢¬ø¬øMultimodal Question Answering√¢¬ø¬ù (MQA), supporting request for information about a displayed object. 3. Three Uses Cases supporting conversational translation applications. In each Use Case, users can specify whether speech or text is used as input and, if it is speech, whether their speech features are preserved in the interpreted speech: a. √¢¬ø¬øUnidirectional Speech Translation√¢¬ø¬ù (UST). b. √¢¬ø¬øBidirectional Speech Translation√¢¬ø¬ù (BST). c. √¢¬ø¬øOne-to-Many Speech Translation√¢¬ø¬ù (MST).","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=STDUD25550","","","","","","","","","","21 Jul 2022","","",""," Standards"
"Pervasive Data Science and AI","N. Davies; N. D. Lane; M. Musolesi",Lancaster University; University of OxfordSamsung AI; University College London,"IEEE Pervasive Computing","27 Nov 2019","2019","18","3","7","8","Recent years have seen an explosion in the use of data science and AI as a central tenant in numerous computing applications, products, research, and innovation. Examples of the success of data science abound‚Äîapplying new machine-learning techniques to problems such as vision and speech recognition and translation has achieved commonplace levels of performance that would have seemed impossible a few years ago. In parallel, developments in pervasive computing increasingly enable us to instrument our physical environment with complex sensors and actuators and create an interconnected world that generates huge volumes of data. The importance of these trends can be seen in the growing momentum of exemplars such as the Internet of Things (IoT), smart environments, and augmented cognition. Pervasive data science is characterized by a focus on the collection, analysis (inference), and use of data (actuation) in pursuit of the vision of ubiquitous computing1 and raises multiple new challenges, demanding new approaches to how we capture, process, and use data in pervasive environments. Beyond the hype, it is clear that our world is becoming increasingly data centric, in which both physical and electronic services depend on the collection, analysis, and application of large volumes of heterogeneous data. In this Special Issue, we present a series of articles that cover different aspects of the exciting work that is currently carried out in this area.","1558-2590","","10.1109/MPRV.2019.2944289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915867","","Special issues and sections;Artificial intelligence;Machine learning;Data science;Sensors;Computational modeling;Predictive models","","","","","","1","IEEE","27 Nov 2019","","","IEEE","IEEE Magazines"
"Plenary talks: Frontiers in recurrent neural network research","A. Graves",Google DeepMind,"2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","21","33","In the last few years, recurrent neural networks (RNNs) have become the Swiss Army knife of large-scale sequence processing. Problems involving long and complex data streams, such as speech recognition, machine translation and reinforcement learning from raw video, are now routinely tackled with RNNs. This talk takes a look at some of the new architectures, applications and training strategies currently being developed in this exciting field.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7965820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965820","","","","","","","","","IEEE","3 Jul 2017","","","IEEE","IEEE Conferences"
"Keynote talk ‚Äî 2: Developing language resources for under-resourced languages in Indonesia","T. Suhardijanto","Department of Linguistics, Faculty of Humanities, University of Indonesia","2016 International Conference on Knowledge Creation and Intelligent Computing (KCIC)","23 Mar 2017","2016","","","23","23","Language resources plays a key role in human language technology especially NLP and computational linguistic works such as machine translation, sentiment analysis, automatic sound recognition, and text-tospeech. Many NLP applications has been built based on existing language resources. Over the past decade, major languages such as English, French, and German have experienced a significant progress and have a tremendous language resources for building application, linguistic research, and future use. Unlike these major languages, many other languages of the world are still neglected from the perspectives of language engineer. As a result, it is difficult to find any existing language resources in minor languages including languages og Indonesia. With regard to Indonesia, it has 742 languages that most of them are under-resourced languages. In this paper, we present an analytical review of existing problems and approaches in our effort to develop language resources for many languages lacking speech and text resources in Indonesia.","","978-1-5090-5231-8","10.1109/KCIC.2016.7883614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883614","","","","","","","","","IEEE","23 Mar 2017","","","IEEE","IEEE Conferences"
"Special Topic on Nonvolatile Memory for Efficient Implementation of Neural/Neuromorphic Computing","S. Yu","Georgia Institute of Technology Atlanta, GA, USA","IEEE Journal on Exploratory Solid-State Computational Devices and Circuits","10 Jun 2019","2019","5","1","ii","iii","In recent years, artificial intelligence (AI) based on machine/deep learning has shown significantly improved accuracy in large-scale visual/auditory recognition and classification tasks, some even surpassing human-level accuracy. In particular, deep neural networks (DNNs) and their variants have proved their efficacy in a wide range of image, video, speech, and biomedical applications. To achieve incremental accuracy improvement, state-of-the-art deep learning algorithms tend to aggressively increase the depth and size of the network, which imposes ever-increasing computational capacity and storage cost in hardware. Although graphic processing units (GPUs) are the dominant technology in the training of the DNN models at the cloud, application-specific integrated circuit (ASIC) hardware accelerators are being developed to run large-scale deep learning algorithms for inference (or even training) on-chip. This provides opportunities to bring the AI closer to the edge device for applications such as autonomous driving, machine translation, and smart wearable devices, where severe constraints exist in performance, power, and area.","2329-9231","","10.1109/JXCDC.2019.2913526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732946","","Special issues and sections;Nonvolatile memory;Random access memory;Neural networks;Synapses;Neurons;Spintronics;Neuromorphic engineering","","","","","","0","OAPA","10 Jun 2019","","","IEEE","IEEE Journals"
"Statistical machine translation gains respect","D. Geer",NA,"Computer","10 Oct 2005","2005","38","10","18","21","Relatively few researchers have worked on approaches that compare and analyze documents and their already-available translations to determine statistically, without prior linguistic knowledge, the likely meanings of phrases. These statistical systems use this information to translate new documents. For years, because processors were not fast enough to handle the extensive computation these systems require, many experts considered statistical systems inferior to rule-based systems. However, when the Speech Group of the US National Institute of Standards and Technology's Information Access Division tested 20 machine translation technologies, a statistical system developed by Google finished in first place. The NIST test results' significance is that Google and other organizations will invest more time, money, and talent into researching this approach. Meanwhile, faster processors and other advances are making statistical translation technology more accurate and thus more useful. However, the approach must still clear several hurdles - such as still inadequate accuracy and problems recognizing idioms - before it can be useful for mission-critical tasks.","1558-0814","","10.1109/MC.2005.353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1516048","Machine translation technology;Linguistics;Statistical translation systems;Translation technology","Snow;Humans;Natural languages;Dictionaries;Mathematical analysis;Mathematical model;Machine intelligence;Laboratories;Speech processing;Feeds","language translation;natural languages;search engines;Internet;document handling","statistical machine translation;search engine;rule based technology;document handling;Internet","","4","","","IEEE","10 Oct 2005","","","IEEE","IEEE Magazines"
"1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","",,"1997 IEEE International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1997","4","","iii","","Presents the front cover of the proceedings.","1520-6149","0-8186-7919-0","10.1109/ICASSP.1997.595303","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=595303","","","signal processing;acoustics;speech processing;equalisers;neural nets;multimedia systems;array signal processing;acoustic signal processing;mobile communication;parameter estimation;echo suppression;audio coding;active noise control;hearing aids;signal detection;digital signal processing chips;speech recognition;speech coding;speech synthesis;speech enhancement;signal reconstruction;adaptive filters;time-frequency analysis;wavelet transforms;digital filters;image processing;image restoration;image enhancement;video signal processing;motion estimation;image coding;pattern recognition","signal processing;blind equalisation;channel identification;tracking;speech-to-speech translation;neural network applications;multimedia human-machine interaction;microphone array signal processing;mobile communication;statistical signal processing;frequency estimation;echo cancellation;audio coding;active noise control;hearing aids;computer music;matched field processing;signal detection;DSP processors;parameter estimation;time-scale analysis;broadcast news recognition;CELP speech coding;language modeling;word spotting;speech synthesis;acoustic modeling;speaker adaptation;speaker verification;language identification;spoken language systems;speech enhancement;speech analysis;speech models;low bit rate speech coding;large vocabulary systems;signal reconstruction;adaptive filters;fast algorithms;time-frequency analysis;wavelets;filterbank design;digital filter design","","","","","IEEE","6 Aug 2002","","","IEEE","IEEE Conferences"
"[Title page]","",,"2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","1","1","The following topics are dealt with: speech recognition; acoustic modeling; robustness in ASR I; language modeling; spoken document retrieval and summarization; spoken/multimodal language understanding and dialog systems; multilingual speech processing and machine translation.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373497","","","acoustic signal processing;information retrieval;language translation;natural language processing;speech processing;speech recognition","speech recognition;acoustic modeling;ASR;language modeling;spoken document retrieval;summarization;spoken language understanding;multimodal language understanding;dialog systems;multilingual speech processing;machine translation","","","","","IEEE","8 Jan 2010","","","IEEE","IEEE Conferences"
"[Front cover]","",,"2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","c1","c1","The following topics were dealt with: audio coding; signal enhancement; source separation; microphone and loudspeaker array signal processing; acoustic echo control; active noise control; sound perception; audio analysis and synthesis; multichannel biomedical signal processing; biomedical image processing; bioinformatics; scalable and multiview video coding; image filtering; stereoscopic and 3-D processing; video segmentation and tracking; feature extraction; image superresolution; biometrics; motion estimation; stereo processing; image coding; remote sensing imagery; image retrieval; image quality assessment; image modeling; video modeling; image restoration; intelligent vehicle applications; security methods for physical devices and media; data hiding methods in speech and audio; authentication; watermarking; fingerprinting; signal decomposition; learning methods; MLSP applications; statistical modelling techniques; signal extraction; dimensionality reduction; multimedia synchronization; multimedia indexing and retrieval; multimodal systems and applications; multimedia database; video transmission and coding systems; beamforming and space-time adaptive processing; sensor networks; sensor array processing; multi-antenna processing for wireless communications; signal processing education; cognitive radio and dynamic spectrum management; resource allocation; optimisation; game theory; precoding; sensor networks; distributed estimation; consensus averaging; information theory and networking; MIMO detection and decoding; OFDM; relay and cooperative systems; channel estimation and equalisation; space-time coding; vector precoding; diversity, blind estimation and performance analysis; cooperative processing over adaptive networks; compressed sensing; Monte Carlo methods for detection and estimation; sampling theory; nonstationary signal analysis; adaptive filters; digital and multirate filtering; signal reconstruction; multimedia social networks; signal processing for neural spike trains; multimedia surveillance; video search and event analysis; voice transformation; interference channels; spectrum sharing; robot audition; consensus gossiping; handling reverberant speech; discriminative training for ASR; robust speech recognition; voice conversion; voice search; spoken term detection; speaker verification; speaker diarization and clustering; speaker recognition; large vocabulary ASR; language and dialect identification; speech adaptation; speech enhancement; speech production, perception and analysis; spoken language understanding; language modeling and machine learning for speech processing; machine translation; dialog system; language acquisition and annotation; speech retrieval; and speech mining.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4959497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4959497","","","active noise control;adaptive filters;audio coding;bioinformatics;blind source separation;channel estimation;cognitive radio;cooperative systems;data encapsulation;feature extraction;fingerprint identification;game theory;image coding;image resolution;image restoration;image retrieval;information theory;interactive systems;language translation;learning (artificial intelligence);medical image processing;medical signal processing;microphone arrays;MIMO systems;Monte Carlo methods;motion estimation;multimedia databases;neurophysiology;OFDM modulation;optimisation;resource allocation;robots;signal reconstruction;speaker recognition;speech enhancement;speech recognition;stereo image processing;video coding;watermarking","audio coding;signal enhancement;source separation;microphone signal processing;loudspeaker array signal processing;acoustic echo control;active noise control;sound perception;audio analysis;audiosynthesis;multichannel biomedical signal processing;bioinformatics;scalable video coding;multiview video coding;image filtering;stereoscopic processing;3-D processing;video segmentation;video tracking;feature extraction;image superresolution;biometrics;motion estimation;stereo processing;image coding;remote sensing imagery;image retrieval;image quality assessment;image modeling;video modeling;image restoration;intelligent vehicle applications;security methods;data hiding;authentication;watermarking;fingerprinting;signal decomposition;learning methods;MLSP applications;statistical modelling;signal extraction;dimensionality reduction;multimedia synchronization;multimedia indexing;multimedia retrieval;multimodal systems;multimedia database;video transmission;video coding systems;beamforming;space-time adaptive processing;sensor networks;sensor array processing;multiantenna processing;wireless communications;signal processing education;cognitive radio;dynamic spectrum management;resource allocation;optimisation;game theory;precoding;distributed estimation;consensus averaging;information theory;networking;MIMO detection;decoding;OFDM;relay systems;cooperative systems;channel estimation;channel equalisation;space-time coding;vector precoding;cooperative processing;adaptive networks;compressed sensing;Monte Carlo methods;sampling theory;nonstationary signal analysis;adaptive filters;digital filtering;multirate filtering;signal reconstruction;multimedia social networks;signal processing;neural spike trains;multimedia surveillance;video search;event analysis;voice transformation;interference channels;spectrum sharing;robot audition;consensus gossiping;handling reverberant speech;discriminative training;robust speech recognition;voice conversion;voice search;spoken term detection;speaker verification;speaker diarization;clustering;speaker recognition;large vocabulary ASR;language identification;dialect identification;speech adaptation;speech enhancement;speech production;speech perception;speech analysis;spoken language understanding;language modeling;machine learning;machine translation;dialog system;language acquisition;language annotation;speech retrieval;speech mining","","","","","IEEE","26 May 2009","","","IEEE","IEEE Conferences"
"Special issue on speech-to-speech machine translation","",,"IEEE Transactions on Speech and Audio Processing","19 Apr 2004","2004","12","3","348","348","Provides notice of upcoming special issues of interest to practitioners and researchers.","1558-2353","","10.1109/TSA.2004.828665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288164","","","","","","","","","IEEE","19 Apr 2004","","","IEEE","IEEE Journals"
"2003 International Conference on Natural Language Processing and Knowledge Engineering (IEEE Cat. No.03EX703)","",,"International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003","22 Mar 2004","2003","","","","","The following topics are dealt with: natural language processing; speech recognition; knowledge representation; word sense disambiguation; pattern extraction; rule extraction; dialogue systems; question and answering; discourse processing; speech synthesis; document processing; text chunking; digital watermarking; speech enhancement; speaker identification; collocation extraction; information retrieval; term extraction; knowledge acquisition; machine translation; word segmentation; text classification; parsing; text summarization; Web monitoring; semantic-oriented language; computational linguistics.","","0-7803-7902-0","10.1109/NLPKE.2003.1275857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1275857","","","natural languages;text analysis;speech recognition;knowledge acquisition;knowledge representation;semantic Web;computational linguistics;speech synthesis;language translation","natural language processing;speech recognition;knowledge representation;word sense disambiguation;pattern extraction;dialogue system;discourse processing;speech synthesis;document processing;text chunking;digital watermarking;speech enhancement;speaker identification;information retrieval;term extraction;knowledge acquisition;machine translation;word segmentation;parsing;text summarization;computational linguistics;semantic Web","","","","","IEEE","22 Mar 2004","","","IEEE","IEEE Conferences"
"Proceedings of the 2005 12th IEEE International Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE '05) (IEEE Cat. No. 05EX1156)","",,"2005 International Conference on Natural Language Processing and Knowledge Engineering","27 Feb 2006","2005","","","","","The following topics are dealt with: natural language processing; machine translation; grammars; computational linguistics; knowledge engineering; speech processing; linguistics; information retrieval; text analysis; speech recognition.","","0-7803-9361-9","10.1109/NLPKE.2005.1598685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1598685","","","speech processing;natural languages;computational linguistics;language translation;grammars;speech recognition;information retrieval;knowledge engineering;text analysis","natural language processing;machine translation;grammars;computational linguistics;knowledge engineering;speech processing;linguistics;information retrieval;text analysis;speech recognition","","","","","IEEE","27 Feb 2006","","","IEEE","IEEE Conferences"
"Special issue on speech-to-speech machine translation","",,"IEEE Transactions on Image Processing","13 Apr 2004","2004","13","4","626","626","Provides notice of upcoming special issues of interest to practitioners and researchers.","1941-0042","","10.1109/TIP.2004.827908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1284400","","","","","","","","","IEEE","13 Apr 2004","","","IEEE","IEEE Journals"
"Special issue on speech-to-speech machine translation","",,"IEEE Transactions on Image Processing","30 Mar 2004","2004","13","3","454","454","Provides notice of upcoming special issues of interest to practitioners and researchers.","1941-0042","","10.1109/TIP.2004.827408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1278370","","","","","","","","","IEEE","30 Mar 2004","","","IEEE","IEEE Journals"
"Special issue on speech-to-speech machine translation","",,"IEEE Transactions on Image Processing","19 Apr 2004","2004","13","5","738","738","Provides notice of upcoming special issues of interest to practitioners and researchers.","1941-0042","","10.1109/TIP.2004.828420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1288202","","","","","","","","","IEEE","19 Apr 2004","","","IEEE","IEEE Journals"
"Special issue on speech-to-speech machine translation","",,"IEEE Transactions on Image Processing","30 Mar 2004","2004","13","2","266","266","Provides notice of upcoming special issues of interest to practitioners and researchers.","1941-0042","","10.1109/TIP.2004.827326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1278343","","","","","","","","","IEEE","30 Mar 2004","","","IEEE","IEEE Journals"
"[Title page]","",,"2010 4th International Universal Communication Symposium","13 Dec 2010","2010","","","1","1","The following topics are dealt with: speech processing; language processing; machine translation; concept maps; data cloud; multilinguality; information extraction; machine learning; word sense disambiguation; 3D visualization; 3D sound processing; virtual reality; human interaction; and ultra realistic communications.","","978-1-4244-7820-0","10.1109/IUCS.2010.5666781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5666781","","","audio signal processing;data visualisation;information retrieval;language translation;learning (artificial intelligence);speech processing;virtual reality;word processing","speech processing;language processing;machine translation;concept maps;data cloud;multilinguality;information extraction;machine learning;word sense disambiguation;3D visualization;3D sound processing;virtual reality;human interaction;ultra realistic communications","","","","","IEEE","13 Dec 2010","","","IEEE","IEEE Conferences"
"2006 IEEE Spoken Language Technology Workshop","",,"2006 IEEE Spoken Language Technology Workshop","19 Mar 2007","2006","","","i","i","The following topics are dealt with: spoken language understanding; multilingual language processing; information extraction and retrieval; spoken and multimodal dialog system; speech recognition and machine translation.","","1-4244-0872-5","10.1109/SLT.2006.326767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123334","","","information retrieval;interactive systems;language translation;linguistics;natural language processing;speech recognition","spoken language understanding;multilingual language processing;information extraction;information retrieval;multimodal dialog system;machine translation;speech recognition","","","","","IEEE","19 Mar 2007","","","IEEE","IEEE Conferences"
"Table of contents","",,"2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT)","14 Jan 2016","2015","","","IX","XI","The following topics are dealt with: AI; natural language processing and information extraction; social media; Web search; text reuse; speech recognition; mobile robot; information retrieval; ontological-semantic graph; and machine translation.","","978-9-5268-3970-7","10.1109/AINL-ISMW-FRUCT.2015.7382978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382978","","","graph theory;information retrieval;Internet;language translation;mobile robots;natural language processing;ontologies (artificial intelligence);search engines;social networking (online);speech recognition;text analysis","machine translation;ontological-semantic graph;information retrieval;mobile robot;speech recognition;text reuse;Web search;social media;information extraction;natural language processing;AI","","","","","","14 Jan 2016","","","IEEE","IEEE Conferences"
"[Front cover]","",,"2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","c1","c1","The following topics are dealt with: loudspeaker; microphone array signal processing; echo cancellation; source separation; music classification; music recognition; signal enhancement; perception; noise reduction; audio analysis; audio synthesis; spatial audio coding; content analysis for music, multimedia, and medicine; room acoustics; acoustic system modeling; biomedical signal processing; electroencephalography; magnetoencephalography; biomedical image processing; image coding; depth estimation; video coding; interpolation and super-resolution; image segmentation; image analysis; image enhancement; image feature extraction; visual search and annotation; video segmentation and tracking; image and video retrieval; image denoising, restoration, and enhancement; image filtering; remote sensing; image resizing and reconstruction; image recognition; video analysis; 3D processing and coding; motion estimation, registration, and tracking; biometrics; parallel and embedded signal processing; communication, error correction, and navigation; optimization; industry technologies; multimedia forensics; watermarking; data hiding; multimedia identification and authentication; network security; blind signal separation; matrix factorizations; learning theory; speech processing; clustering; Bayesian, information-theoretic and graphical learning; biomedicine; multimedia security and forensics; audio visual processing; multimedia communication; multimedia recognition, search, and retrieval; audiovisual and multimodal processing; MIMO radar; sensor array; multichannel signal processing; DOA estimation; source localization and tracking; sensor networks and distributed estimation; compressed sensing; sparse signal modeling; signal processing education; resource allocation; interference management; relay networks; social networks; smart grid; cognitive radio; adaptive filtering; pattern recognition; time-frequency analysis; filter banks; sampling sparsity; acoustic modeling; speech enhancement; speech synthesis; speaker diarization; weighted finite-state transducers; speaker identification; machine learning; pitch, prosody, and voice quality; speaker verification; speech translation; language modeling; spoken language understanding; paralinguistic, nonlinguistic information and data mining; speech retrieval; resource-aware design; multiple radar systems; depth cameras; digitally enhanced analog systems; ray and sound reproducing; and distributed transmit beamforming.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289160","","","adaptive filters;architectural acoustics;array signal processing;audio coding;authorisation;Bayes methods;biometrics (access control);blind source separation;channel bank filters;cognitive radio;compressed sensing;data encapsulation;data mining;direction-of-arrival estimation;echo suppression;electroencephalography;error correction;feature extraction;image denoising;image enhancement;image restoration;image segmentation;information theory;interpolation;learning (artificial intelligence);loudspeakers;magnetoencephalography;medical image processing;microphone arrays;MIMO radar;motion estimation;multimedia communication;navigation;optimisation;remote sensing;resource allocation;sensor arrays;social networking (online);speaker recognition;speech enhancement;time-frequency analysis;video coding;video retrieval;watermarking","loudspeaker;microphone array signal processing;echo cancellation;source separation;music classification;music recognition;signal enhancement;perception;noise reduction;audio analysis;audio synthesis;spatial audio coding;content analysis;room acoustics;acoustic system modeling;biomedical signal processing;electroencephalography;magnetoencephalography;biomedical image processing;image coding;depth estimation;video coding;interpolation;super-resolution;image segmentation;image analysis;image enhancement;image feature extraction;visual search;visual annotation;video segmentation;video tracking;image retrieval;video retrieval;image denoising;image restoration;image filtering;remote sensing;image resizing;image reconstruction;image recognition;video analysis;3D processing;motion estimation;motion registration;motion tracking;biometrics;parallel signal processing;embedded signal processing;error correction;navigation;optimization;industry technologies;multimedia forensics;watermarking;data hiding;multimedia identification;authentication;network security;blind signal separation;matrix factorizations;learning theory;speech processing;speech clustering;Bayesian methods;information theory;graphical learning;biomedicine;multimedia security;audio visual processing;multimedia communication;multimedia recognition;multimedia search;multimedia retrieval;audiovisual processing;multimodal processing;MIMO radar;sensor array;multichannel signal processing;DOA estimation;source localization;source tracking;sensor networks;distributed estimation;compressed sensing;sparse signal modeling;signal processing education;resource allocation;interference management;relay networks;social networks;smart grid;cognitive radio;adaptive filtering;pattern recognition;time-frequency analysis;filter banks;sampling sparsity;acoustic modeling;speech enhancement;speech synthesis;speaker diarization;weighted finite-state transducers;speaker identification;machine learning;pitch;prosody;voice quality;speaker verification;speech translation;spoken language understanding;paralinguistic information;nonlinguistic information;data mining;speech retrieval;resource-aware design;multiple radar systems;depth cameras;digitally enhanced analog systems;ray reproducing;sound reproducing;distributed transmit beamforming","","","","","IEEE","30 Aug 2012","","","IEEE","IEEE Conferences"
"[Title page]","",,"2016 Pattern Recognition Association of South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech)","16 Jan 2017","2016","","","1","1","The following topics are dealt with: HMM-based child speech synthesis; Afrikaans speech synthesis; optical character recognition; statistical machine translation; computer vision; push-broom Lidar; extreme learning machine; automatic pain recognition; defocus deblurring; noncontact access control; moving object tracking; adaptive skin detection; text detection; convolutional neural networks; low default credit scoring; offline signature verification; data classification; DDoS attacks; textual anomaly detection; spam detection; social media; mobile robot path planning; robot programming; educational robots; 2D laser range finder; thermal power generation scheduling; fuzzy logic control; MANETS; flexible robot arm flatness based control; reflex assisted walking; multifunctional robotic walker; modular electric automatic guided vehicles; robotic endoscope; fixed-wing aircraft landing; quadrotor acceleration-based control; and tethered quadrotor UAV landing.","","978-1-5090-3335-5","10.1109/RoboMech.2016.7813190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813190","","","authorisation;automatic guided vehicles;autonomous aerial vehicles;computer network security;computer vision;convolution;digital signatures;educational robots;electric vehicles;endoscopes;feedforward neural nets;finance;fuzzy control;helicopters;image motion analysis;image restoration;language translation;laser ranging;legged locomotion;manipulators;medical robotics;mobile ad hoc networks;object tracking;optical character recognition;optical radar;path planning;pattern classification;power generation scheduling;robot programming;social networking (online);speech synthesis;text detection;thermal power stations;unsolicited e-mail","HMM-based child speech synthesis;Afrikaans speech synthesis;optical character recognition;statistical machine translation;computer vision;push-broom Lidar;extreme learning machine;automatic pain recognition;defocus deblurring;noncontact access control;moving object tracking;adaptive skin detection;text detection;convolutional neural networks;low default credit scoring;offline signature verification;data classification;DDoS attacks;textual anomaly detection;spam detection;social media;mobile robot path planning;robot programming;educational robots;2D laser range finder;thermal power generation scheduling;fuzzy logic control;MANETS;flexible robot arm flatness based control;reflex assisted walking;multifunctional robotic walker;modular electric automatic guided vehicles;robotic endoscope;fixed-wing aircraft landing;quadrotor acceleration-based control;tethered quadrotor UAV landing","","","","","IEEE","16 Jan 2017","","","IEEE","IEEE Conferences"
"[Front cover]","",,"2011 International Conference on Asian Language Processing","2 Jan 2012","2011","","","C1","C1","The following topics are dealt with: phonology; morphology; syntax; language model; semantics; discourse; text understanding; text retrieval; document summarization; machine translation; spoken language processing; linguistic resource; linguistic tool; and language learning.","","978-1-4577-1733-8","10.1109/IALP.2011.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121454","","","computational linguistics;computer aided instruction;information retrieval;language translation;natural languages;speech processing;text analysis","phonology;morphology;syntax;language model;semantics;discourse;text understanding;text retrieval;document summarization;machine translation;spoken language processing;linguistic resource;linguistic tool;language learning","","","","","IEEE","2 Jan 2012","","","IEEE","IEEE Conferences"
"[Title page i]","",,"2008 Second International Symposium on Universal Communication","22 Dec 2008","2008","","","i","i","The following topics are discussed: spoken language processing; ultra-realistic sound technology; home and indoor environments; machine translation; multilingual information processing; ultra-realistic image technology; information analysis for Web contents; language resources; natural language processing; context and situation understanding; multisensory interaction; and smart environments.","","978-0-7695-3433-6","10.1109/ISUC.2008.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724422","","","audio signal processing;image processing;information analysis;Internet;language translation;natural language processing;sensor fusion;speech processing","spoken language processing;ultrarealistic sound technology;home environment;indoor environment;machine translation;multilingual information processing;ultrarealistic image technology;information analysis;Web content;language resources;natural language processing;context understanding;situation understanding;multisensory interaction;smart environment","","","","","IEEE","22 Dec 2008","","","IEEE","IEEE Conferences"
"Table of contents","",,"2014 International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management (HNICEM)","22 Jan 2015","2014","","","1","2","The following topics are dealt with: drug driver system; quadrotor unmanned aerial vehicles; blind source separation; mobile phone store simulation; PWM power controller; computer numerically-controlled LASER machine; remote controlled vehicle; agricultural telemetry system; Android phone; health monitoring system; emotion generation model for robots; MEMS; network access intrusion detection system; digital pest controller; microprocessor; disaster response; solid waste decomposition; hybrid genetic algorithm-neural network; monocular breast self-examination image sequence; fuzzy PLC; crop processing; Web-based smart power distribution unit; food servicing industry; WiFi; integrated bamboo splitting machine; sigma-delta analog to digital converter; antennas; public transit scheduling tool; Magnus wind rotor blades; PV solar tracking; power monitoring system; wearable sensors; distance education system; RFID; CMOS technology; vibration electromagnetic energy harvest; telecommunication access network; photoplethysmograph; vocabulary speech recognition; artificial neural networks; cement production; microalgae drying; image processing; 3D machining; language translation; computer games; sensor fusion; handpump system; robot communication; LED lighting; ubiquitous network; gesture-glove controller; DC-AC converter; motion detection; autonomous underwater vehicle; and XBitTorrent.","","978-1-4799-4020-2","10.1109/HNICEM.2014.7016271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016271","","","agriculture;analogue-digital conversion;antennas;autonomous aerial vehicles;autonomous underwater vehicles;blind source separation;CMOS integrated circuits;computer games;computerised numerical control;crops;DC-AC power convertors;drug delivery systems;emergency management;energy harvesting;laser beam machining;LED lamps;micromechanical devices;microprocessor chips;neural nets;pest control;photoplethysmography;power control;programmable controllers;public transport;pulse width modulation;radiofrequency identification;robots;security of data;sensors;service industries;smart phones;solar power stations;speech recognition;ubiquitous computing;wind turbines","drug driver system;quadrotor unmanned aerial vehicles;blind source separation;mobile phone store simulation;PWM power controller;computer numerically-controlled LASER machine;remote controlled vehicle;agricultural telemetry system;Android phone;health monitoring system;emotion generation model;MEMS;network access intrusion detection system;digital pest controller;microprocessor;disaster response;solid waste decomposition;hybrid genetic algorithm-neural network;monocular breast self-examination;image sequence;fuzzy PLC;crop processing;Web-based smart power distribution unit;food servicing industry;WiFi;integrated bamboo splitting machine;sigma-delta analog to digital converter;antennas;public transit scheduling tool;Magnus wind rotor blades;PV solar tracking;power monitoring system;wearable sensors;distance education system;RFID;CMOS technology;vibration electromagnetic energy harvest;telecommunication access network;photoplethysmograph;vocabulary speech recognition;artificial neural networks;cement production;microalgae drying;image processing;3D machining;language translation;computer games;sensor fusion;handpump system;robot communication;LED lighting;ubiquitous network;gesture-glove controller;DC-AC converter;motion detection;autonomous underwater vehicle;XBitTorrent","","","","","IEEE","22 Jan 2015","","","IEEE","IEEE Conferences"
"[Title page i]","",,"2009 International Conference on Asian Language Processing","15 Jan 2010","2009","","","i","i","The following topics are dealt with: data mining; speech processing; machine translation; natural language processing; and text analysis.","","978-0-7695-3904-1","10.1109/IALP.2009.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380731","","","data mining;natural language processing;speech processing;text analysis","data mining;speech processing;machine translation;natural language processing;text analysis","","","","","IEEE","15 Jan 2010","","","IEEE","IEEE Conferences"
"In the news","G. Lawton",NA,"IEEE Intelligent Systems","5 Jun 2012","2012","27","2","2","7","Hidden Markov models and neural networks are enabling new approaches to understanding and translating spoken language. Google, Microsoft, AT&T, Nuance Communications, and Apple are among the companies driving research in this area. Israeli researchers have developed a computational machine-learning model based on how bacteria communicate and act collectively. Their algorithm could enable development of robots that could form smart teams and also improve work in areas such as swarm computing. Paleontologists have turned to artificial intelligence to analyze satellite imagery and create a model of where best to find sites with fossils. The researchers trained their AI-based application to recognize the spectral signatures of different types of land cover and combined that data with elevation and slope information to identify likely fossil-bearing sites.","1941-1294","","10.1109/MIS.2012.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212520","hidden Markov models;language recognition;machine translation;Google Voice;Microsoft;AT&;T;Nuance Communications;Apple Siri;bacteria;swarm computing;collective decision making;paleontology;fossils;satellite imagery","Hidden Markov models;Google;Speech processing;Decision making;Particle swarm optimization;Machine learning;Machine intelligence","archaeology;biology computing;geology;learning (artificial intelligence);microorganisms;mobile robots;palaeontology","language tool;artificial intelligence;bacteria;intelligent behavior;machine learning;autonomous robot;paleontologists;fossil","","2","","","IEEE","5 Jun 2012","","","IEEE","IEEE Magazines"
"Table of contents","",,"2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","3 Nov 2016","2016","","","1","43","The following topics are dealt with: signal processing; image processing; video processing; speech processing; computer vision; pattern recognition; embedded systems; computer architecture; VLSI; adaptive systems; wireless sensor networks; MANETs; VANETs; distributed systems; natural language processing and machine translation; intelligent informatics; artificial intelligence and machine learning; data engineering; biocomputing; data security; data trust and privacy; steganography; applied informatics; wireless and multimedia communications; and social networks.","","978-1-5090-2029-4","10.1109/ICACCI.2016.7732016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732016","","","biocomputing;computer architecture;computer vision;data handling;data privacy;embedded systems;language translation;learning (artificial intelligence);multimedia communication;natural language processing;pattern recognition;security of data;social networking (online);speech processing;steganography;trusted computing;vehicular ad hoc networks;video signal processing;VLSI;wireless sensor networks","signal processing;image processing;video processing;speech processing;computer vision;pattern recognition;embedded systems;computer architecture;VLSI;adaptive systems;wireless sensor networks;MANETs;VANETs;distributed systems;natural language processing;machine translation;intelligent informatics;artificial intelligence;machine learning;data engineering;biocomputing;data security;data trust;data privacy;steganography;applied informatics;multimedia communications;wireless communications;social networks","","","","","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"In the News","G. Lawton",NA,"IEEE Intelligent Systems","15 Nov 2012","2012","27","5","5","9","Researchers are working on a technique that enables a humanoid robot to learn basic language skills by communicating directly with people. Google and Stanford University scientists have developed a large, cloud-based, deep-learning neural network suitable for complex tasks such as object recognition and machine translation. Scientists have developed an approach designed to let robots work with in concert with humans in factories and other settings.","1941-1294","","10.1109/MIS.2012.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353470","robots;robotics;training;MIT;Julie Shah;machine learning;machine vision;networking;deep learning;neural network;object recognition;Google;Stanford University;cloud computing;robot;speech;language;corpus linguistics;University of Hertfordshire;DeeChee;Caroline Lyon;ITALK;iCub;RobotCub Consortium","","","","","","","","IEEE","15 Nov 2012","","","IEEE","IEEE Magazines"
"IEEE publications: Scanning the issues","",,"IEEE Spectrum","19 Apr 2013","1965","2","9","144","145","Automatic Translation. A lively and searching dissertation on the problems of translation, and the frailties of both humans and machines in undertaking the task, appears in the June issue of the IEEE Transactions on Engineering Writing and Speech.","1939-9340","","10.1109/MSPEC.1965.6501420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6501420","","","","","","","","","IEEE","19 Apr 2013","","","IEEE","IEEE Magazines"
