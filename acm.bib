@article{10.1109/TASLP.2020.2986886,
author = {Kano, Takatomo and Sakti, Sakriani and Nakamura, Satoshi},
title = {End-to-End Speech Translation With Transcoding by Multi-Task Learning for Distant Language Pairs},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986886},
doi = {10.1109/TASLP.2020.2986886},
abstract = {Directly translating spoken utterances from a source language to a target language is challenging because it requires a fundamental transformation in both linguistic and para/non-linguistic features. Traditional speech-to-speech translation approaches concatenate automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech synthesizer (TTS) by text information. The current state-of-the-art models for ASR, MT, and TTS have mainly been built using deep neural networks, in particular, an attention-based encoder-decoder neural network with an attention mechanism. Recently, several works have constructed end-to-end direct speech-to-text translation by combining ASR and MT into a single model. However, the usefulness of these models has only been investigated on language pairs of similar syntax and word order (e.g., English-French or English-Spanish). For syntactically distant language pairs (e.g., English-Japanese), speech translation requires distant word reordering. Furthermore, parallel texts with corresponding speech utterances that are suitable for training end-to-end speech translation are generally unavailable. Collecting such corpora is usually time-consuming and expensive. This article proposes the first attempt to build an end-to-end direct speech-to-text translation system on syntactically distant language pairs that suffer from long-distance reordering. We train the model on English (subject-verb-object (SVO) word order) and Japanese (SOV word order) language pairs. To guide the attention-based encoder-decoder model on this difficult problem, we construct end-to-end speech translation with transcoding and utilize curriculum learning (CL) strategies that gradually train the network for end-to-end speech translation tasks by adapting the decoder or encoder parts. We use TTS for data augmentation to generate corresponding speech utterances from the existing parallel text data. Our experiment results show that the proposed approach provides significant improvements compared with conventional cascade models and the direct speech translation approach that uses a single model without transcoding and CL strategies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1342–1355},
numpages = {14}
}

@article{10.1109/TASLP.2020.3037543,
author = {Chuang, Shun-Po and Liu, Alexander H. and Sung, Tzu-Wei and Lee, Hung-yi},
title = {Improving Automatic Speech Recognition and Speech Translation via Word Embedding Prediction},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3037543},
doi = {10.1109/TASLP.2020.3037543},
abstract = {In this article, we target speech translation (ST). We propose lightweight approaches that generally improve either ASR or end-to-end ST models. We leverage continuous representations of words, known as word embeddings, to improve ASR in cascaded systems as well as end-to-end ST models. The benefit of using word embedding is that word embedding can be obtained easily by training on pure textual data, which alleviates data scarcity issue. Also, word embedding provides additional contextual information to speech models. We motivate to distill the knowledge from word embedding into speech models. In ASR, we use word embeddings as a regularizer to reduce the WER, and further propose a novel decoding method to fuse the semantic relations among words for further improvement. In the end-to-end ST model, we propose leveraging word embeddings as an intermediate representation to enhance translation performance. Our analysis shows that it is possible to map speech signals to semantic space, which motivates future work on applying the proposed methods in spoken language processing tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {93–105},
numpages = {13}
}

@inproceedings{10.1145/3316782.3322746,
author = {Haslwanter, Jean D. Hallewell and Heiml, Michael and Wolfartsberger, Josef},
title = {Lost in Translation: Machine Translation and Text-to-Speech in Industry 4.0},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322746},
doi = {10.1145/3316782.3322746},
abstract = {Small lot sizes are becoming more common in modern manufacturing. Rather than automate every possible product variant, companies may rely on manual assembly to be more flexible. However, it can be difficult for people to remember the steps for every possible product variant. Assistive systems providing instructions can support workers. In this paper, we present a study investigating whether existing machine translation and text-to-speech engines provide sufficient quality to enable on-the-fly translations to provide assistance to workers in their native languages. The results of our tests indicate that machine translation is not yet sufficient for this application.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {333–342},
numpages = {10},
keywords = {computer-assisted instruction, text to speech, assistive systems, TTS, manual assembly, machine translation, MT},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3582515.3609567,
author = {Delnevo, Giovanni and Im, Marcus and Tse, Rita and Lam, Chan-Tong and Tang, Su-Kit and Salomoni, Paola and Pau, Giovanni and Ghini, Vittorio and Mirri, Silvia},
title = {Italian-Chinese Neural Machine Translation: Results and Lessons Learnt},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582515.3609567},
doi = {10.1145/3582515.3609567},
abstract = {Today, access to the Internet provides access to various forms of knowledge like free online lecture series offered by prestigious universities, massive open online courses, films and books, and Wikipedia. In addition, it is possible to join online communities on any topic of interest, get to know people with common interests, exchange thoughts and participate in debates. To enable access to these unprecedented knowledge bases, it is crucial to be able to translate texts into any language known by users. For this reason, Machine Translation has been a very active research field for the last thirty years. In this paper, we investigate the task of Chinese-Italian translations by exploiting Neural Machine Translation approaches. We trained several deep neural networks starting from two already available datasets containing Chinese-Italian parallel corpora. Then, we compared their performance against some of the most common machine translation services freely available online. In particular, we take advantage of Microsoft Translator, Google Translate, DeepL, and ModernMT.},
booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
pages = {455–461},
numpages = {7},
keywords = {Italian Chinese Translation, Neural Machine Translation, Transformer, Big Data},
location = {Lisbon, Portugal},
series = {GoodIT '23}
}

@article{10.1145/3625095,
author = {Liu, Jinhui and Zhang, Feng},
title = {Language Model Method for Collocation Rules of Parts of Speech in Machine Translation System},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3625095},
doi = {10.1145/3625095},
abstract = {With the development of the times, modern society has now entered the Internet of Things (IoT) information age and Machine Translation (MT) plays an important role in increasingly frequent cross-language communication. In recent years, China's artificial intelligence industry has been in a stage of rapid construction, and the scale of its core industries has grown explosively, and a large number of artificial intelligence companies, including issuers, have emerged. Part of speech has always been a major problem in MT. One of the reasons is that there are a large number of multi-category words in Chinese and a large number of polysemy words in English, so part of speech collocation problems account for a large proportion of MT errors, which to some extent affects the credibility and accuracy of the translation. To reduce the error problem in MT of part of speech collocation, this paper used Machine Learning (ML) methods to study the Language Model (LM) of part of speech collocation based on recurrent neural network (NN) and compared it with the traditional statistical LM. In terms of the accuracy rate of the two LMS in the automatic evaluation index of machine translation, the experimental results show that the recursive NN LM established by the ML method had an accuracy rate of 80.42% and 83.57% respectively for the part-of-speech matching rules of the IoT machine translation system in the dialogue between Chinese and English and the translation of articles. The accuracy of traditional statistical LM evaluation was 71.29% and 69.52%, respectively. Compared to traditional statistical LM, the accuracy of translation was higher. This showed that the recurrent NN LM reduced the number of errors in the collocation of parts of speech in MT and improved the accuracy and credibility of MT.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Part Collocation Rules, Neural Networks, Machine Translation, Language Model, Machine Learning}
}

@inproceedings{10.1145/3411170.3411258,
author = {Tse, Rita and Mirri, Silvia and Tang, Su-Kit and Pau, Giovanni and Salomoni, Paola},
title = {Building an Italian-Chinese Parallel Corpus for Machine Translation from the Web},
year = {2020},
isbn = {9781450375597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411170.3411258},
doi = {10.1145/3411170.3411258},
abstract = {In an increasingly globalized world, being able to understand texts in different languages (even more so in different alphabets and charsets) has become a necessity. This can be strategic even while moving and travelling across different countries, characterized by different languages. With this in mind, bilingual corpora become critical resources since they are the basis of every state-of-the-art automatic translation system; moreover, building a parallel corpus is usually a complex and very expensive operation. This paper describes an innovative approach we have defined and adopted to automatically build an Italian-Chinese parallel corpus, with the aim of using it for training an Italian-Chinese Neural Machine Translation. Our main idea is to scrape parallel texts from the Web: we defined a general pipeline, describing each specific step from the selection of the appropriate data sources to the sentence alignment method. A final evaluation was conducted to evaluate the goodness of our approach and its results show that 90% of the sentences were correctly aligned. The corpus we have obtained consists of more than 6,000 sentence pairs (Italian and Chinese), which are the basis for building a Machine Translation system.},
booktitle = {Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {265–268},
numpages = {4},
keywords = {machine translation, sentence alignment, Italian-Chinese corpus, bilingual annotation},
location = {Antwerp, Belgium},
series = {GoodTechs '20}
}

@article{10.1109/TASLP.2023.3244521,
author = {Zhang, Hao and Si, Nianwen and Chen, Yaqi and Zhang, Wenlin and Yang, Xukui and Qu, Dan and Zhang, Wei-Qiang},
title = {Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3244521},
doi = {10.1109/TASLP.2023.3244521},
abstract = {The end-to-end speech translation (E2E-ST) model has gradually become a mainstream paradigm due to its low latency and less error propagation. However, it is non-trivial to train such a model well due to the task complexity and data scarcity. The speech-and-text modality differences result in the E2E-ST model performance usually inferior to the corresponding machine translation (MT) model. Based on the above observation, existing methods often use sharing mechanisms to carry out <italic><bold>implicit knowledge transfer</bold></italic> by imposing various constraints. However, the final model often performs worse on the MT task than the MT model trained alone, which means that the knowledge transfer ability of this method is also limited. To deal with these problems, we propose the FCCL (<underline>F</underline>ine- and <underline>C</underline>oarse- Granularity <underline>C</underline>ontrastive <underline>L</underline>earning) approach for E2E-ST, which makes <italic>explicit knowledge transfer</italic> through cross-modal multi-grained contrastive learning. A key ingredient of our approach is applying contrastive learning at both sentence- and frame-level to give the comprehensive guide for extracting speech representations containing rich semantic information. In addition, we adopt a simple whitening method to alleviate the representation degeneration in the MT model, which adversely affects contrast learning. Experiments on the MuST-C benchmark show that our proposed approach significantly outperforms the state-of-the-art E2E-ST baselines on all eight language pairs. Further analysis indicates that FCCL can free up its capacity from learning grammatical structure information and force more layers to learn semantic information.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1075–1086},
numpages = {12}
}

@inproceedings{10.1145/3544548.3581129,
author = {Angelucci, Margherita and Marshall, Harrison and Tebourbi, Meriem and Seguin, Joshua Paolo and Varghese, Delvin and Olivier, Patrick and Bartindale, Tom},
title = {Action Translate: Supporting Students in Translation Volunteering},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581129},
doi = {10.1145/3544548.3581129},
abstract = {University students are well known for volunteering within non-governmental organisations (NGOs). A significant part of NGO practice is the production of documents that communicate their work to local communities and international stakeholders. However, organisations often struggle to resource translations of these documents, resulting in the exclusion of the very same communities that they want to reach. Although many students are multilingual and are willing to volunteer their time and language skills, there are few structured opportunities configured for such non-professional translation of content in the short-term mode that would fit into the student pattern of availability. We developed Action Translate to specifically support these motivated, non-professional translators within the volunteering constraints of university life. Action Translate leverages machine translation post-editing to support teams of volunteers working on NGO translation projects online. Through analysis of a real-world deployment, we discuss how digital systems can be developed to better support student volunteer translators, specifically in building collegiate interaction and identity as translators for a cause.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {224},
numpages = {14},
keywords = {machine translation, post-editing, translation, volunteering},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1109/TASLP.2022.3178241,
author = {Wang, Qian and Zhang, Jiajun and Zong, Chengqing},
title = {Synchronous Inference for Multilingual Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3178241},
doi = {10.1109/TASLP.2022.3178241},
abstract = {Multilingual neural machine translation allows a single model to translate between multiple language pairs, which greatly reduces the cost of model training and receives much attention recently. Previous studies mainly focus on training stage optimization and improve positive knowledge transfer among languages with different levels of parameter sharing, but ignore the multilingual knowledge transfer during inference although the translation in one language may help the generation of other languages. This work enhances knowledge sharing among multiple target languages in the inference phase. To achieve this, we propose a synchronous inference method that can simultaneously generate translations in multiple languages. During generation, the model that predicts the next word of each language not only based on source sentence and previously predicted segments, but also based on predicted words of other target languages. To maximize the inference stage knowledge sharing, we design a cross-lingual attention module which allows the model to dynamically selects the most relevant information from multiple target languages. The synchronous inference model requires multi-way parallel training data which is scarce. We therefore propose to adopt multi-task learning to incorporate large-scale bilingual data. We evaluate our method on three multilingual translation datasets and prove that the proposed method significantly improve the translation quality and the decoding efficiency compared to strong bilingual and multilingual baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1827–1839},
numpages = {13}
}

@inproceedings{10.1145/3600211.3604675,
author = {Rarrick, Spencer and Naik, Ranjita and Mathur, Varun and Poudel, Sundar and Chowdhary, Vishal},
title = {GATE: A Challenge Set for Gender-Ambiguous Translation Examples},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604675},
doi = {10.1145/3600211.3604675},
abstract = {Although recent years have brought significant progress in improving translation of unambiguously gendered sentences, translation of ambiguously gendered input remains relatively unexplored. When source gender is ambiguous, machine translation models typically default to stereotypical gender roles, perpetuating harmful bias. Recent work has led to the development of "gender rewriters" that generate alternative gender translations on such ambiguous inputs, but such systems are plagued by poor linguistic coverage. To encourage better performance on this task we present and release GATE, a linguistically diverse corpus of gender-ambiguous source sentences along with multiple alternative target language translations. We also provide tools for evaluation and system analysis when using GATE and use them to evaluate our translation rewriter.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {845–854},
numpages = {10},
keywords = {gender bias, social biases, machine translation},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@article{10.1145/3610404,
author = {Haulai, Thangkhanhau and Hussain, Jamal},
title = {Construction of Mizo: English Parallel Corpus for Machine Translation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610404},
doi = {10.1145/3610404},
abstract = {Parallel corpus is a key component of statistical and Neural Machine Translation (NMT). While most research focuses on machine translation, corpus creation studies are limited for many languages, and no research paper on a Mizo–English corpus exists yet. A high-quality parallel corpus is required for Natural Language Processing activities including machine translation, Chatbots, Transliteration, and Cross-Language Information Retrieval. This work aims to investigate parallel corpus creation techniques and apply them to the Mizo–English language pair. Another goal is to test machine translation on the newly constructed corpus. We contributed to LF Aligner tool to support Mizo language for Mizo sentence alignment in corpus development. Our effort created the first large-scale Mizo–English parallel corpus with over 529K sentences. The pre-processed corpus was used for Mizo-to-English NMT. It was evaluated using BLEU, Character F1 Score (ChrF), and Translation Edit Rate (TER) scores. Our system achieved BLEU 45.08, ChrF 65.36, and TER 41.16, setting a new benchmark for Mizo-to-English translation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {220},
numpages = {12},
keywords = {corpus construction, parallel text, Mizo, machine translation, bilingual corpus}
}

@inproceedings{10.1145/3501409.3501602,
author = {Boakye-Yiadom, Adwoa Agyeiwaa and Qin, Mingwei and Jing, Ren},
title = {Research of Automatic Speech Recognition of Asante-Twi Dialect For Translation},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501602},
doi = {10.1145/3501409.3501602},
abstract = {This paper presents a new way of building low-resourced dialect Automatic Speech Recognition (ASR) systems using a small database using the Asante-Twi dialect. Three different ASR systems with different features and methods have been tested and tried using the Kaldi toolkit. For the first and second Asante-Twi ASR systems, Mel Frequency Cepstral Coefficients (MFCC) feature extraction method was used with different context dependent parameters for each and the Perceptual Linear Prediction(PLP) feature extraction method was used for the third ASR system. To enhance the performance of the ASR systems, all feature extraction methods of the systems are improved using Cepstral Mean and Variance Normalization (CMVN) and Delta(Δ) dynamic features. In addition, the acoustic model unit of each ASR system using the GMM-HMM pattern classifier algorithm has been improved by training two context dependent (triphone) models, one on top of the other, and both on top of context independent (monophone) models to deliver better performance. Word Error Rate(WER) is used as metrics for measuring the accuracy performance of the systems. As the correct parameter setting for triphone models were used, the second ASR system saw about 50% reduction in %WER for the first triphone model and about 10% reduction in %WER values for the second triphone model as compared to the first ASR system. Decoding results show that the second ASR system was the most efficient system of all the ASR systems in percent WER because it produced the lowest value of 5.15% WER obtained from context dependent triphone models. The third ASR system, using the same triphone parameters as the second ASR, was the worst performing of all three. Thus, MFFCs are found to be the most suitable feature extraction technique when using noise-free data with context-dependent acoustic models being the best method for GMM-HMM acoustic modeling on a limited amount of data.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1086–1094},
numpages = {9},
keywords = {PLP, GMM-HMM, Asante-Twi dialect, ASR, MFCC, Kaldi toolkit},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.5555/3546258.3546365,
author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
title = {Beyond English-Centric Multilingual Machine Translation},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {107},
numpages = {48},
keywords = {neural networks, many-to-many, bitext mining, multilingual machine translation, model scaling}
}

@article{10.1109/TASLP.2021.3105798,
author = {Leong, Chongman and Liu, Xuebo and Wong, Derek F. and Chao, Lidia S.},
title = {Exploiting Translation Model for Parallel Corpus Mining},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3105798},
doi = {10.1109/TASLP.2021.3105798},
abstract = {Parallel corpus mining (PCM) is beneficial for many corpus-based natural language processing tasks, e.g., machine translation and bilingual dictionary induction, especially in low-resource languages and domains. It relies heavily on cross-lingual representations to model the interdependencies between different languages and determine whether sentences are parallel or not. In this paper, we take the first step towards exploiting the multilingual Transformer translation model to produce expressive sentence representations for PCM. Since the traditional Transformer lacks an immediate sentence representation, we pool the output representation of the encoder as the sentence representation, which is further optimized as a part of the training flow of the translation model. Experiments conducted on the BUCC PCM task show that the proposed method improves mining performance over the existing methods with the assistance of the pre-trained multilingual BERT. To further test the usability of the proposed method, we mine parallel sentences from public resources and find that the mined sentences can indeed enhance low-resource machine translation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2829–2839},
numpages = {11}
}

@inproceedings{10.1145/3377713.3377774,
author = {Kumar, Rashi and Jha, Piyush and Sahula, Vineet},
title = {An Augmented Translation Technique for Low Resource Language Pair: Sanskrit to Hindi Translation},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377774},
doi = {10.1145/3377713.3377774},
abstract = {Neural Machine Translation (NMT) is an ongoing technique for Machine Translation (MT) using an enormous artificial neural network. It has exhibited promising outcomes and has shown incredible potential in solving challenging machine translation exercises. One such exercise is the best approach to furnish great MT to language sets with a little preparing information. In this work, we inspect Zero-Shot Translation (ZST) for a low resource language pair. By working on high resource language pairs for which benchmarks are available, namely Spanish to Portuguese, and training on data sets(Spanish-English and English-Portuguese), we prepare a state of proof for ZST system that gives appropriate results on the available data. Subsequently, we test the same architecture for Sanskrit to Hindi translation for which data is sparse, by training the model on English-Hindi and Sanskrit-English language pairs. To prepare and decipher with the ZST system, we broaden the preparation and interpretation pipelines of the NMT seq2seq model in TensorFlow, incorporating ZST features. Dimensionality reduction of word embedding is performed to reduce the memory usage for data storage and to achieve faster training and translation cycles. In this work, existing helpful technology has been utilized imaginatively to execute our Natural Language Processing (NLP) issue of Sanskrit to Hindi translation. We have constructed a Sanskrit-Hindi parallel corpus of 300 sentences for testing. The data required for the construction of a parallel corpus has been taken from the telecasted news, published on the Department of Public Information, the state government of Madhya Pradesh, India website.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {377–383},
numpages = {7},
keywords = {Sanskrit Hindi Language Translation, Low resource languages, Neural Machine Translation},
location = {Sanya, China},
series = {ACAI '19}
}

@article{10.1109/TASLP.2020.2973896,
author = {Scharenborg, Odette and Ondel, Lucas and Palaskar, Shruti and Arthur, Philip and Ciannella, Francesco and Du, Mingxing and Larsen, Elin and Merkx, Danny and Riad, Rachid and Wang, Liming and Dupoux, Emmanuel and Besacier, Laurent and Black, Alan and Hasegawa-Johnson, Mark and Metze, Florian and Neubig, Graham and St\"{u}ker, Sebastian and Godard, Pierre and M\"{u}ller, Markus},
title = {Speech Technology for Unwritten Languages},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2973896},
doi = {10.1109/TASLP.2020.2973896},
abstract = {Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {964–975},
numpages = {12}
}

@inproceedings{10.1109/ICSE43902.2021.00047,
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
title = {Testing Machine Translation via Referential Transparency},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00047},
doi = {10.1109/ICSE43902.2021.00047},
abstract = {Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {410–422},
numpages = {13},
keywords = {Metamorphic testing, Referential transparency, Testing, Machine translation},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1145/3579163,
author = {Sebastian, Mary Priya and G., Santhosh Kumar},
title = {Malayalam Natural Language Processing: Challenges in Building a Phrase-Based Statistical Machine Translation System},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3579163},
doi = {10.1145/3579163},
abstract = {Statistical Machine Translation (SMT) is a preferred Machine Translation approach to convert the text in a specific language into another by automatically learning translations using a parallel corpus. SMT has been successful in producing quality translations in many foreign languages, but there are only a few works attempted in South Indian languages. The article discusses on experiments conducted with SMT for Malayalam language and analyzes how the methods defined for SMT in foreign languages affect a Dravidian language, Malayalam. The baseline SMT model does not work for Malayalam due to its unique characteristics like agglutinative nature and morphological richness. Hence, the challenge is to identify where precisely the SMT model has to be modified such that it adapts the challenges of the language peculiarity into the baseline model and give better translations for English to Malayalam translation. The alignments between English and Malayalam sentence pairs, subjected to the training process in SMT, plays a crucial role in producing quality output translation. Therefore, this work focuses on improving the translation model of SMT by refining the alignments between English–Malayalam sentence pairs. The phrase alignment algorithms align the verb and noun phrases in the sentence pairs and develop a new set of alignments for the English–Malayalam sentence pairs. These alignment sets refine the alignments formed from Giza++ produced as a result of EM training algorithm. The improved Phrase-Based SMT model trained using these refined alignments resulted in better translation quality, as indicated by the AER and BLUE scores.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {117},
numpages = {51},
keywords = {Dravidian language, Statistical Machine Translation, Natural Language Processing, alignments, Machine Translation, Malayalam}
}

@article{10.1613/jair.1.12699,
author = {Escolano, Carlos and R. Costa-juss\`{a}, Marta and R. Fonollosa, Jos\'{e} A.},
title = {Multilingual Machine Translation: Deep Analysis of Language-Specific Encoder-Decoders},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12699},
doi = {10.1613/jair.1.12699},
abstract = {State-of-the-art multilingual machine translation relies on a shared encoder-decoder. In this paper, we propose an alternative approach based on language-specific encoder-decoders, which can be easily extended to new languages by learning their corresponding modules. To establish a common interlingua representation, we simultaneously train N initial languages. Our experiments show that the proposed approach improves over the shared encoder-decoder for the initial languages and when adding new languages, without the need to retrain the remaining modules. All in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.},
journal = {J. Artif. Int. Res.},
month = {may},
numpages = {18},
keywords = {natural language, machine translation}
}

@inproceedings{10.1145/3313831.3376261,
author = {Liebling, Daniel J. and Lahav, Michal and Evans, Abigail and Donsbach, Aaron and Holbrook, Jess and Smus, Boris and Boran, Lindsey},
title = {Unmet Needs and Opportunities for Mobile Translation AI},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376261},
doi = {10.1145/3313831.3376261},
abstract = {Translation apps and devices are often presented in the context of providing assistance while traveling abroad. However, the spectrum of needs for cross-language communication is much wider. To investigate these needs, we conducted three studies with populations spanning socioeconomic status and geographic regions: (1) United States-based travelers, (2) migrant workers in India, and (3) immigrant populations in the United States. We compare frequent travelers' perception and actual translation needs with those of the two migrant communities. The latter two, with low language proficiency, have the greatest translation needs to navigate their daily lives. However, current mobile translation apps do not meet these needs. Our findings provide new insights on the usage practices and limitations of mobile translation tools. Finally, we propose design implications to help apps better serve these unmet needs.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {mobile, migrants, speech, emerging markets, machine translation, immigrants},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3406095,
author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
title = {A Survey of Multilingual Neural Machine Translation},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3406095},
doi = {10.1145/3406095},
abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in recent years. MNMT has been useful in improving translation quality as a result of translation knowledge transfer (transfer learning). MNMT is more promising and interesting than its statistical machine translation counterpart, because end-to-end modeling and distributed representations open new avenues for research on machine translation. Many approaches have been proposed to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and, hence, deserve further exploration. In this article, we present an in-depth survey of existing literature on MNMT. We first categorize various approaches based on their central use-case and then further categorize them based on resource scenarios, underlying modeling principles, core-issues, and challenges. Wherever possible, we address the strengths and weaknesses of several techniques by comparing them with each other. We also discuss the future directions for MNMT. This article is aimed towards both beginners and experts in NMT. We hope this article will serve as a starting point as well as a source of new ideas for researchers and engineers interested in MNMT.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {99},
numpages = {38},
keywords = {zero-shot, Neural machine translation, survey, multi-source, low-resource, multilingualism}
}

@article{10.1145/3445974,
author = {Lalrempuii, Candy and Soni, Badal and Pakray, Partha},
title = {An Improved English-to-Mizo Neural Machine Translation},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3445974},
doi = {10.1145/3445974},
abstract = {Machine Translation is an effort to bridge language barriers and misinterpretations, making communication more convenient through the automatic translation of languages. The quality of translations produced by corpus-based approaches predominantly depends on the availability of a large parallel corpus. Although machine translation of many Indian languages has progressively gained attention, there is very limited research on machine translation and the challenges of using various machine translation techniques for a low-resource language such as Mizo. In this article, we have implemented and compared statistical-based approaches with modern neural-based approaches for the English–Mizo language pair. We have experimented with different tokenization methods, architectures, and configurations. The performance of translations predicted by the trained models has been evaluated using automatic and human evaluation measures. Furthermore, we have analyzed the prediction errors of the models and the quality of predictions based on variations in sentence length and compared the model performance with the existing baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {61},
numpages = {21},
keywords = {TER, METEOR, Mizo, BLEU, low-resource language, transformer, Neural machine translation}
}

@article{10.1145/3526215,
author = {Chen, Linqing and Li, Junhui and Gong, Zhengxian and Zhang, Min and Zhou, Guodong},
title = {One Type Context Is Not Enough: Global Context-Aware Neural Machine Translation},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3526215},
doi = {10.1145/3526215},
abstract = {How to effectively model global context has been a critical challenge for document-level neural machine translation (NMT). Both preceding and global context have been carefully explored in the sequence-to-sequence (seq2seq) framework. However, previous studies generally map global context into one vector, which is not enough to well represent the entire document since this largely ignores the hierarchy between sentences and words within. In this article, we propose to model global context for source language from both sentence level and word level. Specifically at sentence level, we extract useful global context for the current sentence, while at word level, we compute global context against words within the current sentence. On this basis, both kinds of global context can be appropriately fused before being incorporated into the state-of-the-art seq2seq model, i.e., Transformer. Detailed experimentation on various document-level translation tasks shows that global context at both sentence level and word level significantly improve translation performance. More encouraging, both kinds of global context are complementary. This leads to more improvement when both kinds of global context are used.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {131},
numpages = {14},
keywords = {Neural machine translation, global context-aware, transformer, document-level}
}

@inproceedings{10.1145/3343031.3352587,
author = {Duarte, Amanda Cardoso},
title = {Cross-Modal Neural Sign Language Translation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3352587},
doi = {10.1145/3343031.3352587},
abstract = {Sign Language is the primary means of communication for the majority of the Deaf and hard-of-hearing communities. Current computational approaches in this general research area have focused specifically on sign language recognition and the translation of sign language to text. However, the reverse problem of translating from spoken to sign language has so far not been widely explored. The goal of this doctoral research is to explore sign language translation in this generalized setting, i.e. translating from spoken language to sign language and vice versa. Towards that end, we propose a concrete methodology for tackling the problem of speech to sign language translation and introduce How2Sign, the first public, continuous American Sign Language dataset that enables such research. With a parallel corpus of almost 60 hours of sign language videos (collected with both RGB and depth sensor data) and the corresponding speech transcripts for over 2500 instructional videos, How2Sign is a public dataset of unprecedented scale that can be used to advance not only sign language translation, but also a wide range of sign language understanding tasks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1650–1654},
numpages = {5},
keywords = {dataset, neural networks, american sign language, sign language translation, deep learning},
location = {Nice, France},
series = {MM '19}
}

@article{10.1109/TASLP.2019.2959224,
author = {Nishimura, Yuta and Sudoh, Katsuhito and Neubig, Graham and Nakamura, Satoshi},
title = {Multi-Source Neural Machine Translation With Missing Data},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959224},
doi = {10.1109/TASLP.2019.2959224},
abstract = {Machine translation is rife with ambiguities in word ordering and word choice, and even with the advent of machine-learning methods that learn to resolve this ambiguity based on statistics from large corpora, mistakes are frequent. <italic>Multi-source translation</italic> is an approach that attempts to resolve these ambiguities by exploiting multiple inputs (e.g. sentences in three different languages) to increase translation accuracy. These methods are trained on multilingual corpora, which include the multiple source languages and the target language, and then at test time uses information from both source languages while generating the target. While there are many of these multilingual corpora, such as multilingual translations of TED talks or European parliament proceedings, in practice, many multilingual corpora are not complete due to the difficulty to provide translations in <italic>all</italic> of the relevant languages. Existing studies on multi-source translation did not explicitly handle such situations, and thus are only applicable to complete corpora that have all of the languages of interest, severely limiting their practical applicability. In this article, we examine approaches for multi-source neural machine translation (NMT) that can learn from and translate such incomplete corpora. Specifically, we propose methods to deal with incomplete corpora at both training time and test time. For training time, we examine two methods: (1) a simple method that simply replaces missing source translations with a special NULL symbol, and (2) a data augmentation approach that fills in incomplete parts with source translations created from multi-source NMT. For test-time, we examine methods that use multi-source translation even when only a single source is provided by first translating into an additional auxiliary language using standard NMT, then using multi-source translation on the original source and this generated auxiliary language sentence. Extensive experiments demonstrate that the proposed training-time and test-time methods both significantly improve translation performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {569–580},
numpages = {12}
}

@article{10.1145/3441691,
author = {Maruf, Sameen and Saleh, Fahimeh and Haffari, Gholamreza},
title = {A Survey on Document-Level Neural Machine Translation: Methods and Evaluation},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3441691},
doi = {10.1145/3441691},
abstract = {Machine translation (MT) is an important task in natural language processing (NLP), as it automates the translation process and reduces the reliance on human translators. With the resurgence of neural networks, the translation quality surpasses that of the translations obtained using statistical techniques for most language-pairs. Up until a few years ago, almost all of the neural translation models translated sentences independently, without incorporating the wider document-context and inter-dependencies among the sentences. The aim of this survey article is to highlight the major works that have been undertaken in the space of document-level machine translation after the neural revolution, so researchers can recognize the current state and future directions of this field. We provide an organization of the literature based on novelties in modelling and architectures as well as training and decoding strategies. In addition, we cover evaluation strategies that have been introduced to account for the improvements in document MT, including automatic metrics and discourse-targeted test sets. We conclude by presenting possible avenues for future exploration in this research field.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {45},
numpages = {36},
keywords = {Context-aware neural machine translation}
}

@article{10.1109/TASLP.2020.2995270,
author = {Fan, Yang and Tian, Fei and Xia, Yingce and Qin, Tao and Li, Xiang-Yang and Liu, Tie-Yan},
title = {Searching Better Architectures for Neural Machine Translation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2995270},
doi = {10.1109/TASLP.2020.2995270},
abstract = {Neural architecture search (NAS) has played important roles in the evolution of neural architectures. However, no much attention has been paid to improve neural machine translation (NMT) through NAS approaches. In this work, we propose a gradient-based NAS algorithm for NMT, which automatically discovers architectures with better performances. Compared with previous NAS work, we jointly search the network operations (e.g., LSTM, CNN, self-attention etc) as well as dropout rates to ensure better results. We show that with reasonable resources it is possible to discover novel neural network architectures for NMT, which achieve consistently better performances than Transformer [1], the state-of-the-art NMT model, across different tasks. On WMT'14 English-to-German translation, IWSLT'14 German-to-English translation and WMT'18 Finnish-to-English translation tasks, our discovered architectures could obtain 30.1, 36.1 and 26.4 BLEU scores, which are great improvement over Transformer baselines. We also empirically verify that the discovered model on one task can be transferred to other tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1574–1585},
numpages = {12}
}

@article{10.1145/3524300,
author = {Shi, Shumin and Wu, Xing and Su, Rihai and Huang, Heyan},
title = {Low-Resource Neural Machine Translation: Methods and Trends},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3524300},
doi = {10.1145/3524300},
abstract = {Neural Machine Translation (NMT) brings promising improvements in translation quality, but until recently, these models rely on large-scale parallel corpora. As such corpora only exist on a handful of language pairs, the translation performance is far from the desired effect in the majority of low-resource languages. Thus, developing low-resource language translation techniques is crucial and it has become a popular research field in neural machine translation. In this article, we make an overall review of existing deep learning techniques in low-resource NMT. We first show the research status as well as some widely used low-resource datasets. Then, we categorize the existing methods and show some representative works detailedly. Finally, we summarize the common characters among them and outline the future directions in this field.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {103},
numpages = {22},
keywords = {Low-resource, pivot-based methods, transfer learning, neural machine translation, data augmentation, unsupervised, semi-supervised}
}

@article{10.1109/TASLP.2021.3076863,
author = {Guo, Junliang and Zhang, Zhirui and Xu, Linli and Chen, Boxing and Chen, Enhong},
title = {Adaptive Adapters: An Efficient Way to Incorporate BERT Into Neural Machine Translation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076863},
doi = {10.1109/TASLP.2021.3076863},
abstract = {Large-scale pre-trained language models (e.g., BERT) have attracted great attention in recent years. It is straightforward to fine-tune them on natural language understanding tasks such as text classification, however, effectively and efficiently incorporating them into natural language generation tasks such as neural machine translation remains a challenging problem. In this paper, we integrate two pre-trained BERT models from the source and target language domains into a sequence-to-sequence model by introducing light-weight adapter modules. The adapters are inserted between BERT layers and tuned on downstream tasks, while the parameters of BERT models are fixed during fine-tuning. As pre-trained language models are usually very deep, inserting adapters into all layers will result in a considerable scale of new parameters. To deal with this problem, we introduce latent variables to decide whether using adapters or not in each layer, which are learned during fine-tuning. In this way, the model is able to automatically determine which adapters to use, therefore hugely promoting the parameter efficiency and decoding speed. We evaluate the proposed framework on various neural machine translation tasks. Equipped with parallel sequence decoding, our model consistently outperforms autoregressive baselines while reducing the inference latency by half. With automatic adapter selection, the proposed model further achieves 20% speedup while still outperforming autoregressive baselines. When applied to autoregressive decoding, the proposed model can also achieve comparable performance with the state-of-the-art baseline models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1740–1751},
numpages = {12}
}

@inproceedings{10.1145/3572549.3572646,
author = {Zhao, Shuo},
title = {Computer Corpus-Based Study of Film and Television Translation},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572646},
doi = {10.1145/3572549.3572646},
abstract = {Chinese traditional cultural resources have been fully presented in film and television which have different characteristics in translation and culture. It is believed that film and television translation is an exaggerated form of artistic expression, and vividly shows invisible human emotions, lifestyles and cultural activities in a perceptual way. Corpus-based translation studies have made film and television translationin gratifying progress and become an important paradigm of cultural studies. Corpus translation inherits the mantle of descriptive translatology and draws on corpus linguistic methods. It also has the unique requirements of subtitle translation, which are instantaneity, popularity, cross-culture and listening. Corpus-based film and television text translation is a cross-modal translation activity.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {606–609},
numpages = {4},
keywords = {Cross-culture, Film and Television, Translation, Corpus},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@inproceedings{10.1145/3600211.3604672,
author = {Ghosh, Sourojit and Caliskan, Aylin},
title = {ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five Other Low-Resource Languages},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604672},
doi = {10.1145/3600211.3604672},
abstract = {In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in machine translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT’s accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7th most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g., man = doctor, woman = nurse) or actions (e.g., woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to ‘he’ or ‘she’. We also observe ChatGPT completely failing to translate the English gender-neutral singular pronoun ‘they’ into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AI systems that perform machine translation to better accommodate such low-resource languages.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {901–912},
numpages = {12},
keywords = {Bengali, human-centered design, ChatGPT, machine translation, language models, gender bias},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@article{10.1145/3314936,
author = {Trieu, Hai-Long and Tran, Duc-Vu and Ittoo, Ashwin and Nguyen, Le-Minh},
title = {Leveraging Additional Resources for Improving Statistical Machine Translation on Asian Low-Resource Languages},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314936},
doi = {10.1145/3314936},
abstract = {Phrase-based machine translation (MT) systems require large bilingual corpora for training. Nevertheless, such large bilingual corpora are unavailable for most language pairs in the world, causing a bottleneck for the development of MT. For the Asian language pairs—Japanese, Indonesian, Malay paired with Vietnamese—they are also not excluded from the case, in which there are no large bilingual corpora on these low-resource language pairs. Furthermore, although the languages are widely used in the world, there is no prior work on MT, which causes an issue for the development of MT on these languages. In this article, we conducted an empirical study of leveraging additional resources to improve MT for the Asian low-resource language pairs: translation from Japanese, Indonesian, and Malay to Vietnamese. We propose an innovative approach that lies in two strategies of building bilingual corpora from comparable data and phrase pivot translation on existing bilingual corpora of the languages paired with English. Bilingual corpora were built from Wikipedia bilingual titles to enhance bilingual data for the low-resource languages. Additionally, we introduced a combined model of the additional resources to create an effective solution to improve MT on the Asian low-resource languages. Experimental results show the effectiveness of our systems with the improvement of +2 to +7 BLEU points. This work contributes to the development of MT on low-resource languages, especially opening a promising direction for the progress of MT on the Asian language pairs.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {32},
numpages = {22},
keywords = {pivot methods, semantic similarity, sentence alignment, low-resource languages, Statistical machine translation}
}

@inproceedings{10.1145/3440084.3441214,
author = {Ahmadnia, Benyamin},
title = {Linked Data Effectiveness in Neural Machine Translation},
year = {2021},
isbn = {9781450388894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440084.3441214},
doi = {10.1145/3440084.3441214},
abstract = {Quality of data-driven Machine Translation (MT) systems depends on large volumes of data from which models can be constructed to leverage patterns and knowledge from these datasets. In corpus-based MT systems, Out-Of-Vocabulary (OOV) words and ambiguous translations are the most common sources of error. In this paper, JRC-Names and DBpedia have been employed as Linked Data (LD) to minimize the aforementioned types of errors on top of a Neural MT (NMT) model. Three strategies have been evaluated for exploiting knowledge from LD in translating named entities; 1) Dictionaries, 2) Pre-decoding, and 3) Post-editing. Based on the experimental results, these strategies optimize the benefit of the multilingual LD to NMT application. The experiments on English-Spanish translation as well as English-French translation evaluate the validity of the proposed idea.},
booktitle = {Proceedings of the 2020 4th International Symposium on Computer Science and Intelligent Control},
articleno = {38},
numpages = {4},
keywords = {OOV words, Computational linguistics, Linked data, Natural language processing, Neural machine translation},
location = {Newcastle upon Tyne, United Kingdom},
series = {ISCSIC 2020}
}

@article{10.1162/coli_a_00367,
author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
title = {On the Linguistic Representational Power of Neural Machine Translation Models},
year = {2020},
issue_date = {March 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00367},
doi = {10.1162/coli_a_00367},
abstract = {Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.},
journal = {Comput. Linguist.},
month = {mar},
pages = {1–52},
numpages = {52}
}

@article{10.1613/jair.1.13546,
author = {Haralampieva, Veneta and Caglayan, Ozan and Specia, Lucia},
title = {Supervised Visual Attention for Simultaneous Multimodal Machine Translation},
year = {2022},
issue_date = {Sep 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {74},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13546},
doi = {10.1613/jair.1.13546},
abstract = {There has been a surge in research in multimodal machine translation (MMT), where additional modalities such as images are used to improve translation quality of textual systems. A particular use for such multimodal systems is the task of simultaneous machine translation, where visual context has been shown to complement the partial information provided by the source sentence, especially in the early phases of translation. In this paper, we propose the first Transformer-based simultaneous MMT architecture, which has not been previously explored in simultaneous translation. Additionally, we extend this model with an auxiliary supervision signal that guides the visual attention mechanism using labelled phrase-region alignments. We perform comprehensive experiments on three language directions and conduct thorough quantitative and qualitative analyses using both automatic metrics and manual inspection. Our results show that (i) supervised visual attention consistently improves the translation quality of the simultaneous MMT models, and (ii) fine-tuning the MMT with supervision loss enabled leads to better performance than training the MMT from scratch. Compared to the state-of-the-art, our proposed model achieves improvements of up to 2.3 BLEU and 3.5 METEOR points.},
journal = {J. Artif. Int. Res.},
month = {sep},
numpages = {31}
}

@inproceedings{10.1145/3477495.3532071,
author = {Bai, Yu and Huang, Heyan and Fan, Kai and Gao, Yang and Zhu, Yiming and Zhan, Jiaao and Chi, Zewen and Chen, Boxing},
title = {Unifying Cross-Lingual Summarization and Machine Translation with Compression Rate},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532071},
doi = {10.1145/3477495.3532071},
abstract = {Cross-Lingual Summarization (CLS) is a task that extracts important information from a source document and summarizes it into a summary in another language. It is a challenging task that requires a system to understand, summarize, and translate at the same time, making it highly related to Monolingual Summarization (MS) and Machine Translation (MT). In practice, the training resources for Machine Translation are far more than that for cross-lingual and monolingual summarization. Thus incorporating the Machine Translation corpus into CLS would be beneficial for its performance. However, the present work only leverages a simple multi-task framework to bring Machine Translation in, lacking deeper exploration.In this paper, we propose a novel task, Cross-lingual Summarization with Compression rate (CSC), to benefit Cross-Lingual Summarization by large-scale Machine Translation corpus. Through introducing compression rate, the information ratio between the source and the target text, we regard the MT task as a special CLS task with a compression rate of 100%. Hence they can be trained as a unified task, sharing knowledge more effectively. However, a huge gap exists between the MT task and the CLS task, where samples with compression rates between 30% and 90% are extremely rare. Hence, to bridge these two tasks smoothly, we propose an effective data augmentation method to produce document-summary pairs with different compression rates. The proposed method not only improves the performance of the CLS task, but also provides controllability to generate summaries in desired lengths. Experiments demonstrate that our method outperforms various strong baselines in three cross-lingual summarization datasets. We released our code and data at https://github.com/ybai-nlp/CLS_CR.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1087–1097},
numpages = {11},
keywords = {machine translation, compression rate, cross-lingual summarization},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3544548.3580790,
author = {Robertson, Samantha and Wang, Zijie J. and Moritz, Dominik and Kery, Mary Beth and Hohman, Fred},
title = {Angler: Helping Machine Translation Practitioners Prioritize Model Improvements},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580790},
doi = {10.1145/3544548.3580790},
abstract = {Machine learning (ML) models can fail in unexpected ways in the real world, but not all model failures are equal. With finite time and resources, ML practitioners are forced to prioritize their model debugging and improvement efforts. Through interviews with 13 ML practitioners at Apple, we found that practitioners construct small targeted test sets to estimate an error’s nature, scope, and impact on users. We built on this insight in a case study with machine translation models, and developed Angler, an interactive visual analytics tool to help practitioners prioritize model improvements. In a user study with 7 machine translation experts, we used Angler to understand prioritization practices when the input space is infinite, and obtaining reliable signals of model quality is expensive. Our study revealed that participants could form more interesting and user-focused hypotheses for prioritization by analyzing quantitative summary statistics and qualitatively assessing data by reading sentences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {832},
numpages = {20},
keywords = {Model evaluation, machine translation, visual analytics},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3531146.3533244,
author = {Mehandru, Nikita and Robertson, Samantha and Salehi, Niloufar},
title = {Reliable and Safe Use of Machine Translation in Medical Settings},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533244},
doi = {10.1145/3531146.3533244},
abstract = {Language barriers between patients and clinicians contribute to disparities in quality of care. Machine Translation (MT) tools are widely used in healthcare settings, but even small mistranslations can have life-threatening consequences. We study how MT is currently used in medical settings through a qualitative interview study with 20 clinicians–physicians, surgeons, nurses, and midwives. We find that clinicians face challenges stemming from lack of time and resources, cultural barriers, and medical literacy rates, as well as accountability in cases of miscommunication. Clinicians have devised strategies to aid communication in the face of language barriers including back translation, non-verbal communication, and testing patient understanding. We propose design implications for machine translation systems including combining neural MT with pre-translated medical phrases, integrating translation support with multimodal communication, and providing interactive support for testing mutual understanding.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2016–2025},
numpages = {10},
keywords = {Machine Translation in Medicine, Clinical Decision-Making, Reliability of AI Systems},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.1145/3357612,
author = {Jung, Hun-Young and Lee, Jong-Hyeok and Min, Eunju and Na, Seung-Hoon},
title = {Word Reordering for Translation into Korean Sign Language Using Syntactically-Guided Classification},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3357612},
doi = {10.1145/3357612},
abstract = {Machine translation aims to break the language barrier that prevents communication with others and increase access to information. Deaf people face huge language barriers in their daily lives, including access to digital and spoken information. There are very few digital resources for sign language processing. In this article, we present a transfer-based machine translation system for translating Korean-to-Korean Sign Language (KSL) glosses, mainly composed of (1) dictionary-based lexical transfer and (2) a hybrid syntactic transfer based on a data-driven model. In particular, we formulate complicated word reordering problems in syntactic transfer as multi-class classification tasks and propose “syntactically guided” data-driven syntactic transfer. The core part of our study is a neural classification model for reordering order-important constituent pairs with a reordering task that is newly designed for Korean-to-KSL translation. The experiment results evaluated on news transcript data show that the proposed system achieves a BLEU score of 0.512 and a RIBES score of 0.425, significantly improving upon the baseline system performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {31},
numpages = {20},
keywords = {neural networks, sentence embedding, Sign language, word reordering, machine translation}
}

@article{10.1613/jair.1.13566,
author = {Saunders, Danielle},
title = {Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey},
year = {2022},
issue_date = {Dec 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {75},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13566},
doi = {10.1613/jair.1.13566},
abstract = {The development of deep learning techniques has allowed Neural Machine Translation (NMT) models to become extremely powerful, given sufficient training data and training time. However, systems struggle when translating text from a new domain with a distinct style or vocabulary. Fine-tuning on in-domain data allows good domain adaptation, but requires sufficient relevant bilingual data. Even if this is available, simple fine-tuning can cause overfitting to new data and catastrophic forgetting of previously learned behaviour. We survey approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains. We divide techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure. We finally highlight the benefits of domain adaptation and multidomain adaptation techniques to other lines of NMT research.},
journal = {J. Artif. Int. Res.},
month = {dec},
numpages = {74}
}

@inproceedings{10.1145/3572549.3572564,
author = {Zhou, Xinshan and Liu, Yang and Yin, Chunjie},
title = {Translation Comprehension Misunderstandings of English Majors in the Information Era: — A Case Study of Offline English-Chinese Translation Test},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572564},
doi = {10.1145/3572549.3572564},
abstract = {In the past, English major students carried heavy dictionaries while learning English. Nowadays, they can obtain more information quickly and efficiently by using electronic devices. This is a double-edged sword. On the one hand, it saves students time to consult a dictionary. On the other hand, such learning methods rely heavily upon translation software, which is harmful. This study would connect online learning with an offline translation test. To identify the causes of translation comprehension misunderstandings, 60 junior students in a language education university in China were selected as research participants. After collecting and analyzing their offline translation test papers, the students were divided into three groups for interview. The results showed that the leading causes that affect translation performance are internal and external factors. The former are students’ bilingual abilities and background knowledge of the source text; the latter include mother tongue interference and the length of the text and the way of teachers’ teaching, which are difficult for students to change through their own efforts. It is necessary to improve the translation ability of English majors by identifying the advantages and disadvantages of using translation software in the information era. Teachers and students should use it well instead of abusing it.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {90–95},
numpages = {6},
keywords = {Translation software, Online learning, Translation, Misunderstandings},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@inproceedings{10.1145/3531146.3534638,
author = {Robertson, Samantha and D\'{\i}az, Mark},
title = {Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534638},
doi = {10.1145/3531146.3534638},
abstract = {Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2223–2238},
numpages = {16},
keywords = {explainable machine learning, computer-mediated communication, machine translation, human-AI interaction},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1109/ICSE.2019.00021,
author = {Tufano, Michele and Pantiuchina, Jevgenija and Watson, Cody and Bavota, Gabriele and Poshyvanyk, Denys},
title = {On Learning Meaningful Code Changes via Neural Machine Translation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00021},
doi = {10.1109/ICSE.2019.00021},
abstract = {Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL.Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {25–36},
numpages = {12},
keywords = {neural-machine translation, empirical study},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3362789.3362849,
author = {V\'{a}zquez, Maximiliano Fr\'{\i}as and P\'{e}rez, Francisco Seoane},
title = {Hate Speech in Spain Against Aquarius Refugees 2018 in Twitter},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362849},
doi = {10.1145/3362789.3362849},
abstract = {High-profile events can trigger online hate speech, which in turn modify attitudes and offline behavior towards stigmatized groups. This paper addresses the first path of this process by using manual and computational methods to analyze the complete stream of Twitter messages in Spanish referring the boat Aquarius (N = 24,254) From the rejection of Italy, until the arrival at the Spanish port of Valencia, which was a milestone for the entry of refugees and was highly covered by the media. We found that most of the messages revolved around few topics and were mostly positive, but a significant part of negative messages included hate speech towards refugees and rejection of politicians. Supporting our hypothesis, online hate speech grew after the announcement. The general positive sentiment paradoxically increased, and the language sentiment became less negative. We discuss the theoretical and practical implications, and acknowledge limitations referred to the examined timeframe, suggesting more longitudinal analyses.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {906–910},
numpages = {5},
keywords = {Supervised learning, Machine learning, xenophobia, Hate speech},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.1109/TASLP.2022.3153264,
author = {Liang, Xiaobo and Wu, Lijun and Li, Juntao and Qin, Tao and Zhang, Min and Liu, Tie-Yan},
title = {Multi-Teacher Distillation With Single Model for Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153264},
doi = {10.1109/TASLP.2022.3153264},
abstract = {Knowledge distillation (KD) is an effective strategy for neural machine translation (NMT) to improve the performance of a student model. Usually, the teacher can guide the student to be better by distilling the soft label or data knowledge from the teacher itself. However, the data diversity and teacher knowledge are limited with only one teacher model. Though a natural solution is to adopt multiple randomized teacher models, one big shortcoming is that the model parameters and training costs are largely increased with the number of teacher models. In this work, we explore to mimic multiple teacher distillation from the sub-network space and permuted variants of one single teacher model. Specifically, we train a teacher by multiple sub-network extraction paradigms: sub-layer reordering, layer-drop, and dropout variants. In doing so, one teacher model can provide multiple outputs variants and causes neither additional parameters nor much extra training cost. Experiments on 8 IWSLT datasets: IWSLT14 En <inline-formula><tex-math notation="LaTeX">$leftrightarrow$</tex-math></inline-formula> De, En <inline-formula><tex-math notation="LaTeX">$leftrightarrow$</tex-math></inline-formula> Es and IWSLT17 En <inline-formula><tex-math notation="LaTeX">$leftrightarrow$</tex-math></inline-formula> Fr, En <inline-formula><tex-math notation="LaTeX">$leftrightarrow$</tex-math></inline-formula> Zh and the large WMT14 EN <inline-formula><tex-math notation="LaTeX">$to$</tex-math></inline-formula> DE translation tasks show that our method even achieves nearly comparable performance with multiple teacher models with different randomized parameters, both word-level and sequence-level knowledge distillation. Our code is available online at <uri>https://github.com/dropreg/RLD</uri>.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {992–1002},
numpages = {11}
}

@article{10.1109/TASLP.2020.3042006,
author = {Duan, Chaoqun and Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhu, Conghui and Zhao, Tiejun},
title = {Modeling Future Cost for Neural Machine Translation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3042006},
doi = {10.1109/TASLP.2020.3042006},
abstract = {Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, in this article, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a future cost representation is learned based on the current generated target word and its contextual information to compute an additional loss to guide the training of the NMT model. Furthermore, the learned future cost representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {770–781},
numpages = {12}
}

@inproceedings{10.1145/3536221.3556621,
author = {Nayak, Shravan and Schuler, Christian and Saha, Debjoy and Baumann, Timo},
title = {A Deep Dive Into Neural Synchrony Evaluation for Audio-Visual Translation},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556621},
doi = {10.1145/3536221.3556621},
abstract = {We present a comprehensive analysis of the neural audio-visual synchrony evaluation tool SyncNet. We assess the agreement of SyncNet scores vis-a-vis human perception and whether we can use these as a reliable metric for evaluating audio-visual lip-synchrony in generation tasks with no ground truth reference audio-video pair. We further look into the underlying elements in audio and video which vitally affect synchrony using interpretable explanations from SyncNet predictions and analyse its susceptibility by introducing adversarial noise. SyncNet has been used in numerous papers on visually-grounded text-to-speech for scenarios such as dubbing. We focus on this scenario which features many local asynchronies (something that SyncNet isn’t made for).},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {642–647},
numpages = {6},
keywords = {dubbing, audio-visual synchrony, speech-lip synchrony},
location = {Bengaluru, India},
series = {ICMI '22}
}

@article{10.1109/TASLP.2020.2996077,
author = {Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun and Yang, Muyun and Zhao, Hai},
title = {Towards More Diverse Input Representation for Neural Machine Translation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2996077},
doi = {10.1109/TASLP.2020.2996077},
abstract = {Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1586–1597},
numpages = {12}
}

@inbook{10.1145/3233795.3233811,
author = {Waibel, Alexander},
title = {Multimodal Dialogue Processing for Machine Translation},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233811},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {577–620},
numpages = {44}
}

@article{10.1109/TASLP.2020.3036776,
author = {Mart\'{\i}n-Do\~{n}as, Juan Manuel and Jensen, Jesper and Tan, Zheng-Hua and Gomez, Angel M. and Peinado, Antonio M.},
title = {Online Multichannel Speech Enhancement Based on Recursive EM and DNN-Based Speech Presence Estimation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3036776},
doi = {10.1109/TASLP.2020.3036776},
abstract = {This article presents a recursive expectation-maximization algorithm for online multichannel speech enhancement. A deep neural network mask estimator is used to compute the speech presence probability, which is then improved by means of statistical spatial models of the noisy speech and noise signals. The clean speech signal is estimated using beamforming, single-channel linear postfiltering and speech presence masking. The clean speech statistics and speech presence probabilities are finally used to compute the acoustic parameters for beamforming and postfiltering by means of maximum likelihood estimation. This iterative procedure is carried out on a frame-by-frame basis. The algorithm integrates the different estimates in a common statistical framework suitable for online scenarios. Moreover, our method can successfully exploit spectral, spatial and temporal speech properties. Our proposed algorithm is tested in different noisy environments using the multichannel recordings of the CHiME-4 database. The experimental results show that our method outperforms other related state-of-the-art approaches in noise reduction performance, while allowing low-latency processing for real-time applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {3080–3094},
numpages = {15}
}

@inproceedings{10.1145/3299819.3299844,
author = {Elnaggar, Ahmed and Gebendorfer, Christoph and Glaser, Ingo and Matthes, Florian},
title = {Multi-Task Deep Learning for Legal Document Translation, Summarization and Multi-Label Classification},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299844},
doi = {10.1145/3299819.3299844},
abstract = {The digitalization of the legal domain has been ongoing for a couple of years. In that process, the application of different machine learning (ML) techniques is crucial. Tasks such as the classification of legal documents or contract clauses as well as the translation of those are highly relevant. On the other side, digitized documents are barely accessible in this field, particularly in Germany. Today, deep learning (DL) is one of the hot topics with many publications and various applications. Sometimes it provides results outperforming the human level. Hence this technique may be feasible for the legal domain as well. However, DL requires thousands of samples to provide decent results. A potential solution to this problem is multi-task DL to enable transfer learning. This approach may be able to overcome the data scarcity problem in the legal domain, specifically for the German language. We applied the state of the art multi-task model on three tasks: translation, summarization, and multi-label classification. The experiments were conducted on legal document corpora utilizing several task combinations as well as various model parameters. The goal was to find the optimal configuration for the tasks at hand within the legal domain. The multi-task DL approach outperformed the state of the art results in all three tasks. This opens a new direction to integrate DL technology more efficiently in the legal domain.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {9–15},
numpages = {7},
keywords = {Multi-label, Summarization, Translation, Multi-task Deep Learning, Classifcation},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@article{10.1109/TASLP.2020.2982282,
author = {Sun, Haipeng and Wang, Rui and Chen, Kehai and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
title = {Unsupervised Neural Machine Translation With Cross-Lingual Language Representation Agreement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982282},
doi = {10.1109/TASLP.2020.2982282},
abstract = {Unsupervised cross-lingual language representation initialization methods such as unsupervised bilingual word embedding (UBWE) pre-training and cross-lingual masked language model (CMLM) pre-training, together with mechanisms such as denoising and back-translation, have advanced unsupervised neural machine translation (UNMT), which has achieved impressive results on several language pairs, particularly French-English and German-English. Typically, UBWE focuses on initializing the word embedding layer in the encoder and decoder of UNMT, whereas the CMLM focuses on initializing the entire encoder and decoder of UNMT. However, UBWE/CMLM training and UNMT training are independent, which makes it difficult to assess how the quality of UBWE/CMLM affects the performance of UNMT during UNMT training. In this paper, we first empirically explore relationships between UNMT and UBWE/CMLM. The empirical results demonstrate that the performance of UBWE and CMLM has a significant influence on the performance of UNMT. Motivated by this, we propose a novel UNMT structure with cross-lingual language representation agreement to capture the interaction between UBWE/CMLM and UNMT during UNMT training. Experimental results on several language pairs demonstrate that the proposed UNMT models improve significantly over the corresponding state-of-the-art UNMT baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1170–1182},
numpages = {13}
}

@article{10.1145/3382039,
author = {Hong, Jonggi and Vaing, Christine and Kacorri, Hernisa and Findlater, Leah},
title = {Reviewing Speech Input with Audio: Differences between Blind and Sighted Users},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3382039},
doi = {10.1145/3382039},
abstract = {Speech input is a primary method of interaction for blind mobile device users, yet the process of dictating and reviewing recognized text through audio only (i.e., without access to visual feedback) has received little attention. A recent study found that sighted users could identify only about half of automatic speech recognition (ASR) errors when listening to text-to-speech output of the ASR results. Blind screen reader users, in contrast, may be better able to identify ASR errors through audio due to their greater use of speech interaction and increased ability to comprehend synthesized speech. To compare the experiences of blind and sighted users with speech input and ASR errors, as well as to compare their ability to identify ASR errors through audio-only interaction, we conducted a lab study with 12 blind and 12 sighted participants. The study included a semi-structured interview portion to qualitatively understand experiences with ASR, followed by a controlled speech input task to quantitatively compare participants’ ability to identify ASR errors in their dictated text. Findings revealed differences between blind and sighted participants in terms of how they use speech input and their level of concern for ASR errors (e.g., blind users were more highly concerned). In the speech input task, blind participants were able to identify only 40% of ASR errors, which, counter to our hypothesis, was not significantly different from sighted participants’ performance. In depth analysis of speech input, ASR errors, and strategy of identifying ASR errors scrutinized how participants entered a text with speech input and reviewed it. Our findings indicate the need for future work on how to support blind users in confidently using speech input to generate accurate, error-free text.},
journal = {ACM Trans. Access. Comput.},
month = {apr},
articleno = {2},
numpages = {28},
keywords = {text entry, ASR errors, dictation, visual impairment, synthesized speech, Speech input, blind}
}

@article{10.1109/TASLP.2018.2888814,
author = {Deena, Salil and Hasan, Madina and Doulaty, Mortaza and Saz, Oscar and Hain, Thomas},
title = {Recurrent Neural Network Language Model Adaptation for Multi-Genre Broadcast Speech Recognition and Alignment},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2888814},
doi = {10.1109/TASLP.2018.2888814},
abstract = {Recurrent neural network language models RNNLMs generally outperform $n$-gram language models when used in automatic speech recognition ASR. Adapting RNNLMs to new domains is an open problem and current approaches can be categorised as either feature-based or model based. In feature-based adaptation, the input to the RNNLM is augmented with auxiliary features whilst model-based adaptation includes model fine-tuning and the introduction of adaptation layers in the network. In this paper, the properties of both types of adaptation are investigated on multi-genre broadcast speech recognition. Existing techniques for both types of adaptation are reviewed and the proposed techniques for model-based adaptation, namely the linear hidden network adaptation layer and the $K$-component adaptive the RNNLM, are investigated. Moreover, new features derived from the acoustic domain are investigated for the RNNLM adaptation. The contributions of this paper include two hybrid adaptation techniques: the fine-tuning of feature-based RNNLMs and a feature-based adaptation layer. Moreover, the semi-supervised adaptation of RNNLMs using genre information is also proposed. The ASR systems were trained using 700 h of multi-genre broadcast speech. The gains obtained when using the RNNLM adaptation techniques proposed in this paper are consistent when using RNNLMs trained on an in-domain set of 10M words and on a combination of in-domain and out-of-domain sets of 660 M words, with approx. $text{10}{%}$ perplexity and $text{2}{%}$ relative word error rate improvements on a 28.3 h. test set. The best RNNLM adaptation techniques for ASR are also evaluated on a lightly supervised alignment of subtitles task for the same data, where the use of RNNLM adaptation leads to an absolute increase in the F–measure of $text{0.5}{%}$.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {572–582},
numpages = {11}
}

@article{10.1109/TASLP.2018.2848698,
author = {Dubey, Harishchandra and Sangwan, Abhijeet and Hansen, John H. L.},
title = {Leveraging Frequency-Dependent Kernel and DIP-Based Clustering for Robust Speech Activity Detection in Naturalistic Audio Streams},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2848698},
doi = {10.1109/TASLP.2018.2848698},
abstract = {Speech activity detection SAD is front-end in most speech systems, e.g., speaker verification, speech recognition&nbsp;etc. Supervised SAD typically leverages machine learning models trained on annotated data. For applications like zero-resource speech processing and NIST-OpenSAT-2017 public safety communications task, it might not be feasible to collect SAD annotations. SAD is challenging for naturalistic audio streams containing multiple noise-sources simultaneously. We propose a novel frequency-dependent kernel FDK based SAD features. FDK provides enhanced spectral decomposition from which several statistical descriptors are derived. FDK statistical descriptors are combined by principal component analysis into one-dimensional FDK-SAD features. We further proposed two decision backends: First, variable model-size Gaussian mixture model VMGMM; and second, Hartigan dip-based robust feature clustering. While VMGMM is a model-based approach, the DipSAD is nonparametric. We used both backends for comparative evaluations in two phases: first, standalone SAD performance; and second, the effect of SAD on text-dependent speaker verification using RedDots data. The NIST-OpenSAD-2015 and NIST-OpenSAT-2017 corpora are used for standalone SAD evaluations. We establish two Center for Robust Speech Systems CRSS corpora namely CRSS-PLTL-II and CRSS long-duration naturalistic noise corpus. The CRSS corpora facilitate standalone SAD evaluations on naturalistic audio streams. We performed comparative studies of the proposed approaches with multiple baselines including SohnSAD, rSAD, semisupervised Gaussian mixture model, and Gammatone spectrogram features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2056–2071},
numpages = {16}
}

@inproceedings{10.1145/3486011.3486492,
author = {Blanco-Herrero, David and S\'{a}nchez-Holgado, Patricia},
title = {Fake News and Hate Speech: Who is to Blame? Study of the Perceptions of Spanish Citizens about the Actors Responsible for the Production and Spread of Fake News and Hate Speech},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486492},
doi = {10.1145/3486011.3486492},
abstract = {Fake news and hate speech are among the biggest challenges of online communication. A survey to 421 Spanish citizens tries to discover who they consider responsible for the production and spread of these phenomena. In general, politicians and radical groups are most commonly blamed for the production of fake news, hate speech and for fake news spreading hate speech, although important differences can be found based on gender, age and political ideology, being women, older people and left-wing people most worried about these phenomena. The study also observed that citizens tend to believe that those producing and, to a lesser extent, those sharing this content are aware of its potential consequences. These observations allow a deeper understanding of disinformation and hatred, helping designing strategies to combat them, and highlighting the need to approach them together.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {448–451},
numpages = {4},
keywords = {Fake news, Citizens survey, Spain, Hate speech, Disinformation},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/3397482.3450714,
author = {Gupta, Ananya},
title = {User-Controlled Content Translation in Social Media},
year = {2021},
isbn = {9781450380188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397482.3450714},
doi = {10.1145/3397482.3450714},
abstract = {As it has become increasingly common for social network users to write and view post in languages other than English, most social networks now provide machine translations to allow posts to be read by an audience beyond native speakers. However, authors typically cannot view the translations of their posts and have little control over these translations. To address this issue, I am developing a prototype that will provide authors with transparency of and more personalized control over the translation of their posts.},
booktitle = {26th International Conference on Intelligent User Interfaces - Companion},
pages = {96–98},
numpages = {3},
keywords = {machine translation, social network sites, non-sensitive, transparency, sensitive, control},
location = {College Station, TX, USA},
series = {IUI '21 Companion}
}

@inproceedings{10.1145/3594409.3594415,
author = {Wang, Zhipeng and Xu, Hongjing and Chen, Shuoying and Guo, Yuhang},
title = {Forward Translation to Mix Data for Speech Translation},
year = {2023},
isbn = {9781450398398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594409.3594415},
doi = {10.1145/3594409.3594415},
abstract = {End-to-End speech translation means that using a model to translate speech in one language into text in another language. Currently, the main challenge in the field of speech translation is data scarcity. Existing works solve this problem by using text information or applying data augmentation. However, these works only focus on the exploitation of a single corpus, ignoring the full use of existing human-labeled different-sources data. In this paper, we introduce a simple method to solve the data scarcity problem: training a model with simply mixed data and applying the forward translation method to expand the training set. We perform experiments on covost v2 French-English and mTEDx French-English. Our experiments demonstrate that combining the mixture of speech translation corpora with forward translation can yield a better result than the method without mixing.},
booktitle = {Proceedings of the 2023 7th International Conference on Innovation in Artificial Intelligence},
pages = {178–182},
numpages = {5},
keywords = {Forward-translation, Domain adaption, Data scarcity, Speech translation},
location = {Harbin, China},
series = {ICIAI '23}
}

@inproceedings{10.1145/3486011.3486494,
author = {Latorre, Juan Pablo and Amores, Javier J.},
title = {Topic Modelling of Racist and Xenophobic YouTube Comments. Analyzing Hate Speech against Migrants and Refugees Spread through YouTube in Spanish},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486494},
doi = {10.1145/3486011.3486494},
abstract = {Racist and xenophobic hate speech transmitted online is of increasing concern, as social media provide the most suitable channel for its rapid, uncontrolled and massive spread. However, although this type of hate has been studied on diverse social platforms, the academy has not yet focused the attention on the speech generated on YouTube. This work aims to fill this gap in the literature, analyzing the racist and xenophobic hate speech that is spread through YouTube comments in Spanish. Specifically, a manual classification has been carried out to detect hate messages, as well as latent sentiments in comments. On the other hand, a topic modelling was carried out exclusively on hate comments. The results show the characteristics of the racist/xenophobic discourse that spreads through YouTube in Spanish, which, in general terms, represents migration as a burden and a threat, and calls for a government that carries out tougher and more restrictive immigration measures.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {456–460},
numpages = {5},
keywords = {hate speech, social media, migration, topic modelling, racism, sentiment analysis, YouTube, xenophobia},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@article{10.1145/3473331,
author = {Muneer, Iqra and Nawab, Rao Muhammad Adeel},
title = {Cross-Lingual Text Reuse Detection Using Translation Plus Monolingual Analysis for English-Urdu Language Pair},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3473331},
doi = {10.1145/3473331},
abstract = {Cross-Lingual Text Reuse Detection (CLTRD) has recently attracted the attention of the research community due to a large amount of digital text readily available for reuse in multiple languages through online digital repositories. In addition, efficient machine translation systems are freely and readily available to translate text from one language into another, which makes it quite easy to reuse text across languages, and consequently difficult to detect it. In the literature, the most prominent and widely used approach for CLTRD is Translation plus Monolingual Analysis (T+MA). To detect CLTR for English-Urdu language pair, T+MA has been used with lexical approaches, namely, N-gram Overlap, Longest Common Subsequence, and Greedy String Tiling. This clearly shows that T+MA has not been thoroughly explored for the English-Urdu language pair. To fulfill this gap, this study presents an in-depth and detailed comparison of 26 approaches that are based on T+MA. These approaches include semantic similarity approaches (semantic tagger based approaches, WordNet-based approaches), probabilistic approach (Kullback-Leibler distance approach), monolingual word embedding-based approaches siamese recurrent architecture, and monolingual sentence transformer-based approaches for English-Urdu language pair. The evaluation was carried out using the CLEU benchmark corpus, both for the binary and the ternary classification tasks. Our extensive experimentation shows that our proposed approach that is a combination of 26 approaches obtained an F1 score of 0.77 and 0.61 for the binary and ternary classification tasks, respectively, and outperformed the previously reported approaches [41] (F1 = 0.73) for the binary and (F1 = 0.55) for the ternary classification tasks) on the CLEU corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {oct},
articleno = {26},
numpages = {18},
keywords = {word embedding approach, semantic approaches, probabilistic approach, English-Urdu language pairs, monolingual sentence transformer based approaches, deep Learning approach}
}

@article{10.1145/3606694,
author = {Donoso, Diego and Saavedra, Jose M.},
title = {Survey on Sketch-to-Photo Translation},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3606694},
doi = {10.1145/3606694},
abstract = {Sketch-based understanding is involved in human communication and cognitive development, making it essential in visual perception. A specific task in this domain is sketch-to-photo translation, where a model produces realistic images from simple drawings. To this end, large paired training datasets are commonly required, which is impractical in real applications. Thus, this work studies conditional generative models for sketch-to-photo translation, overcoming the lack of training datasets by a self-supervised approach that produces sketch-photo pairs from a target catalog. Our study shows the benefit of cycle-consistency loss and UNet architectures that, together with the proposed dataset generation, improve performance in real applications like eCommerce. Our results also reveal the weakness of conditional DDPMs for generating images resembling the input sketch, even though they achieve a high FID score.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {22},
numpages = {25},
keywords = {deep learning, sketch-to-photo translation, conditional GANs, Generative models}
}

@inproceedings{10.1145/3549206.3549258,
author = {Lahoti, Anshul and Gurugubelli, Krishna and Arroyave, Juan Rafel Orozco and Vuppala, Anil Kumar},
title = {Shifted Delta Cepstral Coefficients with RNN to Improve the Detection of Parkinson’s Disease from the Speech},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549258},
doi = {10.1145/3549206.3549258},
abstract = {Parkinson’s disease (PD) is a progressive neurodegenerative disorder of the central nervous system identified by motor and non-motor activities abnormalities. PD affects respiration, laryngeal, articulation, resonance, and prosodic aspects of speech production. Detection of PD from speech is a non-invasive approach useful for automatic screening. Perceptual attributes of speech due to PD are manifested as temporal variations in speech. In this regard, current work investigated the use of LSTM and BiLSTM networks with shifted delta cepstral (SDC) features to detect PD from speech. Further in BiLSTM networks, a multi-head attention mechanism is introduced, assuming that each head captures distinct information to detect PD. SDC features obtained from MFCCs, and SFFCCs are used for developing the PD detection system. The performance of the experiments is validated using the PC-GITA database. The experimental results revealed that BiLSTM networks give a relative improvement of 4-5% over the LSTM networks. The use of a multi-head attention mechanism further improved the detection accuracy of the PD detection system, showing that it can capture various discriminative features.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {284–288},
numpages = {5},
keywords = {PC-GITA database., Temporal variations, Parkinson’s detection, Shifted delta cepstral coefficients},
location = {Noida, India},
series = {IC3-2022}
}

@inproceedings{10.1145/3582580.3582597,
author = {Xing, Hao},
title = {Application Study of AI-Powered Non-Translatables Function in Computer-Aided Translation},
year = {2023},
isbn = {9781450398015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582580.3582597},
doi = {10.1145/3582580.3582597},
abstract = {In the era of artificial intelligence, computer-aided translation (CAT) tools are also constantly integrating and adding new functions based on artificial intelligence technology. The purpose is to further improve translation efficiency, shorten project cycle and reduce translation cost through the adoption of new technology. With the wide application of computer-aided translation tools in translation teaching and other fields, people put forward higher requirements for the human-machine interactive translation system based on artificial intelligence. Artificial intelligence technology, which can also be used as an educational technology, has been paid more attention to its application in teaching fields. AI-powered non-translatables function can efficiently assist automatic pre-translation and post-editing. This thesis analyzes the processing and application of AI-powered non-translatables function in computer-aided translation, and expects to provide reference for the development of computer-aided translation industry and education based on artificial intelligence and help for the research of artificial intelligence in natural language processing and computer-aided translation.},
booktitle = {Proceedings of the 2022 5th International Conference on Education Technology Management},
pages = {104–109},
numpages = {6},
keywords = {post-editing, artificial intelligence technology, computer-aided translation, translation teaching, AI-powered non-translatables function, pre-translation},
location = {Lincoln, United Kingdom},
series = {ICETM '22}
}

@article{10.1109/TASLP.2022.3226332,
author = {Lyu, Xinglin and Li, Junhui and Zhang, Min and Ding, Chenchen and Tanaka, Hideki and Utiyama, Masao},
title = {Refining History for Future-Aware Neural Machine Translation},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3226332},
doi = {10.1109/TASLP.2022.3226332},
abstract = {Neural machine translation uses a decoder to generate target words auto-regressively by predicting the next target word conditioned on a given source sentence and its previously predicted target words, i.e, its translation history, which suffers from two limitations: 1) the prediction of next word depends heavily on the quality of its history information. Moreover, the discrepancy between training and inference exacerbates this limitation; 2) this left-to-right decoding way cannot make full use of the target-side future information, which leads to the issue of unbalanced outputs. On the one hand, we alleviate the first limitation with a history-refining module, which learns to examine the quality of each history word by assigning it a confidence score. The confidence score is further used as a gate to control the amount of its word embedding flowing to the decoder. On the other hand, we attack the second limitation with a future-foreseeing module, which learns the distribution of future translation at each decoding time step. More importantly, we further propose refining history for future-aware NMT since the two modules can be closely incorporated as they focus on different kinds of context. Experimental results on various translation tasks with different scaled datasets, including WMT English<inline-formula><tex-math notation="LaTeX">$leftrightarrow$</tex-math></inline-formula>{German, French, Romanian}, show that our proposed approach achieves significant improvements over strong Transformer-based NMT baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {500–512},
numpages = {13}
}

@article{10.1109/TASLP.2021.3138714,
author = {Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro},
title = {Integrating Prior Translation Knowledge Into Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138714},
doi = {10.1109/TASLP.2021.3138714},
abstract = {Neural machine translation (NMT), which is an encoder-decoder joint neural language model with an attention mechanism, has achieved impressive results on various machine translation tasks in the past several years. However, the language model attribute of NMT tends to produce fluent yet sometimes unfaithful translations, which hinders the improvement of translation capacity. In response to this problem, we propose a simple and efficient method to integrate prior translation knowledge into NMT in a universal manner that is compatible with neural networks. Meanwhile, it enables NMT to consider the crossing language translation knowledge from the source-side of the training pipeline of NMT, thereby making full use of the prior translation knowledge to enhance the performance of NMT. The experimental results on two large-scale benchmark translation tasks demonstrated that our approach achieved a significant improvement over a strong baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {330–339},
numpages = {10}
}

@inproceedings{10.1145/3291280.3291794,
author = {Coughlin, Ryan and Setthawong, Rachsuda and Setthawong, Pisal},
title = {An Improved English-Thai Translation Framework for Non-Timing Aligned Parallel Corpora Using Bleualign with Explicit Feedback},
year = {2018},
isbn = {9781450365680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291280.3291794},
doi = {10.1145/3291280.3291794},
abstract = {One significant resource for language translation using Statistical Machine Translation (SMT) is parallel corpora. SMT model works well with timing aligned parallel corpora. However, imperfectly aligned sentences in the bilingual corpus typically leads to poorer translation in the final translation after training the SMT model. A major challenge in effectively applying nontiming aligned parallel corpora in the SMT model has not been thoroughly researched. The goal of this paper is to improve the accuracy of an English to Thai Statistical Machine Translation (SMT) model by improving the sentence alignment of parallel corpora. This work proposes an improved English-Thai translation framework for non-timing aligned Parallel corpora using an improved alignment algorithm: Bleualign with explicit user feedback. The generated model can then be applied to the Moses SMT training system to generate English-Thai translation. This experiment uses both English and Thai subtitles obtained from TED (www.ted.com) to build the parallel corpora. The TED corpora sentences are not timing aligned, and this research will try to generate an alignment model to be applied on the Moses SMT training system. The result shows that the model using our proposed algorithm outperforms two traditional alignment models: Gale-Church, Bleualign with the highest BLEU score of 0.36.},
booktitle = {Proceedings of the 10th International Conference on Advances in Information Technology},
articleno = {14},
numpages = {8},
keywords = {statistical machine translation (SMT), English-Thai translation framework, phrase alignment, nontiming parallel corpora},
location = {Bangkok, Thailand},
series = {IAIT 2018}
}

@inproceedings{10.1145/3394231.3397890,
author = {Cao, Rui and Lee, Roy Ka-Wei and Hoang, Tuan-Anh},
title = {DeepHate: Hate Speech Detection via Multi-Faceted Text Representations},
year = {2020},
isbn = {9781450379892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394231.3397890},
doi = {10.1145/3394231.3397890},
abstract = {Online hate speech is an important issue that breaks the cohesiveness of online social communities and even raises public safety concerns in our societies. Motivated by this rising issue, researchers have developed many traditional machine learning and deep learning methods to detect hate speech in online social platforms automatically. However, most of these methods have only considered single type textual feature, e.g., term frequency, or using word embeddings. Such approaches neglect the other rich textual information that could be utilized to improve hate speech detection. In this paper, we propose DeepHate, a novel deep learning model that combines multi-faceted text representations such as word embeddings, sentiments, and topical information, to detect hate speech in online social platforms. We conduct extensive experiments and evaluate DeepHate on three large publicly available real-world datasets. Our experiment results show that DeepHate outperforms the state-of-the-art baselines on the hate speech detection task. We also perform case studies to provide insights into the salient features that best aid in detecting hate speech in online social platforms.},
booktitle = {Proceedings of the 12th ACM Conference on Web Science},
pages = {11–20},
numpages = {10},
keywords = {Online Toxic Content, Social Media, Hate Speech Detection},
location = {Southampton, United Kingdom},
series = {WebSci '20}
}

@inproceedings{10.1145/3394171.3413785,
author = {Gomez, Raul and Liu, Yahui and De Nadai, Marco and Karatzas, Dimosthenis and Lepri, Bruno and Sebe, Nicu},
title = {Retrieval Guided Unsupervised Multi-Domain Image to Image Translation},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413785},
doi = {10.1145/3394171.3413785},
abstract = {Image to image translation aims to learn a mapping that transforms an image from one visual domain to another. Recent works assume that images descriptors can be disentangled into a domain-invariant content representation and a domain-specific style representation. Thus, translation models seek to preserve the content of source images while changing the style to a target visual domain. However, synthesizing new images is extremely challenging especially in multi-domain translations, as the network has to compose content and style to generate reliable and diverse images in multiple domains. In this paper we propose the use of an image retrieval system to assist the image-to-image translation task. First, we train an image-to-image translation model to map images to multiple domains. Then, we train an image retrieval model using real and generated images to find images similar to a query one in content but in a different domain. Finally, we exploit the image retrieval system to fine-tune the image-to-image translation model and generate higher quality images. Our experiments show the effectiveness of the proposed solution and highlight the contribution of the retrieval network, which can benefit from additional unlabeled data and help image-to-image translation models in the presence of scarce data.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3164–3172},
numpages = {9},
keywords = {image-to-image translation, gans, unsupervised learning, retrieval system},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1145/3569010,
author = {Sharma, Vijay Kumar and Mittal, Namita and Vidyarthi, Ankit and Gupta, Deepak},
title = {Exploring Web-Based Translation Resources Applied to Hindi-English Cross-Lingual Information Retrieval},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3569010},
doi = {10.1145/3569010},
abstract = {Internet users perceive a multilingual web but are unfamiliar with it due to communication in their regional language called Cross-Lingual Information Retrieval (CLIR). In CLIR, a translation technique is used to translate the user queries into the target documents language. Conventional translation techniques are based on either a manual dictionary or a parallel corpus. While the trending Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) techniques are trained on a parallel corpus. NMT is not so mature for Hindi-English translation, according to the literature, SMT performs better than the NMT. SMT provides a static translation due to the limited vocabularies in the available parallel corpus. It may not provide the translations for missing or unseen words while the web provides a dynamic interface where multiple users are updating information at the same time. The web may provide the translations for missing or unseen words, therefore, the web is effectively used for technically developed languages like English, German, Spanish, Russian, and Chinese. In this paper, different web resources such as Wikipedia, Hindi WordNet &amp; Indo WordNet, ConceptNet, and online dictionary-based translation techniques are proposed and applied to Hindi-English CLIR. Wikipedia-based translation approach incorporates three modules, i.e., exactly matched, partially matched, and disambiguation to address the issues of wrong inter-wiki links, partially matched terms, and ambiguous articles. Hindi WordNet &amp; Indo WorNet attribute ”English synset” and ConceptNet attributes ”Related term” &amp; ”Synonymy” are used for obtaining translations. Further, WordNet path similarity is used to disambiguate translations. Various online dictionaries are available that return multiple relevant and irrelevant translations. The proposed approaches are compared to the SMT where the Wikipedia-based approach achieves approximately similar mean average precision to SMT.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
keywords = {Indo WordNet, Web Resources, Statistical Machine Translation, ConceptNet, Hindi WordNet, Wikipedia, Online Dictionary}
}

@article{10.1109/TASLP.2019.2937190,
author = {Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
title = {Neural Machine Translation With Sentence-Level Topic Context},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937190},
doi = {10.1109/TASLP.2019.2937190},
abstract = {Traditional neural machine translation NMT methods use the word-level context to predict target language translation while neglecting the sentence-level context, which has been shown to be beneficial for translation prediction in statistical machine translation. This paper represents the sentence-level context as latent topic representations by using a convolution neural network, and designs a topic attention to integrate source sentence-level topic context information into both attention-based and Transformer-based NMT. In particular, our method can improve the performance of NMT by modeling source topics and translations jointly. Experiments on the large-scale LDC Chinese-to-English translation tasks and WMT’14 English-to-German translation tasks show that the proposed approach can achieve significant improvements compared with baseline systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {1970–1984},
numpages = {15}
}

@inproceedings{10.1145/3598469.3598553,
author = {Sutharapu, Hashwanth and Duggal, Akshit and Tiwari, Sanju and Vakaj, Edlira and Ortiz-Rodriguez, Fernando and Barrera-Hernandez, Ruben},
title = {Dialect Translation of English Language to Telangana: Mexin Project},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598469.3598553},
doi = {10.1145/3598469.3598553},
abstract = {Generally Telangana dialect is frequently spoken in vocal daily interactions. Official Telugu is the language used in books, newspapers, academic journals, and other types of literature. Telangana only produces a small quantity of literature and written material in documentary series form. Despite numerous attempts, the Telangana language’s range is still confined to vocal forms. We are attempting to build a dataset of Telangana words, that are obtained from various documents, novels, essays, plays, and everyday interactions of native speakers, to mitigate this barrier and enable the electronic profusion of Telangana dialect. The first phase of the work consisted of extracting some research papers relevant to the topic and gaining some more insight into the objective focused. We then moved on to collect words in the Telangana language as a second phase, i.e., making a dataset. Then using other methods such as tokenization we began with the third phase of our project to implement the proposed work where finally conversion of Telangana dialects is translated to English..},
booktitle = {Proceedings of the 24th Annual International Conference on Digital Government Research},
pages = {671–673},
numpages = {3},
keywords = {NLP, Tokenization, Translation, Dialect},
location = {Gda?sk, Poland},
series = {DGO '23}
}

@article{10.1145/3618110,
author = {Nuthakki, Praveena and Katamaneni, Madhavi and J. N., Chandra Sekhar and Gubbala, Kumari and Domathoti, Bullarao and Maddumala, Venkata Rao and Jetti, Kumar Raja},
title = {Deep Learning Based Multilingual Speech Synthesis Using Multi Feature Fusion Methods},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3618110},
doi = {10.1145/3618110},
abstract = {The poor intelligibility and out-of-the-ordinary nature of the traditional concatenation speech synthesis technologies are two major problems. CNN's context deep learning approaches aren't robust enough for sensitive speech synthesis. Our suggested approach may satisfy such needs and modify the complexities of voice synthesis. The suggested model's minimal aperiodic distortion makes it an excellent candidate for a communication recognition model. Our suggested method is as close to human speech as possible, despite the fact that speech synthesis has a number of audible flaws. Additionally, there is excellent hard work to be done in incorporating sentiment analysis into text categorization using natural language processing. The intensity of feeling varies greatly from nation to country. To improve their voice synthesis outputs, models need to include more and more concealed layers &amp; nodes into the updated mixture density network. For our suggested algorithm to perform at its best, we need a more robust network foundation and optimization methods. We hope that after reading this article and trying out the example data provided, both experienced researchers and those just starting out would have a better grasp of the steps involved in creating a deep learning approach. Overcoming fitting issues with less data in training, the model is making progress. More space is needed to hold the input parameters in the DL-based method.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Natural Language Processing, Speech to Text, Machine Learning, Deep Learning}
}

@article{10.1109/TASLP.2020.2985066,
author = {Kodrasi, Ina and Bourlard, Herv\'{e}},
title = {Spectro-Temporal Sparsity Characterization for Dysarthric Speech Detection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2985066},
doi = {10.1109/TASLP.2020.2985066},
abstract = {To assist the clinical diagnosis and treatment of neurological diseases that cause speech dysarthria such as Parkinson's disease&nbsp;(PD), it is of paramount importance to craft robust features which can be used to automatically discriminate between healthy and dysarthric speech. Since dysarthric speech of patients suffering from PD is breathy, semi-whispery, and is characterized by abnormal pauses and imprecise articulation, it can be expected that its spectro-temporal sparsity differs from the spectro-temporal sparsity of healthy speech. While we have recently successfully used temporal sparsity characterization for dysarthric speech detection, characterizing spectral sparsity poses the challenge of constructing a valid feature vector from signals with a different number of unaligned time frames. Further, although several non-parametric and parametric measures of sparsity exist, it is unknown which sparsity measure yields the best performance in the context of dysarthric speech detection. The objective of this paper is to demonstrate the advantages of spectro-temporal sparsity characterization for automatic dysarthric speech detection. To this end, we first provide a numerical analysis of the suitability of different non-parametric and parametric measures (i.e., <inline-formula><tex-math notation="LaTeX">$l_1$</tex-math></inline-formula>-norm, kurtosis, Shannon entropy, Gini index, shape parameter of a Chi distribution, and shape parameter of a Weibull distribution) for sparsity characterization. It is shown that kurtosis, the Gini index, and the parametric sparsity measures are advantageous sparsity measures, whereas the <inline-formula><tex-math notation="LaTeX">$l_1$</tex-math></inline-formula>-norm and entropy measures fail to robustly characterize the temporal sparsity of signals with a different number of time frames. Second, we propose to characterize the spectral sparsity of an utterance by initially time-aligning it to the same utterance uttered by a (arbitrarily selected) reference speaker using dynamic time warping. Experimental results on a Spanish database of healthy and dysarthric speech show that estimating the spectro-temporal sparsity using the Gini index or the parametric sparsity measures and using it as a feature in a support vector machine results in a high classification accuracy of 83.3%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1210–1222},
numpages = {13}
}

@article{10.1109/TASLP.2021.3085119,
author = {Li, Guanlin and Liu, Lemao and Zhu, Conghui and Wang, Rui and Zhao, Tiejun and Shi, Shuming},
title = {Detecting Source Contextual Barriers for Understanding Neural Machine Translation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3085119},
doi = {10.1109/TASLP.2021.3085119},
abstract = {In machine translation evaluation, the traditional wisdom measures model's generalization ability in an average sense, for example by using corpus BLEU. However, the statistics of corpus BLEU cannot provide comprehensive understanding and fine-grained analysis on model's generalization ability. As a remedy, this paper attempts to understand NMT at fine-grained level, by detecting contextual barriers within an unseen input sentence that <italic>cause</italic> the degradation in model's translation quality. It proposes a principled definition of source contextual barriers as well as its modified version which is tractable in computation and operates at word-level. Based on the modified one, three simple methods are proposed for barrier detection by search-aware risk estimation through counterfactual generation. Extensive analyses are conducted on those detected contextual barrier words on both Zh <inline-formula><tex-math notation="LaTeX">$Leftrightarrow$</tex-math></inline-formula> En NIST benchmarks. Potential usages motivated from barrier words are also discussed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {3158–3169},
numpages = {12}
}

@inproceedings{10.1145/3323503.3360292,
author = {Ver\'{\i}ssimo, Vin\'{\i}cius and Silva, Cec\'{\i}lia and Hanael, Vitor and Moraes, Caio and Costa, Rostand and Maritan, Tiago and Aschoff, Manuella and Gaud\^{e}ncio, Tha\'{\i}s},
title = {A Study on the Use of Sequence-to-Sequence Neural Networks for Automatic Translation of Brazilian Portuguese to LIBRAS},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360292},
doi = {10.1145/3323503.3360292},
abstract = {The World Health Organization estimates that approximately 466 million people have some level of hearing loss. This significant portion of the world population faces several challenges in accessing information. The main problem is that the languages in which the deaf community can perceive and produce in a natural way are sign languages (SL). An alternative to dealing with this would be the translation of the content from an oral language to SL. However, when it comes to accessing online content, it is necessary to consider translating SL not only for audio or video content, but also for more complex text on websites. This is already a difficult task by itself for the volume involved, and it also addresses some additional challenges, related to the high cost of human interpreter service and the great dynamism of Internet content. In this context, one of the most promising approaches to such scenarios is the use of machine translation applications from oral to sign language. This work evaluates the use of neural network models usually used in natural language processing for the production of LIBRAS glosses from texts in Portuguese. Using a 2k factorial experiment design, we evaluated the impact of several aspects such as database size, types of models and training parameters in the quality of automatic translation obtained. The results of the experiments were very promising and point to an initial superiority of the LightConv model in most of the evaluated scenarios.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {101–108},
numpages = {8},
keywords = {accessibility, neural networks, machine translation, sign language, deep learning},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3536220.3563689,
author = {Dubagunta, S. Pavankumar and Moneta, Edoardo and Theocharopoulos, Eleni and Magimai Doss, Mathew},
title = {Towards Automatic Prediction of Non-Expert Perceived Speech Fluency Ratings},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536220.3563689},
doi = {10.1145/3536220.3563689},
abstract = {Automatic speech fluency prediction has been mainly approached from the perspective of computer aided language learning, where the system tends to predict ratings similar to those of the human experts. Speech fluency prediction, however, can be questioned in a more relaxed social setting, where the ratings arise usually from non-experts; indeed, everyday assessments of fluency are appraised by our social environment and encounters; these encounters due to globalisation are becoming of international nature and therefore being a non-expert has become a norm. This paper explores the latter direction, i.e., prediction of non-expert perceived speech fluency ratings, which has not been studied in the speech technology literature, to the best of our knowledge. Toward that, we investigate several approaches, namely, (a) low-level descriptor feature functionals, (b) bag-of-audio word based approach and (c) neural network based end-to-end acoustic modelling approach. Our investigations on speech data collected from 54 speakers and rated by seven non-experts demonstrate that non-expert speech fluency ratings can be systematically predicted, with the best performing system yielding a Pearson’s correlation coefficient of 0.66 and a Spearman’s correlation coefficient of 0.67 with the median human scores.},
booktitle = {Companion Publication of the 2022 International Conference on Multimodal Interaction},
pages = {7–11},
numpages = {5},
keywords = {speech assessment, low level descriptors, raw waveform modelling, bag of audio words, Perceived fluency},
location = {Bengaluru, India},
series = {ICMI '22 Companion}
}

@inproceedings{10.1145/3543873.3587660,
author = {Perez-Martin, Jesus and Gomez-Robles, Jorge and Guti\'{e}rrez-Fandi\~{n}o, Asier and Adsul, Pankaj and Rajanala, Sravanthi and Lezcano, Leonardo},
title = {Cross-Lingual Search for e-Commerce Based on Query Translatability and Mixed-Domain Fine-Tuning},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587660},
doi = {10.1145/3543873.3587660},
abstract = {Online stores in the US offer a unique scenario for Cross-Lingual Information Retrieval (CLIR) due to the mix of Spanish and English in user queries. Machine Translation (MT) provides an opportunity to lift relevance by translating the Spanish queries to English before delivering them to the search engine. However, polysemy-derived problems, high latency and context scarcity in product search, make generic MT an impractical solution. The wide diversity of products in marketplaces injects non-translatable entities, loanwords, ambiguous morphemes, cross-language ambiguity and a variety of Spanish dialects in the communication between buyers and sellers, posing a thread to the accuracy of MT. In this work, we leverage domain adaptation on a simplified architecture of Neural Machine Translation (NMT) to make both latency and accuracy suitable for e-commerce search. Our NMT model is fine-tuned on a mixed-domain corpus based on engagement data expanded with catalog back-translation techniques. Beyond accuracy, and given that translation is not the goal but the means to relevant results, the problem of Query Translatability is addressed by a classifier on whether the translation should be automatic or explicitly requested. We assembled these models into a query translation system that we tested and launched at Walmart.com , with a statistically significant lift in Spanish GMV and an nDCG gain for Spanish queries of +70%.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {892–898},
numpages = {7},
keywords = {product search, query classification, domain adaptation, neural machine translation, cross-lingual information retrieval, back-translation, fine-tuning},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3606695,
author = {Bhowmick, Rajat Subhra and Ganguli, Isha and Paul, Ananya and Paul, Jayanta and Sil, Jaya},
title = {Improving Indic Code-Mixed to Monolingual Translation Using Mixed Script Augmentation, Generation &amp; Transfer Learning},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3606695},
doi = {10.1145/3606695},
abstract = {The use of code-mixed languages (written in Roman character) on social media platforms is prevalent in multilingual nations. Translation from code-mixed to monolingual is necessary for social media analysis, content filtering, and targeted advertising. Training translation models from scratch is difficult due to the scarcity of available code-mixed resources and the extremely noisy nature of real-time code-mixed sentences. At the moment, multilingual state-of-the-art language models are routinely used for multilingual applications. However, multilingual models are ineffective in handling code-mixed sentences as it is usually written in Roman script but contain words from at least two languages. In the paper, two data augmentation techniques are proposed to improve code-mixed to monolingual translation, one based on script augmentation and the other on code-mixed sentence generation. The proposed approach converts the code-mixed sentences into ‘Mixed Script form’ that restore the native language words in the sentences with corresponding native language scripts. The novelty of the work is that the multilingual language models include each language’s linguistic competence, preserving context in the monolingual sentences, not possible in the earlier models. Using an mT5 model, denoising and mixed-script switching are performed, followed by monolingual translation with another mT5 model. Code-mixed sentences are generated by employing a simple code-mixed sentence generating technique using monolingual parallel inputs. Two different Indic language sets, namely Hindi-English and Bengali-English are applied and in each case, the proposed approach outperforms straight uni-script (Roman) code-mixed to monolingual translation.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
keywords = {Language Translation, Code-Mixing, Language Generation}
}

@inproceedings{10.1145/3514221.3520150,
author = {Tang, Jiawei and Luo, Yuyu and Ouzzani, Mourad and Li, Guoliang and Chen, Hongyang},
title = {Sevi: Speech-to-Visualization through Neural Machine Translation},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3520150},
doi = {10.1145/3514221.3520150},
abstract = {Data visualization is a powerful tool for understating information through visual cues. However, allowing novices to create visualization artifacts for what they want to see is not easy, just as not everyone can write SQL queries. Arguably, the most natural way to specify what to visualize is through natural language or speech, similar to our daily search on Google or Apple Siri, leaving to the system the task of reasoning about what to visualize and how.In this demo, we present Sevi an end-to-end data visualization system that acts as a virtual assistant to allow novices to create visualizations through either natural language or speech. Sevi is powered by two main components: Speech2Text which is based on Google Cloud Speech-to-Text Rest API, and Text2VIS, which uses an end-to-end neural machine translation model called ncNet trained using a cross-domain benchmark called nvBench. Both ncNet and nvBench have been developed by us. We will walk the audience through two general domain datasets, one related to COVID-19 and the other on NBA player statistics, to highlight how Sevi enables novices to easily create data visualizations. Because nvBench contains Text2VIS training samples from 105 domains (e.g., sport, college, hospital, etc.), the audience can play with speech or text input with any of these domains.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2353–2356},
numpages = {4},
keywords = {speech-to-visualization, natural language-to-visualization},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3316782.3322780,
author = {Bird, Jordan J. and Wanner, Elizabeth and Ek\'{a}rt, Anik\'{o} and Faria, Diego R.},
title = {Accent Classification in Human Speech Biometrics for Native and Non-Native English Speakers},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322780},
doi = {10.1145/3316782.3322780},
abstract = {Accent classification provides a biometric path to high resolution speech recognition. This preliminary study explores various methods of human accent recognition through classification of locale. Classical, ensemble, timeseries and deep learning techniques are all explored and compared. A set of diphthong vowel sounds are recorded from participants from the United Kingdom and Mexico, and then formed into a large static dataset of statistical descriptions by way of their Mel-frequency Cepstral Coefficients (MFCC) at a sample window length of 0.02 seconds. Using both flat and timeseries data, various machine learning models are trained and compared to the scientific standard Hidden Markov Model (HMM). Results through 10 fold cross validation show that a vote of average probabilities between a Random Forest and Long Short-term Memory Neural Network result in a classification accuracy of 94.74%, outperforming the speech classification standard Hidden Markov Model by a 5% increase in accuracy.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {554–560},
numpages = {7},
keywords = {accent recognition, speech recognition, voice assistants, machine learning, computational linguistics, biometrics},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@article{10.1145/3589001,
author = {Ghosal, Sayani and Jain, Amita and Tayal, Devendra Kumar and Menon, Varun G. and Kumar, Akshi},
title = {Inculcating Context for Emoji Powered Bengali Hate Speech Detection Using Extended Fuzzy SVM and Text Embedding Models},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3589001},
doi = {10.1145/3589001},
abstract = {The massive growth of social webs offer opportunities to communicate with diverse languages, unstructured text, informal posts, misspelled contents and emojis. Social media users feel comfortable to express their emotions specially emotions with high intensity (hate speech) in their mother tongue. Hate speech in any form targets groups and individuals that may trigger antisocial activities, hate crimes, and terrorist acts. Bengali social media users use Bengali for posting implicit or indirect hate text. Existing Bengali hate speech detection research considers explicit hate speech detection but in actual hate is expressed more in implicit way. In order to detect both implicit and explicit hate speech from low resource content, social webs need highly efficient automated tools. Researchers applied discriminative learning approaches (i.e. SVM, MLP, CNN) to distinguish hate text with only clear-cut outcomes in detecting direct hate speech. The proposed novel Bengali hate speech detection model considers two parallel approaches: (i) It applies extended fuzzy SVM classifier for class imbalanced dataset (FSVMCIL) and multilingual BERT (mBERT) text embedding model to detect first hate label; (ii) Morphological analysis method to detect implicit and explicit hate content with the hate similarity (HS) scheme for second hate label. Linking both labeling methods, this research extracts contextual Bengali hate speech from informal text. This novel HS method considers Word2Vec word embedding model and Bengali hate lexicon. It also considers emoji to text conversion for efficient contextual analysis. This study also conducts extensive experiments for various categories with the Bengali hate speech dataset. It also evaluates the proposed model performance considering weighted F1 score, precision, recall and accuracy parameters. Results reveal significant improvement in Bengali hate speech detection with 2.35% increase in F1- score and 9.11 % increase in accuracy.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {Hate Speech Detection, Indian language, Word2Vec, Multilingual BERT, Low Resource Language, Fuzzy Support Vector Machine, Natural Language Processing}
}

@article{10.1145/3406205,
author = {Fang, Hui and Shi, Hongmei and Zhang, Jiuzhou},
title = {Heuristic Bilingual Graph Corpus Network to Improve English Instruction Methodology Based on Statistical Translation Approach},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3406205},
doi = {10.1145/3406205},
abstract = {The number of sentence pairs in the bilingual corpus is a key to translation accuracy in computational machine translations. However, if the amount goes beyond a certain degree, the increasing number of cases has less impact on the translation while the construction of translation systems requires a considerable amount of time and energy, thus preventing the development of a statistical translation by the computer. This article offers a number of classifications for measuring the amount of information for each pair of sentences, using the Heuristic Bilingual Graph Corpus Network (HBGCN) to form an improved method of corpus selection that takes the difference between the first amount of information between the pairs of sentences into account. Using a graphic-based selector method as a training set, they achieve a close translation result through our experiments with the whole body and achieve better results than basic results for the following based on the Document Inverse Frequency (DIF) ranking approach.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {41},
numpages = {14},
keywords = {document inverse frequency, corpus selection method, machine translation, Heuristic bilingual graph}
}

@inproceedings{10.1145/3505284.3532968,
author = {Carvalho Afonso, Marcelo and Almeida, Pedro and Be\c{c}a, Pedro and Silva, Telmo and Covalenco, Iulia},
title = {Usability Of Text-To-Speech Technology in Creating News Podcasts Using Portuguese Of Portugal},
year = {2022},
isbn = {9781450392129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505284.3532968},
doi = {10.1145/3505284.3532968},
abstract = {The increase in the consumption of digital formats has, in many cases, been a penalty for traditional media companies. In the adaptation to digital, the transformation of written news into audio formats, that guarantee spatio-temporal flexibility in its consumption, is one of the differentiating options. Artificial intelligence tools can help accelerate and automate the digitalization processes. It is, therefore, the objective of this paper to evaluate the integration of Text-to-Speech (TTS) technology in the process of creating news podcasts. The study comprised two surveys. The first corresponding to the validation of TTS services in Portuguese from Portugal, and the second for the validation of three models of news podcasts containing human voice, synthesized voice via TTS, and a hybrid model with TTS voice and human voice. The results point to a general acceptance of the integration of voices generated by TTS in news podcasts without prejudice to the consumer experience.},
booktitle = {Proceedings of the 2022 ACM International Conference on Interactive Media Experiences},
pages = {363–368},
numpages = {6},
keywords = {Speech synthesis, Digital Journalism, Media Formats, Text-To-Speech, News, News Podcast},
location = {Aveiro, JB, Portugal},
series = {IMX '22}
}

@inproceedings{10.1145/3405755.3406139,
author = {Clark, Leigh and Cowan, Benjamin R. and Roper, Abi and Lindsay, Stephen and Sheers, Owen},
title = {Speech Diversity and Speech Interfaces: Considering an Inclusive Future through Stammering},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406139},
doi = {10.1145/3405755.3406139},
abstract = {The number of speech interfaces and services made available through them continue to grow. This has opened up interactions to people who rely on speech as a critical modality for interacting with systems. However, people with diverse speech patterns such as those who stammer are at risk of being negatively affected or excluded from speech interface interaction. In this paper, we consider what an inclusive speech interface future may look like for people who stammer. In doing so, we identify three key challenges: (1) developing effective speech recognition, (2) understanding the user experiences of people who stammer and (3) supporting speech interfaces designers through appropriate heuristics. We believe the interdisciplinary and cross-community strengths of venues like CUI are well positioned to address these challenges going forward.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {24},
numpages = {3},
keywords = {Stammer, inclusivity, speech interface, accessibility, speech diversity, stutter},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3544548.3581280,
author = {Sum, Cella M and Tran, Anh-Ton and Lin, Jessica and Kuo, Rachel and Bennett, Cynthia L and Harrington, Christina and Fox, Sarah E},
title = {Translation as (Re)Mediation: How Ethnic Community-Based Organizations Negotiate Legitimacy},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581280},
doi = {10.1145/3544548.3581280},
abstract = {Ethnic community-based organizations (CBOs) play an essential role in supporting the wellbeing of immigrants and refugees. CBO workers often act as linguistic and cultural translators between communities, government, and health and social service systems. However, resource constraints, technological barriers, and pressures to be data-driven require workers to perform additional forms of translation to ensure their organizations’ survival. Drawing on 16 interviews with members of 7 Asian American and Pacific Islander CBOs, we examine opportunities and barriers concerning their technology-mediated work practices. We identify two circumstances where CBO workers perform translation: (1) as legitimacy work to build trust with funders and communities, and (2) as (re)mediation in attending to technological barriers and resisting hegemonic systems that treat their communities as “other.” By unpacking the politics of translation work across these sites, we position CBO workers as a critical source for HCI research and practice as it seeks to support community wellbeing.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {603},
numpages = {14},
keywords = {AAPI, migrant, diaspora, legitimacy work, Translation work, ethnic community-based organizations, remediation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1109/TASLP.2018.2837223,
author = {Wang, Rui and Utiyama, Masao and Finch, Andrew and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
title = {Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2837223},
doi = {10.1109/TASLP.2018.2837223},
abstract = {Neural machine translation NMT has been prominent in many machine translation tasks. However, in some domain-specific tasks, only the corpora from similar domains can improve translation performance. If out-of-domain corpora are directly added into the in-domain corpus, the translation performance may even degrade. Therefore, domain adaptation techniques are essential to solve the NMT domain problem. Most existing methods for domain adaptation are designed for the conventional phrase-based machine translation. For NMT domain adaptation, there have been only a few studies on topics such as fine tuning, domain tags, and domain features. In this paper, we have four goals for sentence level NMT domain adaptation. First, the NMT's internal sentence embedding is exploited and the sentence embedding similarity is used to select out-of-domain sentences that are close to the in-domain corpus. Second, we propose three sentence weighting methods, i.e., sentence weighting, domain weighting, and batch weighting, to balance the data distribution during NMT training. Third, in addition, we propose dynamic training methods to adjust the sentence selection and weighting during NMT training. Fourth, to solve the multidomain problem in a real-world NMT scenario where the domain distributions of training and testing data often mismatch, we proposed a multidomain sentence weighting method to balance the domain distributions of training data and match the domain distributions of training and testing data. The proposed methods are evaluated in international workshop on spoken language translation IWSLT English-to-French/German tasks and a multidomain English-to-French task. Empirical results show that the sentence selection and weighting methods can significantly improve the NMT performance, outperforming the existing baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1727–1741},
numpages = {15}
}

@inproceedings{10.1145/3351095.3375685,
author = {Wan, Evelyn and de Groot, Aviva and Jameson, Shazade and P\u{a}un, Mara and L\"{u}cking, Phillip and Klumbyte, Goda and L\"{a}mmerhirt, Danny},
title = {Lost in Translation: An Interactive Workshop Mapping Interdisciplinary Translations for Epistemic Justice},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375685},
doi = {10.1145/3351095.3375685},
abstract = {There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a "marker for truth" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {692},
numpages = {1},
keywords = {workflow, interdisciplinary collaboration, critical theory, algorithm development, epistemic justice, methodologies},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3362789.3362850,
author = {Fr\'{\i}as-V\'{a}zquez, Maximiliano and Arcila, Carlos},
title = {Hate Speech against Central American Immigrants in Mexico: Analysis of Xenophobia and Racism in Politicians, Media and Citizens},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362850},
doi = {10.1145/3362789.3362850},
abstract = {This doctoral thesis project is carried out with the support of the Observatorio de lo Contenidos Audiovisuales (OCA) and the doctoral program of Education in the Knowledge Society of the University of Salamanca. This project seeks to analyze messages published in the social network of Twitter, to identify the origin of hate speech against Central American migrants in Mexico according to the three spheres of the agenda building (Media, Politics and Public) to determine where it starts and which of the three has more influence on the propagation of this discourse. In order to carry out this research, the Twitter API will be used to download the messages posted on the social network, and automated feelings analysis tools based on supervised automatic machine learning will be used for this, the feelings analysis tool will be used on this thesis is AutoCop, which was developed by members of the OCA and used for investigations that handle large scale data.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {956–960},
numpages = {5},
keywords = {Machine learning, Hate speech, xenophobia, Supervised learning},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.1109/TASLP.2021.3060813,
author = {Zhao, Guanlong and Ding, Shaojin and Gutierrez-Osuna, Ricardo},
title = {Converting Foreign Accent Speech Without a Reference},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3060813},
doi = {10.1109/TASLP.2021.3060813},
abstract = {Foreign accent conversion (FAC) is the problem of generating a synthetic voice that has the voice identity of a second-language (L2) learner and the pronunciation patterns of a native (L1) speaker. This synthetic voice has been referred to as a “golden-speaker” in the pronunciation-training literature. FAC is generally achieved by building a voice-conversion model that maps utterances from a source (L1) speaker onto the target (L2) speaker. As such, FAC requires that a reference utterance from the L1 speaker be available at synthesis time. This greatly restricts the application scope of the FAC system. In this work, we propose a “reference-free” FAC system that eliminates the need for reference L1 utterances at synthesis time, and transforms L2 utterances directly. The system is trained in two steps. First, a conventional FAC procedure is used to create a golden-speaker using utterances from a reference L1 speaker (which are then discarded) and the L2 speaker. Second, a pronunciation-correction model is trained to convert L2 utterances to match the golden-speaker utterances obtained in the first step. At synthesis time, the pronunciation-correction model directly transforms a novel L2 utterance into its golden-speaker counterpart. Our results show that the system reduces foreign accents in novel L2 utterances, achieving a 20.5% relative reduction in word-error-rate of an American English automatic speech recognizer and a 19% reduction in perceptual ratings of foreign accentedness obtained through listening tests. Over 73% of the listeners also rated golden-speaker utterances as having the same voice identity as the original L2 utterances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2367–2381},
numpages = {15}
}

@article{10.1109/TASLP.2019.2905778,
author = {Liu, Yuanyuan and Lee, Tan and Law, Thomas and Lee, Kathy Yuet-Sheung},
title = {Acoustical Assessment of Voice Disorder With Continuous Speech Using ASR Posterior Features},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2905778},
doi = {10.1109/TASLP.2019.2905778},
abstract = {Traditionally acoustical assessment of voice disorder relies on simple and homogeneous speech samples like sustained vowels. Continuous speech is believed to be more representative of the daily function of voice and more preferable in clinical practice. This paper describes an attempt on automating voice assessment with continuous speech utterances. The proposed system makes use of a novel type of features that are derived from phone posterior probabilities outputted by a deep neural network based automatic speech recognition ASR system. These ASR-based voice features are designed to effectively quantify the mismatch between disordered voice and normal voice. Prediction of voice disorder severity is carried out first at utterance-level and subsequently the prediction scores for individual utterances from a subject are combined to give an overallassessment on the subject. With a low-dimension ASR-based feature vector, the utterance-level prediction accuracy is comparable to that with conventional features with a much higher dimension. By jointly using the ASR features and conventional voice features, a subject-level prediction accuracy of over $text{80}{%}$ on three severity classes can be achieved. Subjects with mild disorder and those with severe disorder could be perfectly distinguished by the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1047–1059},
numpages = {13}
}

@article{10.1162/coli_a_00377,
author = {V\'{a}zquez, Ra\'{u}l and Raganato, Alessandro and Creutz, Mathias and Tiedemann, J\"{o}rg},
title = {A Systematic Study of Inner-Attention-Based Sentence Representations                    in Multilingual Neural Machine Translation},
year = {2020},
issue_date = {June 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00377},
doi = {10.1162/coli_a_00377},
abstract = {Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning. We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability to encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.},
journal = {Comput. Linguist.},
month = {jun},
pages = {387–424},
numpages = {38}
}

@inproceedings{10.1145/3446132.3446404,
author = {Zhou, Maoxian and Secha, Jia and Cai, Rangjia},
title = {Domain Adaptation for Tibetan-Chinese Neural Machine Translation},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446404},
doi = {10.1145/3446132.3446404},
abstract = {The meaning of the same word or sentence is likely to change in different semantic contexts, which challenges general-purpose translation system to maintain stable performance across different domains. Therefore, domain adaptation is an essential researching topic in Neural Machine Translation practice. In order to efficiently train translation models for different domains, in this work we take the Tibetan-Chinese general translation model as the parent model, and obtain two domain-specific Tibetan-Chinese translation models with small-scale in-domain data. The empirical results indicate that the method provides a positive approach for domain adaptation in low-resource scenarios, resulting in better bleu metrics as well as faster training speed over our general baseline models.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {77},
numpages = {5},
keywords = {Tibetan-Chinese machine translation, Neural Machine Translation, domain adaptation, transfer learning},
location = {Sanya, China},
series = {ACAI '20}
}

@inproceedings{10.1145/3375708.3380312,
author = {Dorris, Wyatt and Hu, Ruijia (Roger) and Vishwamitra, Nishant and Luo, Feng and Costello, Matthew},
title = {Towards Automatic Detection and Explanation of Hate Speech and Offensive Language},
year = {2020},
isbn = {9781450371155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375708.3380312},
doi = {10.1145/3375708.3380312},
abstract = {The use of hate speech and offensive language online has become widely recognized as a critical social problem plaguing today's Internet users. Previous research in the detection of hate speech and offensive language has primarily focused on using machine learning approaches to naively detect hate speech and offensive language, without explaining the reasons for their detection. In this work, we introduce a novel hate speech and offensive language defense system called HateDefender, which consists of a detection model based on deep Long Short-term Memory (LSTM) neural networks and an explanation model based on the gating signals of LSTMs. HateDefender effectively detects hate speech and offensive language (average accuracy of 90.82% and 89.10% on hate speech and offensive language, respectively) and explains their factors by pinpointing the exact words that are responsible for causing them. Our system uses these explanations for the effective intervention of such incidents online.},
booktitle = {Proceedings of the Sixth International Workshop on Security and Privacy Analytics},
pages = {23–29},
numpages = {7},
keywords = {explanation, lstm, offensive language detection, hate speech detection},
location = {New Orleans, LA, USA},
series = {IWSPA '20}
}

@inproceedings{10.1145/3544548.3581281,
author = {Danielescu, Andreea and Horowit-Hendler, Sharone A and Pabst, Alexandria and Stewart, Kenneth Michael and Gallo, Eric M and Aylett, Matthew Peter},
title = {Creating Inclusive Voices for the 21st Century: A Non-Binary Text-to-Speech for Conversational Assistants},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581281},
doi = {10.1145/3544548.3581281},
abstract = {As voice assistant usage continues to grow, their homogeneity becomes even more problematic with the UNESCO report, “I’d Blush if I could” showing that designing only feminine voice assistants encourages negative behavior, both with virtual assistants and with real people [3]. While masculine text-to-speech (TTS) voices exist, ones that cover the full range of gender presentations, such as non-binary or gender-ambiguous voices are largely missing. In this paper, we present a method of creating a non-binary TTS voice and an example voice, Sam, created with input from the non-binary and transgender communities. We have open-sourced the resulting voice, along with the process and data used to create it. Finally, we present results from a large-scale survey showing that non-binary individuals are more likely to prefer a non-binary voice assistant compared to cisgendered individuals and discuss differences across age and gender.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {390},
numpages = {17},
keywords = {text-to-speech, voice user interfaces., voice assistants, gender},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1109/TASLP.2021.3090973,
author = {Liu, Yuanyuan and Penttil\"{a}, Nelly and Ihalainen, Tiina and Lintula, Juulia and Convey, Rachel and R\"{a}s\"{a}nen, Okko},
title = {Language-Independent Approach for Automatic Computation of Vowel Articulation Features in Dysarthric Speech Assessment},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3090973},
doi = {10.1109/TASLP.2021.3090973},
abstract = {Imprecise vowel articulation can be observed in people with Parkinson's disease (PD). Acoustic features measuring vowel articulation have been demonstrated to be effective indicators of PD in its assessment. Standard clinical vowel articulation features of vowel working space area (VSA), vowel articulation index (VAI) and formants centralization ratio (FCR), are derived the first two formants of the three corner vowels /a/, /i/ and /u/. Conventionally, manual annotation of the corner vowels from speech data is required before measuring vowel articulation. This process is time-consuming. The present work aims to reduce human effort in clinical analysis of PD speech by proposing an automatic pipeline for vowel articulation assessment. The method is based on automatic corner vowel detection using a language universal phoneme recognizer, followed by statistical analysis of the formant data. The approach removes the restrictions of prior knowledge of speaking content and the language in question. Experimental results on a Finnish PD speech corpus demonstrate the efficacy and reliability of the proposed automatic method in deriving VAI, VSA, FCR and F2i/F2u (the second formant ratio for vowels /i/ and /u/). The automatically computed parameters are shown to be highly correlated with features computed with manual annotations of corner vowels. In addition, automatically and manually computed vowel articulation features have comparable correlations with experts' ratings on speech intelligibility, voice impairment and overall severity of communication disorder. Language-independence of the proposed approach is further validated on a Spanish PD database, PC-GITA, as well as on TORGO corpus of English dysarthric speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2228–2243},
numpages = {16}
}

@article{10.1145/3592791,
author = {Khanmohammadi, Reza and Mirshafiee, Mitra Sadat and Rezaee Jouryabi, Yazdan and Mirroshandel, Seyed Abolghasem},
title = {Prose2Poem: The Blessing of Transformers in Translating Prose to Persian Poetry},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592791},
doi = {10.1145/3592791},
abstract = {Persian poetry has consistently expressed its philosophy, wisdom, speech, and rationale based on its couplets, making it an enigmatic language on its own to both native and non-native speakers. Nevertheless, the noticeable gap between Persian prose and poems has left the two pieces of literature mediumless. Having curated a parallel corpus of prose and their equivalent poems, we introduce a novel Neural Machine Translation approach for translating prose to ancient Persian poetry using transformer-based language models in an exceptionally low-resource setting. Translating input prose into ancient Persian poetry presents two primary challenges: In addition to being reasonable in conveying the same context as the input prose, the translation must also satisfy poetic standards. Hence, we designed our method consisting of three stages. First, we trained a transformer model from scratch to obtain an initial translations of the input prose. Next, we designed a set of heuristics to leverage contextually rich initial translations and produced a poetic masked template. In the last stage, we pretrained different variations of BERT on a poetry corpus to use the masked language modelling technique to obtain final translations. During the evaluation process, we considered both automatic and human assessment. The final results demonstrate the eligibility and creativity of our novel heuristically aided approach among Literature professionals and non-professionals in generating novel Persian poems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {170},
numpages = {18},
keywords = {low-resource language, Machine translation, transformers, Persian poetry}
}

@inproceedings{10.1145/3448748.3448804,
author = {Miao, Yuyang and Lou, Xinyu and Wu, Han},
title = {The Diagnosis of Parkinson's Disease Based on Gait, Speech Analysis and Machine Learning Techniques},
year = {2021},
isbn = {9781450390002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448748.3448804},
doi = {10.1145/3448748.3448804},
abstract = {Parkinson's disease (PD) is a long-term degenerative disorder of the central nervous system. The common symptoms are tremor, rigidity, slowness of movement, and difficulty with walking at early stages. Currently, PD can't be cured. And there are not really effective methods to diagnose it. However, machine learning is a new way for the diagnosis of PD. It can build a model from PD patients' dataset, which can help classify PD and healthy people. In this review, the applications of machine learning for PD diagnosis by algorithms and data are analyzed. Several machine learning classifiers are briefly introduced, including artificial neural network (ANN), support vector machine (SVM), Naive Bayes (NB), K-Nearest Neighbor (k-NN). Next, the basis of gait analysis is introduced, including gait circle and gait data, and then, each step of the machine learning processing is focused on. Two ways are concentrated to analyze speech signals - support vector machine (SVM) and artificial neural network (ANN). This review presents that machine learning has good performances for the diagnosis of PD. However, it can only be a diagnosis tool to help doctors because of its limited generalization. In the future, people should explore more effective algorithms with better generalization.},
booktitle = {Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing},
pages = {358–371},
numpages = {14},
keywords = {Speech Analysis, Parkinson's Disease, Support Vector Machines, Machine Learning, Neural Networks, Diagnosis, Classification, Gait Analysis},
location = {Harbin, China},
series = {BIC 2021}
}

@inproceedings{10.1145/3600160.3605088,
author = {Sarlas, Athanasios and Kalafatelis, Alexandros and Alexandridis, Georgios and Kourtis, Michail-Alexandros and Trakadas, Panagiotis},
title = {Exploring Federated Learning for Speech-Based Parkinson’s Disease Detection},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3605088},
doi = {10.1145/3600160.3605088},
abstract = {Parkinson’s Disease is the second most prevalent neurodegenerative disorder, currently affecting as high as 3% of the global population. Research suggests that up to 80% of patients manifest phonatory symptoms as early signs of the disease. In this respect, various systems have been developed that identify high risk patients by analyzing their speech using recordings obtained from natural dialogues and reading tasks conducted in clinical settings. However, most of them are centralized models, where training and inference take place on a single machine, raising concerns about data privacy and scalability. To address these issues, the current study migrates an existing, state-of-the-art centralized approach to the concept of federated learning, where the model is trained in multiple independent sessions on different machines, each with its own dataset. Therefore, the main objective is to establish a proof of concept for federated learning in this domain, demonstrating its effectiveness and viability. Moreover, the study aims to overcome challenges associated with centralized machine learning models while promoting collaborative and privacy-preserving model training.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {102},
numpages = {6},
keywords = {Speech Articulation, Parkinson’s Disease, Federating Learning},
location = {Benevento, Italy},
series = {ARES '23}
}

@inproceedings{10.1145/3319619.3321951,
author = {Bird, Jordan J. and Wanner, Elizabeth and Ek\'{a}rt, Anik\'{o} and Faria, Diego R.},
title = {Phoneme Aware Speech Recognition through Evolutionary Optimisation},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3321951},
doi = {10.1145/3319619.3321951},
abstract = {Phoneme awareness provides the path to high resolution speech recognition to overcome the difficulties of classical word recognition. Here we present the results of a preliminary study on Artificial Neural Network (ANN) and Hidden Markov Model (HMM) methods of classification for Human Speech Recognition through Diphthong Vowel sounds in the English Phonetic Alphabet, with a specific focus on evolutionary optimisation of bio-inspired classification methods. A set of audio clips are recorded by subjects from the United Kingdom and Mexico. For each recording, the data were pre-processed, using Mel-Frequency Cepstral Coefficients (MFCC) at a sliding window of 200ms per data object, as well as a further MFCC timeseries format for forecast-based models, to produce the dataset. We found that an evolutionary optimised deep neural network achieves 90.77% phoneme classification accuracy as opposed to the best HMM of 150 hidden units achieving 86.23% accuracy. Many of the evolutionary solutions take substantially longer to train than the HMM, however one solution scoring 87.5% (+1.27%) requires fewer resources than the HMM.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {362–363},
numpages = {2},
keywords = {phoneme awareness, computational linguistics, speech recognition, evolutionary optimisation, artificial neural networks},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1145/3344274,
author = {Fischer, Kerstin and Niebuhr, Oliver and Jensen, Lars C. and Bodenhagen, Leon},
title = {Speech Melody Matters—How Robots Profit from Using Charismatic Speech},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3344274},
doi = {10.1145/3344274},
abstract = {In this article, we address to what extent the proverb “the sound makes the music” also applies to human-robot interaction, and whether robots could profit from using speech characteristics similar to those used by charismatic speakers like Steve Jobs. In three empirical studies, we investigate the effects of using Steve Jobs’ and Mark Zuckerberg's speech characteristics during the generation of robot speech on the robot's persuasiveness and its impressionistic evaluation. The three studies address different human-robot interaction situations, which range from online questionnaires to real-time interactions with a large service robot, yet all involve both behavioral measures and users’ assessments. The results clearly show that robots can profit from using charismatic speech.},
journal = {J. Hum.-Robot Interact.},
month = {dec},
articleno = {4},
numpages = {21},
keywords = {social functions of speech, prosody, persuasion, robot personality, charisma, Human-robot interaction}
}

@inproceedings{10.1145/3587103.3594136,
author = {S\'{a}nchez-Granados, \'{A}ngel Francisco},
title = {Application of Software Visualization for Syntax-Directed Translation Learning},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594136},
doi = {10.1145/3587103.3594136},
abstract = {The aim of this doctoral thesis is to develop a visualisation model to improve the learning process of syntax-driven translation, for which a software tool is being created. This program, aimed both to teachers and students, will allow to load a grammar and generate the visualization with the provided input. A generation API is currently available for teachers to annotate their specifications and generate the visualization, although the goal in the future is to do this process through an automatic annotation of the specifications.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {613–614},
numpages = {2},
keywords = {language processors, teaching of computer sciences, software visualisation, syntax-driven translators},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@article{10.1145/3485469,
author = {Kang, Xiaomian and Zhao, Yang and Zhang, Jiajun and Zong, Chengqing},
title = {Enhancing Lexical Translation Consistency for Document-Level Neural Machine Translation},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3485469},
doi = {10.1145/3485469},
abstract = {Document-level neural machine translation (DocNMT) has yielded attractive improvements. In this article, we systematically analyze the discourse phenomena in Chinese-to-English translation, and focus on the most obvious ones, namely lexical translation consistency. To alleviate the lexical inconsistency, we propose an effective approach that is aware of the words which need to be translated consistently and constrains the model to produce more consistent translations. Specifically, we first introduce a global context extractor to extract the document context and consistency context, respectively. Then, the two types of global context are integrated into a encoder enhancer and a decoder enhancer to improve the lexical translation consistency. We create a test set to evaluate the lexical consistency automatically. Experiments demonstrate that our approach can significantly alleviate the lexical translation inconsistency. In addition, our approach can also substantially improve the translation quality compared to sentence-level Transformer.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {59},
numpages = {21},
keywords = {Document-level translation, lexical consistency, discourse phenomena, neural machine translation}
}

@article{10.1145/3511888,
author = {Kaur, Kamaldeep and Singh, Parminder},
title = {Impact of Feature Extraction and Feature Selection Algorithms on Punjabi Speech Emotion Recognition Using Convolutional Neural Network},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511888},
doi = {10.1145/3511888},
abstract = {As a challenge to refine the spontaneity and productivity of a machine and human coherence, speech emotion recognition has been an overriding area of research. The trustability and fulfillment of emotion recognition are largely involved with the feature extraction and selection processes. An important role is played in exploring and distinguishing audio content during the feature extraction phase. Also, the features that have been extracted should be resilient to a number of disturbances and reliable enough for an adequate classification system. This article focuses on three main components of a Speech Emotion Recognition (SER) process. The first one is the optimal feature extraction method for a Punjabi SER system. The second one is the use of an appropriate feature selection method that selects effectual features from the ones extracted in the first step and removes the redundant features to improve the conduct of emotion recognition. The third one is the classification model that has been used further for emotion recognition. So the scope of this article is to explain the three main steps of the Punjabi SER system: feature extraction, feature selection, and emotion recognition with classifier. The results have been calculated and compared for number of feature set combinations, with and without a feature selection process. A total of 10 experiments are carried out, and various performance metrics such as precision, recall, F1-score, accuracy, and so on, are used to demonstrate the results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {100},
numpages = {23},
keywords = {feature extraction, CNN, features, feature selection, speech, Punjabi}
}

@article{10.1109/TASLP.2018.2864648,
author = {Li, Qiang and Wong, Derek F. and Chao, Lidia S. and Zhu, Muhua and Xiao, Tong and Zhu, Jingbo and Zhang, Min},
title = {Linguistic Knowledge-Aware Neural Machine Translation},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2864648},
doi = {10.1109/TASLP.2018.2864648},
abstract = {Recently, researchers have shown an increasing interest in incorporating linguistic knowledge into neural machine translation NMT. To this end, previous works choose either to alter the architecture of NMT encoder to incorporate syntactic information into the translation model, or to generalize the embedding layer of the encoder to encode additional linguistic features. The former approach mainly focuses on injecting the syntactic structure of the source sentence into the encoding process, leading to a complicated model that lacks the flexibility to incorporate other types of knowledge. The latter extends word embeddings by considering additional linguistic knowledge as features to enrich the word representation. It thus does not explicitly balance the contribution from word embeddings and the contribution from additional linguistic knowledge. To address these limitations, this paper proposes a knowledge-aware NMT approach that models additional linguistic features in parallel to the word feature. The core idea is that we propose modeling a series of linguistic features at the word level knowledge block using a recurrent neural network RNN. And in sentence level, those word-corresponding feature blocks are further encoded using a RNN encoder. In decoding, we propose a knowledge gate and an attention gate to dynamically control the proportions of information contributing to the generation of target words from different sources. Extensive experiments show that our approach is capable of better accounting for importance of additional linguistic, and we observe significant improvements from 1.0 to 2.3 BLEU points on Chinese$leftrightarrow$ English and English$rightarrow$German translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2341–2354},
numpages = {14}
}

@article{10.1145/3433607,
author = {Hair, Adam and Ballard, Kirrie J. and Markoulli, Constantina and Monroe, Penelope and Mckechnie, Jacqueline and Ahmed, Beena and Gutierrez-Osuna, Ricardo},
title = {A Longitudinal Evaluation of Tablet-Based Child Speech Therapy with Apraxia World},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3433607},
doi = {10.1145/3433607},
abstract = {Digital games can make speech therapy exercises more enjoyable for children and increase their motivation during therapy. However, many such games developed to date have not been designed for long-term use. To address this issue, we developed Apraxia World, a speech therapy game specifically intended to be played over extended periods. In this study, we examined pronunciation improvements, child engagement over time, and caregiver and automated pronunciation evaluation accuracy while using our game over a multi-month period. Ten children played Apraxia World at home during two counterbalanced 4-week treatment blocks separated by a 2-week break. In one treatment phase, children received pronunciation feedback from caregivers and in the other treatment phase, utterances were evaluated with an automated framework built into the game. We found that children made therapeutically significant speech improvements while using Apraxia World, and that the game successfully increased engagement during speech therapy practice. Additionally, in offline mispronunciation detection tests, our automated pronunciation evaluation framework outperformed a traditional method based on goodness of pronunciation scoring. Our results suggest that this type of speech therapy game is a valid complement to traditional home practice.},
journal = {ACM Trans. Access. Comput.},
month = {mar},
articleno = {3},
numpages = {26},
keywords = {computer-aided pronunciation training (CAPT), childhood apraxia of speech (CAS), speech sound disorders (SSDs), serious games, Games for health}
}

@article{10.1109/TASLP.2020.2998277,
author = {Yuan, Yougen and Xie, Lei and Leung, Cheung-Chi and Chen, Hongjie and Ma, Bin},
title = {Fast Query-by-Example Speech Search Using Attention-Based Deep Binary Embeddings},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2998277},
doi = {10.1109/TASLP.2020.2998277},
abstract = {State-of-the-art query-by-example (QbE) speech search approaches usually use recurrent neural network (RNN) based acoustic word embeddings (AWEs) to represent variable-length speech segments with fixed-dimensional vectors, and thus simple cosine distances can be measured over the embedded vectors of both the spoken query and the search content. In this paper, we aim to improve search accuracy and speed for the AWE-based QbE approach in low-resource scenario. First, multi-head self-attentive mechanism is introduced for learning a sequence of attention weights for all time steps of RNN outputs while attending to different positions of a speech segment. Second, as the real-valued AWEs suffer from substantial computation in similarity measure, a hashing layer is adopted for learning deep binary embeddings, and thus binary pattern matching can be directly used for fast QbE speech search. The proposed approach of self-attentive deep hashing network is effectively trained with three specifically-designed objectives: a penalization term, a triplet loss, and a quantization loss. Experiments show that our approach improves the relative search speed by 8 times and mean average precision (MAP) by 18.9%, as compared with the previous best real-valued embedding approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {1988–2000},
numpages = {13}
}

@inproceedings{10.1145/3409334.3452068,
author = {Subedi, Ishan Mani and Singh, Maninder and Ramasamy, Vijayalakshmi and Walia, Gursimran Singh},
title = {Application of Back-Translation: A Transfer Learning Approach to Identify Ambiguous Software Requirements},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452068},
doi = {10.1145/3409334.3452068},
abstract = {Ambiguous requirements are problematic in requirement engineering as various stakeholders can debate on the interpretation of the requirements leading to a variety of issues in the development stages. Since requirement specifications are usually written in natural language, analyzing ambiguous requirements is currently a manual process as it has not been fully automated to meet the industry standards. In this paper, we used transfer learning by using ULMFiT where we pre-trained our model to a general-domain corpus and then fine-tuned it to classify ambiguous vs unambiguous requirements (target task). We then compared its accuracy with machine learning classifiers like SVM, Linear Regression, and Multinomial Naive Bayes. We also used back translation (BT) as a text augmentation technique to see if it improved the classification accuracy. Our results showed that ULMFiT achieved higher accuracy than SVM (Support Vector Machines), Logistic Regression and Multinomial Naive Bayes for our initial data set. Further by augmenting requirements using BT, ULMFiT got a higher accuracy than SVM, Logistic Regression, and Multinomial Naive Bayes classifier, improving the initial performance by 5.371%. Our proposed research provides some promising insights on how transfer learning and text augmentation can be applied to small data sets in requirements engineering.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {130–137},
numpages = {8},
keywords = {transfer learning, neural networks, requirement engineering and quality, machine learning},
location = {Virtual Event, USA},
series = {ACM SE '21}
}

@inproceedings{10.1145/3395035.3425640,
author = {Nayak, Shravan and Baumann, Timo and Bhattacharya, Supratik and Karakanta, Alina and Negri, Matteo and Turchi, Marco},
title = {See Me Speaking? Differentiating on Whether Words Are Spoken On Screen or Off to Optimize Machine Dubbing},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425640},
doi = {10.1145/3395035.3425640},
abstract = {Dubbing is the art of finding a translation from a source into a target language that can be lip-synchronously revoiced, i. e., that makes the target language speech appear as if it was spoken by the very actors all along. Lip synchrony is essential for the full-fledged reception of foreign audiovisual media, such as movies and series, as violated constraints of synchrony between video (lips) and audio (speech) lead to cognitive dissonance and reduce the perceptual quality. Of course, synchrony constraints only apply to the translation when the speaker's lips are visible on screen. Therefore, deciding whether to apply synchrony constraints requires an automatic method for detecting whether an actor's lips are visible on screen for a given stretch of speech or not. In this paper, we attempt, for the first time, to classify on- from off-screen speech based on a corpus of real-world television material that has been annotated word-by-word for the visibility of talking lips on screen. We present classification experiments in which we classify},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {130–134},
numpages = {5},
keywords = {multi-modal speech processing, dubbing, audiovisual machine translation, activity recognition},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3584871.3584878,
author = {Tijerina, Issac G and Datta, Soma},
title = {A Survey on Current Speech to Text Analysis to Help Programmers Dictate Code},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584878},
doi = {10.1145/3584871.3584878},
abstract = {Abstract: The focus of this study is to survey the usage of Speech to Text in programming and the general application in other fields. The findings are n then applied to further the application of Speech to Text with coding. It was found that the state of modern Speech to Text is in constant motion. Research and development are done in this field to improve Speech to Text and apply it to various fields. It applies to medical fields, education, machinery control, and others. It is being seen that while being used, there is still a struggle with a user's accent if it differs from the native accent of the language. This study selects 31 articles and has been split into content, application, and Speech To Text (STT).},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {50–57},
numpages = {8},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@inproceedings{10.1145/3405755.3406120,
author = {Dubiel, Mateusz and Halvey, Martin and Gallegos, Pilar Oplustil and King, Simon},
title = {Persuasive Synthetic Speech: Voice Perception and User Behaviour},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406120},
doi = {10.1145/3405755.3406120},
abstract = {Previous research indicates that synthetic speech can be as persuasive as human speech. However, there is a lack of empirical validation on interactive goal-oriented tasks. In our two-stage study (online listening test and lab evaluation), we compared participants' perception of the persuasiveness of synthetic voices created from speech in a debating style vs. speech from audio-books. Participants interacted with our Conversational Agent (CA) to complete 4 flight-booking tasks and were asked to evaluate the voice, message and perceived personal qualities. We found that participants who interacted with the CA using the voice created from debating style speech rated it as significantly more truthful and more involved than the CA using the audio-book-based voice. However, there was no difference in how frequently each group followed the CA's recommendations. We hope our investigation will provoke discussion about the impact of different synthetic voices on users' perceptions of CAs in goal-oriented tasks.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {6},
numpages = {9},
keywords = {User Behaviour, Speech Perception, Speech Synthesis},
location = {Bilbao, Spain},
series = {CUI '20}
}

@article{10.1145/3626187,
author = {Chen, Yaqi and Zhang, Wenlin and Zhang, Hao and Qu, Dan and Yang, Xu-Kui},
title = {Task-Based Meta Focal Loss for Multilingual Low-Resource Speech Recognition},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3626187},
doi = {10.1145/3626187},
abstract = {Low-resource automatic speech recognition is a challenging task due to a lack of labeled training data. To resolve this issue, multilingual meta-learning learns a better model initialization from many source language tasks for fast adaptation to unseen target languages. However, for diverse source languages, the quantity and difficulty vary greatly because of their different data scales and phonological systems. These differences lead to task-quantity and task-difficulty imbalance issues and thus a failure of multilingual meta-learning ASR. In this work, we propose a task-based meta focal loss (TMFL) approach to address this tough challenge. Specifically, we introduce a hard-task moderator and update the meta-parameters using gradients from both the support set and query set. Our proposed approach focuses more on hard tasks and makes full use of the data from hard tasks. Moreover, we analyze the significance of the hard task moderator and interpret its significance at the sample level. Experiment results show that the proposed method, TMFL, significantly outperforms the state-of-the-art multilingual meta learning on all target languages for the IARPA BABEL and OpenSLR datasets, especially under very-low-resource conditions. In particular, it can reduce CER from 72 (% ) to 60 (% ) by fine-tuning the pre-trained model with about 22 hours of Vietnamese data.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {speech recognition, IARPA-BABEL, meta learning, focal loss, OpenSLR, low-resource}
}

@inproceedings{10.1145/3331184.3331222,
author = {Zbib, Rabih and Zhao, Lingjun and Karakos, Damianos and Hartmann, William and DeYoung, Jay and Huang, Zhongqiang and Jiang, Zhuolin and Rivkin, Noah and Zhang, Le and Schwartz, Richard and Makhoul, John},
title = {Neural-Network Lexical Translation for Cross-Lingual IR from Text and Speech},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331222},
doi = {10.1145/3331184.3331222},
abstract = {We propose a neural network model to estimate word translation probabilities for Cross-Lingual Information Retrieval (CLIR). The model estimates better probabilities for word translations than automatic word alignments alone, and generalizes to unseen source-target word pairs. We further improve the lexical neural translation model (and subsequently CLIR), by incorporating source word context, and by encoding the character sequences of input source words to generate translations of out-of-vocabulary words. To be effective, neural network models typically need training on large amounts of data labeled directly on the final task, in this case relevance to queries. In contrast, our approach only requires parallel data to train the translation model, and uses an unsupervised model to compute CLIR relevance scores.We report results on the retrieval of text and speech documents from three morphologically complex languages with limited training data resources (Swahili, Tagalog, and Somali) and short English queries. Despite training on only about 2M words of parallel training data for each language, we obtain neural network translation models that are very effective for this task. We also obtain further improvements using (i) a modified relevance model, which uses the probability of occurrence of a translation of each query term in the source document, and (ii) confusion networks (instead of 1-best output) that encode multiple transcription alternatives in the output of an Automatic Speech Recognition (ASR) system.We achieve overall MAP relative improvements of up to 24% on Swahili, 50% on Tagalog, and 39% on Somali over the baseline probabilistic model, and larger improvements over monolingual retrieval from machine translation output.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {645–654},
numpages = {10},
keywords = {cross-lingual information retrieval, speech recognition, machine translation, probabilistic modeling, neural networks},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1109/TASLP.2019.2941587,
author = {Liu, Xuebo and Wong, Derek F. and Chao, Lidia S. and Liu, Yang},
title = {Latent Attribute Based Hierarchical Decoder for Neural Machine Translation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2941587},
doi = {10.1109/TASLP.2019.2941587},
abstract = {Neural machine translation NMT has achieved state-of-the-art performance in many translation tasks. However, because the computational cost increases with the size of the search space for predicting the target words, the translation quality of NMT is constrained by the limited vocabulary. To alleviate this problem, we propose a novel dynamic hierarchical decoder for NMT to utilize all of the target words in the training and decoding process. In the proposed model, a target word is represented by two latent attribute vectors rather than a word vector.&nbsp;The model is trained to dynamically put together those words that share similar linguistic attributes. The prediction of a target word is, therefore, turned into the prediction of attribute vectors, where the $mathrm{softmax}$ functions are performed at the attribute level. This greatly reduces the model size and the decoding time.&nbsp;Our experimental results demonstrate that the proposed model significantly outperforms the NMT baselines in both Chinese-English and English-German translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2103–2112},
numpages = {10}
}

@article{10.1145/3605778,
author = {Khurana, Surbhi and Dev, Amita and Bansal, Poonam},
title = {SER: Performance Evaluation of CNN Model Along with an Overview of Available Indic Speech Datasets, and Transition of Classifiers From Traditional to Modern Era},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3605778},
doi = {10.1145/3605778},
abstract = {Speech emotion recognition (SER) is a rapidly evolving field in affective computing and human-computer interaction. In general, a SER system extracts and classifies prominent elements called features from a pre-processed speech signal to target the presence of speaker's certain emotion. This paper explores the utilization of deep learning classifiers in SER and surveys available datasets in both Indic and international languages. The paper highlights the significance of SER in enhancing human-computer interaction and presents deep learning as an effective approach to handle the complexity of speech signals. Various deep learning architectures, including Convolution Neural Networks (CNNs), Recurrent Neural Network (RNNs), and hybrid models, are analysed in terms of training methodology, and performance on benchmark datasets. Additionally, the paper conducts a comprehensive survey of publicly available datasets for speech emotion recognition, considering emotional categories, language diversity, recording conditions, and sample sizes. Challenges in adapting deep learning models to these datasets, such as data augmentation and cross-lingual transfer learning, are discussed. Moreover, the CNN based model is analysed on accuracy, precision, recall and F-1 score on Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset with the value 84%, 85%, 84% and 84% resp. The review concludes with key findings, emphasizing the strengths and limitations of deep learning classifiers for SER. It identifies the need for standardized evaluation protocols, exploration of transfer learning across languages, and development of robust and culturally diverse datasets as future research directions.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
keywords = {Speech features, Transfer learning, Ant lion optimization, CNN, Speech Emotion recognition, Classifiers}
}

@article{10.1162/coli_a_00374,
author = {Mohiuddin, Tasnim and Joty, Shafiq},
title = {Unsupervised Word Translation with Adversarial                    Autoencoder},
year = {2020},
issue_date = {June 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00374},
doi = {10.1162/coli_a_00374},
abstract = {Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from machine translation to transfer learning. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of refinement procedures sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, refinement with Procrustes solution and refinement with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the supervised system. Along with performing comprehensive ablation studies to understand the contribution of different components of our adversarial model, we also conduct a thorough analysis of the refinement procedures to understand their effects.},
journal = {Comput. Linguist.},
month = {jun},
pages = {257–288},
numpages = {32}
}

@inproceedings{10.1145/3323503.3360305,
author = {de Oliveira, Caio C\'{e}sar Moraes and R\^{e}go, Tha\'{\i}s Gaudencio do and Lima, Manuella Aschoff Cavalcanti Brand\~{a}o and de Ara\'{u}jo, Tiago Maritan Ugulino},
title = {Analysis of Rule-Based Machine Translation and Neural Machine Translation Approaches for Translating Portuguese to LIBRAS},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360305},
doi = {10.1145/3323503.3360305},
abstract = {In this paper, we propose a rule-based machine translator for Brazilian Portuguese to Brazilian Sign Language translation. This translator was implemented using the part of speech tagging and lemmatization techniques with implementations in Aelius and CoGrOO, respectively. Then, we developed a convolutional translator seeking to replicate the rule-based translation with a sintetic corpus. In this corpus, preprocessing techniques such as substitution of names, numbers and spelling errors by symbols were applied to improve processing. The translator were tested on two corpus, Bosque e OpenSub (extracted from a site of subtitles), of 69 and 36.858 lines respectively, and compared with translations generated by interpreters and translations generated by the application VLibras (LAVID-UFPB).},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {117–124},
numpages = {8},
keywords = {sign language, machine translation, neural machine translation},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3437963.3441668,
author = {De La Pe\~{n}a Sarrac\'{e}n, Gretel Liz},
title = {Multilingual and Multimodal Hate Speech Analysis in Twitter},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441668},
doi = {10.1145/3437963.3441668},
abstract = {Automatic hate speech detection has become a crucial task nowadays, due the increase of hate on the Internet and its negative consequences. Therefore, in our PhD we propose the design and implementation of methods for the automatic processing of hate messages. The study is focused on the hate messages on Twitter. The hypothesis on which the research is based is that the prediction of hate speech, considering textual content, can be improved by the combination of features such as the activity and communities of users, as well as the images that can be shared with the tweets. In this way, we intend to develop strategies for the automatic detection of hate with multimodal and also multilingual (both in English and Spanish) approaches. Furthermore, our research includes the study of counter-narrative as an alternative to mitigate the effects of hate speech. To address the problem, we employ deep learning techniques, deepening the study of approaches based on representation with graphs.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1109–1110},
numpages = {2},
keywords = {deep learning, countering hate, multimodal hate speech detection},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3572549.3572629,
author = {Zhao, Gang and Yi, Jiarong and Chu, Jie and Zhang, Yinan and Yin, Jianghua},
title = {Design and Implementation of the Teacher-Student Dialogue Automatic Speech Recognition Tool Based on Classroom Verbal Characteristics},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572629},
doi = {10.1145/3572549.3572629},
abstract = {Speech behavior is a tool for teachers and students to express and communicate in the classroom, and analyzing the content of classroom teacher-student dialogue is one of the key basic technologies to break through the automatic analysis of teaching videos. However, the current discourse research tools rely on the universal speech recognition cloud platform, and there are problems such as confusion between teacher-student role and unclear discourse boundaries for classroom teacher-student dialogue recognition. Therefore, this paper designs and develops teacher-student dialogue recognition tools after classifing and encoding the auditory information data and summarizing the general rules of speech characteristics in classroom teaching videos. To achieve the purpose of analyzing classroom teaching videos, education researchers can use the tool to intercept teaching videos, obtain boundary points of teacher-student dialogue, and recognize their speech content. Experimental results show that the tool has stable performance, which can quickly and accurately process audio signals, segment and cluster teacher and student voiceprint features. Finally, it produce structured text containing the time point of the discourse boundary, the teacher-student role label and the discourse content, which will provide data support for analyzing further classroom teaching video behavior.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {503–508},
numpages = {6},
keywords = {Speech recognition, Classroom verbal characteristics, Teaching video, Speaker diarization, Teacher-student dialogue},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@inproceedings{10.1145/3447535.3462495,
author = {Vitiugin, Fedor and Senarath, Yasas and Purohit, Hemant},
title = {Efficient Detection of Multilingual Hate Speech by Using Interactive Attention Network with Minimal Human Feedback},
year = {2021},
isbn = {9781450383301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447535.3462495},
doi = {10.1145/3447535.3462495},
abstract = {Online hate speech on social media has become a critical problem for social network services that has been further fueled by the self-isolation in the COVID-2019 pandemic. Current studies have primarily focused on detecting hate speech in one language due to the complexity of the task; however, hate speech has no boundaries across the languages and geographies in the real world nowadays. This demands further investigation on multilingual hate speech detection methods, with strong requirements for model interpretability to effectively understand the context of the model errors. In this paper, we propose a Multilingual Interactive Attention Network (MLIAN) model for hate speech detection on multilingual social media text corpora, by building upon the attention networks for interpretability and human-in-the-loop paradigm for model adaptability. This model interactively learns to give attention to the relevant contextual words and leverage the labels for the hate target mentions from the simulated human feedback. We evaluated the proposed model on SemEval-2019 Task 5 datasets in English and Spanish. Extensive experimentation of model training in both settings of single and multiple language data demonstrates the superior performance of our model (with AUC more than 84%) compared to the strong baselines. Our results show that human feedback not only improves the model performance but also helps to improve the interpretability of the model by establishing a strong connection between the learned attention weights and semantic frames for the text across languages. Further, an analysis of the amount of human feedback required to achieve reliable and increased model performance shows that less than 4% of training data is sufficient. The application of the MLIAN method can inform future studies on multilingual hate speech.},
booktitle = {Proceedings of the 13th ACM Web Science Conference 2021},
pages = {130–138},
numpages = {9},
keywords = {Human-in-the-loop Machine Learning, Hate Speech Detection, Social Media},
location = {Virtual Event, United Kingdom},
series = {WebSci '21}
}

@inproceedings{10.1145/3388818.3389159,
author = {Rao, Wenbi and Zhang, Ji and Wu, Jianwei},
title = {Improved BLSTM RNN Based Accent Speech Recognition Using Multi-Task Learning and Accent Embeddings},
year = {2020},
isbn = {9781450376952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388818.3389159},
doi = {10.1145/3388818.3389159},
abstract = {A major challenge in Automatic speech recognition (ASR) systems for Mandarin is to be able to handle speakers with different kinds of accents. ASR systems that are trained using single-task learning underperform due to poor generalization ability when confronted with a new accent. In this paper, we explore how to use accent sentences information that accent embeddings and multi-task learning on the basis of the bidirectional long short term memory (BLSTM) to improve accent speech recognition. Firstly we consider augmenting the speech input with accent information in the form of embeddings extracted by a standalone network. Then we propose multi-task learning architecture that we jointly learn an accent classifier and a multi-accent acoustic model. Experiments with these methods demonstrate that we obtain a 4% average relative improvement in word error rate over a multi-accent baseline system.},
booktitle = {Proceedings of the 2020 2nd International Conference on Image, Video and Signal Processing},
pages = {1–6},
numpages = {6},
keywords = {Accented speech recognition, multi-task learning, accent sentences embeddings, acoustic model},
location = {Singapore, Singapore},
series = {IVSP '20}
}

@article{10.1145/3523282,
author = {Gong, Yan},
title = {Study on Machine Translation Teaching Model Based on Translation Parallel Corpus and Exploitation for Multimedia Asian Information Processing},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3523282},
doi = {10.1145/3523282},
abstract = {Text in one language can be mechanically translated into another language using machine translation (MT). It is possible to anticipate a sequence of words, generally modeling full sentences using machine translation in a single integrated model. Human language's flexibility makes automatic translation an artificial intelligence (AI) challenge of the highest order. A single model rather than a pipeline of fine-tuned models is now the best way to attain state-of-the-art outcomes in machine translation. For example, words having numerous meanings, phrases that use more than one grammatical structure, and other grammar issues make it difficult for a machine to translate; however, many misinterpretations translate to be a breeze. A teacher's job is to assist pupils in overcoming the emotional and cognitive obstacles that stand in the way of developing effective problem-solving abilities. Students will benefit from developing problem-solving abilities since they will apply what they have learned to new circumstances. MT-AI, machine translation technology, and products have been employed in a wide range of applications, including business travel, tourism, and cross-lingual information retrieval. Text translation and phonetic translation are two types of translations that focus on the content of the source language. It is possible to create self-learning systems by injecting machine learning techniques into existing software and then observing the results of such injection. Computer software can translate a massive volume of text in a short period. It takes longer for a human translator to perform the same work as a computer program. The simulation investigation is developed based on correctness and effectiveness, demonstrating the proposed framework's reliability of 95.1%.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
keywords = {Human Translation, Artificial Intelligence, Student, Machine Translation}
}

@inproceedings{10.1145/3503047.3503120,
author = {Zhou, Maoxian and Secha, Jia and Cai, Rangjia},
title = {Research on Tibetan-Chinese Neural Machine Translation Integrating Syntactic Information},
year = {2022},
isbn = {9781450385862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503047.3503120},
doi = {10.1145/3503047.3503120},
abstract = {In recent years, Neural Networks have gradually replaced other methods in the field of Machine Translation and become the mainstream which have excellent performance in many languages. However, the performance of Neural Machine Translation mainly relies on large-scale parallel corpora, which is not ideal for low-resource languages, especially Tibetan-Chinese Machine Translation. In order to obtain the best translation performance with more external information on the basis of limited corpus, this paper introduces syntactic information, that is, adding part-of-speech(POS) tags as input features in the training process. Experiments verify the effectiveness of this method, which can improve the translation performance to a certain extent.},
booktitle = {Proceedings of the 3rd International Conference on Advanced Information Science and System},
articleno = {68},
numpages = {4},
keywords = {Tibetan-Chinese Machine Translation, syntactic information, Part-of-speech tagging, NMT},
location = {Sanya, China},
series = {AISS '21}
}

@inproceedings{10.1145/3447548.3467209,
author = {Bouyarmane, Karim},
title = {GEM: Translation-Free Zero-Shot Global Entity Matcher for Global Catalogs},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467209},
doi = {10.1145/3447548.3467209},
abstract = {We propose a modular BiLSTM / CNN / Transformer deep-learning encoder architecture, together with a data synthesis and training approach, to solve the problem of matching catalog products across different languages, different local catalogs, and different catalog data contributors. The end-to-end model relies solely on raw natural language textual data in the catalog entries and on images of the products, without any feature engineering, and is entirely translation-free, not requiring the translation of the catalog natural-language data to the same base language for inference. We report experiments results on a 4-languages-scope model (English, French, German, Spanish) matching entities from 4 local catalogs (UK, France, Germany, Spain) of a retail website. We demonstrate that the model achieves performance comparable to state-of-the-art existing entity matchers that operate within a single language, and that the model achieves high-performance zero-shot inference on language pairs not seen in training.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2593–2600},
numpages = {8},
keywords = {hierarchical neural networks, record matching, record representation learning, deduplication, cross-lingual matching, record embedding},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1145/3389021,
author = {Xu, Fan and Luo, Jian and Wang, Mingwen and Zhou, Guodong},
title = {Speech-Driven End-to-End Language Discrimination toward Chinese Dialects},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389021},
doi = {10.1145/3389021},
abstract = {Language discrimination among similar languages, varieties, and dialects is a challenging natural language processing task. The traditional text-driven focus leads to poor results. In this article, we explore the effectiveness of speech-driven features toward language discrimination among Chinese dialects. First, we systematically explore the appropriateness of speech-driven MFCC features toward CNN-based language discrimination. Then, we design an end-to-end speech recognition model based on HMM-DNN to predict Chinese dialect words. We adopt attention mechanism to extract the discriminative words related to different Chinese dialects. Finally, through a CNN, we combine the word-level embedding and the MFCC-based features. Evaluation of two benchmark Chinese dialect corpora shows the appropriateness and effectiveness of the proposed speech-driven approach to fine-grained Chinese dialect discrimination compared to the state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {62},
numpages = {24},
keywords = {language discrimination, Speech-driven features, text-driven features, attention, Chinese dialect}
}

@article{10.1145/3312575,
author = {Costa-Juss\`{a}, Marta R. and Casas, No\'{e} and Escolano, Carlos and Fonollosa, Jos\'{e} A. R.},
title = {Chinese-Catalan: A Neural Machine Translation Approach Based on Pivoting and Attention Mechanisms},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3312575},
doi = {10.1145/3312575},
abstract = {This article innovatively addresses machine translation from Chinese to Catalan using neural pivot strategies trained without any direct parallel data. The Catalan language is very similar to Spanish from a linguistic point of view, which motivates the use of Spanish as pivot language. Regarding neural architecture, we are using the latest state-of-the-art, which is the Transformer model, only based on attention mechanisms. Additionally, this work provides new resources to the community, which consists of a human-developed gold standard of 4,000 sentences between Catalan and Chinese and all the others United Nations official languages (Arabic, English, French, Russian, and Spanish). Results show that the standard pseudo-corpus or synthetic pivot approach performs better than cascade.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {43},
numpages = {8},
keywords = {Neural machine translation, pivot approaches, Chinese-Catalan, transformer}
}

@article{10.1145/3377323,
author = {Corazza, Michele and Menini, Stefano and Cabrio, Elena and Tonelli, Sara and Villata, Serena},
title = {A Multilingual Evaluation for Online Hate Speech Detection},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3377323},
doi = {10.1145/3377323},
abstract = {The increasing popularity of social media platforms such as Twitter and Facebook has led to a rise in the presence of hate and aggressive speech on these platforms. Despite the number of approaches recently proposed in the Natural Language Processing research area for detecting these forms of abusive language, the issue of identifying hate speech at scale is still an unsolved problem. In this article, we propose a robust neural architecture that is shown to perform in a satisfactory way across different languages; namely, English, Italian, and German. We address an extensive analysis of the obtained experimental results over the three languages to gain a better understanding of the contribution of the different components employed in the system, both from the architecture point of view (i.e., Long Short Term Memory, Gated Recurrent Unit, and bidirectional Long Short Term Memory) and from the feature selection point of view (i.e., ngrams, social network–specific features, emotion lexica, emojis, word embeddings). To address such in-depth analysis, we use three freely available datasets for hate speech detection on social media in English, Italian, and German.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {10},
numpages = {22},
keywords = {text classification, Hate speech detection, social media, multilingual data}
}

@inproceedings{10.1145/3603163.3609158,
author = {Guimar\~{a}es, Samuel and Kakizaki, Gabriel and Melo, Philipe and Silva, M\'{a}rcio and Murai, Fabricio and Reis, Julio C. S. and Benevenuto, Fabr\'{\i}cio},
title = {Anatomy of Hate Speech Datasets: Composition Analysis and Cross-Dataset Classification},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603163.3609158},
doi = {10.1145/3603163.3609158},
abstract = {Manifestations of hate speech in different scenarios are increasingly frequent on social platforms. In this context, there is a large number of works that propose solutions for identifying this type of content in these environments. Most efforts to automatically detect hate speech follow the same process of supervised learning, using annotators to label a predefined set of messages, which are, in turn, used to train classifiers. However, annotators can create labels for different classification tasks, with divergent definitions of hate speech, binary or multi-label schemes, and various methodologies for collecting data. In this context, we examine the principal publicly available datasets for hate speech research. We investigate the types of hate speech (e.g., ethnicity, religion, sexual orientation) present in their composition, explore their content beyond the labels, and use cross-dataset classification to examine the use of the labeled data beyond its original work. Our results reveal interesting insights toward a better understanding of the hate speech phenomenon and improving its detection on social platforms.Warning. This paper contains offensive words and tweet examples.},
booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
articleno = {33},
numpages = {11},
keywords = {Datasets, Classification, HateBase, Hate Speech, Toxicity, Offensive Speech, Abusive Speech},
location = {Rome, Italy},
series = {HT '23}
}

@inproceedings{10.5555/3382225.3382258,
author = {Khaefi, Muhammad Rizal and Idzalika, Rajius and Amin, Imaduddin and Pramestri, Zakiya and Jutta, Pamungkas and Riyadi, Yulistina and Hodge, George and Lee, Jong Gun},
title = {Estimating the Quality of Crowdsourced Translations Based on the Characteristics of Source and Target Words and Participants},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Text-based media possess a wealth of insights that can be mined to understand perceptions and actions. Researchers and public officials can use these data to inform development policy and humanitarian action. An important step in analyzing text-based databases, such as social media, is the creation of taxonomies which are used to filter information relevant to topics of interest. We worked with thousands of online volunteers to translate 2,137 keywords or phrases in English to formal or vernacular expressions in 29 different languages with the aim of understanding human responses to natural disasters, as well as developing sets of corpus on non popular languages (non English and non EU languages) which still has limited studies. In processing the data set, we faced a challenge in selecting a set of quality translations for each language. This paper aims to estimate the quality of the crowdsourced translations by non-professional translators. This paper presents an extensive empirical study using 91 features from 29 languages corpora to describe (a) translators, (b) source expressions, and (c) translated expressions. Our results show that our approach exploring two regression models and two supervised learning methods produces better results than a baseline approach with a commonly used metric, namely peer-review scores.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {151–158},
numpages = {8},
keywords = {crowdsourcing translation, translation quality estimation, text analysis},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3465336.3475102,
author = {Jiang, Aiqi and Zubiaga, Arkaitz},
title = {Cross-Lingual Capsule Network for Hate Speech Detection in Social Media},
year = {2021},
isbn = {9781450385510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465336.3475102},
doi = {10.1145/3465336.3475102},
abstract = {Most hate speech detection research focuses on a single language, generally English, which limits their generalisability to other languages. In this paper we investigate the cross-lingual hate speech detection task, tackling the problem by adapting the hate speech resources from one language to another. We propose a cross-lingual capsule network learning model coupled with extra domain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves state-of-the-art performance on benchmark datasets from AMI@Evalita2018 and AMI@Ibereval2018 involving three languages: English, Spanish and Italian, outperforming state-of-the-art baselines on all six language pairs.},
booktitle = {Proceedings of the 32nd ACM Conference on Hypertext and Social Media},
pages = {217–223},
numpages = {7},
keywords = {social media, capsule network, hate speech detection, cross-lingual learning},
location = {Virtual Event, USA},
series = {HT '21}
}

@article{10.1145/3624562,
author = {Yaffe, Philip},
title = {Lost and Found in Translation},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2023},
number = {September},
url = {https://doi.org/10.1145/3624562},
doi = {10.1145/3624562},
abstract = {Each "Communication Corner" essay is self-contained; however, they build on each other. For best results, before reading this essay and doing the exercise, go to the first essay "How an Ugly Duckling Became a Swan," then read each succeeding essay.Most people write in their native language, which is why too many pay too little attention to how precisely they say what they want to say. "Well, everyone will understand." People who write or translate into other languages know this isn't true.},
journal = {Ubiquity},
month = {sep},
articleno = {1},
numpages = {5}
}

@article{10.1109/TASLP.2022.3212829,
author = {Liu, Yuanyuan and Reddy, Mittapalle Kiran and Penttil\"{a}, Nelly and Ihalainen, Tiina and Alku, Paavo and R\"{a}s\"{a}nen, Okko},
title = {Automatic Assessment of Parkinson's Disease Using Speech Representations of Phonation and Articulation},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3212829},
doi = {10.1109/TASLP.2022.3212829},
abstract = {Speech from people with Parkinson's disease (PD) are likely to be degraded on phonation, articulation, and prosody. Motivated to describe articulation deficits comprehensively, we investigated 1) the universal phonological features that model articulation manner and place, also known as speech attributes, and 2) glottal features capturing phonation characteristics. These were further supplemented by, and compared with, prosodic features using a popular compact feature set and standard MFCC. Temporal characteristics of these features were modeled by convolutional neural networks. Besides the features, we were also interested in the speech tasks for collecting data for automatic PD speech assessment, like sustained vowels, text reading, and spontaneous monologue. For this, we utilized a recently collected Finnish PD corpus (PDSTU) as well as a Spanish database (PC-GITA). The experiments were formulated as regression problems against expert ratings of PD-related symptoms, including ratings of speech intelligibility, voice impairment, overall severity of communication disorder on PDSTU, as well as on the Unified Parkinson's Disease Rating Scale (UPDRS) on PC-GITA. The experimental results show: 1) the speech attribute features can well indicate the severity of pathologies in parkinsonian speech; 2) combining phonation features with articulatory features improves the PD assessment performance, but requires high-quality recordings to be applicable; 3) read speech leads to more accurate automatic ratings than the use of sustained vowels, but not if the amount of speech is limited to correspond to the sustained vowels in duration; and 4) jointly using data from several speech tasks can further improve the automatic PD assessment performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {242–255},
numpages = {14}
}

@inproceedings{10.1145/3423958.3423973,
author = {Bouchardon, Serge and Meza, Nohelia},
title = {Translating a Web-Based Work of Digital Literature into Several Languages: Issues and Feedback},
year = {2020},
isbn = {9781450388856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423958.3423973},
doi = {10.1145/3423958.3423973},
abstract = {In 2010, the digital literature piece D\'{e}prise was published online (http://deprise.fr or http://lossofgrasp.com). Progressively, this web-based interactive narrative has been translated from French into English (2010), Italian (2011), Spanish (2013), and Portuguese (2016), and more recently in Arabic, Chinese, German, Hungarian and Polish (2020). Every translation has led to an intercultural and transcreative process [14] between the translators and the author. In this paper, we explore the following question: what does the translation of a web-based piece teach us about digital literature, but also about the Digital and the Web itself? We have asked the English, Italian, Spanish, and Portuguese translators of the work for feedback on the translation process. We have used the written exchanges with the translators to question the modes of collaboration between author and translator and the importance of translating specific dimensions of web-based digital literature. Additionally, we briefly discuss some future research trajectories on translating web-based digital literature: role of indirect translation [11], cultural dimension of the works, and translation as reinvented memory.},
booktitle = {Proceedings of the 3rd International Conference on Web Studies},
pages = {9–15},
numpages = {7},
keywords = {Transcreation, Multilingualism, Translation, Digital Literature},
location = {Hammamet, Tunisia},
series = {WS.3 2020}
}

@article{10.1109/TASLP.2021.3066303,
author = {Michelsanti, Daniel and Tan, Zheng-Hua and Zhang, Shi-Xiong and Xu, Yong and Yu, Meng and Yu, Dong and Jensen, Jesper},
title = {An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3066303},
doi = {10.1109/TASLP.2021.3066303},
abstract = {<italic>Speech enhancement</italic> and <italic>speech separation</italic> are two related tasks, whose purpose is to extract either one or more target speech signals, respectively, from a mixture of sounds generated by several sources. Traditionally, these tasks have been tackled using signal processing and machine learning techniques applied to the available acoustic signals. Since the visual aspect of speech is essentially unaffected by the acoustic environment, <italic>visual information</italic> from the target speakers, such as lip movements and facial expressions, has also been used for speech enhancement and speech separation systems. In order to efficiently fuse acoustic and visual information, researchers have exploited the flexibility of data-driven approaches, specifically <italic>deep learning</italic>, achieving strong performance. The ceaseless proposal of a large number of techniques to extract features and fuse multimodal information has highlighted the need for an overview that comprehensively describes and discusses audio-visual speech enhancement and separation based on deep learning. In this paper, we provide a systematic survey of this research topic, focusing on the main elements that characterise the systems in the literature: <italic>acoustic features</italic>; <italic>visual features</italic>; <italic>deep learning methods</italic>; <italic>fusion techniques</italic>; <italic>training targets</italic> and <italic>objective functions</italic>. In addition, we review deep-learning-based methods for <italic>speech reconstruction from silent videos</italic> and <italic>audio-visual sound source separation for non-speech signals</italic>, since these methods can be more or less directly applied to audio-visual speech enhancement and separation. Finally, we survey commonly employed <italic>audio-visual speech datasets</italic>, given their central role in the development of data-driven approaches, and <italic>evaluation methods</italic>, because they are generally used to compare different systems and determine their performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1368–1396},
numpages = {29}
}

@inproceedings{10.1145/3498851.3499022,
author = {Zhu-Zhou, Fangfang and Gil-Pita, Roberto and Garc\'{\i}a-G\'{o}mez, Joaqu\'{\i}n and Rosa-Zurera, Manuel},
title = {Noise and Codification Effect on Emotional Speech Classification Systems},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3499022},
doi = {10.1145/3498851.3499022},
abstract = {Emotions are reactions that all human beings experience daily, e.g., joy, sadness, fear, anger. They can be manifested through speech: when we speak, words are always accompanied by our emotions. This fact affects the muscular movements of the respiratory system and the larynx, thus modifying the timbre, tone, intensity, and intonation of what we say. There are many acoustic databases labeled with emotions available in the public domain. However, most of them were created under non-real-world circumstances, i.e., actors recreated emotions, and emotions were labeled under fictitious conditions where noise is absent. Another drawback of the design of emotion recognition systems is the lack of enough patterns in the available databases, thus driving to generalization problems and leading to overfitting. In this paper, a system is developed in order to verify the noise and the codification effect. Results have shown a performance deterioration in both cases, increasing the error probability from 28.54% to 65.29% in the first case and from 28.54% to 40.09% in the second case. Furthermore, an enlargement of the training set is proposed, creating new virtual patterns consisting of the original patterns with the addition of different values of signal-to-noise ratio and its effect in the design of an emotion classification system, confirming an improvement of the test error probability.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {278–283},
numpages = {6},
keywords = {Speech Processing, Noise Effect, Codification Effect, Enlarging Datasets, Emotional Speech},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3336294.3336313,
author = {Fernandez-Amoros, David and Heradio, Ruben and Mayr-Dorn, Christoph and Egyed, Alexander},
title = {A Kconfig Translation to Logic with One-Way Validation System},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336313},
doi = {10.1145/3336294.3336313},
abstract = {Automated analysis of variability models is crucial for managing software system variants, customized for different market segments or contexts of use. As most approaches for automated analysis are built upon logic engines, they require having a Boolean logic translation of the variability models. However, the translation of some significant languages to Boolean logic is remarkably non-trivial. The contribution of this paper is twofold: first, a translation of the Kconfig language is presented; second, an approach to test the translation for any given model is provided. The proposed translation has been empirically tested with the introduced validation procedure on five open-source projects.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {303–308},
numpages = {6},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1109/TASLP.2020.2967567,
author = {Moro-Vel\'{a}quez, Laureano and Hern\'{a}ndez-Garc\'{\i}a, Estefan\'{\i}a and G\'{o}mez-Garc\'{\i}a, Jorge A. and Godino-Llorente, Juan I. and Dehak, Najim},
title = {Analysis of the Effects of Supraglottal Tract Surgical Procedures in Automatic Speaker Recognition Performance},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2967567},
doi = {10.1109/TASLP.2020.2967567},
abstract = {This article evaluates the impact in the performance of state-of-the-art automatic speaker recognition schemes of three surgical procedures modifying the supraglottal tract structures of speakers. To do so, a new corpus (Cuco) was recorded, containing the speech of 107 speakers before and after surgery. Speakers were divided into four groups depending on the type of surgery: tonsillectomy, functional endoscopy sinus surgery (FESS), septoplasty, and controls. The analyzed speaker recognition schemes were i-vectors, i-vectors with supervised Universal Background Model, i-vectors employing Time-delay Deep Neural Networks and x-vectors. In all cases, probabilistic linear discriminant analysis was employed in the back-end. Results show changes in the speech of patients who underwent tonsillectomy or FESS after surgery in contrast to controls or patients who had a septoplasty, where not significant variations are observed. These changes increase the Equal Error Rate (EER) of the analyzed speaker recognition schemes for the septoplasty and FESS groups when employing enrollment data recorded before the surgery. Moreover, surgery has a similar influence in the speech of female and male speakers with respect to the analyzed schemes. In consequence, results suggest that it is advisable to update the speaker's enrollment speech after three months following supraglottal tract surgery to ensure that the effects of the operation and post-operative recovery period do not influence the performance of the automatic speaker recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {798–812},
numpages = {15}
}

@article{10.1145/3529759,
author = {Retta, Ephrem Afele and Almekhlafi, Eiad and Sutcliffe, Richard and Mhamed, Mustafa and Ali, Haider and Feng, Jun},
title = {A New Amharic Speech Emotion Dataset and Classification Benchmark},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3529759},
doi = {10.1145/3529759},
abstract = {In this article we present the Amharic Speech Emotion Dataset (ASED), which covers four dialects (Gojjam, Wollo, Shewa, and Gonder) and five different emotions (neutral, fearful, happy, sad, and angry). We believe it is the first Speech Emotion Recognition (SER) dataset for the Amharic language. Sixty-five volunteer participants, all native speakers of Amharic, recorded 2,474 sound samples, 2 to 4 seconds in length. Eight judges (two for each dialect) assigned emotions to the samples with high agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available for download. Next, we developed a four-layer variant of the well-known VGG model, which we call VGGb. Three experiments were then carried out using VGGb for SER, using ASED. First, we investigated which features work best for Amharic, FilterBank, Mel Spectrogram, or Mel-frequency Cepstral Coefficient (MFCC). This was done by training three VGGb SER models on ASED, using FilterBank, Mel Spectrogram, and MFCC features, respectively. Four forms of training were tried, standard cross-validation and three variants based on sentences, dialects, and speaker groups. Thus, a sentence used for training would not be used for testing, and the same for a dialect and speaker group. MFCC features were superior under all four training schemes. MFCC was therefore adopted for Experiment 2, where VGGb and three well-known existing models were compared on ASED: RESNet50, AlexNet, and LSTM. VGGb was found to have very good accuracy (90.73%) as well as the fastest training time. In Experiment 3, the performance of VGGb was compared when trained on two existing SER datasets—RAVDESS (English) and EMO-DB (German)—as well as on ASED (Amharic). Results are comparable across these languages, with ASED being the highest. This suggests that VGGb can be successfully applied to other languages. We hope that ASED will encourage researchers to explore the Amharic language and to experiment with other models for Amharic SER.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {20},
numpages = {22},
keywords = {classifiers, Amharic dataset, Speech emotion recognition, feature extraction}
}

@article{10.1109/TASLP.2018.2846402,
author = {Do, Quoc Truong and Sakti, Sakriani and Nakamura, Satoshi},
title = {Sequence-to-Sequence Models for Emphasis Speech Translation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2846402},
doi = {10.1109/TASLP.2018.2846402},
abstract = {Speech-to-speech translation S2ST systems are capable of breaking language barriers in cross-lingual communication by translating speech across languages. Recent studies have introduced many improvements that allow existing S2ST systems to handle not only linguistic meaning but also paralinguistic information such as emphasis by proposing additional emphasis estimation and translation components. However, the approach used for emphasis translation is not optimal for sequence translation tasks and fails to easily handle the long-term dependencies of words and emphasis levels. It also requires the quantization of emphasis levels and treats them as discrete labels instead of continuous values. Moreover, the whole translation pipeline is fairly complex and slow because all components are trained separately without joint optimization. In this paper, we make two contributions: 1 we propose an approach that can handle continuous emphasis levels based on sequence-to-sequence models, and 2 we combine machine and emphasis translation into a single model, which greatly simplifies the translation pipeline and make it easier to perform joint optimization. Our results on an emphasis translation task indicate that our translation models outperform previous models by a large margin in both objective and subjective tests. Experiments on a joint translation model also show that our models can perform joint translation of words and emphasis with one-word delays instead of full-sentence delays while preserving the translation performance of both tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1873–1883},
numpages = {11}
}

@article{10.5555/3455716.3455831,
author = {Gassiat, \'{E}lisabeth and Le Corff, Sylvain and Leh\'{e}ricy, Luc},
title = {Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Space},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {115},
numpages = {40},
keywords = {deconvolution, nonparametric estimation, latent variable models}
}

@article{10.1109/TASLP.2021.3133216,
author = {Jorge, Javier and Gim\'{e}nez, Adri\`{a} and Silvestre-Cerd\`{a}, Joan Albert and Civera, Jorge and Sanchis, Albert and Juan, Alfons},
title = {Live Streaming Speech Recognition Using Deep Bidirectional LSTM Acoustic Models and Interpolated Language Models},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3133216},
doi = {10.1109/TASLP.2021.3133216},
abstract = {Although Long-Short Term Memory (LSTM) networks and deep Transformers are now extensively used in offline ASR, it is unclear how best offline systems can be adapted to work with them under the streaming setup. After gaining considerable experience on this regard in recent years, in this paper we show how an optimized, low-latency streaming decoder can be built in which bidirectional LSTM acoustic models, together with general interpolated language models, can be nicely integrated with minimal perfomance degradation. In brief, our streaming decoder consists of a one-pass, real-time search engine relying on a limited-duration window sliding over time and a number of ad hoc acoustic and language model pruning techniques. Extensive empirical assessment is provided on truly streaming tasks derived from the well-known LibriSpeech and TED talks datasets, as well as from TV shows on a main Spanish broadcasting station.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {148–161},
numpages = {14}
}

@article{10.1109/TASLP.2022.3207349,
author = {Bie, Xiaoyu and Leglaive, Simon and Alameda-Pineda, Xavier and Girin, Laurent},
title = {Unsupervised Speech Enhancement Using Dynamical Variational Autoencoders},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3207349},
doi = {10.1109/TASLP.2022.3207349},
abstract = {Dynamical variational autoencoders (DVAEs) are a class of deep generative models with latent variables, dedicated to model time series of high-dimensional data. DVAEs can be considered as extensions of the variational autoencoder (VAE) that include temporal dependencies between successive observed and/or latent vectors. Previous work has shown the interest of using DVAEs over the VAE for speech spectrograms modeling. Independently, the VAE has been successfully applied to speech enhancement in noise, in an unsupervised noise-agnostic set-up that requires neither noise samples nor noisy speech samples at training time, but only requires clean speech signals. In this paper, we extend these works to DVAE-based single-channel unsupervised speech enhancement, hence exploiting both speech signals unsupervised representation learning and dynamics modeling. We propose an unsupervised speech enhancement algorithm that combines a DVAE speech prior pre-trained on clean speech signals with a noise model based on nonnegative matrix factorization, and we derive a variational expectation-maximization (VEM) algorithm to perform speech enhancement. The algorithm is presented with the most general DVAE formulation and is then applied with three specific DVAE models to illustrate the versatility of the framework. Experimental results show that the proposed DVAE-based approach outperforms its VAE-based counterpart, as well as several supervised and unsupervised noise-dependent baselines, especially when the noise type is unseen during training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2993–3007},
numpages = {15}
}

@inproceedings{10.1145/3579051.3579065,
author = {Wu, Zhanglin and Zhang, Min and Zhu, Ming and Li, Yinglu and Zhu, Ting and Yang, Hao and Peng, Song and Qin, Ying},
title = {KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579065},
doi = {10.1145/3579051.3579065},
abstract = {BERTScore is an effective and robust automatic metric for reference-based machine translation evaluation. In this paper, we incorporate multilingual knowledge graph into BERTScore and propose a metric named KG-BERTScore, which linearly combines the results of BERTScore and bilingual named entity matching for reference-free machine translation evaluation. From the experimental results on WMT19 QE as a metric without references shared tasks, our metric KG-BERTScore gets higher overall correlation with human judgements than the current state-of-the-art metrics for reference-free machine translation evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore and the parameter for linear combination are also studied in this paper.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {121–125},
numpages = {5},
keywords = {pre-trained multilingual model, BERTScore, multilingual knowledge graph, machine translation evaluation, KG-BERTScore},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@inproceedings{10.1145/3531146.3533117,
author = {Markl, Nina},
title = {Language Variation and Algorithmic Bias: Understanding Algorithmic Bias in British English Automatic Speech Recognition},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533117},
doi = {10.1145/3531146.3533117},
abstract = {All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {521–534},
numpages = {14},
keywords = {speech and language technologies, algorithmic bias, language variation, speech recognition},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.1145/3425604,
author = {Pinto, Dennis and Arnau, Jose-Mar\'{\i}a and Gonz\'{a}lez, Antonio},
title = {Design and Evaluation of an Ultra Low-Power Human-Quality Speech Recognition System},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3425604},
doi = {10.1145/3425604},
abstract = {Automatic Speech Recognition (ASR) has experienced a dramatic evolution since pioneer development of Bell Lab’s single-digit recognizer more than 50 years ago. Current ASR systems have taken advantage of the tremendous improvements in AI during the past decade by incorporating Deep Neural Networks into the system and pushing their accuracy to levels comparable to that of humans. This article describes and characterizes a representative ASR system with state-of-the-art accuracy and proposes a hardware platform capable of decoding speech in real-time with a power dissipation close to 1 Watt. The software is based on the so-called hybrid approach with a vocabulary of 200K words and RNN-based language model re-scoring, whereas the hardware consists of a commercially available low-power processor along with two accelerators used for the most compute-intensive tasks. The article shows that high performance can be obtained with very low power, enabling the deployment of these systems in extremely power-constrained environments such as mobile and IoT devices.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {41},
numpages = {19},
keywords = {low-power hardware, Hardware accelerators}
}

@inproceedings{10.1145/3477495.3531717,
author = {Liu, Yuqi and Hu, Chengcheng and Lin, Jimmy},
title = {Another Look at Information Retrieval as Statistical Translation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531717},
doi = {10.1145/3477495.3531717},
abstract = {Over two decades ago, Berger and Lafferty proposed "information retrieval as statistical translation" (IRST), a simple and elegant method for ad hoc retrieval based on the noisy channel model. At the time, they lacked the large-scale human-annotated datasets necessary to properly train their models. In this paper, we ask the simple question: What if Berger and Lafferty had access to datasets such as the MS MARCO passage ranking dataset that we take for granted today? The answer to this question tells us how much of recent improvements in ranking can be solely attributed to having more data available, as opposed to improvements in models (e.g., pretrained transformers) and optimization techniques (e.g., contrastive loss). In fact, Boytsov and Kolter recently began to answer this question with a replication of Berger and Lafferty's model, and this work can be viewed as another independent replication effort, with generalizations to additional conditions not previously explored, including replacing the sum of translation probabilities with ColBERT's MaxSim operator. We confirm that while neural models (particularly pretrained transformers) have indeed led to great advances in retrieval effectiveness, the IRST model proposed decades ago is quite effective if provided sufficient training data.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2749–2754},
numpages = {6},
keywords = {ms marco, noisy channel model},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3587267,
author = {Huang, Xin and Zhang, Jiajun and Zong, Chengqing},
title = {Contrastive Adversarial Training for Multi-Modal Machine Translation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3587267},
doi = {10.1145/3587267},
abstract = {The multi-modal machine translation task is to improve translation quality with the help of additional visual input. It is expected to disambiguate or complement semantics while there are ambiguous words or incomplete expressions in the sentences. Existing methods have tried many ways to fuse visual information into text representations. However, only a minority of sentences need extra visual information as complementary. Without guidance, models tend to learn text-only translation from the major well-aligned translation pairs. In this article, we propose a contrastive adversarial training approach to enhance visual participation in semantic representation learning. By contrasting multi-modal input with the adversarial samples, the model learns to identify the most informed sample that is coupled with a congruent image and several visual objects extracted from it. This approach can prevent the visual information from being ignored and further fuse cross-modal information. We examine our method in three multi-modal language pairs. Experimental results show that our model is capable of improving translation accuracy. Further analysis shows that our model is more sensitive to visual information.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {157},
numpages = {18},
keywords = {multi-modal machine translation, adversarial training, Contrastive Learning}
}

@inproceedings{10.1145/3593013.3594049,
author = {Papakyriakopoulos, Orestis and Choi, Anna Seo Gyeong and Thong, William and Zhao, Dora and Andrews, Jerone and Bourke, Rebecca and Xiang, Alice and Koenecke, Allison},
title = {Augmented Datasheets for Speech Datasets and Ethical Decision-Making},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594049},
doi = {10.1145/3593013.3594049},
abstract = {Speech datasets are crucial for training Speech Language Technologies (SLT); however, the lack of diversity of the underlying training data can lead to serious limitations in building equitable and robust SLT products, especially along dimensions of language, accent, dialect, variety, and speech impairment—and the intersectionality of speech features with socioeconomic and demographic features. Furthermore, there is often a lack of oversight on the underlying training data—commonly built on massive web-crawling and/or publicly available speech—with regard to the ethics of such data collection. To encourage standardized documentation of such speech data components, we introduce an augmented datasheet for speech datasets1, which can be used in addition to “Datasheets for Datasets” [78]. We then exemplify the importance of each question in our augmented datasheet based on in-depth literature reviews of speech data used in domains such as machine learning, linguistics, and health. Finally, we encourage practitioners—ranging from dataset creators to researchers—to use our augmented datasheet to better define the scope, properties, and limits of speech datasets, while also encouraging consideration of data-subject protection and user community empowerment. Ethical dataset creation is not a one-size-fits-all process, but dataset creators can use our augmented datasheet to reflexively consider the social context of related SLT applications and data sources in order to foster more inclusive SLT products downstream.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {881–904},
numpages = {24},
keywords = {speech, transparency, datasheets, ethics, datasets},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3469595.3469618,
author = {Edwards, Justin and Janssen, Christian and Gould, Sandy and Cowan, Benjamin R.},
title = {Eliciting Spoken Interruptions to Inform Proactive Speech Agent Design},
year = {2021},
isbn = {9781450389983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469595.3469618},
doi = {10.1145/3469595.3469618},
abstract = {Current speech agent interactions are typically user-initiated, limiting the interactions they can deliver. Future functionality will require agents to be proactive, sometimes interrupting users. Little is known about how these spoken interruptions should be designed, especially in urgent interruption contexts. We look to inform design of proactive agent interruptions through investigating how people interrupt others engaged in complex tasks. We therefore developed a new technique to elicit human spoken interruptions of people engaged in other tasks. We found that people interrupted sooner when interruptions were urgent. Some participants used access rituals to forewarn interruptions, but most rarely used them. People balanced speed and accuracy in timing interruptions, often using cues from the task they interrupted. People also varied phrasing and delivery of interruptions to reflect urgency. We discuss how our findings can inform speech agent design and how our paradigm can help gain insight into human interruptions in new contexts.},
booktitle = {Proceedings of the 3rd Conference on Conversational User Interfaces},
articleno = {23},
numpages = {12},
keywords = {multitasking, urgency, interruptions, speech interfaces, proactive agents},
location = {Bilbao (online), Spain},
series = {CUI '21}
}

@inproceedings{10.1145/3536221.3556587,
author = {Do, Tiffany D. and Akter, Mamtaj and Choudhary, Zubin and Azevedo, Roger and McMahan, Ryan P.},
title = {The Effects of an Embodied Pedagogical Agent’s Synthetic Speech Accent on Learning Outcomes},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556587},
doi = {10.1145/3536221.3556587},
abstract = {Modern text-to-speech engines can be an effective speech choice for embodied virtual pedagogical agents. However, it is not known how synthesized accents influence learning outcomes and perceptions of the agent. In this paper, we conducted a between-subjects experiment (n=60) to determine the effect of a pedagogical agent’s machine synthesized text-to-speech accent (United States English or Indian English) on learning outcomes and perceptions of the agent for students in the United States. Our results indicate that learner gender interacts with synthesized speech accent to significantly affect learning outcomes and perceptions of the agent. Our results reveal that a foreign synthetic speech accent may affect the learning outcomes of female university students (n=30), but not male university students (n=30). Finally, our results indicate that learner gender interacts with synthesized speech accent to affect perceptions of the pedagogical agent’s human-likeness. We provide novel insights on the differences between male and female learners for interactions with pedagogical agents with synthetic TTS accents.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {198–206},
numpages = {9},
keywords = {pedagogical agents, accent, synthetic speech},
location = {Bengaluru, India},
series = {ICMI '22}
}

@inproceedings{10.1145/3581791.3596862,
author = {Shahid, Irtaza and Roy, Nirupam},
title = {"Is This My President Speaking?" Tamper-Proofing Speech in Live Recordings},
year = {2023},
isbn = {9798400701108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581791.3596862},
doi = {10.1145/3581791.3596862},
abstract = {Malicious editing of audiovisual content has emerged as a popular tool for targeted defamation, spreading disinformation, and triggering political unrest. Public speeches and statements of political leaders, public figures, or celebrities are particularly at target due to their effectiveness in influencing the masses. Ubiquitous audiovisual recording of live speeches with smart devices and unrestricted content sharing and redistributing on social media make it difficult to address this threat using existing authentication techniques. Given public recordings of live events lack source control over the media, standard solutions falter. This paper presents TalkLock, a speech integrity verification system that can enable live speakers to protect their speeches from malicious alterations even when the speech is recorded by any member of the audience. The core idea is to generate meta-information from the speech signal in real-time and disseminate it through a secure QR code-based screen-camera communication. The QR code when recorded along with the speech embeds the meta-information in the content and it can be used later for independent verification in stand-alone applications or online platforms. A user study with live speech and real-world experiments with different types of voices, languages, environments, and distances show that TalkLock can verify fake content with 94.4% accuracy.},
booktitle = {Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services},
pages = {219–232},
numpages = {14},
keywords = {speech verification, QR code, deepfake, voice features},
location = {Helsinki, Finland},
series = {MobiSys '23}
}

@inproceedings{10.1145/3368089.3409756,
author = {Gupta, Shashij and He, Pinjia and Meister, Clara and Su, Zhendong},
title = {Machine Translation Testing via Pathological Invariance},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409756},
doi = {10.1145/3368089.3409756},
abstract = {Machine translation software has become heavily integrated into our daily lives due to the recent improvement in the performance of deep neural networks. However, machine translation software has been shown to regularly return erroneous translations, which can lead to harmful consequences such as economic loss and political conflicts. Additionally, due to the complexity of the underlying neural models, testing machine translation systems presents new challenges. To address this problem, we introduce a novel methodology called PatInv. The main intuition behind PatInv is that sentences with different meanings should not have the same translation. Under this general idea, we provide two realizations of PatInv that given an arbitrary sentence, generate syntactically similar but semantically different sentences by: (1) replacing one word in the sentence using a masked language model or (2) removing one word or phrase from the sentence based on its constituency structure. We then test whether the returned translations are the same for the original and modified sentences. We have applied PatInv to test Google Translate and Bing Microsoft Translator using 200 English sentences. Two language settings are considered: English-Hindi (En-Hi) and English-Chinese (En-Zh). The results show that PatInv can accurately find 308 erroneous translations in Google Translate and 223 erroneous translations in Bing Microsoft Translator, most of which cannot be found by the state-of-the-art approaches.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {863–875},
numpages = {13},
keywords = {Pathological Invariance, Testing, Machine translation},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3377811.3380339,
author = {He, Pinjia and Meister, Clara and Su, Zhendong},
title = {Structure-Invariant Testing for Machine Translation},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380339},
doi = {10.1145/3377811.3380339},
abstract = {In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of "similar" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {961–973},
numpages = {13},
keywords = {machine translation, structural invariance, metamorphic testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3588766,
author = {Yi, Nian and Shao, Chenze and Wumaier, Aishan},
title = {Integrating Reconstructor and Post-Editor into Neural Machine Translation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588766},
doi = {10.1145/3588766},
abstract = {Neural machine translation (NMT) mainly comprises the encoder and decoder. The encoder is mainly used to extract the feature vector of the source language sentence. The decoder predicts the next token according to the feature vector extracted by the encoder and the information of the current moment. In this process, there is no guarantee that the features extracted by the encoder are indistinguishable from the meaning of the sentences in the source language. There is also no guarantee that the decoder can accurately predict the corresponding character. These issues can lead to over-translation and under-translation issues in the translated results. Previous researchers alleviated this problem by calculating the gap between the reconstructed source-language sentences and the source-language sentences. Inspired by this method, we propose to integrate a reconstructor and a post-editor into NMT during the training. The reconstructor takes the translation of NMT as input to reconstruct the source sentence, and the post-editor takes the translation as input and post-edits it to predict the target sentence. Through the training of the reconstructor and the post-editor, the semantics of the translation are forced to follow the source sentence and the target sentence. Experimental results show that our approach can effectively improve the performance of NMT on multiple translation tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {163},
numpages = {15},
keywords = {reconstructor, neural machine translation, loss function, post-editor, Neural network}
}

@article{10.1145/3567592,
author = {Ranathunga, Surangika and Lee, En-Shiun Annie and Prifti Skenduli, Marjana and Shekhar, Ravi and Alam, Mehreen and Kaur, Rishemjit},
title = {Neural Machine Translation for Low-Resource Languages: A Survey},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567592},
doi = {10.1145/3567592},
abstract = {Neural Machine Translation (NMT) has seen tremendous growth in the last ten years since the early 2000s and has already entered a mature phase. While considered the most widely used solution for Machine Translation, its performance on low-resource language pairs remains sub-optimal compared to the high-resource counterparts due to the unavailability of large parallel corpora. Therefore, the implementation of NMT techniques for low-resource language pairs has been receiving the spotlight recently, thus leading to substantial research on this topic. This article presents a detailed survey of research advancements in low-resource language NMT (LRL-NMT) and quantitative analysis to identify the most popular techniques. We provide guidelines to select the possible NMT technique for a given LRL data setting based on our findings. We also present a holistic view of the LRL-NMT research landscape and provide recommendations to enhance the research efforts further.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {229},
numpages = {37},
keywords = {data augmentation, semi-supervised NMT, multilingual NMT, transfer learning, zero-shot translation, Neural machine translation, pivoting, unsupervised NMT, low-resource languages}
}

@article{10.1109/TASLP.2021.3078364,
author = {Narendra, N.P. and Schuller, Bj\"{o}rn and Alku, Paavo},
title = {The Detection of Parkinson's Disease From Speech Using Voice Source Information},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078364},
doi = {10.1109/TASLP.2021.3078364},
abstract = {Developing automatic methods to detect Parkinson's disease (PD) from speech has attracted increasing interest as these techniques can potentially be used in telemonitoring health applications. This article studies the utilization of voice source information in the detection of PD using two classifier architectures: traditional pipeline approach and end-to-end approach. The former consists of feature extraction and classifier stages. In feature extraction, the baseline acoustic features—consisting of articulation, phonation, and prosody features—were computed and voice source information was extracted using glottal features that were estimated by iterative adaptive inverse filtering (IAIF) and quasi-closed phase (QCP) glottal inverse filtering methods. Support vector machine classifiers were developed utilizing the baseline and glottal features extracted from every speech utterance and the corresponding <italic>healthy/PD</italic> labels. The end-to-end approach uses deep learning models which were trained using both raw speech waveforms and raw voice source waveforms. In the latter, two glottal inverse filtering methods (IAIF and QCP) and zero frequency filtering method were utilized. The deep learning architecture consists of a combination of convolutional layers followed by a multilayer perceptron. Experiments were performed using PC-GITA speech database. From the traditional pipeline systems, the highest classification accuracy (67.93%) was given by combination of baseline and QCP-based glottal features. From the end-to-end-systems, the highest accuracy (68.56%) was given by the system trained using QCP-based glottal flow signals. Even though classification accuracies were modest for all systems, the study is encouraging as the extraction of voice source information was found to be most effective in both approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1925–1936},
numpages = {12}
}

@inproceedings{10.5555/3382225.3382450,
author = {Lekea, Ioanna K. and Karampelas, Panagiotis},
title = {Detecting Hate Speech within the Terrorist Argument: A Greek Case},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {This paper presents a methodology for automatically detecting the presence of hate speech within the terrorist argument. Hate speech can be used by a terrorist group as a means of judging possible targets' guilt and deciding on their punishment, as well as a means of making people to accept acts of terror or even as propaganda for possibly attracting new members. In this paper, we examine both ideology expressed and practices employed by the Revolutionary Organization 17 November (hereafter 17N) that operated in Greece between the years of 1975 and 2002. Within this line of thought, we will focus on the ideological justification, ethical standing and deployment of the terrorist operations as presented in the communiqu\'{e}s published by 17N, emphasizing on the use of hate speech as a means of justifying their choices and actions, as well as a way of reaching out to Greek people. To decide on how the automatic classification will be performed, we experimented with different text analyzing techniques such as critical discourse and content analysis and based on the preliminary results of these techniques a classification algorithm is proposed that can classify the communiqu\'{e}s in three categories depending on the presence of hate speech. The methodology was tested over the existing dataset with all the communiqu\'{e}s and the corresponding results are discussed.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1084–1091},
numpages = {8},
keywords = {ideology, greek terrorism, tactics, hate speech, critical discourse analysis, ethics, targets, content analysis},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1145/3587932,
author = {Bala Das, Sudhansu and Biradar, Atharv and Kumar Mishra, Tapas and Kr. Patra, Bidyut},
title = {Improving Multilingual Neural Machine Translation System for Indic Languages},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3587932},
doi = {10.1145/3587932},
abstract = {The Machine Translation System (MTS) serves as effective tool for communication by translating text or speech from one language to another language. Recently, neural machine translation (NMT) has become popular for its performance and cost-effectiveness. However, NMT systems are restricted in translating low-resource languages as a huge quantity of data is required to learn useful mappings across languages. The need for an efficient translation system becomes obvious in a large multilingual environment like India. Indian languages (ILs) are still treated as low-resource languages due to unavailability of corpora. In order to address such an asymmetric nature, the multilingual neural machine translation (MNMT) system evolves as an ideal approach in this direction. The MNMT converts many languages using a single model, which is extremely useful in terms of training process and lowering online maintenance costs. It is also helpful for improving low-resource translation. In this article, we propose an MNMT system to address the issues related to low-resource language translation. Our model comprises two MNMT systems, i.e., for English-Indic (one-to-many) and for Indic-English (many-to-one) with a shared encoder-decoder containing 15 language pairs (30 translation directions). Since most of IL pairs have a scanty amount of parallel corpora, not sufficient for training any machine translation model, we explore various augmentation strategies to improve overall translation quality through the proposed model. A state-of-the-art transformer architecture is used to realize the proposed model. In addition, the article addresses the use of language relationships (in terms of dialect, script, etc.), particularly about the role of high-resource languages of the same family in boosting the performance of low-resource languages. Moreover, the experimental results also show the advantage of back-translation and domain adaptation for ILs to enhance the translation quality of both source and target languages. Using all these key approaches, our proposed model emerges to be more efficient than the baseline model in terms of evaluation metrics, i.e., BLEU (BiLingual Evaluation Understudy) score for a set of ILs.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {169},
numpages = {24},
keywords = {Multilingual neural machine translation system (MNMT), corpus, low resource language, BLEU score, Indic languages (ILs)}
}

@article{10.1145/3583684,
author = {Shi, Xuewen and Huang, Heyan and Jian, Ping and Tang, Yi-Kun},
title = {Approximating to the Real Translation Quality for Neural Machine Translation via Causal Motivated Methods},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3583684},
doi = {10.1145/3583684},
abstract = {It is hard to evaluate translations objectively and accurately, which limits the applications of machine translation. In this article, we assume that the above phenomenon is caused by noise interference during translation evaluation, and we handle the problem through a perspective of causal inference. We assume that the observable translation score is affected by the unobservable true translation quality and some noise simultaneously. If there is a variable that is related to the noise and independent to the true translation quality, the related noise can be eliminated by removing the effect of that variable from the observed score. Based on the above causality hypothesis, this article studies the length bias problem of beam search for neural machine translation (NMT) and the input related noise problem of translation quality estimation (QE). For the NMT length bias problem, we conduct the experiments on four typical NMT tasks (Uyghur–Chinese, Chinese–English, English–German, and English–French) with different scales of datasets. Comparing with previous approaches, the proposed causal motivated method is model-agnostic and does not require supervised training. For QE tasks, we conduct the experiments on the WMT’20 submissions. Experimental results show that the denoised QE results gain better Pearson’s correlation scores with human assessed scores compared to the original submissions. Further analyses on the NMT and QE tasks also demonstrate the rationality of the empirical assumptions made on our methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {126},
numpages = {26},
keywords = {Neural machine translation, quality estimation, causal inference, half-sibling regression}
}

@inproceedings{10.1145/3544548.3581465,
author = {Su, Zixiong and Fang, Shitao and Rekimoto, Jun},
title = {LipLearner: Customizable Silent Speech Interactions on Mobile Devices},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581465},
doi = {10.1145/3544548.3581465},
abstract = {Silent speech interface is a promising technology that enables private communications in natural language. However, previous approaches only support a small and inflexible vocabulary, which leads to limited expressiveness. We leverage contrastive learning to learn efficient lipreading representations, enabling few-shot command customization with minimal user effort. Our model exhibits high robustness to different lighting, posture, and gesture conditions on an in-the-wild dataset. For 25-command classification, an F1-score of 0.8947 is achievable only using one shot, and its performance can be further boosted by adaptively learning from more data. This generalizability allowed us to develop a mobile silent speech interface empowered with on-device fine-tuning and visual keyword spotting. A user study demonstrated that with LipLearner, users could define their own commands with high reliability guaranteed by an online incremental learning scheme. Subjective feedback indicated that our system provides essential functionalities for customizable silent speech interactions with high usability and learnability.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {696},
numpages = {21},
keywords = {Few-shot Learning, Silent Speech Interface, Customization, Lipreading},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3571732,
author = {Madan, Chetan and Diddee, Harshita and Kumar, Deepika and Mittal, Mamta},
title = {CodeFed: Federated Speech Recognition for Low-Resource Code-Switching Detection},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3571732},
doi = {10.1145/3571732},
abstract = {One common constraint in the practical application of speech recognition is Code Switching. The issue of code-switched languages is especially aggravated in the context of Indian languages - since most massively multilingual models are trained on corpora that are not representative of the diverse set of Indian languages. An associated constraint with such systems is the privacy-intrusive nature of the applications that aim to collate such representative data. To collectively mitigate both problems, this works presents CodeFed: A federated learning-based code-switching detection model that can be deployed to collaboratively trained by leveraging private data from multiple users, without compromising their privacy. Using a representative low-resource Indic dataset, we demonstrate the superior performance of a collaboratively trained global model that is trained using federated learning on three low-resource Indic languages - Gujarati, Tamil and Telugu and draw a comparison of the model with respect to most current work in the field. Finally, to evaluate the practical realizability of the proposed system, CodeFed also discusses the system overview of the label generation architecture which may accompany CodeFed’s possible real-time deployment.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
keywords = {Federated Learning, Low Resource Indian Languages, Speech Processing, Mobile Computing, Code-Switching}
}

@inproceedings{10.1145/3568162.3578633,
author = {Amioka, Saya and Janssens, Ruben and Wolfert, Pieter and Ren, Qiaoqiao and Pinto Bernal, Maria Jose and Belpaeme, Tony},
title = {Limitations of Audiovisual Speech on Robots for Second Language Pronunciation Learning},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3578633},
doi = {10.1145/3568162.3578633},
abstract = {The perception of audiovisual speech plays an important role in infants' first language acquisition and continues to be important for language understanding beyond infancy. Beyond that, the perception of speech and congruent lip motion supports language understanding for adults, and it has been suggested that second language learning benefits from audiovisual speech, as it helps learners distinguish speech sounds in the target language. In this paper, we study whether congruent audiovisual speech on a robot facilitates the learning of Japanese pronunciation. 27 native-Dutch speaking participants were trained in Japanese pronunciation by a social robot. The robot demonstrated 30 Japanese words of varying complexity using either congruent audiovisual speech, incongruent visual speech, or computer-generated audiovisual speech. Participants were asked to imitate the robot's pronunciation, recordings of which were rated by native Japanese speakers. Against expectation, the results showed that congruent audiovisual speech resulted in lower pronunciation performance than low-fidelity or incongruent speech. We show that our learners, being native Dutch speakers, are only very weakly sensitive to audiovisual Japanese speech which possibly explains why learning performance does not seem to benefit from audiovisual speech.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {359–367},
numpages = {9},
keywords = {audiovisual speech, multi-modal interaction, human-robot interaction, orofacial animations, robot-assisted language learning},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@article{10.1145/3342353,
author = {Han, Dong and Li, Junhui and Li, Yachao and Zhang, Min and Zhou, Guodong},
title = {Explicitly Modeling Word Translations in Neural Machine Translation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342353},
doi = {10.1145/3342353},
abstract = {In this article, we show that word translations can be explicitly incorporated into NMT effectively to avoid wrong translations. Specifically, we propose three cross-lingual encoders to explicitly incorporate word translations into NMT: (1) Factored encoder, which encodes a word and its translation in a vertical way; (2) Gated encoder, which uses a gated mechanism to selectively control the amount of word translations moving forward; and (3) Mixed encoder, which stitchingly learns a word and its translation annotations over sequences where words and their translations are alternatively mixed. Besides, we first use a simple word dictionary approach and then a word sense disambiguation (WSD) approach to effectively model the word context for better word translation. Experimentation on Chinese-to-English translation demonstrates that all proposed encoders are able to improve the translation accuracy for both traditional RNN-based NMT and recent self-attention-based NMT (hereafter referred to as Transformer). Specifically, Mixed encoder yields the most significant improvement of 2.0 in BLEU on the RNN-based NMT, while Gated encoder improves 1.2 in BLEU on Transformer. This indicates the usefulness of an WSD approach in modeling word context for better word translation. This also indicates the effectiveness of our proposed cross-lingual encoders in explicitly modeling word translations to avoid wrong translations in NMT. Finally, we discuss in depth how word translations benefit different NMT frameworks from several perspectives.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {15},
numpages = {17},
keywords = {word sense disambiguation, cross-lingual encoder, Neural machine translation, word translation}
}

@article{10.1145/3409463,
author = {Fazal, Muhammad Abu Ul and Ferguson, Sam and Johnston, Andrew},
title = {Evaluation of Information Comprehension in Concurrent Speech-Based Designs},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3409463},
doi = {10.1145/3409463},
abstract = {In human-computer interaction, particularly in multimedia delivery, information is communicated to users sequentially, whereas users are capable of receiving information from multiple sources concurrently. This mismatch indicates that a sequential mode of communication does not utilise human perception capabilities as efficiently as possible. This article reports an experiment that investigated various speech-based (audio) concurrent designs and evaluated the comprehension depth of information by comparing comprehension performance across several different formats of questions (main/detailed, implied/stated). The results showed that users, besides answering the main questions, were also successful in answering the implied questions, as well as the questions that required detailed information, and that the pattern of comprehension depth remained similar to that seen to a baseline condition, where only one speech source was presented. However, the participants answered more questions correctly that were drawn from the main information, and performance remained low where the questions were drawn from detailed information. The results are encouraging to explore the concurrent methods further for communicating multiple information streams efficiently in human-computer interaction, including multimedia.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {dec},
articleno = {129},
numpages = {19},
keywords = {listening comprehension, comprehension depth, concurrent streaming, audio streams, Concurrent speech, speech perception, auditory display, intermittent 8 continuous speech presentation, voice-based interaction, audio spatial location, concurrent speech-based information comprehension}
}

@article{10.1145/3610774,
author = {Ma, Yuwei and Qian, Yu},
title = {Methods of Improving Japanese-Chinese Machine Translation System through Machine Learning and Human-Computer Interaction},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610774},
doi = {10.1145/3610774},
abstract = {With globalization, the exchange of people between different countries is becoming more frequent. Due to different languages, there are serious obstacles to personnel exchanges. To a large extent, they hinder the growth of industries such as economy, culture and tourism in each country. The emergence of Machine Translation (MT) has effectively improved the problem of language barriers, and greatly reduced the workload of translators in text translation. However, MT does not have the same flexible flexibility as human translation. It just translates the text word by word, which is often difficult to meet people's higher needs. This paper proposed to build a Japanese-Chinese MT system and integrate machine learning and Human-Computer Interaction (HCI) technology into the system. To further enhance the efficiency of the system, enhancement algorithms were also applied to the system to optimize the performance of the system. From the experimental results, in terms of BLEU (Bilingual Evaluation Understudy) index, the average BLEU index of the algorithm in this paper was 8.59, and that of the traditional algorithm was 6.55. In terms of translation precision, the average precision of the algorithm in this paper was 91.53%, while that of the traditional algorithm was 87.28%. In terms of translation readability, the average readability of the algorithm in this paper was 93.32%, while that of the traditional algorithm was 89.22%. By comparison, the average BLEU index of the algorithm in this paper has increased by 2.04; the average accuracy of translation increased by 4.25%; the average readability increased by 4.1%. From the above data, it was evident that the enhancement algorithm can optimize the performance of the Japanese-Chinese MT system well.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
keywords = {Human-Computer Interaction, Enhancement Algorithm, Machine Translation, Japanese-Chinese Machine Translation System, Machine Learning}
}

@inproceedings{10.1145/3409334.3452059,
author = {Qiu, Jiabao and Moh, Melody and Moh, Teng-Sheng},
title = {Fast Streaming Translation Using Machine Learning with Transformer},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452059},
doi = {10.1145/3409334.3452059},
abstract = {Machine Translation is the usage of machine learning techniques in translation from one language to another. It has recently been applied to streaming translation, also known as automatic subtitling. The most common challenge in this area is the trade-off between correctness and speed. Due to its real-time feature, streaming translation needs high speed as it has strict playtime constraints. This paper proposes an enhanced Transformer model for fast streaming translation. The proposed machine-learning method is described, implemented, and evaluated based on a common German-English bilingual dataset. The evaluation results have shown that the proposed system successfully achieved a good speed in the training phase, and a high speed in the actual translating phrase that is fast enough for real-time applications, while also maintaining robust correctness. We believe the proposed Transformer model is a significant contribution to natural-language processing, and would be useful for other real-time translation applications.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {9–16},
numpages = {8},
keywords = {machine translation, natural language processing, machine learning, neural networks},
location = {Virtual Event, USA},
series = {ACM SE '21}
}

@inproceedings{10.1145/3585088.3595283,
author = {Patel, Manooshree and Chalageri, Swapneel},
title = {GlotBot: Hybrid Language Translator for Secondary Level Mathematics Classrooms},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3595283},
doi = {10.1145/3585088.3595283},
abstract = {As the place where so many individuals make lifelong friends, learn how to navigate the world, and grow into themselves, compassion must start in the classroom. However, current American classrooms sometimes lack the very principles of equality and inclusion which underlie compassionate communities. One such case can be seen in the systemic lingual ostracization of students who are not native English speakers (English language learners). As our student design partners indicated, students’ and teachers’ mutual understanding despite differences in language, contributes to a feeling of “love and comfort in the classroom”. Inspired by our student design partners’ proposal of the Robo-Assistant, we decided to answer the Interaction Design and Children (IDC) 2023 research and design challenge with GlotBot! GlotBot is a mobile application to be used by teachers while delivering classroom instruction. GlotBot is designed for the secondary mathematics classroom, a prime setting in which students’ English language proficiencies are unfairly conflated with their mathematical abilities. GlotBot translanguages; it generates hybrid translations of teacher speech in real-time by translating non-technical terms into Spanish, but keeping technical terms (“coordinate plane”, “graph”) in English. The teacher can choose to repeat this hybrid translation out loud, to create more access points to the content for English language learners. The main objective of GlotBot is to assist teachers in delivering an equitable pedagogy to create an inclusive and compassionate classroom space that celebrates our nation’s lingual diversity.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {439–443},
numpages = {5},
keywords = {mobile application, language accessibility, hybrid translation, secondary mathematics classrooms, inclusive education},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{10.1145/3362789.3362842,
author = {Blanco-Herrero, David and Calder\'{o}n, Carlos Arcila},
title = {Spread and Reception of Fake News Promoting Hate Speech against Migrants and Refugees in Social Media: Research Plan for the Doctoral Programme Education in the Knowledge Society},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362842},
doi = {10.1145/3362789.3362842},
abstract = {The growing number of cases of hate speech against migrants and refugees substantially obeys to the relevance of social media and the presence of fake news in them. This research will triangulate three methods to study how fake contents in social media contribute to the spread of hate speech against these collectives. A survey will analyze the opinions of citizens -producers and receivers of fake or hateful content in social media--about the topic; using social network analysis (SNA) we will study how these contents spread in social media; and an experimental survey will observe how these contents are received and to what extent they are believed by citizens. The expected results will allow a multidimensional and complete view of the whole communication system--production, transmission and reception of messages- of hate speech through fake news and social media, combining some of the most relevant and urgent topics of current Western societies.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {949–955},
numpages = {7},
keywords = {Social media, Hate speech, Fake news, Survey, Social Network Analysis, Spain, Migrants and refugees, Experiment},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.1145/3617653,
author = {Pathak, Dhrubajyoti and Nandi, Sukumar and Sarmah, Priyankoo},
title = {Part-of-Speech Tagger for Assamese Using Ensembling Approach},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617653},
doi = {10.1145/3617653},
abstract = {Ensemble system for part-of-speech (POS) tagging is beneficial for many resource-poor languages that do not have enough annotated training data to train Deep Learning (DL, also named Deep Neural Network) based POS taggers. An Ensemble system is a better choice to incorporate the linguistic features of a language and leverage the benefits of various types of POS taggers. In this work, we present our experiment of developing an ensemble tagger for Assamese, a low-resource, morphologically rich scheduled language of India, spoken by more than 15 million people. Despite the success of modern neural-network-based models in sequence tagging tasks, it has yet to receive attention in developing tasks such as POS in a resource-poor language such as Assamese. We develop a POS tagging model based on the BiLSTM-CRF architecture with a corpus of 404k tokens. We cover several word embeddings during training. Among all the experiments, the top two POS tagging models achieve tagging F1 scores of 0.746 and 0.745. We observe that the DL-based taggers are not able to achieve decent accuracy. It may be due to the inability to capture the linguistic features of the language or due to comparatively less annotated data. So, we build another POS tagger using a rule-based approach considering several morphological phenomenons of the language and get an F1 score of 0.85. Subsequently, we integrate the top two DL-based taggers with the rule-based ones and develop a new POS tagger using an ensemble approach, of which we get an improved F1 score of 0.925. Performance improvement of our new ensemble POS taggers over the baseline taggers suggests that integration of the taggers combines the qualities of all taggers in the new tagger. Therefore, this study also states ensemble taggers are more suitable for highly inflectional, morphologically rich resource-poor languages.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Word embedding, Assamese language, Language model, Part of speech (POS) tagger, Deep Learning, Ensemble POS tagger, Sequence labeling model}
}

@inproceedings{10.1145/3572549.3572641,
author = {Eun, Nahyun and Ou, Soobin and Kim, Mijin and Yoo, Chaewon and Lee, Jongwoo},
title = {Speech-Recognizing KIOSK Mobile Application for the Visually Impaired},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572641},
doi = {10.1145/3572549.3572641},
abstract = {Recently, KIOSKs can be easily found in many places due to the increased labor cost burden and technology development. In particular, the restaurant industry increases the kiosk placement rate. However, the visually impaired have problems with KIOSKs consisting of touch screens and even describe it as a 'wall.' Based on speech recognition, we propose software that helps visually impaired people use kiosks independently. The speech recognition kiosk proposed in this paper guides the ordering process to users through sound. And users order foods by voice. This paper introduces the functional design of the speech-recognition kiosk, the implementation of the prototype, and the operation process of the application.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {575–580},
numpages = {6},
keywords = {speech recognition, the visually impaired, android, KIOSK},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@article{10.1145/3382189,
author = {Tan, Liling and Li, Maggie Yundi and Kok, Stanley},
title = {E-Commerce Product Categorization via Machine Translation},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382189},
doi = {10.1145/3382189},
abstract = {E-commerce platforms categorize their products into a multi-level taxonomy tree with thousands of leaf categories. Conventional methods for product categorization are typically based on machine learning classification algorithms. These algorithms take product information as input (e.g., titles and descriptions) to classify a product into a leaf category. In this article, we propose a new paradigm based on machine translation. In our approach, we translate a product’s natural language description into a sequence of tokens representing a root-to-leaf path in a product taxonomy. In our experiments on two large real-world datasets, we show that our approach achieves better predictive accuracy than a state-of-the-art classification system for product categorization. In addition, we demonstrate that our machine translation models can propose meaningful new paths between previously unconnected nodes in a taxonomy tree, thereby transforming the taxonomy into a directed acyclic graph. We discuss how the resultant taxonomy directed acyclic graph promotes user-friendly navigation, and how it is more adaptable to new products.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jul},
articleno = {11},
numpages = {14},
keywords = {classification, E-commerce, machine translation}
}

@inproceedings{10.1145/3503252.3531309,
author = {Grolman, Edita and Binyamini, Hodaya and Shabtai, Asaf and Elovici, Yuval and Morikawa, Ikuya and Shimizu, Toshiya},
title = {HateVersarial: Adversarial Attack Against Hate Speech Detection Algorithms on Twitter},
year = {2022},
isbn = {9781450392075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503252.3531309},
doi = {10.1145/3503252.3531309},
abstract = {Machine learning (ML) models are commonly used to detect hate speech, which is considered one of the main challenges of online social networks. However, ML models have been shown to be vulnerable to well-crafted input samples referred to as adversarial examples. In this paper, we present an adversarial attack against hate speech detection models and explore the attack’s ability to: (1) prevent the detection of a hateful user, which should result in termination of the user’s account, and (2) classify normal users as hateful, which may lead to the termination of a legitimate user’s account. The attack is targeted at ML models that are trained on tabular, heterogeneous datasets (such as the datasets used for hate speech detection) and attempts to determine the minimal number of the most influential mutable features that should be altered in order to create a successful adversarial example. To demonstrate and evaluate the attack, we used the open and publicly available “Hateful Users on Twitter” dataset. We show that under a black-box assumption (i.e., the attacker does not have any knowledge on the attacked model), the attack has a 75% success rate, whereas under a white-box assumption (i.e., the attacker has full knowledge on the attacked model), the attack has an 88% success rate.},
booktitle = {Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {143–152},
numpages = {10},
keywords = {hate speech, Twitter, social media, adversarial attack},
location = {Barcelona, Spain},
series = {UMAP '22}
}

@article{10.1109/TASLP.2019.2957871,
author = {Beerends, John G. and Neumann, Niels M. P. and van den Broek, Egon L. and Llagostera Casanovas, Anna and Menendez, Jovana Torres and Schmidmer, Christian and Berger, Jens},
title = {Subjective and Objective Assessment of Full Bandwidth Speech Quality},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957871},
doi = {10.1109/TASLP.2019.2957871},
abstract = {With the introduction of fullband speech coding the question arises what role frequency components above 14&nbsp;kHz play in speech quality assessment. On the one hand, our results show that bandwidth limitation from 24&nbsp;kHz down to 14&nbsp;kHz is not audible to even the most critical subject. On the other hand, 14–24&nbsp;kHz band limited, audible levels of noise clearly decrease the perceived quality, especially for young subjects with healthy ears. Furthermore, modern high-quality voice links, using the latest speech codecs, often apply advanced buffering schemes that introduce a new type of audible degradation: micropauses. We investigated the impact of i) bandwidth limitation, ii) coding schemes, iii) micropause, and iv) noise on the perceived quality. Subjective results and objective predictions based on ITU-T recommendation P.863 POLQA are compared. For accurate prediction of the impact of micropauses and noise degradations small model adaptations are suggested. In contrast codec degradations and bandwidth limitation are already predicted with very high accuracy by POLQA: <italic>r</italic> = 0.98, RMSE<sup>*</sup> = 0.05 Mean Opinion Score (MOS).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {440–449},
numpages = {10}
}

@article{10.1145/3477536,
author = {Yu, Zhiqiang and Yu, Zhengtao and Xian, Yantuan and Huang, Yuxin and Guo, Junjun},
title = {Improving Chinese-Vietnamese Neural Machine Translation with Linguistic Differences},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3477536},
doi = {10.1145/3477536},
abstract = {We present a simple, efficient data augmentation approach for boosting Chinese-Vietnamese neural machine translation performance by leveraging the linguistic difference between the two languages. We first define the formalized representation of modifier symmetry, which is one of the most representative linguistic differences between Chinese and Vietnamese. We then propose and test two data augmentation strategies for leveraging the linguistic difference, which can be integrated naturally with different translation models. Results indicate that both strategies can introduce linguistic rules to boost translation accuracy. Tests on Chinese-Vietnamese benchmarks show significant accuracy improvements. To facilitate studies in this domain, we also release an open-source toolkit1 with flexible implementation for Chinese-Vietnamese linguistic difference tagging.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {22},
numpages = {12},
keywords = {linguistic difference, data augmentation, Neural machine translation, Chinese-Vietnamese}
}

@article{10.1145/3610582,
author = {Sethi, Nandini and Dev, Amita and Bansal, Poonam and Sharma, Deepak Kumar and Gupta, Deepak},
title = {A Pragmatic Analysis of Machine Translation Techniques for Preserving the Authenticity of the Sanskrit Language},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610582},
doi = {10.1145/3610582},
abstract = {Machine Translation has been a field of study for over six decades, but it has acquired substantial prominence in the last decade as processing capacity in personal computers has increased. The purpose of this paper is to discuss the usage of Sanskrit as a source, target, or supporting language in various Machine Translation systems. To investigate Machine Translation, researchers use a variety of strategies, including corpus-based, direct, and rule-based approaches. The primary goal of employing Sanskrit in Machine Translation is to evaluate its appropriateness, lexicon, and performance when proper Machine Translation methods are used. The research examines various modelling strategies for developing a machine translation system, specifically Statistical and Neural Machine Translation, in order to bridge the gap between Sanskrit and its current successor, Hindi. Interpretations are formed in Statistical Machine Translation by matching words from the source and target languages with statistical models and bilingual text corpora to learn parameters. Neural Machine Translation, on the other hand, uses an artificial neural network to predict the likelihood of a word sequence, frequently modelling entire phrases within a single integrated model. Neural Machine Translation is implemented using an encoder-decoder architecture with an attention mechanism. One of the most significant contributions of this paper is the use of different data sources, data collecting, and scraping to create a complete dataset. According to the study's findings, Neural Machine Translation outperforms the Statistical Machine Translation modelling technique. Furthermore, the paper examines the distinctive qualities of the Sanskrit language as well as the difficulties encountered by researchers in digesting Sanskrit while constructing the machine translation system. This study investigates the use of Sanskrit in Machine Translation and analyses several modelling methods, such as Statistical and Neural Machine Translation. The paper emphasizes the advantages of Neural Machine Translation and discusses the unique characteristics and challenges of the Sanskrit language in machine translation development.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
keywords = {Bilingual Dictionary, Corpus-Based Machine Translation (CBMT), Natural Language Processing (NLP), Sanskrit, Parallel Corpora, part of speech (POS), Interlingua}
}

@article{10.1145/3617371,
author = {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and Jhaveri, Rutvij H. and Banik, Debajyoty},
title = {Automatic Resource Augmentation for Machine Translation in Low Resource Language: EnIndic Corpus},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617371},
doi = {10.1145/3617371},
abstract = {Parallel corpus is the primary ingredient of machine translation. It is required to train the statistical machine translation (SMT) and neural machine translation (NMT) systems. There is a lack of good quality parallel corpus for Hindi to English. Comparable corpora for a given language pair are comparatively easy to find, but this cannot be used directly in SMT or NMT systems. As a result, we generate a parallel corpus from the comparable corpus. For this purpose, the sentences (which are translations of each other) are mined from the comparable corpus to prepare the parallel corpus. The proposed algorithm uses the length of the sentence and word translation model to align sentence pairs that are translations of each other. Then, the sentence pairs that are poor translations of each other (measured by a similarity score based on IBM model 1 translation probability) are filtered out. We apply this algorithm to comparable corpora, which are crawled from speeches of the President and Vice-President of India, and mined parallel corpora out of them. The prepared parallel corpus contains good quality aligned sentences (with 96.338% f-score). Subsequently, incorrect sentence pairs are filtered out manually to make the corpus in qualitative practical use. Finally, we gather various sentences from different sources to prepare the EnIndic corpus, which comprises 1,656,207 English-Hindi sentence pairs (miscellaneous domain). We have deployed this prepared largest English-Hindi parallel corpus at https://github.com/debajyoty/EnIndic.git and the source code at https://github.com/debajyoty/EnIndicSourceCode.git.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
keywords = {Linguistic Resources and Natural Language Processing, Comparable Corpus, Machine Translation, Parallel Corpus}
}

@article{10.1145/3608947,
author = {Kim, Hwichan and Tosho, Hirasawa and Moon, Sangwhan and Okazaki, Naoaki and Komachi, Mamoru},
title = {North Korean Neural Machine Translation through South Korean Resources},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3608947},
doi = {10.1145/3608947},
abstract = {South and North Korea both use the Korean language. However, Korean natural language processing (NLP) research has mostly focused on South Korean language. Therefore, existing NLP systems in the Korean language, such as neural machine translation (NMT) systems, cannot properly process North Korean inputs. Training a model using North Korean data is the most straightforward approach to solving this problem, but the data to train NMT models are insufficient. To solve this problem, we constructed a parallel corpus to develop a North Korean NMT model using a comparable corpus. We manually aligned parallel sentences to create evaluation data and automatically aligned the remaining sentences to create training data. We trained a North Korean NMT model using our North Korean parallel data and improved North Korean translation quality using South Korean resources such as parallel data and a pre-trained model. In addition, we propose Korean-specific pre-processing methods, character tokenization, and phoneme decomposition to use the South Korean resources more efficiently. We demonstrate that the phoneme decomposition consistently improves the North Korean translation accuracy compared to other pre-processing methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {223},
numpages = {22},
keywords = {north korean machine translation, pre-process, parallel data construction, Low resource}
}

@inproceedings{10.1145/3377713.3377752,
author = {Liu, Xinyue and Wang, Weixuan},
title = {Multiway Attention for Neural Machine Translation},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377752},
doi = {10.1145/3377713.3377752},
abstract = {Neural machine translation (NMT) with source side attention has achieved remarkable performance. Nevertheless, all existing attention mechanisms employ only one attention function. However, several different attention functions have been proposed. They have different mechanisms and capture different information of the sentences, thus a single attention function does not perform well. In the paper, we propose the multiway attention neural machine translation model (MA-NMT) which employs multiple attention functions in the attention mechanism to calculate the weight of each source word when predicting the next target word. Specially, we design three attention functions to get the contextual information. Then we combine the features from all attention function to obtain the final semantic representation. The results of experiments on the English-German translation task demonstrate that the proposed MA-NMT improves the performance than the baseline NMT models.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {265–270},
numpages = {6},
keywords = {Attention mechanism, Neural machine translation, Deep learning},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3293339.3293341,
author = {Ram R, Vijay Sundar and Devi, Sobha Lalitha},
title = {Overview of Verb Phrase Translation in Machine Translation: English to Tamil and Hindi to Tamil},
year = {2018},
isbn = {9781450362085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293339.3293341},
doi = {10.1145/3293339.3293341},
abstract = {We present an overview of verb phrase translation in machine translation from English to Tamil and Hindi to Tamil track, where English, Hindi and Tamil belong to three different language families, namely, Indo-European, Indo-Aryan and Dravidian family respectively. Verb phrases carry syntactic information such as tense, aspect, modal, and PNG (person, number and gender) other than the main verb. The characteristics of verb phrase vary between languages, which make the task challenging. We have five registrations and three out of the five registered teams submitted their runs. The runs are evaluated based on the correctness of tense, aspect, modal (TAM) and PNG of the verb phrase. The performance scores of the participants show the complexity of the task.},
booktitle = {Proceedings of the 10th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {6–10},
numpages = {5},
keywords = {English-Tamil, Verb phrase translation, Hindi-Tamil},
location = {Gandhinagar, India},
series = {FIRE '18}
}

@article{10.1145/3469721,
author = {Baruah, Rupjyoti and Mundotiya, Rajesh Kumar and Singh, Anil Kumar},
title = {Low Resource Neural Machine Translation: Assamese to/from Other Indo-Aryan (Indic) Languages},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3469721},
doi = {10.1145/3469721},
abstract = {Machine translation (MT) systems have been built using numerous different techniques for bridging the language barriers. These techniques are broadly categorized into approaches like Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). End-to-end NMT systems significantly outperform SMT in translation quality on many language pairs, especially those with the adequate parallel corpus. We report comparative experiments on baseline MT systems for Assamese to other Indo-Aryan languages (in both translation directions) using the traditional Phrase-Based SMT as well as some more successful NMT architectures, namely basic sequence-to-sequence model with attention, Transformer, and finetuned Transformer. The results are evaluated using the most prominent and popular standard automatic metric BLEU (BiLingual Evaluation Understudy), as well as other well-known metrics for exploring the performance of different baseline MT systems, since this is the first such work involving Assamese. The evaluation scores are compared for SMT and NMT models for the effectiveness of bi-directional language pairs involving Assamese and other Indo-Aryan languages (Bangla, Gujarati, Hindi, Marathi, Odia, Sinhalese, and Urdu). The highest BLEU scores obtained are for Assamese to Sinhalese for SMT (35.63) and the Assamese to Bangla for NMT systems (seq2seq is 50.92, Transformer is 50.01, and finetuned Transformer is 50.19). We also try to relate the results with the language characteristics, distances, family trees, domains, data sizes, and sentence lengths. We find that the effect of the domain is the most important factor affecting the results for the given data domains and sizes. We compare our results with the only existing MT system for Assamese (Bing Translator) and also with pairs involving Hindi.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {19},
numpages = {32},
keywords = {NMT, Machine translation, Assamese, sequence-to-sequence, low resource, Indo-Aryan, Transformer, SMT, finetuned Transformer}
}

@inproceedings{10.1145/3428690.3429160,
author = {Pudo, Miko\l{}aj and Wi\'{s}niewski, Adrian and Janicki, Artur},
title = {Improved Weighted Loss Function for Training End-of-Speech Detection Models},
year = {2021},
isbn = {9781450389242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428690.3429160},
doi = {10.1145/3428690.3429160},
abstract = {In this paper we propose an improved system for the detection of end of speech (EOS) events in noisy environments, needed, for example, in voice interfaces of mobile devices. Our solution is based on a deep neural network composed of convolutional, feed-forward and LSTM layers. For the input data we use mel-frequency cepstral coefficients (MFCC). The main novelty of our solution is the metric used during the training process of the model: our loss function returns higher values the later the model recognizes the EOS event. We confront this approach with the loss functions previously used, where such a delay was not considered. The experiments run on the TIMIT corpus, as well as additional evaluations on the other types of audio data, showed that our solution is significantly more robust to noisy and far-field environments compared to the baseline solution.},
booktitle = {Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {25–29},
numpages = {5},
keywords = {voice activity detection, machine learning, mobile devices, voice interface, speech processing, deep neural network, end-of-speech detection},
location = {Chiang Mai, Thailand},
series = {MoMM '20}
}

@inproceedings{10.1145/3443279.3443310,
author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},
title = {Character-Level Transformer-Based Neural Machine Translation},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443310},
doi = {10.1145/3443279.3443310},
abstract = {Neural machine translation (NMT) is nowadays commonly applied at the subword level, using byte-pair encoding. A promising alternative approach focuses on character-level translation, which simplifies processing pipelines in NMT considerably. This approach, however, must consider relatively longer sequences, rendering the training process prohibitively expensive. In this paper, we discuss a Transformer-based approach, that we compare, both in speed and in quality to the Transformer at subword and character levels, as well as previously developed character-level models. We evaluate our models on 4 language pairs from WMT'15: DE-EN, CS-EN, FI-EN and RU-EN. The proposed architecture can be trained on a single GPU and is 34% faster than the character-level Transformer; still, the obtained results are at least on par with it. In addition, our proposed model outperforms the subword-level model in FI-EN and shows close results in CS-EN. To stimulate further research in this area and close the gap with subword-level NMT, we make all our code and models publicly available.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {149–156},
numpages = {8},
keywords = {Neural Machine Translation, Natural Language Processing, Character-Level Translation},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@article{10.1109/TASLP.2022.3198546,
author = {Qian, Yanmin and Gong, Xun and Huang, Houjun},
title = {Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3198546},
doi = {10.1109/TASLP.2022.3198546},
abstract = {The variety and complexity of accents pose a huge challenge to robust Automatic Speech Recognition (ASR). Some previous work has attempted to address such problems, however most of the current approaches either require prior knowledge about the target accent, or cannot handle unseen accents and accent-unspecific standard speech. In this work, we aim to improve multi-accent speech recognition in the end-to-end (E2E) framework with a novel layer-wise adaptation architecture. Firstly, we propose a robust deep accent representation learning architecture to obtain accurate accent embedding, and some advanced schemes are designed to further boost the quality of accent embeddings, including phone posteriorgram (PPG) feature, TTS based data augmentation in the training stage, test-time augmentation and multi-embedding fusion in the testing stage. Then, the layer-wise adaptation with accent embeddings is developed for fast accent adaptation in ASR, and two types of adapter layers are designed, including the gated adapter layer and multi-basis adapter layer. Compared to the usual two-pass adaptation, these adapter layers are injected between the ASR encoder layers to encode the accent information in ASR flexibly, and perform fast adaption on the corresponding speech accent. The experiments on Accent AESRC corpus show that the proposed deep accent representation learning can capture accurate accent knowledge, and get high performance on accent classification. The new layer-wise adaptation architecture with the accurate accent embedding outperforms the other traditional methods, and obtains consistent <inline-formula><tex-math notation="LaTeX">$sim$</tex-math></inline-formula>15% relative word error rate (WER) reduction on all kinds of testing scenarios, including seen accents, unseen accents and accent-unspecific standard speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2842–2853},
numpages = {12}
}

@inproceedings{10.1145/3485768.3485806,
author = {Hernandez Urbano Jr., Rommel and Uy Ajero, Jeffrey and Legaspi Angeles, Angelic and Hacar Quintos, Maria Nikki and Regalado Imperial, Joseph Marvin and Llabanes Rodriguez, Ramon},
title = {A BERT-Based Hate Speech Classifier from Transcribed Online Short-Form Videos},
year = {2021},
isbn = {9781450390156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485768.3485806},
doi = {10.1145/3485768.3485806},
abstract = {With the rise of human-centric technologies such as social media platforms, the amount of hate also continues to grow proportionally with the increasing number of users worldwide. TikTok is one of the most-used social media platforms due to its feature that allows users to express themselves via creating and sharing short-form videos based on any desired topic and content. In addition, it has also become a platform for political discourse and mudslinging as users can freely express an opinion and indirectly debate with random people online. In this study, we propose the use of BERT, a complex bidirectional transformer-based model, for the task of automatic hate speech detection from speech transcribed from Tagalog TikTok videos. Results of our experiments show that a BERT-based hate speech classifier scores 61% F1. We also extended the task beyond several algorithms such as LSTM, Na\"{\i}ve Bayes, and Decision Tree and found out that traditional methods such as a simple Bernoulli Na\"{\i}ve Bayes approach remain at par with the BERT model.},
booktitle = {2021 5th International Conference on E-Society, E-Education and E-Technology},
pages = {186–192},
numpages = {7},
keywords = {Filipino Language, Bidirectional Encoder Representations from Transformers (BERT), TikTok, Hate Speech},
location = {Taipei, Taiwan},
series = {ICSET 2021}
}

@article{10.1145/3490488,
author = {Cao, Jialun and Li, Meiziniu and Li, Yeting and Wen, Ming and Cheung, Shing-Chi and Chen, Haiming},
title = {SemMT: A Semantic-Based Testing Approach for Machine Translation Systems},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3490488},
doi = {10.1145/3490488},
abstract = {Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2% and 15.4% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {34e},
numpages = {36},
keywords = {semantic similarity, testing, semantic equivalent, Machine translation, metamorphic testing}
}

@inproceedings{10.1145/3330089.3330113,
author = {Taghbalout, Imane and Allah, Fadoua Ataa},
title = {A Hybrid Approach for Amazigh-English Machine Translation},
year = {2018},
isbn = {9781450361019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330089.3330113},
doi = {10.1145/3330089.3330113},
abstract = {In this paper, we present our hybrid methodology for building a bidirectional Amazigh-English machine translation. The architecture of the proposed system is based on both Interlingua-Based Machine Translation (IBMT) and Statistical-Based Machine Translation (SBMT) approaches. Amazigh is a less-resourced language. It does not have parallel corpora with enough size. So, using statistical approach for such language will not be a good choice, because this approach requires large parallel corpora to well train probabilistic models, and to ensure a translation of good quality. Since we dispose of an Amazigh IBMT based deconverter, we thought, firstly, to use it in building an Amazigh-English parallel corpus. This latter have been exploited to train the necessary models in SBMT.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
articleno = {15},
numpages = {6},
keywords = {translation model, Statistical machine translation, hybrid machine translation, Amazigh language, language model, Corpus generation},
location = {Hammamet, Tunisia},
series = {ICSENT 2018}
}

@article{10.1109/TASLP.2022.3169629,
author = {Hosseini, Maryam and Celotti, Luca and Plourde, \'{E}ric},
title = {End-to-End Brain-Driven Speech Enhancement in Multi-Talker Conditions},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3169629},
doi = {10.1109/TASLP.2022.3169629},
abstract = {Single-channel speech enhancement algorithms have seen great improvements over the past few years. Despite these improvements, they still lack the efficiency of the auditory system in extracting attended auditory information in the presence of competing speakers. Recently, it has been shown that the attended auditory information can be decoded from the brain activity of the listener. In this paper, we propose two novel end-to-end deep learning methods referred to as the Brain Enhanced Speech Denoiser (BESD) and the U-shaped Brain Enhanced Speech Denoiser (U-BESD) respectively, that take advantage of this fact to denoise a multi-talker speech mixture without considering further background noises or reverberations. We use a Feature-wise Linear Modulation (FiLM) between the brain activity and the sound mixture, to better extract the features of the attended speaker to perform speech enhancement. We show, using electroencephalography (EEG) signals recorded from the listener, that both BESD and U-BESD successfully extract the attended speaker without any prior information about this speaker. Moreover, U-BESD also outperforms a current state-of-the-art approach that also uses brain activity to perform enhancement. The proposed neural network-based methods would thus make great candidates for realistic applications where no prior information about the attended speaker is available, such as hearing aids, cellphones, or noise cancelling headphones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1718–1733},
numpages = {16}
}

@inproceedings{10.1145/3380851.3416741,
author = {R. Hardin, Ashley and Ito, Junko and Sasaki, Aiko},
title = {Writing for Human and Machine Translation: Best Practices for Technical Writers},
year = {2020},
isbn = {9781450375252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380851.3416741},
doi = {10.1145/3380851.3416741},
abstract = {The Red Hat Localization Services team translates technical product documentation from English to other languages required for Red Hat customers. Translators from Localization Services collaborate with Red Hat technical writers to clarify statements in the documentation and answer technical questions related to the documentation as they translate content. This industry insight report examines the team's translation workflow and explores how the machine translation technology used by the Localization Services team impacts the translation progress, for better and for worse. We take a closer look at how this intersection of technology and design both limits and enhances the documentation design and translation processes. This report also examines the role of the technical writer in the translation process, how the writer's documentation design decisions impact the machine translation process, and how the writer should be influenced by the translation process as a whole. This report discusses several methods that can be used by technical writers to write clear and concise content for both human and machine translation. Taking a proactive approach to writing for a global audience makes the translations process more efficient and increases content quality.},
booktitle = {Proceedings of the 38th ACM International Conference on Design of Communication},
articleno = {6},
numpages = {3},
keywords = {product documentation, machine translation, localization, technical communication},
location = {Denton, TX, USA},
series = {SIGDOC '20}
}

@inproceedings{10.1145/3477495.3531808,
author = {Lou, Chenwei and Gao, Jun and Yu, Changlong and Wang, Wei and Zhao, Huan and Tu, Weiwei and Xu, Ruifeng},
title = {Translation-Based Implicit Annotation Projection for Zero-Shot Cross-Lingual Event Argument Extraction},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531808},
doi = {10.1145/3477495.3531808},
abstract = {Zero-shot cross-lingual event argument extraction (EAE) is a challenging yet practical problem in Information Extraction. Most previous works heavily rely on external structured linguistic features, which are not easily accessible in real-world scenarios. This paper investigates a translation-based method to implicitly project annotations from the source language to the target language. With the use of translation-based parallel corpora, no additional linguistic features are required during training and inference. As a result, the proposed approach is more cost effective than previous works on zero-shot cross-lingual EAE. Moreover, our implicit annotation projection approach introduces less noises and hence is more effective and robust than explicit ones. Experimental results show that our model achieves the best performance, outperforming a number of competitive baselines. The thorough analysis further demonstrates the effectiveness of our model compared to explicit annotation projection approaches.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2076–2081},
numpages = {6},
keywords = {cross-lingual learning, word alignment, event extraction},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3372278.3390740,
author = {Giro-i-Nieto, Xavier},
title = {One Perceptron to Rule Them All: Language, Vision, Audio and Speech},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390740},
doi = {10.1145/3372278.3390740},
abstract = {Deep neural networks have boosted the convergence of multimedia data analytics in a unified framework shared by practitioners in natural language, vision and speech. Image captioning, lip reading or video sonorization are some of the first applications of a new and exciting field of research exploiting the generalization properties of deep neural representation. This tutorial will firstly review the basic neural architectures to encode and decode vision, text and audio, to later review the those models that have successfully translated information across modalities.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {7–8},
numpages = {2},
keywords = {joint embeddings, deep learning, multimodal, cross-modal},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@article{10.1145/3610773,
author = {Gezmu, Andargachew Mekonnen and N\"{u}rnberger, Andreas},
title = {Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610773},
doi = {10.1145/3610773},
abstract = {Neural approaches, which are currently state-of-the-art in many areas, have contributed significantly to the exciting advancements in machine translation. However, Neural Machine Translation (NMT) requires a substantial quantity and good quality parallel training data to train the best model. A large amount of training data, in turn, increases the underlying vocabulary exponentially. Therefore, several proposed methods have been devised for relatively limited vocabulary due to constraints of computing resources such as system memory. Encoding words as sequences of subword units for so-called open-vocabulary translation is an effective strategy for solving this problem. However, the conventional methods for splitting words into subwords focus on statistics-based approaches that mainly conform to agglutinative languages. In these languages, the morphemes have relatively clean boundaries. These methods still need to be thoroughly investigated for their applicability to fusion languages, which is the main focus of this article. Phonological and orthographic processes alter the borders of constituent morphemes of a word in fusion languages. Therefore, it makes it difficult to distinguish the actual morphemes that carry syntactic or semantic information from the word’s surface form, the form of the word as it appears in the text. We, thus, resorted to a word segmentation method that segments words by restoring the altered morphemes. We also compared conventional and morpheme-based NMT subword models. We could prove that morpheme-based models outperform conventional subword models on a benchmark dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {231},
numpages = {19},
keywords = {morpheme-based word segmentation, Neural machine translation, low-resource languages, fusion languages, transformers}
}

@inproceedings{10.1145/3527188.3561921,
author = {Obremski, David and Hering, Helena Babette and Friedrich, Paula and Lugrin, Birgit},
title = {Mixed-Cultural Speech for Intelligent Virtual Agents - the Impact of Different Non-Native Accents Using Natural or Synthetic Speech in the English Language},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3561921},
doi = {10.1145/3527188.3561921},
abstract = {This paper presents an exploratory study investigating the impact of non-native accented speech on the perception of Intelligent Virtual Agents (IVAs). In an online study, native English speakers watched a video of an IVA holding a monologue whilst speaking English with either a Spanish, Hindi or Mandarin accent that was either recorded by native speakers of that respective language (natural speech) or synthetically generated (synthetic speech). The results showed a significant impact of naturalness of speech on the IVAs perceived warmth and a significant interaction of accent and naturalness of speech on its perceived competence. The naturalness of speech impacted the participants’ perception of the IVA as a non-native speaker of English, and the correctness of the attributed mother tongue in the Spanish and the Mandarin accent condition. These results are a valuable contribution to research on mixed-cultural IVAs in general and non-native speech as a cultural cue more specifically.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {67–75},
numpages = {9},
keywords = {mixed-cultural, intelligent virtual agents, non-native accent},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@inproceedings{10.1145/3283812.3283823,
author = {White, Robert and Krinke, Jens},
title = {TestNMT: Function-to-Test Neural Machine Translation},
year = {2018},
isbn = {9781450360555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283812.3283823},
doi = {10.1145/3283812.3283823},
abstract = {Test generation can have a large impact on the software engineering process by decreasing the amount of time and effort required to maintain a high level of test coverage. This increases the quality of the resultant software while decreasing the associated effort. In this paper, we present TestNMT, an experimental approach to test generation using neural machine translation. TestNMT aims to learn to translate from functions to tests, allowing a developer to generate an approximate test for a given function, which can then be adapted to produce the final desired test. We also present a preliminary quantitative and qualitative evaluation of TestNMT in both cross-project and within-project scenarios. This evaluation shows that TestNMT is potentially useful in the within-project scenario, where it achieves a maximum BLEU score of 21.2, a maximum ROUGE-L score of 38.67, and is shown to be capable of generating approximate tests that are easy to adapt to working tests.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering},
pages = {30–33},
numpages = {4},
keywords = {Neural machine translation, software testing},
location = {Lake Buena Vista, FL, USA},
series = {NL4SE 2018}
}

@inproceedings{10.1145/3449726.3459441,
author = {Feng, Ben and Liu, Dayiheng and Sun, Yanan},
title = {Evolving Transformer Architecture for Neural Machine Translation},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459441},
doi = {10.1145/3449726.3459441},
abstract = {The transformer models have achieved great success on neural machine translation tasks in recent years. However, the hyper-parameters of the transformer are often manually designed by expertise, where the layer is often regularly stacked together without exploring potentially promising ordering patterns. In this paper, we propose a transformer architecture design algorithm based on genetic algorithm, which can automatically find the proper layer ordering pattern and hyper-parameters for the tasks at hand. The experimental results show that the models designed by the proposed algorithm outperform the vanilla transformer on the widely used machine translation benchmark, which reveals that the performance of transformer architecture can be improved by adjusting layer ordering pattern and hyper-parameters by the proposed algorithm.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {273–274},
numpages = {2},
keywords = {transformer, machine translation, genetic algorithm},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1109/TASLP.2021.3057831,
author = {Zhang, Jiacheng and Luan, Huanbo and Sun, Maosong and Zhai, Feifei and Xu, Jingfang and Liu, Yang},
title = {Neural Machine Translation With Explicit Phrase Alignment},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3057831},
doi = {10.1109/TASLP.2021.3057831},
abstract = {While neural machine translation has achieved state-of-the-art translation performance, it is unable to capture the alignment between the input and output during the translation process. The lack of alignment in neural machine translation models leads to three problems: it is hard to (1) interpret the translation process, (2) impose lexical constraints, and (3) impose structural constraints. These problems not only increase the difficulty of designing new architectures for neural machine translation, but also limit its applications in practice. To alleviate these problems, we propose to introduce explicit phrase alignment into the translation process of arbitrary neural machine translation models. The key idea is to build a search space similar to that of phrase-based statistical machine translation for neural machine translation where phrase alignment is readily available. We design a new decoding algorithm that can easily impose lexical and structural constraints. Experiments show that our approach makes the translation process of neural machine translation more interpretable without sacrificing translation quality. In addition, our approach achieves significant improvements in lexically and structurally constrained translation tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1001–1010},
numpages = {10}
}

@inproceedings{10.1145/3501247.3539017,
author = {Stan, George Vlad and Baart, Andr\'{e} and Dittoh, Francis and Akkermans, Hans and Bon, Anna},
title = {A Lightweight Downscaled Approach to Automatic Speech Recognition for Small Indigenous Languages},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539017},
doi = {10.1145/3501247.3539017},
abstract = {Development of fully featured Automatic Speech Recognition (ASR) systems for a complete language vocabulary generally requires large data repositories, massive computing power, and a stable digital network infrastructure. These conditions are not met in the case of many indigenous languages. Based on our research for over a decade in West Africa, we present a lightweight and downscaled approach to AI-based ASR and describe a set of associated experiments. The aim is to produce a variety of limited-vocabulary ASRs as a basis for the development of practically useful (mobile and radio) voice-based information services that fit needs, preferences and knowledge of local rural communities.},
booktitle = {Proceedings of the 14th ACM Web Science Conference 2022},
pages = {451–458},
numpages = {8},
keywords = {voice-based technologies, under-resourced/indigenous languages, low resource environments, automatic speech recognition, neural networks, machine learning},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@article{10.1145/3448252,
author = {Arora, Karunesh Kumar and Agrawal, Shyam Sunder},
title = {Source-Side Reordering to Improve Machine Translation between Languages with Distinct Word Orders},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3448252},
doi = {10.1145/3448252},
abstract = {English and Hindi have significantly different word orders. English follows the subject-verb-object (SVO) order, while Hindi primarily follows the subject-object-verb (SOV) order. This difference poses challenges to modeling this pair of languages for translation. In phrase-based translation systems, word reordering is governed by the language model, the phrase table, and reordering models. Reordering in such systems is generally achieved during decoding by transposing words within a defined window. These systems can handle local reorderings, and while some phrase-level reorderings are carried out during the formation of phrases, they are weak in learning long-distance reorderings. To overcome this weakness, researchers have used reordering as a step in pre-processing to render the reordered source sentence closer to the target language in terms of word order. Such approaches focus on using parts-of-speech (POS) tag sequences and reordering the syntax tree by using grammatical rules, or through head finalization. This study shows that mere head finalization is not sufficient for the reordering of sentences in the English-Hindi language pair. It describes various grammatical constructs and presents a comparative evaluation of reorderings with the original and the head-finalized representations. The impact of the reordering on the quality of translation is measured through the BLEU score in phrase-based statistical systems and neural machine translation systems. A significant gain in BLEU score was noted for reorderings in different grammatical constructs.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {69},
numpages = {18},
keywords = {SOV, Machine translation, SVO, word reordering}
}

@inproceedings{10.1145/3572549.3572581,
author = {Souza, Adriana and Freitas, Diamantino},
title = {A Dynamic Model for Pauses in the Synthesized Speech of Mathematical Expressions in MathML},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572581},
doi = {10.1145/3572549.3572581},
abstract = {Voice synthesizers still present several challenges in the speech of mathematical content, as spoken mathematics has quite peculiar rules. In the synthesized speech, pauses help blind and visually impaired students identify the limits of mathematical operators and subexpressions. However, most studies on pauses define them uniformly or use simple punctuation marks to force the synthesizer to introduce pauses in certain parts of the expression. Speech is a dynamic process and pauses in expressions also need to be dynamic to make the synthetic speech of expressions more natural, as this can help in memorizing this type of content. This work proposed a dynamic model of pauses for mathematical expressions. Collected math expressions spoken by teachers were used to create the model. These expressions were useful for identifying patterns and creating a linear regression model. Blind and visually impaired students evaluated the model. Some improvements were observed when we compared the synthesized mathematical expressions with the model and Audiomath, the parameter tool used in this study.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {194–200},
numpages = {7},
keywords = {Mathematics, Accessibility, Visual Impairment, Synthesized Speech},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@inproceedings{10.1145/3568562.3568646,
author = {Pham Van, Hanh and Le Thanh, Huong},
title = {Improving Khmer-Vietnamese Machine Translation with Data Augmentation Methods},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568646},
doi = {10.1145/3568562.3568646},
abstract = {Machine translation has achieved significant improvements with the development of neural models. However, such approaches require large-scale parallel data, which are hard to collect for low-resource language pairs. This paper solves this problem by applying a pretrained multilingual model and fine-tuning it with a low-resource bilingual dataset. In addition, we propose two data-augmentation strategies to receive new training data, including: (i) back-translating with the dataset from the source language; (ii) translating sentences from the source language to the target one through a pivot language. The proposed approach is applied to the Khmer-Vietnamese machine translation. Experimental results show that our proposed approach gains 4.426% BLEU score higher than the Google translator model using a test set of 2000 Khmer-Vietnamese sentence pairs.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {276–282},
numpages = {7},
keywords = {mBART, Data Augmentation, Neural Machine Translation, Khmer-Vietnamese Translation},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3442381.3450137,
author = {Saha, Punyajoy and Mathew, Binny and Garimella, Kiran and Mukherjee, Animesh},
title = {“Short is the Road That Leads from Fear to Hate”: Fear Speech in Indian WhatsApp Groups},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450137},
doi = {10.1145/3442381.3450137},
abstract = {WhatsApp is the most popular messaging app in the world. Due to its popularity, WhatsApp has become a powerful and cheap tool for political campaigning being widely used during the 2019 Indian general election, where it was used to connect to the voters on a large scale. Along with the campaigning, there have been reports that WhatsApp has also become a breeding ground for harmful speech against various protected groups and religious minorities. Many such messages attempt to instil fear among the population about a specific (minority) community. According to research on inter-group conflict, such ‘fear speech’ messages could have a lasting impact and might lead to real offline violence. In this paper, we perform the first large scale study on fear speech across thousands of public WhatsApp groups discussing politics in India. We curate a new dataset and try to characterize fear speech from this dataset. We observe that users writing fear speech messages use various events and symbols to create the illusion of fear among the reader about a target community. We build models to classify fear speech and observe that current state-of-the-art NLP models do not perform well at this task. Fear speech messages tend to spread faster and could potentially go undetected by classifiers built to detect traditional toxic speech due to their low toxic nature. Finally, using a novel methodology to target users with Facebook ads, we conduct a survey among the users of these WhatsApp groups to understand the types of users who consume and share fear speech. We believe that this work opens up new research questions that are very different from tackling hate speech which the research community has been traditionally involved in. We have made our code and dataset public for other researchers.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1110–1121},
numpages = {12},
keywords = {Islamophobia, survey, fear speech, hate speech, WhatsApp, classification},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3265751,
author = {Mrinalini, K. and Nagarajan, T. and Vijayalakshmi, P.},
title = {Pause-Based Phrase Extraction and Effective OOV Handling for Low-Resource Machine Translation Systems},
year = {2018},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3265751},
doi = {10.1145/3265751},
abstract = {Machine translation is the core problem for several natural language processing research across the globe. However, building a translation system involving low-resource languages remains a challenge with respect to statistical machine translation (SMT). This work proposes and studies the effect of a phrase-induced hybrid machine translation system for translation from English to Tamil, under a low-resource setting. Unlike conventional hybrid MT systems, the free-word ordering feature of the target language Tamil is exploited to form a re-ordered target language model and to extend the parallel text corpus for training the SMT. In the current work, a novel rule-based phrase-extraction method, implemented using parts-of-speech (POS) and place-of-pause in both languages is proposed, which is used to pre-process the training corpus for developing the back-off phrase-induced SMT. Further, out-of-vocabulary (OOV) words are handled using speech-based transliteration and two-level thesaurus intersection techniques based on the POS tag of the OOV word. To ensure that the input with OOV words does not skip phrase-level translation in the hierarchical model, a phrase-level example-based machine translation approach is adopted to find the closest matching phrase and perform translation followed by OOV replacement. The proposed system results in a bilingual evaluation understudy score of 84.78 and a translation edit rate of 19.12. The performance of the system is compared in terms of adequacy and fluency, with existing translation systems for this specific language pair, and it is observed that the proposed system outperforms its counterparts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {12},
numpages = {22},
keywords = {place-of-pause based phrase extraction, Low-resource machine translation, thesaurus intersection, PL-EBMT, POS}
}

@inproceedings{10.1145/3484824.3484907,
author = {Das, Nabanita and Padhy, Neelamadhab},
title = {A Bibliometric and Co-Occurrence Analysis of Speech Processing Literature Published from the Year 2015 to Mid of June 2021},
year = {2022},
isbn = {9781450387637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484824.3484907},
doi = {10.1145/3484824.3484907},
abstract = {Speech processing technology will continue to be a fast-increasing area of signal processing for many years to come, based on present trends. Research in this field is useful in a variety of fields, including voice identification, interactive voice systems, emotion recognition, call center automation, virtual assistants, and robotics. Artificial intelligence, machine learning, and signal processing have all made significant strides recently, making this viable. The goal was for authors/co-authors of documents to evaluate speech processing in Scopus in terms of research trends and leading countries. The search syntax was based on the title of the article. Our article summarizes the research that has been done on this topic over the last five years. This study looked at 5,082 "speech processing" records from January 1, 2015, to June 14, 2021, which were collected from the Scopus database using the keywords "Speech Processing" and "keyword in the title (item)" and then evaluated with VOSviewer software. Several countries are ranked substantially higher in Scopus in terms of journal coverage, according to this analysis. The United States, China, Japan, India, and the United Kingdom are the major contributors. The interpretation of the results is determined by the search phrase used and the database or information system under investigation. According to this survey, in the area of speech processing, 73 percent overall research papers published in the last five years are conference papers. The studied co-occurrence relationship between keyword in the title clearly demonstrates that this topic of research is steadily being explored. This analysis may inspire and motivates the research community who are working in this discipline.},
booktitle = {Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence},
pages = {237–243},
numpages = {7},
keywords = {Voice Recognition, Bibliometric analysis, Speech Processing, Publications},
location = {Windhoek, Namibia},
series = {DSMLAI '21'}
}

@inproceedings{10.1145/3536221.3556608,
author = {Obremski, David and Hering, Helena Babette and Friedrich, Paula and Lugrin, Birgit},
title = {Exploratory Study on the Perception of Intelligent Virtual Agents With Non-Native Accents Using Synthetic and Natural Speech in German},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3556608},
doi = {10.1145/3536221.3556608},
abstract = {This paper presents an exploratory study which investigates the impact of different non-native accents and the naturalness of speech on the correct assignment of an Intelligent Virtual Agent’s (IVA) mother tongue, as well as its perceived warmth, competence and intelligibility. An online-experiment with a between subjects design was conducted, in which the participants, who were native speakers of German, watched a video of an IVA that spoke German with a non-native accent. The IVA’s speech was either synthetically generated or pre-recorded using non-native speakers. The participants experienced an IVA with either a Turkish, Italian or Polish accent, based on the most frequent accents in the German-speaking area. The results revealed that the IVA’s accent impacted its perceived warmth, but not its perceived competence and intelligibility. The IVA’s naturalness of speech played no role in its classification as a non-native speaker of German but on the correctness of the assigned mother tongue within the Polish accent condition. These results give valuable insight in the perception of non-native speaking IVAs and constitute helpful implications for future research with mixed-cultural IVAs.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {15–24},
numpages = {10},
keywords = {synthetic speech, non-native accent, intelligent virtual agents},
location = {Bengaluru, India},
series = {ICMI '22}
}

@article{10.1145/3571073,
author = {Yi̇rmi̇be\c{s}o\u{g}lu, Zeynep and G\"{u}ng\"{o}r, Tunga},
title = {Morphologically Motivated Input Variations and Data Augmentation in Turkish-English Neural Machine Translation},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3571073},
doi = {10.1145/3571073},
abstract = {Success of neural networks in natural language processing has paved the way for neural machine translation (NMT), which rapidly became the mainstream approach in machine translation. Significant improvement in translation performance has been achieved with breakthroughs such as encoder-decoder networks, attention mechanism, and Transformer architecture. However, the necessity of large amounts of parallel data for training an NMT system and rare words in translation corpora are issues yet to be overcome. In this article, we approach NMT of the low-resource Turkish-English language pair. We employ state-of-the-art NMT architectures and data augmentation methods that exploit monolingual corpora. We point out the importance of input representation for the morphologically rich Turkish language and make a comprehensive analysis of linguistically and non-linguistically motivated input segmentation approaches. We prove the effectiveness of morphologically motivated input segmentation for the Turkish language. Moreover, we show the superiority of the Transformer architecture over attentional encoder-decoder models for the Turkish-English language pair. Among the employed data augmentation approaches, we observe back-translation to be the most effective and confirm the benefit of increasing the amount of parallel data on translation quality. This research demonstrates a comprehensive analysis on NMT architectures with different hyperparameters, data augmentation methods, and input representation techniques, and proposes ways of tackling the low-resource setting of Turkish-English NMT.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {92},
numpages = {31},
keywords = {low-resource, attention, Transformer, data augmentation, Neural machine translation, word segmentation, encoder-decoder, morphology}
}

@article{10.1145/3510580,
author = {Su, Chao and Huang, Heyan and Shi, Shumin and Jian, Ping},
title = {Improving Neural Machine Translation by Transferring Knowledge from Syntactic Constituent Alignment Learning},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3510580},
doi = {10.1145/3510580},
abstract = {Statistical machine translation (SMT) models rely on word-, phrase-, and syntax-level alignments. But neural machine translation (NMT) models rarely explicitly learn the phrase- and syntax-level alignments. In this article, we propose to improve NMT by explicitly learning the bilingual syntactic constituent alignments. Specifically, we first utilize syntactic parsers to induce syntactic structures of sentences, and then we propose two ways to utilize the syntactic constituents in a perceptual (not adversarial) generator-discriminator training framework. One way is to use them to measure the alignment score of sentence-level training examples, and the other is to directly score the alignments of constituent-level examples generated with an algorithm based on word-level alignments from SMT. In our generator-discriminator framework, the discriminator is pre-trained to learn constituent alignments and distinguish the ground-truth translation from the fake ones, while the generative translation model is fine-tuned to receive the alignment knowledge and to generate translations that best approximate the true ones. Experiments and analysis show that the learned constituent alignments can help improve the translation results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {91},
numpages = {15},
keywords = {discriminator-generator framework, Neural machine translation, syntactic constituent, constituent alignment}
}

@article{10.1145/3491065,
author = {Mao, Zhuoyuan and Chu, Chenhui and Kurohashi, Sadao},
title = {Linguistically Driven Multi-Task Pre-Training for Low-Resource Neural Machine Translation},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3491065},
doi = {10.1145/3491065},
abstract = {In the present study, we propose novel sequence-to-sequence pre-training objectives for low-resource machine translation (NMT): Japanese-specific sequence to sequence (JASS) for language pairs involving Japanese as the source or target language, and English-specific sequence to sequence (ENSS) for language pairs involving English. JASS focuses on masking and reordering Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on phrase structure masking and reordering tasks. Experiments on ASPEC Japanese–English &amp; Japanese–Chinese, Wikipedia Japanese–Chinese, News English–Korean corpora demonstrate that JASS and ENSS outperform MASS and other existing language-agnostic pre-training methods by up to +2.9 BLEU points for the Japanese–English tasks, up to +7.0 BLEU points for the Japanese–Chinese tasks and up to +1.3 BLEU points for English–Korean tasks. Empirical analysis, which focuses on the relationship between individual parts in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and ENSS. Adequacy evaluation using LASER, human evaluation, and case studies reveals that our proposed methods significantly outperform pre-training methods without injected linguistic knowledge and they have a larger positive impact on the adequacy as compared to the fluency.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {68},
numpages = {29},
keywords = {pre-training, Low-resource neural machine translation, linguistically-driven}
}

@article{10.1145/3321124,
author = {Yin, Yongjing and Su, Jinsong and Wen, Huating and Zeng, Jiali and Liu, Yang and Chen, Yidong},
title = {POS Tag-Enhanced Coarse-to-Fine Attention for Neural Machine Translation},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3321124},
doi = {10.1145/3321124},
abstract = {Although neural machine translation (NMT) has certain capability to implicitly learn semantic information of sentences, we explore and show that Part-of-Speech (POS) tags can be explicitly incorporated into the attention mechanism of NMT effectively to yield further improvements. In this article, we propose an NMT model with tag-enhanced attention mechanism. In our model, NMT and POS tagging are jointly modeled via multi-task learning. Besides following common practice to enrich encoder annotations by introducing predicted source POS tags, we exploit predicted target POS tags to refine attention model in a coarse-to-fine manner. Specifically, we first implement a coarse attention operation solely on source annotations and target hidden state, where the produced context vector is applied to update target hidden state used for target POS tagging. Then, we perform a fine attention operation that extends the coarse one by further exploiting the predicted target POS tags. Finally, we facilitate word prediction by simultaneously utilizing the context vector from fine attention and the predicted target POS tags. Experimental results and further analyses on Chinese-English and Japanese-English translation tasks demonstrate the superiority of our proposed model over the conventional NMT models. We release our code at https://github.com/middlekisser/PEA-NMT.git.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {46},
numpages = {14},
keywords = {Neural machine translation, attention model, POS tags}
}

@article{10.1109/TASLP.2021.3097939,
author = {Li, Xintong and Liu, Lemao and Tu, Zhaopeng and Li, Guanlin and Shi, Shuming and Meng, Max Q.-H.},
title = {Attending From Foresight: A Novel Attention Mechanism for Neural Machine Translation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3097939},
doi = {10.1109/TASLP.2021.3097939},
abstract = {Machines translation&nbsp;(MT) is an essential task in natural language processing or even in artificial intelligence. Statistical machine translation has been the dominant approach to MT for decades, but recently neural machine translation achieves increasing interest because of its appealing model architecture and impressive translation performance. In neural machine translation, an attention model is used to identify the aligned source words for the next target word, i.e., target foresight word, to select translation context. However, it does not make use of any information about this target foresight word at all. Previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrating substantial alignment tasks. However, this approach cannot be applied in machine translation tasks where the target foresight word is unavailable. This paper proposes several novel enhanced attention models by introducing hidden information&nbsp;(such as part-of-speech) of the target foresight word for the translation task. We incorporate the novel enhanced attention employing hidden information about the target foresight word into both recurrent and self-attention-based neural translation models and theoretically justify that such hidden information can make translation prediction easier. Empirical experiments on four datasets further verify that the proposed attention models deliver significant improvements in translation quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2606–2616},
numpages = {11}
}

@inproceedings{10.1145/3486949.3486966,
author = {Akinobu, Yuka and Obara, Momoka and Kajiura, Teruno and Takano, Shiho and Tamura, Miyu and Tomioka, Mayu and Kuramitsu, Kimio},
title = {Is Neural Machine Translation Approach Accurate Enough for Coding Assistance?},
year = {2021},
isbn = {9781450391252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486949.3486966},
doi = {10.1145/3486949.3486966},
abstract = {Coding assistance with deep learning is an emerging concern that has recently attracted much attention in the software development community. To integrate coding assistance with deep learning compactly, we focus on neural machine translation (NMT), which allows users to translate natural language descriptions into expressions in a programming language such as Python. A rising problem here is the limited availability of parallel corpora, which is essential to train better NMT models. To overcome the problem, we propose a transcompiler-based back-translation, a data augmentation method that generates parallel corpora from numerous source code repositories. In this paper, we present our initial experimental results by comparing several NMT models that are built upon the existing corpora and our corpora. The resulting BLEU indicates that our proposed model is accurate enough to allow coding assistance in the future.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Beyond Code: No Code},
pages = {23–28},
numpages = {6},
keywords = {neural machine translation, code generation, back-translation},
location = {Chicago, IL, USA},
series = {BCNC 2021}
}

@inproceedings{10.1145/3289100.3289116,
author = {Ilham, Addarrazi and Hassan, Satori and Khalid, Satori},
title = {Building A First Amazigh Database For Automatic Audiovisual Speech Recognition System},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289116},
doi = {10.1145/3289100.3289116},
abstract = {An audiovisual speech recognition (AVSR) system is a communication method combines the acoustic and the visual modalities. In order to develop efficient AVSR systems, concurrently recorded audio and video data is necessary. We describe in this paper the recording and evaluation of an audiovisual database which is as far as we know the first audiovisual corpus use Amazigh language. This database is called AmDigit_AVSR (Amazigh Digit _ Audiovisual speech recognition system) which is contains 4000 video and audio files of 40 speakers utter the 10 first Amazigh digits.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {94–99},
numpages = {6},
keywords = {Audiovisual corpus, speech recognition, Amazigh language, Lip-reading},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@article{10.1145/3389790,
author = {Marie, Benjamin and Fujita, Atsushi},
title = {Iterative Training of Unsupervised Neural and Statistical Machine Translation Systems},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389790},
doi = {10.1145/3389790},
abstract = {Recent work achieved remarkable results in training neural machine translation (NMT) systems in a fully unsupervised way, with new and dedicated architectures that only rely on monolingual corpora. However, previous work also showed that unsupervised statistical machine translation (USMT) performs better than unsupervised NMT (UNMT), especially for distant language pairs. To take advantage of the superiority of USMT over UNMT, and considering that SMT suffers from well-known limitations overcome by NMT, we propose to define UNMT as NMT trained with the supervision of synthetic parallel data generated by USMT. This way we can exploit USMT up to its limits while ultimately relying on full-fledged NMT models to generate translations. We show significant improvements in translation quality over previous work and also that further improvements can be obtained by alternatively and iteratively training USMT and UNMT. Without the need of a dedicated architecture for UNMT, our simple approach can straightforwardly benefit from any recent and future advances in supervised NMT. Our systems achieve a new state-of-the-art for unsupervised machine translation in all of our six translation tasks for five diverse language pairs, surpassing even supervised SMT or NMT in some tasks. Furthermore, our analysis shows how crucial the comparability between the monolingual corpora used for unsupervised training is in improving translation quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {68},
numpages = {21},
keywords = {knowledge acquisition, semantic similarity, Machine translation, phrase table induction, low-resource language pair}
}

@inproceedings{10.1145/3416506.3423576,
author = {Phan, Hung and Jannesari, Ali},
title = {Statistical Machine Translation Outperforms Neural Machine Translation in Software Engineering: Why and How},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423576},
doi = {10.1145/3416506.3423576},
abstract = {Neural Machine Translation (NMT) is the current trend approach in Natural Language Processing (NLP) to solve the problem of auto- matically inferring the content of target language given the source language. The ability of NMT is to learn deep knowledge inside lan- guages by deep learning approaches. However, prior works show that NMT has its own drawbacks in NLP and in some research problems of Software Engineering (SE). In this work, we provide a hypothesis that SE corpus has inherent characteristics that NMT will confront challenges compared to the state-of-the-art translation engine based on Statistical Machine Translation. We introduce a problem which is significant in SE and has characteristics that challenges the abil- ity of NMT to learn correct sequences, called Prefix Mapping. We implement and optimize the original SMT and NMT to mitigate those challenges. By the evaluation, we show that SMT outperforms NMT for this research problem, which provides potential directions to optimize the current NMT engines for specific classes of parallel corpus. By achieving the accuracy from 65% to 90% for code tokens generation of 1000 Github code corpus, we show the potential of using MT for code completion at token level.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {3–12},
numpages = {10},
keywords = {Neural Machine Translation, Statistical Machine Translation},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1145/3343031.3351066,
author = {K R, Prajwal and Mukhopadhyay, Rudrabha and Philip, Jerin and Jha, Abhishek and Namboodiri, Vinay and Jawahar, C V},
title = {Towards Automatic Face-to-Face Translation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351066},
doi = {10.1145/3343031.3351066},
abstract = {In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Face-to-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1428–1436},
numpages = {9},
keywords = {neural machine translation, voice transfer, translation systems, lip synthesis, cross-language talking face generation, speech to speech translation},
location = {Nice, France},
series = {MM '19}
}

@article{10.1109/TASLP.2020.2999724,
author = {Li, Huayang and Huang, Guoping and Cai, Deng and Liu, Lemao},
title = {Neural Machine Translation With Noisy Lexical Constraints},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2999724},
doi = {10.1109/TASLP.2020.2999724},
abstract = {In neural machine translation, lexically constrained decoding generates translation outputs strictly including the constraints predefined by users, and it is beneficial to improve translation quality at the cost of more decoding overheads if the constraints are perfect. Unfortunately, those constraints may contain mistakes in real-world situations and incorrect constraints will undermine lexically constrained decoding. In this article, we propose a novel framework that is capable of improving the translation quality even if the constraints are noisy. The key to our framework is to treat the lexical constraints as external memories. More concretely, it encodes the constraints by a memory encoder and then leverages the memories by a memory integrator. Experiments demonstrate that our framework can not only deliver substantial BLEU gains in handling noisy constraints, but also achieve speedup in decoding. These results motivate us to apply our models to a new scenario where the constraints are generated without the help of users. Experiments show that our models can indeed improve the translation quality with the automatically generated constraints.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1864–1874},
numpages = {11}
}

@inproceedings{10.1145/3568199.3568218,
author = {Jiang, Shuang},
title = {Multi Strategy Machineenglish Translation System Based on Machine Learning Algorithm},
year = {2023},
isbn = {9781450397551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568199.3568218},
doi = {10.1145/3568199.3568218},
abstract = {The advent of the big data era has brought unprecedented data to machine translation, which is particularly important for data-driven models and algorithms. The degree of public quotation of source text is becoming more and more difficult to understand, but the overall translation performance is improving. On this basis, the multi Strategy machineenglish translation (MSMT) system based on MLA is designed in this paper. Firstly, the MSMT system and machineenglish translation process are analyzed, and the Interactive MSMT system is discussed. Then the neural network translation model of machine learning is proposed. In order to improve the performance of the MSMT system based on MLA proposed in this paper, The results show that in the case of full-text retrieval, the time of English translation is greatly increased, and these time are basically spent in finding and matching these sentences. The sentences that cannot be translated due to the failure of feature index can be translated by the Multi Strategy machine English translation system based on MLA proposed in this paper, which verifies the rationality of the translation system proposed in this paper.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Machine Intelligence},
pages = {119–123},
numpages = {5},
keywords = {English Translation, Machine Learning Algorithm, Translation System Design, Multi Strategy Machine},
location = {Hangzhou, China},
series = {MLMI '22}
}

@inproceedings{10.1145/3394171.3413715,
author = {Lin, Huan and Meng, Fandong and Su, Jinsong and Yin, Yongjing and Yang, Zhengyuan and Ge, Yubin and Zhou, Jie and Luo, Jiebo},
title = {Dynamic Context-Guided Capsule Network for Multimodal Machine Translation},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413715},
doi = {10.1145/3394171.3413715},
abstract = {Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on https://github.com/DeepLearnXMU/MM-DCCN.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1320–1329},
numpages = {10},
keywords = {multimodal machine translation, capsule network, transformer},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1109/TASLP.2019.2946480,
author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Luo, Jiebo},
title = {Future-Aware Knowledge Distillation for Neural Machine Translation},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946480},
doi = {10.1109/TASLP.2019.2946480},
abstract = {Although future context is widely regarded useful for word prediction in machine translation, it is quite difficult in practice to incorporate it into neural machine translation. In this paper, we propose a future-aware knowledge distillation framework (FKD) to address this issue. In the FKD framework, we learn to distill future knowledge from a backward neural language model (teacher) to future-aware vectors (student) during the training phase. The future-aware vector for each word position is computed in a bridge network and optimized towards the corresponding hidden state in the backward neural language model via a knowledge distillation mechanism. We further propose an algorithm to jointly train the neural machine translation model, neural language model and knowledge distillation module end-to-end. The learned future-aware vectors are incorporated into the attention layer of the decoder to provide full-range context information during the decoding phase. Experiments on the NIST Chinese-English and WMT English-German translation tasks show that the proposed method significantly improves translation quality and word alignment.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2278–2287},
numpages = {10}
}

@inproceedings{10.1145/3372885.3373827,
author = {Wang, Qingxiang and Brown, Chad and Kaliszyk, Cezary and Urban, Josef},
title = {Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar},
year = {2020},
isbn = {9781450370974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372885.3373827},
doi = {10.1145/3372885.3373827},
abstract = {In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. In our context informal mathematics refers to human-written mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. To train these models we also prepared four informal-to-formal datasets. We compare and analyze our results according to whether the model is supervised or unsupervised. In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation.},
booktitle = {Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {85–98},
numpages = {14},
keywords = {Proof Assistants, Automating Formalization, Machine Learning, Mizar, Neural Machine Translation},
location = {New Orleans, LA, USA},
series = {CPP 2020}
}

@inproceedings{10.1145/3386527.3405916,
author = {Piech, Chris and Abu-El-Haija, Sami},
title = {Human Languages in Source Code: Auto-Translation for Localized Instruction},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3405916},
doi = {10.1145/3386527.3405916},
abstract = {Computer science education has promised open access around the world, but access is largely determined by what human language you speak. As younger students learn computer science it is less appropriate to assume that they should learn English beforehand. To that end, we present CodeInternational, the first tool to translate code between human languages. To develop a theory of non-English code, and inform our translation decisions, we conduct a study of public code repositories on GitHub. The study is to the best of our knowledge the first on human-language in code and covers 2.9 million Java repositories. To demonstrate CodeInternational's educational utility, we build an interactive version of the popular English-language Karel reader and translate it into 100 spoken languages. Our translations have already been used in classrooms around the world, and represent a first step in an important open CS-education problem.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {167–174},
numpages = {8},
keywords = {source-code, human-language, translation, github},
location = {Virtual Event, USA},
series = {L@S '20}
}

@article{10.1145/3591207,
author = {Sethi, Nandini and Dev, Amita and Bansal, Poonam},
title = {A Novel Neural Machine Translation Approach for Low-Resource Sanskrit-Hindi Language Pair},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3591207},
doi = {10.1145/3591207},
abstract = {Sanskrit is one of the earliest native languages and is correctly described as "the gods' language" because of its wide use in Indian religious literature from the past. However, it is becoming less popular in modern India. Due in significant part to the need for more materials for translation both in and out of Sanskrit, it is no longer commonly utilized. This study explores the feasibility of using machine translation (MT) to provide a link between Sanskrit and, one of the earliest native languages, and its contemporary descendant Hindi. A study was conducted between existing modelling methodologies, notably Statistical machine translation (SMT), and the proposed novel deep learning-based Machine translation strategy using a manually created parallel corpus for the Sanskrit-Hindi language pair. While SMT creates interpretations by mapping phrases from the languages of the source and destination, statistical models, and bilingual text corpora for learning parameters, neural machine translation (NMT) frequently models entire phrases in a single integrated model, using a convolutional neural network to calculate the probability of a word sequence. The proposed NMT model is implemented using an encoder-decoder with an attention mechanism paradigm and the inclusion of gated recurrent units. Our approach involved development of a novel model for Sanskrit-Hindi machine translation using deep learning and the creation of parallel corpora for the Sanskrit-Hindi language pair. The proposed model is evaluated on automated and human-based metrics, and results show that our proposed deep learning-based model outperforms statistical modelling techniques on Moses, surpassing them both with a BLEU score of 53.8% compared to 34.56%. This article examines the undiscovered area of machine translation from Sanskrit to Hindi and discusses the main benefits and drawbacks of statistical and neural machine translation while providing a fresh viewpoint on the subject.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
keywords = {Low-resource, Deep Learning, Sanskrit, Statistical-based, Language processing, Neural Machine Translation}
}

@inproceedings{10.1145/3508230.3508238,
author = {Prombut, Naris and Waijanya, Sajjaporn and Promrit, Nuttachot},
title = {Feature Extraction Technique Based on Conv1D and Conv2D Network for Thai Speech Emotion Recognition},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508238},
doi = {10.1145/3508230.3508238},
abstract = {Speech Emotion Recognition is one of the challenges in Natural Language Processing (NLP) area. There are many factors used to identify emotions in speech, such as pitch, intensity, frequency, duration, and speakers' nationality. This paper implements a speech emotion recognition model specifically for Thai language by classifying it into 5 emotions: Angry, Frustrated, Neutral, Sad, and Happy. This research uses a dataset from VISTEC-depa AI Research Institute of Thailand. There are 21,562 sounds (scripts) divided into 70% of training data and 30% of test data. We use the Mel spectrogram and Mel-frequency Cepstral Coefficients (MFCC) technique for feature extraction and 1D Convolutional Neural Network (Conv1D) all together with 2D Convolutional Neural Network (Conv2D), to classify emotions. With respect to the result, MFCC with Conv2D provides the highest accuracy at 80.59%, and is higher than the baseline study, which is of 71.35%.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {54–60},
numpages = {7},
keywords = {Conv1D, Thai Speech Emotion Recognition, MFCC, Conv2D, Mel spectrogram},
location = {Sanya, China},
series = {NLPIR '21}
}

@article{10.1145/3610611,
author = {Song, Haiyue and Dabre, Raj and Chu, Chenhui and Kurohashi, Sadao and Sumita, Eiichiro},
title = {SelfSeg: A Self-Supervised Sub-Word Segmentation Method for Neural Machine Translation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610611},
doi = {10.1145/3610611},
abstract = {Sub-word segmentation is an essential pre-processing step for Neural Machine Translation (NMT). Existing work has shown that neural sub-word segmenters are better than Byte-Pair Encoding (BPE), however, they are inefficient, as they require parallel corpora, days to train, and hours to decode. This article introduces SelfSeg, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora. SelfSeg takes as input a word in the form of a partially masked character sequence, optimizes the word generation probability, and generates the segmentation with the maximum posterior probability, which is calculated using a dynamic programming algorithm. The training time of SelfSeg depends on word frequencies, and we explore several word frequency normalization strategies to accelerate the training phase. Additionally, we propose a regularization mechanism that allows the segmenter to generate various segmentations for one word. To show the effectiveness of our approach, we conduct MT experiments in low-, middle-, and high-resource scenarios, where we compare the performance of using different segmentation methods. The experimental results demonstrate that, on the low-resource ALT dataset, our method achieves more than 1.2 BLEU score improvement compared with BPE and SentencePiece, and a 1.1 score improvement over Dynamic Programming Encoding (DPE) and Vocabulary Learning via Optimal Transport (VOLT), on average. The regularization method achieves approximately a 4.3 BLEU score improvement over BPE and a 1.2 BLEU score improvement over BPE-dropout, the regularized version of BPE. We also observed significant improvements on IWSLT15 Vi→En, WMT16 Ro→En, and WMT15 Fi→En datasets and competitive results on the WMT14 De→En and WMT14 Fr→En datasets. Furthermore, our method is 17.8\texttimes{} faster during training and up to 36.8\texttimes{} faster during decoding in a high-resource scenario compared to DPE. We provide extensive analysis, including why monolingual word-level data is enough to train SelfSeg.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {215},
numpages = {24},
keywords = {Subword segmentation, self-supervised learning, subword regularization, machine translation, efficient NLP}
}

@article{10.1145/3342482,
author = {Ji, Yatu and Hou, Hongxu and Chen, Junjie and Wu, Nier},
title = {Adversarial Training for Unknown Word Problems in Neural Machine Translation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342482},
doi = {10.1145/3342482},
abstract = {Nearly all of the work in neural machine translation (NMT) is limited to a quite restricted vocabulary, crudely treating all other words the same as an &lt; unk&gt; symbol. For the translation of language with abundant morphology, unknown (UNK) words also come from the misunderstanding of the translation model to the morphological changes. In this study, we explore two ways to alleviate the UNK problem in NMT: a new generative adversarial network (added value constraints and semantic enhancement) and a preprocessing technique that mixes morphological noise. The training process is like a win-win game in which the players are three adversarial sub models (generator, filter, and discriminator). In this game, the filter is to emphasize the discriminator’s attention to the negative generations that contain noise and improve the training efficiency. Finally, the discriminator cannot easily discriminate the negative samples generated by the generator with filter and human translations. The experimental results show that the proposed method significantly improves over several strong baseline models across various language pairs and the newly emerged Mongolian-Chinese task is state-of-the-art.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {17},
numpages = {12},
keywords = {generative adversarial network, UNK, Neural machine translation, value iteration}
}

@inproceedings{10.1145/3485447.3512261,
author = {Solovev, Kirill and Pr\"{o}llochs, Nicolas},
title = {Hate Speech in the Political Discourse on Social Media: Disparities Across Parties, Gender, and Ethnicity},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512261},
doi = {10.1145/3485447.3512261},
abstract = {Social media has become an indispensable channel for political communication. However, the political discourse is increasingly characterized by hate speech, which affects not only the reputation of individual politicians but also the functioning of society at large. In this work, we empirically analyze how the amount of hate speech in replies to posts from politicians on Twitter depends on personal characteristics, such as their party affiliation, gender, and ethnicity. For this purpose, we employ Twitter’s Historical API to collect every tweet posted by members of the 117th U.&nbsp;S. Congress for an observation period of more than six months. Additionally, we gather replies for each tweet and use machine learning to predict the amount of hate speech they embed. Subsequently, we implement hierarchical regression models to analyze whether politicians with certain characteristics receive more hate speech. We find that tweets are particularly likely to receive hate speech in replies if they are authored by (i) persons of color from the Democratic party, (ii) white Republicans, and (iii) women. Furthermore, our analysis reveals that more negative sentiment (in the source tweet) is associated with more hate speech (in replies). However, the association varies across parties: negative sentiment attracts more hate speech for Democrats (vs. Republicans). Altogether, our empirical findings imply significant differences in how politicians are treated on social media depending on their party affiliation, gender, and ethnicity.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3656–3661},
numpages = {6},
keywords = {hate speech, explanatory modeling, political discourse, Social media, computational social science, sentiment analysis, disparities},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3384544.3384565,
author = {Bai, Tiangang and Hou, Hongxu and Ji, Yatu},
title = {Sub-Word Embedding Auxiliary Encoding in Mongolian-Chinese Neural Machine Translation},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384565},
doi = {10.1145/3384544.3384565},
abstract = {For low-resource Mongolian-Chinese neural machine translation (NMT), the common pre-processing methods such as byte pair encoding (BPE) and tokenization, are unable to recognize Mongolian special character, which leads to the loss of complete sentence information. The translation quality of low-frequency words is undesirable due to the problem of data sparsity. In this paper, we firstly propose a process method for Mongolian special character, which can transform the Mongolian special characters into explicit form to decrease the pre-processing error. Secondly, according to the morphological knowledge of Mongolian, we generate the sub-word embedding with large scale monolingual corpus to enhance the contextual information of the representation of low-frequency words. The experiments show that 1) Mongolian special character processing can minimize the semantic loss, 2) systems with sub-word embedding from large scale monolingual corpus can capture the semantic information of low-frequency words effectively 3) the proposed approaches can improve 1-2 BLEU points above the baselines.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {292–296},
numpages = {5},
keywords = {Neural machine translation, Mongolian-Chinese machine translation, Special character processing, Sub-word embedding},
location = {Langkawi, Malaysia},
series = {ICSCA '20}
}

@inproceedings{10.1145/3540250.3549102,
author = {Chen, Simin and Liu, Cong and Haque, Mirazul and Song, Zihe and Yang, Wei},
title = {NMTSloth: Understanding and Testing Efficiency Degradation of Neural Machine Translation Systems},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549102},
doi = {10.1145/3540250.3549102},
abstract = {Neural Machine Translation (NMT) systems have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of NMT systems, which is of paramount importance due to often vast translation demands and real-time requirements, has surprisingly received little attention. In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art NMT systems. By analyzing the working mechanism and implementation of 1455 public-accessible NMT systems, we observe a fundamental property in NMT systems that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of NMT systems instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that NMT systems would have to go through enough iterations to satisfy the pre-configured threshold. We present NMTSloth, which develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level, which sufficiently delays the appearance of EOS and forces these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of NMTSloth, we conduct a systematic evaluation on three public-available NMT systems: Google T5, AllenAI WMT14, and Helsinki-NLP translators. Experimental results show that NMTSloth can increase NMT systems' response latency and energy consumption by 85% to 3153% and 86% to 3052%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by NMTSloth significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1148–1160},
numpages = {13},
keywords = {software testing, Machine learning, neural machine translation},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3457682.3457711,
author = {Huang, Li and Chen, Wenyu and Qu, Hong},
title = {Accelerating Transformer for Neural Machine Translation},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457711},
doi = {10.1145/3457682.3457711},
abstract = {Neural Machine Translation (NMT) models based on Transformer achieve promising progress in both translation quality and training speed. Such a strong framework adopts parallel structures that greatly improve the decoding speed without losing quality. However, due to the self-attention network in decoder that cannot maintain the parallelization under the auto-regressive scheme, the Transformer did not enjoy the same speed performance as training when inference. In this work, with simplicity and feasibility in mind, we introduce a gated cumulative attention network to replace the self-attention part in Transformer decoder to maintain the parallelization property in the inference phase. The gated cumulative attention network includes two sub-layers, a gated linearly cumulative layer that creates the relationship between already predicted tokens and current representation, and a feature fusion layer that enhances the representation with a feature fusion operation. The proposed method was evaluated on WMT17 datasets with 12 language pair groups. Experimental results show the effectiveness of the proposed method and also demonstrated that the proposed gated cumulative attention network has adequate ability as an alternative to the self-attention part in the Transformer decoder.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {191–197},
numpages = {7},
keywords = {Machine Translation, Natural Language Processing, Neural Networks},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1145/3310132,
author = {Vacher, Michel and Aman, Fr\'{e}d\'{e}ric and Rossato, Solange and Portet, Fran\c{c}ois and Lecouteux, Benjamin},
title = {Making Emergency Calls More Accessible to Older Adults Through a Hands-Free Speech Interface in the House},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3310132},
doi = {10.1145/3310132},
abstract = {Wearable personable emergency response (PER) systems are the mainstream solution for allowing frail and isolated individuals to call for help in an emergency. However, these devices are not well adapted to all users and are often not worn all the time, meaning they are not available when needed. This article presents a Voice User Interface system for emergency-call recognition. The interface is designed to permit hands-free interaction using natural language. Crucially, this allows a call for help to be registered without necessitating physical proximity to the system. The system is based on an ASR engine and is tested on a corpus collected to simulate realistic situations. The corpus contains French speech from 4 older adults and 13 younger people wearing an old-age simulator to hamper their mobility, vision, and hearing. On-line evaluation of the preliminary system showed an emergency-call error rate of 27%. Subsequent off-line experimentation improved the results (call error rate 24%), demonstrating that emergency-call recognition in the home is achievable. Another contribution of this work is the corpus, which is made available for research with the hope that it will facilitate related research and quicker development of robust methods for automatic emergency-call recognition in the home.},
journal = {ACM Trans. Access. Comput.},
month = {jun},
articleno = {8},
numpages = {25},
keywords = {assistive technology, Specific voice recognition, ambient assisted living, emergency call}
}

@article{10.1145/3389791,
author = {Zhou, Long and Zhang, Jiajun and Kang, Xiaomian and Zong, Chengqing},
title = {Deep Neural Network--Based Machine Translation System Combination},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389791},
doi = {10.1145/3389791},
abstract = {Deep neural networks (DNNs) have provably enhanced the state-of-the-art natural language process (NLP) with their capability of feature learning and representation. As one of the more challenging NLP tasks, neural machine translation (NMT) becomes a new approach to machine translation and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy and word coverage. It is therefore a promising direction to combine the advantages of both NMT and SMT. In this article, we propose a deep neural network--based system combination framework leveraging both minimum Bayes-risk decoding and multi-source NMT, which take as input the N-best outputs of NMT and SMT systems and produce the final translation. In particular, we apply the proposed model to both RNN and self-attention networks with different segmentation granularity. We verify our approach empirically through a series of experiments on resource-rich Chinese⇒English and low-resource English⇒Vietnamese translation tasks. Experimental results demonstrate the effectiveness and universality of our proposed approach, which significantly outperforms the conventional system combination methods and the best individual system output.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {65},
numpages = {19},
keywords = {low-resource translation, minimal Bayes-risk decoding, NMT, SMT, DNN, system combination}
}

@article{10.1145/3341726,
author = {Imankulova, Aizhan and Sato, Takayuki and Komachi, Mamoru},
title = {Filtered Pseudo-Parallel Corpus Improves Low-Resource Neural Machine Translation},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3341726},
doi = {10.1145/3341726},
abstract = {Large-scale parallel corpora are essential for training high-quality machine translation systems; however, such corpora are not freely available for many language translation pairs. Previously, training data has been augmented by pseudo-parallel corpora obtained by using machine translation models to translate monolingual corpora into the source language. However, in low-resource language pairs, in which only low-accurate machine translation systems can be used, translation quality degrades when a pseudo-parallel corpus is naively used. To improve machine translation performance with low-resource language pairs, we propose a method to effectively expand the training data via filtering the pseudo-parallel corpus using quality estimation based on sentence-level round-trip translation. For experiments with three language pairs that utilized small, medium, and large size parallel corpora, BLEU scores significantly improved for low-resource language pairs. Additionally, the effects of iterative bootstrapping on translation performance quality is investigated; resultingly, it is confirmed that bootstrapping can further improve the translation performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {oct},
articleno = {24},
numpages = {16},
keywords = {low-resource language pairs, sentence-level similarity metrics, Pseudo-parallel corpus, filtering, round-trip translation, bootstrapping}
}

@article{10.1109/TASLP.2022.3231714,
author = {Qi, Jun and Yang, Chao-Han Huck and Chen, Pin-Yu and Tejedor, Javier},
title = {Exploiting Low-Rank Tensor-Train Deep Neural Networks Based on Riemannian Gradient Descent With Illustrations of Speech Processing},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3231714},
doi = {10.1109/TASLP.2022.3231714},
abstract = {This work focuses on designing low-complexity hybrid tensor networks by considering trade-offs between the model complexity and practical performance. Firstly, we exploit a low-rank tensor-train deep neural network (TT-DNN) to build an end-to-end deep learning pipeline, namely LR-TT-DNN. Secondly, a hybrid model combining LR-TT-DNN with a convolutional neural network (CNN), which is denoted as CNN+(LR-TT-DNN), is set up to boost the performance. Instead of randomly assigning large TT-ranks for TT-DNN, we leverage Riemannian gradient descent to determine a TT-DNN associated with small TT-ranks. Furthermore, CNN+(LR-TT-DNN) consists of convolutional layers at the bottom for feature extraction and several TT layers at the top to solve regression and classification problems. We separately assess the LR-TT-DNN and CNN+(LR-TT-DNN) models on speech enhancement and spoken command recognition tasks. Our empirical evidence demonstrates that the LR-TT-DNN and CNN+(LR-TT-DNN) models with fewer model parameters can outperform the TT-DNN and CNN+(TT-DNN) counterparts.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {633–642},
numpages = {10}
}

@article{10.1145/3226045,
author = {Li, Maoxi and Wang, Mingwen},
title = {Optimizing Automatic Evaluation of Machine Translation with the ListMLE Approach},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3226045},
doi = {10.1145/3226045},
abstract = {Automatic evaluation of machine translation is critical for the evaluation and development of machine translation systems. In this study, we propose a new model for automatic evaluation of machine translation. The proposed model combines standard n-gram precision features and sentence semantic mapping features with neural features, including neural language model probabilities and the embedding distances between translation outputs and their reference translations. We optimize the model with a representative list-wise learning to rank approach, ListMLE, in terms of human ranking assessments. The experimental results on WMT’2015 Metrics task indicated that the proposed approach yields significantly better correlations with human assessments than several state-of-the-art baseline approaches. In particular, the results confirmed that the proposed list-wise learning to rank approach is useful and powerful for optimizing automatic evaluation metrics in terms of human ranking assessments. Deep analysis also demonstrated that optimizing automatic metrics with the ListMLE approach is a reasonable method and adding the neural features can gain considerable improvements compared with the traditional features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {2},
numpages = {18},
keywords = {segment-level consistency, learning to rank, system-level correlation, recurrent neural network language model, word embedding, Automatic evaluation of machine translation}
}

@article{10.1145/3599969,
author = {Nguyen, Long H. B. and Nguyen, Binh and Le, Binh and Dinh, Dien},
title = {Exploring Graph-Based Transformer Encoder for Low-Resource Neural Machine Translation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3599969},
doi = {10.1145/3599969},
abstract = {The Transformer is commonly used in Neural Machine Translation (NMT), but it faces issues with over-parameterization in low-resource settings. This means that simply increasing the model parameters significantly will not lead to improved performance. In this study, we propose a graph-based approach that slightly increases the parameters while significantly outperforming the scaled version of the Transformer. We accomplish this by utilizing Graph Neural Networks to encode Universal Conceptual Cognitive Annotation (UCCA), allowing the linguistic features of UCCA to be incorporated into the word embeddings. This improves the performance of the NMT system since the word embedding is now more capable and informative. Experimental results demonstrate that the proposed method outperforms the scaled Transformer model by +0.4, +0.41, and +0.33 BLEU, respectively, in English-Vietnamese/French/Czech datasets. Furthermore, this method reduces the number of parameters by 47% when compared to the scaled Transformer. A thorough analysis of error patterns reveals that the proposed method provides structural awareness to translation systems. Our code is available at: https://github.com/nqbinh17/UCCA_GNN.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
keywords = {Universal Conceptual Cognitive Annotation, Neural Machine Translation, Graph Neural Networks}
}

@inproceedings{10.1145/3342827.3342833,
author = {Adlaon, Kristine Mae M. and Marcos, Nelson},
title = {Building the Language Resource for a Cebuano-Filipino Neural Machine Translation System},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342833},
doi = {10.1145/3342827.3342833},
abstract = {Parallel corpus is a critical resource in machine learning based translation. The task of collecting, extracting, and aligning texts in order to build an acceptable corpus for doing translation is very tedious most especially for low-resource languages. In this paper, we present the efforts made to build a parallel corpus for Cebuano and Filipino from two different domains: biblical texts and the web. For the biblical resource, subword unit translation for verbs and copy-able approach for nouns were applied to correct inconsistencies in translation. This correction mechanism was applied as a preprocessing technique. On the other hand, for Wikipedia being the main web resource, commonly occurring topic segments were extracted from both the source and the target languages. These observed topic segments are unique in 4 different categories. The identification of these topic segments may be used for automatic extraction of sentences. A Recurrent Neural Network was used to implement the translation using OpenNMT sequence modeling tool in TensorFlow. The two different corpora were then evaluated by using them as two separate inputs in the neural network. Results have shown a difference in BLEU score in both corpora.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {127–132},
numpages = {6},
keywords = {recurrent neural network, Cebuano-Filipino translation, Language resources, OpenNMT, natural language processing, neural machine translation},
location = {Tokushima, Japan},
series = {NLPIR '19}
}

@article{10.1109/TASLP.2018.2860287,
author = {Wang, Xing and Tu, Zhaopeng and Zhang, Min},
title = {Incorporating Statistical Machine Translation Word Knowledge Into Neural Machine Translation},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860287},
doi = {10.1109/TASLP.2018.2860287},
abstract = {Neural machine translation NMT has gained more and more attention in recent years, mainly due to its simplicity yet state-of-the-art performance. However, previous research has shown that NMT suffers from several limitations: source coverage guidance, translation of rare words, and the limited vocabulary, while statistical machine translation SMT has complementary properties that correspond well to these limitations. It is straightforward to improve the translation performance by combining the advantages of two kinds of models. This paper proposes a general framework for incorporating the SMT word knowledge into NMT to alleviate above word-level limitations. In our framework, the NMT decoder makes more accurate word prediction by referring to the SMT word recommendations in both training and testing phases. Specifically, the SMT model offers informative word recommendations based on the NMT decoding information. Then, we use the SMT word predictions as prior knowledge to adjust the NMT word generation probability, which unitizes a neural network based classifier to digest the discrete word knowledge. In this paper, we use two model variants to implement the framework, one with a gating mechanism and the other with a direct competition mechanism. Experimental results on Chinese-to-English and English-to-German translation tasks show that the proposed framework can take advantage of the SMT word knowledge and consistently achieve significant improvements over NMT and SMT baseline systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2255–2266},
numpages = {12}
}

@inproceedings{10.1145/3319921.3319936,
author = {Qin, Ying and Zhang, Jie and Lu, Xiaoping},
title = {The Gap between NMT and Professional Translation from the Perspective of Discourse},
year = {2019},
isbn = {9781450361286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319921.3319936},
doi = {10.1145/3319921.3319936},
abstract = {Neural Machine Translation (NMT) attracts an intensification of research efforts in recent years. The claim that the quality of machine translation is matching human translator requires extraordinary evidences. It is almost certain that machine translation so far cannot replace or exceed human translator, but what is the gap between NMT and professional translator? We truly investigate the quality difference between the well-known Google Neural Machine Translation (GNMT) and professional translators at the document level. We build an evaluation model based on the discourse cohesion theory to compare the usage of grammatical and semantical devices in Chinese-to-English translations including reference, substitution, ellipsis, conjunction and lexical cohesion. The evaluation results show that there is a significant gap between NMT and professional translator in document translations. In a comprehensive point of view, for the professional translator, translation is a kind of arts; while for the computers, it is still a kind of technologies.},
booktitle = {Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence},
pages = {50–54},
numpages = {5},
keywords = {Discourse cohesion, Quality evaluation, Discourse analysis, Machine translation},
location = {Suzhou, China},
series = {ICIAI '19}
}

@inproceedings{10.1145/3474085.3475303,
author = {Song, Yuqing and Chen, Shizhe and Jin, Qin and Luo, Wei and Xie, Jun and Huang, Fei},
title = {Product-Oriented Machine Translation with Cross-Modal Cross-Lingual Pre-Training},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475303},
doi = {10.1145/3474085.3475303},
abstract = {Translating e-commercial product descriptions, a.k.a product-oriented machine translation (PMT), is essential to serve e-shoppers all over the world. However, due to the domain specialty, the PMT task is more challenging than traditional machine translation problems. Firstly, there are many specialized jargons in the product description, which are ambiguous to translate without the product image. Secondly, product descriptions are related to the image in more complicated ways than standard image descriptions, involving various visual aspects such as objects, shapes, colors or even subjective styles. Moreover, existing PMT datasets are small in scale to support the research. In this paper, we first construct a large-scale bilingual product description dataset called Fashion-MMT, which contains over 114k noisy and 40k manually cleaned description translations with multiple product images. To effectively learn semantic alignments among product images and bilingual texts in translation, we design a unified product-oriented cross-modal cross-lingual model for pre-training and fine-tuning. Experiments on the Fashion-MMT and Multi30k datasets show that our model significantly outperforms the state-of-the-art models even pre-trained on the same dataset. It is also shown to benefit more from large-scale noisy data to improve the translation quality. We will release the dataset and codes at https://github.com/syuqings/Fashion-MMT.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2843–2852},
numpages = {10},
keywords = {product-oriented machine translation, multimodal transformer, dataset, pre-training},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3349341.3349505,
author = {Shi, Yan and Shi, Chunrang and Zhou, Zehua},
title = {Common Problems and Optimization Strategies of Machine Translation of Headlines of Online Scientific News},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349505},
doi = {10.1145/3349341.3349505},
abstract = {Machine translation is getting more and more used. By a contrast analysis of 188 pieces of English headlines of online news of science and technology from the website of New York Times and its Chinese counters from the website of Huanqiu, this paper summarizes the problems of machine translation of headlines of online news of science and technology. The problems are fuzzy expression, semantic exaggeration, semantic narrowing, misusing idioms, abusing foreign language and guiding the readers to get vulgar information. The translators should improve the versions by machine translation. This paper presents four optimization strategies of machine translation of headlines of online scientific news: changing complexity to brief, using appropriate rhetoric devices, use opportunely punctuations and using the idioms familiar to readers.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {755–758},
numpages = {4},
keywords = {Headlines of online scientific news, Machine translation, Strategy},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@article{10.1145/3540260,
author = {Priyadarshi, Ankur and Saha, Sujan Kumar},
title = {A Study on the Performance of Recurrent Neural Network Based Models in Maithili Part of Speech Tagging},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3540260},
doi = {10.1145/3540260},
abstract = {This article presents our effort in developing a Maithili Part of Speech (POS) tagger. Substantial effort has been devoted to developing POS taggers in several Indian languages, including Hindi, Bengali, Tamil, Telugu, Kannada, Punjabi, and Marathi; but Maithili did not achieve much attention from the research community. Maithili is one of the official languages of India, with around 50 million native speakers. So, we worked on developing a POS tagger in Maithili. For the development, we use a manually annotated in-house Maithili corpus containing 56,126 tokens. The tagset contains 27 tags. We train a conditional random fields (CRF) classifier to prepare a baseline system that achieves an accuracy of 82.67%. Then, we employ several recurrent neural networks (RNN)-based models, including Long-short Term Memory (LSTM), Gated Recurrent Unit (GRU), LSTM with a CRF layer (LSTM-CRF), and GRU with a CRF layer (GRU-CRF) and perform a comparative study. We also study the effect of both word embedding and character embedding in the task. The highest accuracy of the system is 91.53%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {feb},
articleno = {32},
numpages = {16},
keywords = {Recurrent neural network, neural model for NLP, Maithili language, Part of speech tagging}
}

@inproceedings{10.1145/3446434.3446553,
author = {Muravev, Yury},
title = {Machine Translation and Legal Tech in Legal Translation Training},
year = {2021},
isbn = {9781450388900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446434.3446553},
doi = {10.1145/3446434.3446553},
abstract = {The study aims to analyze the use of legal tech and machine translation software in legal translation training. It presents a comparative research of various IT tools and applications in foreign language learning environments, their capabilities and limitations in higher education institutions, as well as the author's recommendations on integrating them in the ESP-teaching process. This research's primary methods are case study method, analysis of software teaching materials, descriptive statistics, and translation studies methods.The study results may improve the output quality of machine translation systems and legal tech software development for legal translation training. The secondary goal is to find software-based teaching methods that may enhance the learning motivation of Legal English students by realistic scenarios of business simulation games. The novelty aspect is implementing adjustable frames in the tasks involving legal tech use in a classroom setting. The study results show that legal tech software may be successfully applied in the acceleration and facilitation of ESP teaching and blended learning.},
booktitle = {Proceedings of the International Scientific Conference - Digital Transformation on Manufacturing, Infrastructure and Service},
articleno = {36},
numpages = {7},
keywords = {English for specific purposes (ESP), machine translation software, Legal English, legal translation, blended learning, computer-assisted language learning (CALL)},
location = {Saint Petersburg, Russian Federation},
series = {DTMIS '20}
}

@article{10.1145/3358414,
author = {Yu, Hongfei and Zhou, Xiaoqing and Duan, Xiangyu and Zhang, Min},
title = {Layer-Wise De-Training and Re-Training for ConvS2S Machine Translation},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3358414},
doi = {10.1145/3358414},
abstract = {The convolutional sequence-to-sequence (ConvS2S) machine translation system is one of the typical neural machine translation (NMT) systems. Training the ConvS2S model tends to get stuck in a local optimum in our pre-studies. To overcome this inferior behavior, we propose to de-train a trained ConvS2S model in a mild way and retrain to find a better solution globally. In particular, the trained parameters of one layer of the NMT network are abandoned by re-initialization while other layers’ parameters are kept at the same time to kick off re-optimization from a new start point and safeguard the new start point not too far from the previous optimum. This procedure is executed layer by layer until all layers of the ConvS2S model are explored. Experiments show that when compared to various measures for escaping from the local optimum, including initialization with random seeds, adding perturbations to the baseline parameters, and continuing training (con-training) with the baseline models, our method consistently improves the ConvS2S translation quality across various language pairs and achieves better performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {26},
numpages = {15},
keywords = {ConvS2S, neural machine translation, local optimum}
}

@inproceedings{10.1145/3511047.3537688,
author = {Polignano, Marco and Colavito, Giuseppe and Musto, Cataldo and de Gemmis, Marco and Semeraro, Giovanni},
title = {Lexicon Enriched Hybrid Hate Speech Detection with Human-Centered Explanations},
year = {2022},
isbn = {9781450392327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511047.3537688},
doi = {10.1145/3511047.3537688},
abstract = {The phenomenon of hate messages on the web is unfortunately in continuous expansion and evolution. Even if the big companies that offer their users a social network service have expressly included in their terms of services rules against hate messages, they are still produced at a huge rate. Therefore, moderators are often employed to monitor these platforms and use their critical skills to decide if the content is offensive or not. Unfortunately, this censorship process is complex and costly in terms of human resources. The system we propose in this work is a system that supports moderators by providing them a set of candidate elements to censor with annexed explanations in natural language. It will then be a task of the human operator to understand if to proceed with the censorship and eventually supply feedback to the result of the classification algorithm to extend its data set of examples and improve its future performances. The proposed system has been designed to merge information coming from data, syntactic tags and a manually annotated lexicon. The messages are then processed through deep learning approaches based on both transformer and deep neural network architecture. The output is consequently supported by an explanation in a human-like form. The model has been evaluated on three state-of-the-art datasets showing excellent effectiveness and clear and understandable explanations.},
booktitle = {Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {184–191},
numpages = {8},
keywords = {Deep Learning, Hate Lexicon, Transformer Model, Explanation, Hate speech, Transparency},
location = {Barcelona, Spain},
series = {UMAP '22 Adjunct}
}

@inproceedings{10.1145/3309700.3338457,
author = {Ishmam, Alvi Md. and Arman, Jawad and Sharmin, Sadia},
title = {Towards the Development of the Bengali Language Corpus from Public Facebook Pages for Hate Speech Research},
year = {2020},
isbn = {9781450366793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309700.3338457},
doi = {10.1145/3309700.3338457},
abstract = {Online abusive or hateful speech detection in different languages on Social Networking Sites (SNS) have drawn the attention of researchers recently. Hateful comments in public Facebook pages ignite social mishaps in Bangladesh. In this paper, we have discussed the development and annotation of the corpus of hateful speech in the Bengali language on public Facebook pages. We have classified hateful comments into six major classes based on the social aspects of Bangladesh. The corpus (4753 comments) is the maiden contribution as a publicly available data set that can be enhanced and utilized for future hate speech research in SNS.},
booktitle = {Proceedings of Asian CHI Symposium 2019: Emerging HCI Research Collection},
pages = {141–146},
numpages = {6},
keywords = {hate speech, Bengali language, Facebook page, annotated corpus},
location = {Glasgow, Scotland, United Kingdom},
series = {AsianHCI '19}
}

@inproceedings{10.1145/3287324.3293840,
author = {Khayrallah, Huda and Knowles, Rebecca and Duh, Kevin and Post, Matt},
title = {An Interactive Teaching Tool for Introducing Novices to Machine Translation},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293840},
doi = {10.1145/3287324.3293840},
abstract = {The first step in the research process is developing an understanding of the problem at hand. Novices may be interested in learning about machine translation (MT), but often lack experience and intuition about the task of translation (either by human or machine) and its challenges. The goal of this work is to allow students to interactively discover why MT is an open problem, and encourage them to ask questions, propose solutions, and test intuitions. We present a hands-on activity in which students build and evaluate their own MT systems using curated parallel texts. By having students hand-engineer MT system rules in a simple user interface, which they can then run on real data, they gain intuition about why early MT research took this approach, where it fails, and what features of language make MT a challenging problem even today. Developing translation rules typically strikes novices as an obvious approach that should succeed, but the idea quickly struggles in the face of natural language complexity. This interactive, intuition-building exercise can be augmented by a discussion of state-of-the-art MT techniques and challenges, focusing on areas or aspects of linguistic complexity that the students found difficult. We envision this lesson plan being used in the framework of a larger AI or natural language processing course (where only a small amount of time can be dedicated to MT) or as a standalone activity. We describe and release the tool that supports this lesson, as well as accompanying data.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1276},
numpages = {1},
keywords = {machine translation, active learning},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3511808.3557662,
author = {Sidiropoulos, Georgios and Vakulenko, Svitlana and Kanoulas, Evangelos},
title = {On the Impact of Speech Recognition Errors in Passage Retrieval for Spoken Question Answering},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557662},
doi = {10.1145/3511808.3557662},
abstract = {Interacting with a speech interface to query a Question Answering (QA) system is becoming increasingly popular. Typically, QA systems rely on passage retrieval to select candidate contexts and reading comprehension to extract the final answer. While there has been some attention to improving the reading comprehension part of QA systems against errors that automatic speech recognition (ASR) models introduce, the passage retrieval part remains unexplored. However, such errors can affect the performance of passage retrieval, leading to inferior end-to-end performance. To address this gap, we augment two existing large-scale passage ranking and open domain QA datasets with synthetic ASR noise and study the robustness of lexical and dense retrievers against questions with ASR noise. Furthermore, we study the generalizability of data augmentation techniques across different domains; with each domain being a different language dialect or accent. Finally, we create a new dataset with questions voiced by human users and use their transcriptions to show that the retrieval performance can further degrade when dealing with natural ASR noise instead of synthetic ASR noise.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4485–4489},
numpages = {5},
keywords = {asr, dense retrieval, passage retrieval, spoken question answering},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3418059,
author = {Sun, Haipeng and Wang, Rui and Utiyama, Masao and Marie, Benjamin and Chen, Kehai and Sumita, Eiichiro and Zhao, Tiejun},
title = {Unsupervised Neural Machine Translation for Similar and Distant Language Pairs: An Empirical Study},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3418059},
doi = {10.1145/3418059},
abstract = {Unsupervised neural machine translation (UNMT) has achieved remarkable results for several language pairs, such as French–English and German–English. Most previous studies have focused on modeling UNMT systems; few studies have investigated the effect of UNMT on specific languages. In this article, we first empirically investigate UNMT for four diverse language pairs (French/German/Chinese/Japanese–English). We confirm that the performance of UNMT in translation tasks for similar language pairs (French/German–English) is dramatically better than for distant language pairs (Chinese/Japanese–English). We empirically show that the lack of shared words and different word orderings are the main reasons that lead UNMT to underperform in Chinese/Japanese–English. Based on these findings, we propose several methods, including artificial shared words and pre-ordering, to improve the performance of UNMT for distant language pairs. Moreover, we propose a simple general method to improve translation performance for all these four language pairs. The existing UNMT model can generate a translation of a reasonable quality after a few training epochs owing to a denoising mechanism and shared latent representations. However, learning shared latent representations restricts the performance of translation in both directions, particularly for distant language pairs, while denoising dramatically delays convergence by continuously modifying the training data. To avoid these problems, we propose a simple, yet effective and efficient, approach that (like UNMT) relies solely on monolingual corpora: pseudo-data-based unsupervised neural machine translation. Experimental results for these four language pairs show that our proposed methods significantly outperform UNMT baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {10},
numpages = {17},
keywords = {similar and distant language pairs, Unsupervised neural machine translation, pseudo-data-based unsupervised neural machine translation}
}

@article{10.1145/3609222,
author = {Lalrempuii, Candy and Soni, Badal},
title = {Investigating Unsupervised Neural Machine Translation for Low-Resource Language Pair English-Mizo via Lexically Enhanced Pre-Trained Language Models},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3609222},
doi = {10.1145/3609222},
abstract = {The vast majority of languages in the world at present are considered to be low-resource languages. Since the availability of large parallel data is crucial for the success of most modern machine translation approaches, improving machine translation for low-resource languages is a key challenge. Most unsupervised techniques for translation benefit closely related languages with monolingual data of substantial quantity. To facilitate research in this direction for the extremely low resource language pair English (en) and Mizo (lus), we have developed a parallel and monolingual corpus for the Mizo language from various news websites. We explore Unsupervised Neural Machine Translation (UNMT) based on the developed monolingual data. We observe that cross-lingual embedding (CLWE) initializations on subword segmented data during pre-training, based on both masked language modelling and sequence-to-sequence generation tasks, improve translation performance. We experiment with cross-lingual alignment and combined alignment and joint training for learning the cross-lingual embedding representations. We also report baseline performances and the impact of CLWE initialization using semi-supervised and supervised neural machine translation. Empirical results show that both CLWE initializations work well for the distant pair English-Mizo compared to the baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {209},
numpages = {18},
keywords = {Unsupervised neural machine translation, low resource languages, Mizo, cross-lingual word embeddings}
}

@inproceedings{10.1145/3405755.3406160,
author = {Dickhaut, Ernestine and Thies, Laura Friederike and Janson, Andreas and Ro\ss{}nagel, Alexander and Leimeister, Jan Marco},
title = {Towards a New Methodology to Capture the Legal Compatibility of Conversational Speech Agents},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406160},
doi = {10.1145/3405755.3406160},
abstract = {Higher legal standards with regards to the data protection of individuals such as the EU General Data Protection Regulation (EU-GDPR) are increasing the pressure on developers of IT artifacts. This is especially prevalent when considering conversational speech agents (CSA), which are collecting data in new ways and thus are oftentimes producing conflicts with existing law regulations. For this purpose, we introduce the law simulation method, which is a well-known evaluation method among law researchers for capturing the legal compatibility of IT artifacts such as CSA. With this rigorous method, we are able to derive actionable guidance for CSA developers to evaluate developer efforts for increasing legal compatibility. To illustrate our methodological approach, we describe in this paper key steps of the method with respect to the evaluation of CSA. We briefly discuss how this procedure can serve as the foundation for a new evaluation method of legally compatible systems in information systems.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {56},
numpages = {4},
keywords = {Conversational speech agent, legal compatibility, evaluation method},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3597926.3598081,
author = {Wang, Jun and Li, Yanhui and Huang, Xiang and Chen, Lin and Zhang, Xiaofang and Zhou, Yuming},
title = {Back Deduction Based Testing for Word Sense Disambiguation Ability of Machine Translation Systems},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598081},
doi = {10.1145/3597926.3598081},
abstract = {Machine translation systems have penetrated our daily lives, providing translation services from source language to target language to millions of users online daily. Word Sense Disambiguation (WSD) is one of the essential functional requirements of machine translation systems, which aims to determine the exact sense of polysemes in the given context. Commercial machine translation systems (e.g., Google Translate) have been shown to fail in identifying the proper sense and consequently cause translation errors. However, to our knowledge, no prior studies focus on testing such WSD bugs for machine translation systems. To tackle this challenge, we propose a novel testing method Back Deduction based Testing for Word Sense Disambiguation (BDTD). Our method’s main idea is to obtain the hidden senses of source words via back deduction from the target language, i.e., employ translation words in the target language to deduce senses of original words identified in the translation procedure. To evaluate BDTD, we conduct an extensive empirical study with millions of sentences under three popular translators, including Google Translate and Bing Microsoft Translator. The experimental results indicate that BDTD can identify a considerable number of WSD bugs with high accuracy, more than 80%, under all three translators.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {601–613},
numpages = {13},
keywords = {Back Deduction, Word Sense Disambiguation, Machine Translation, Software Testing},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3431727,
author = {Zhang, Longtu and Komachi, Mamoru},
title = {Using Sub-Character Level Information for Neural Machine Translation of Logographic Languages},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3431727},
doi = {10.1145/3431727},
abstract = {Logographic and alphabetic languages (e.g., Chinese vs. English) have different writing systems linguistically. Languages belonging to the same writing system usually exhibit more sharing information, which can be used to facilitate natural language processing tasks such as neural machine translation (NMT). This article takes advantage of the logographic characters in Chinese and Japanese by decomposing them into smaller units, thus more optimally utilizing the information these characters share in the training of NMT systems in both encoding and decoding processes. Experiments show that the proposed method can robustly improve the NMT performance of both “logographic” language pairs (JA–ZH) and “logographic + alphabetic” (JA–EN and ZH–EN) language pairs in both supervised and unsupervised NMT scenarios. Moreover, as the decomposed sequences are usually very long, extra position features for the transformer encoder can help with the modeling of these long sequences. The results also indicate that, theoretically, linguistic features can be manipulated to obtain higher share token rates and further improve the performance of natural language processing systems.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {31},
numpages = {15},
keywords = {logographic languages, unsupervised NMT, Neural machine translation, shared information}
}

@article{10.1109/TASLP.2020.3021347,
author = {Yang, Mingming and Wang, Rui and Chen, Kehai and Wang, Xing and Zhao, Tiejun and Zhang, Min},
title = {A Novel Sentence-Level Agreement Architecture for Neural Machine Translation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3021347},
doi = {10.1109/TASLP.2020.3021347},
abstract = {In neural machine translation (NMT), there is a natural correspondence between source and target sentences. The traditional NMT method does not explicitly model the translation agreement on sentence-level. In this article, we propose a comprehensive and novel sentence-level agreement architecture to alleviate this problem. It directly minimizes the difference between the representations of the source-side and target-side sentence on sentence-level. First, we compare a variety of sentence representation strategies and propose a “Gated Sum” sentence representation to achieve better sentence semantic information. Then, rather than a single-layer sentence-level agreement architecture, we further propose a multi-layer sentence agreement architecture to make the source and target semantic spaces closer layer by layer. The proposed agreement module can be integrated into NMT as an additional training objective function, and can also be used to enhance the representation of the source-side sentences. Experiments on the NIST Chinese-to-English and the WMT English-to-German translation tasks show that the proposed agreement architecture achieves significant improvements over state-of-the-art baselines, demonstrating the effectiveness and necessity of exploiting sentence-level agreement for NMT.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2585–2597},
numpages = {13}
}

@article{10.1109/TASLP.2020.2977776,
author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
title = {Machine Speech Chain},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2977776},
doi = {10.1109/TASLP.2020.2977776},
abstract = {Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop machine speech chain model based on deep learning. The sequence-to-sequence model in closed-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning framework that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved performance over that from separate systems that were only trained with labeled data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {976–989},
numpages = {14}
}

@inproceedings{10.1145/3510003.3510206,
author = {Sun, Zeyu and Zhang, Jie M. and Xiong, Yingfei and Harman, Mark and Papadakis, Mike and Zhang, Lu},
title = {Improving Machine Translation Systems via Isotopic Replacement},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510206},
doi = {10.1145/3510003.3510206},
abstract = {Machine translation plays an essential role in people's daily international communication. However, machine translation systems are far from perfect. To tackle this problem, researchers have proposed several approaches to testing machine translation. A promising trend among these approaches is to use word replacement, where only one word in the original sentence is replaced with another word to form a sentence pair. However, precise control of the impact of word replacement remains an outstanding issue in these approaches.To address this issue, we propose CAT, a novel word-replacement-based approach, whose basic idea is to identify word replacement with controlled impact (referred to as isotopic replacement). To achieve this purpose, we use a neural-based language model to encode the sentence context, and design a neural-network-based algorithm to evaluate context-aware semantic similarity between two words. Furthermore, similar to TransRepair, a state-of-the-art word-replacement-based approach, CAT also provides automatic fixing of revealed bugs without model retraining.Our evaluation on Google Translate and Transformer indicates that CAT achieves significant improvements over TransRepair. In particular, 1) CAT detects seven more types of bugs than TransRepair; 2) CAT detects 129% more translation bugs than TransRepair; 3) CAT repairs twice more bugs than TransRepair, many of which may bring serious consequences if left unfixed; and 4) CAT has better efficiency than TransRepair in input generation (0.01s v.s. 0.41s) and comparable efficiency with TransRepair in bug repair (1.92s v.s. 1.34s).},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1181–1192},
numpages = {12},
keywords = {neural networks, machine translation, machine learning testing, testing and repair},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3556677.3556691,
author = {Zhang, Wu and Lam, Tung Yeung and Chan, Mee Yee},
title = {Using Translation Memory to Improve Neural Machine Translations},
year = {2022},
isbn = {9781450396936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556677.3556691},
doi = {10.1145/3556677.3556691},
abstract = {In this paper, we describe a way of using translation memory (TM) to improve the translation quality and stability of neural machine translation (NMT) systems, especially when the sentences to be translated have high similarity with sentences stored in the TM. The difference between the sentences to be translated and the sentences stored in the TM may only be in a few phrases. Our TM comprises not only paired sentences (i.e., a sentence in the source language paired with its translation in the target language) but also paired phrases. Translation quality is improved using good phrase translations for the differing phrases. The NMT system is used to assist phrase translation. We tested our TM on 3,000 English-Chinese paired sentences which were randomly picked from recent annual reports published and submitted to the Hong Kong Stock Exchange. Our TM translations achieved a significant BLEU improvement for high similar sentences compared with our NMT translations.},
booktitle = {Proceedings of the 2022 6th International Conference on Deep Learning Technologies},
pages = {49–54},
numpages = {6},
keywords = {English-Chinese translation, translation memories, neural machine translation},
location = {Xi'an, China},
series = {ICDLT '22}
}

@article{10.1145/3314938,
author = {Gao, Shengxiang and Huang, Jihao and Xue, Mingya and Yu, Zhengtao and Wang, Zhuo and Zhang, Yang},
title = {Syntax-Based Chinese-Vietnamese Tree-to-Tree Statistical Machine Translation with Bilingual Features},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314938},
doi = {10.1145/3314938},
abstract = {Because of the scarcity of bilingual corpora, current Chinese--Vietnamese machine translation is far from satisfactory. Considering the differences between Chinese and Vietnamese, we investigate whether linguistic differences can be used to supervise machine translation and propose a method of syntax-based Chinese--Vietnamese tree-to-tree statistical machine translation with bilingual features. Analyzing the syntax differences between Chinese and Vietnamese, we define some linguistic difference-based rules, such as attributive position, time adverbial position, and locative adverbial position, and create rewards for similar rules. These rewards are integrated into the extraction of tree-to-tree translation rules, and we optimize the pruning of the search space during the decoding phase. The experiments on Chinese--Vietnamese bilingual sentence translation show that the proposed method performs better than several compared methods. Further, the results show that syntactic difference features, with search pruning, can improve the accuracy of machine translation without degrading the efficiency.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {36},
numpages = {20},
keywords = {tree-to-tree, pruning optimization, linguistic features, Statistical machine translation, Chinese-Vietnamese}
}

@inproceedings{10.1145/3364908.3365300,
author = {Kang, Zhehan},
title = {Spoken Language to Sign Language Translation System Based on HamNoSys},
year = {2019},
isbn = {9781450362412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364908.3365300},
doi = {10.1145/3364908.3365300},
abstract = {Sign languages use visual-manual modality to convey information, thereby enabling communication between hearing-impaired people. However, there is no fully developed technology that enables the communication between hearing-impaired people and those with no hearing disabilities. Current approaches to sign language translation are based on videos and pictures that are difficult to edit after being recorded. Here, we propose a new framework based on speech recognition, natural language processing, and 3D virtual human technology. Our current method (1) provides a new translation method based on a visual human, and the gestures can be easily edited using the HamNoSys keyboard; (2) can be used in the translation of various languages, including Chinese sign language(CSL), American sign language (ASL), and British sign language(BSL); (3) achieves grammar conversion between sign and spoken languages. Through the preliminary test of a simple conversation, the unilateral translation from spoken to sign languages is achieved. Further improvements will be obtained by the incorporation of more vocabulary and grammar conversion rules.},
booktitle = {Proceedings of the 2019 International Symposium on Signal Processing Systems},
pages = {159–164},
numpages = {6},
keywords = {Virtual human, HamNoSys, Speech recognition, Sign language translation, Natural language processing},
location = {Beijing, China},
series = {SSPS '19}
}

@inproceedings{10.1145/3412841.3442099,
author = {Mallick, Ritam and Susan, Seba and Agrawal, Vaibhaw and Garg, Rizul and Rawal, Prateek},
title = {Context- and Sequence-Aware Convolutional Recurrent Encoder for Neural Machine Translation},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442099},
doi = {10.1145/3412841.3442099},
abstract = {Neural Machine Translation1 model is a sequence-to-sequence converter based on neural networks. Existing models use recurrent neural networks to construct both the encoder and decoder modules. In alternative research, the recurrent networks were substituted by convolutional neural networks for capturing the syntactic structure in the input sentence and decreasing the processing time. We incorporate the goodness of both approaches by proposing a convolutional-recurrent encoder for capturing the context information as well as the sequential information from the source sentence. Word embedding and position embedding of the source sentence is performed prior to the convolutional encoding layer which is basically a n-gram feature extractor capturing phrase-level context information. The rectified output of the convolutional encoding layer is added to the original embedding vector, and the sum is normalized by layer normalization. The normalized output is given as a sequential input to the recurrent encoding layer that captures the temporal information in the sequence. For the decoder, we use the attention-based recurrent neural network. Translation task on the German-English dataset verifies the efficacy of the proposed approach from the higher BLEU scores achieved as compared to the state of the art.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {853–856},
numpages = {4},
keywords = {neural machine translation, encoder, decoder, context, convolutional, sequence-to-sequence model, recurrent},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3349341.3349420,
author = {Tong, Jiajun and Zhang, Jujian and Liu, Zhengdong},
title = {Research and Implementation of Globalize JS in Improving the Level of Machine Translation},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349420},
doi = {10.1145/3349341.3349420},
abstract = {At present, there are three basic solutions for localization in industrial production environment: traditional source code modification, micro-service architecture model and browser plug-in model. Now, this paper propose a new solution NSX, which uses machine learning and machine translation technology to implement L2 and L3 internationally and reduce the coding burden of developers. Reduce the possibility of localization leading to functional problems; Eliminate QE work in localization functional testing.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {297–302},
numpages = {6},
keywords = {Globalizes, machine translation, NLP, globalization},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3443279.3443286,
author = {Kessikbayeva, Gulshat and Cicekli, Ilyas},
title = {Impact of Statistical Language Model on Example Based Machine Translation System between Kazakh and Turkish Languages},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443286},
doi = {10.1145/3443279.3443286},
abstract = {In this paper a hybrid example based machine translation system between Kazakh and Turkish languages is presented. The system mainly based on example based machine translation method which is supported by a statistical language model for the target language. Translation templates are learned at morphological level from a bilingual parallel corpus of Turkish and Kazakh languages. Translations can be performed at both directions using these learned translation templates. Our main aim with this hybrid example based machine translation system is to obtain more accurate translation results by pre-gained knowledge from target language resource. One of the reasons that we propose this hybrid approach is that monolingual language resources are more widely available than bilingual language resources.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {112–118},
numpages = {7},
keywords = {Language Model, Machine Translation, Natural Language Processing},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@article{10.1145/3365244,
author = {Yu, Zhiqiang and Yu, Zhengtao and Guo, Junjun and Huang, Yuxin and Wen, Yonghua},
title = {Efficient Low-Resource Neural Machine Translation with Reread and Feedback Mechanism},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365244},
doi = {10.1145/3365244},
abstract = {How to utilize information sufficiently is a key problem in neural machine translation (NMT), which is effectively improved in rich-resource NMT by leveraging large-scale bilingual sentence pairs. However, for low-resource NMT, lack of bilingual sentence pairs results in poor translation performance; therefore, taking full advantage of global information in the encoding-decoding process is effective for low-resource NMT. In this article, we propose a novel reread-feedback NMT architecture (RFNMT) for using global information. Our architecture builds upon the improved sequence-to-sequence neural network and consists of a double-deck attention-based encoder-decoder framework. In our proposed architecture, the information generated by the first-pass encoding and decoding process flows to the second-pass encoding process for more sufficient parameters initialization and information use. Specifically, we first propose a “reread” mechanism to transfer the outputs of the first-pass encoder to the second-pass encoder, and then the output is used for the initialization of the second-pass encoder. Second, we propose a “feedback” mechanism that transfers the first-pass decoder’s outputs to a second-pass encoder via an important weight model and an improved gated recurrent unit (GRU). Experiments on multiple datasets show that our approach achieves significant improvements over state-of-the-art NMT systems, especially in low-resource settings.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {34},
numpages = {13},
keywords = {neural machine translation, reread, feedback, Low-resource}
}

@inproceedings{10.1145/3372938.3372957,
author = {Berrichi, Safae and Mazroui, Azzeddine},
title = {Guiding Word Alignment with Prior Knowledge to Improve English-Arabic Machine Translation},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372957},
doi = {10.1145/3372938.3372957},
abstract = {Word alignment is a crucial step during the development process of machine translation system. However, a major challenge for researchers still remains the lack of an effective English/Arabic alignment system. In this article, we propose a new alignment process, which is based on morphological preprocessing and incorporation of a bilingual dictionary as an additional source to support some alignment choices. Test results show that the use of the dictionary has improved the quality of alignment and therefore increase the BLEU score by 5%.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {19},
numpages = {5},
keywords = {Bilingual Dictionary, Statistical Machine Translation, English-Arabic languages, Word Alignment},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3589462.3589488,
author = {Revesz, Peter Zsolt},
title = {A Generalization of the Chomsky-Halle Phonetic Representation Using Real Numbers for Robust Speech Recognition in Noisy Environments},
year = {2023},
isbn = {9798400707445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589462.3589488},
doi = {10.1145/3589462.3589488},
abstract = {Speech recognition is difficult when the speech signal is weak or occurs in a noisy environment. This paper presents an efficient and robust method that can reconstruct the standard pronunciation of English phonemes and words given a weak or noisy signal. The reconstruction is based on a novel representation of the reconstruction task as a problem of data retrieval from a database in two different cases: (1) when the phonemes are represented in the database as binary tuples and the input is also a binary tuple from which deletion errors occur, and (2) when the phonemes are represented in the database and in the input as tuples of real values ranging between 0 and 1. In the latter case, the input phoneme could contain both a higher or lower value than the standard phoneme in the database that is intended by the speaker. For case (2) a theorem is proven regarding when the data retrieval can be expected to be reliable.},
booktitle = {Proceedings of the 27th International Database Engineered Applications Symposium},
pages = {156–160},
numpages = {5},
keywords = {Hamming distance, Noise, Neural network, Signal, Speech recognition, Phoneme, Error},
location = {Heraklion, Crete, Greece},
series = {IDEAS '23}
}

@inproceedings{10.1145/3377811.3380420,
author = {Sun, Zeyu and Zhang, Jie M. and Harman, Mark and Papadakis, Mike and Zhang, Lu},
title = {Automatic Testing and Improvement of Machine Translation},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380420},
doi = {10.1145/3377811.3380420},
abstract = {This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs. Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer. Grey-box repair fixes 30% bugs on average for Transformer. Manual inspection indicates that the translations repaired by our approach improve consistency in 87% of cases (degrading it in 2%), and that our repairs have better translation acceptability in 27% of the cases (worse in 8%).},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {974–985},
numpages = {12},
keywords = {testing and repair, machine translation, translation consistency},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3464458.3464462,
author = {Basu, Chumki and Venkatesan, Sridhar and Chiang, Cho-Yu Jason and Leslie, Nandi and Kamhoua, Charles},
title = {Generating Targeted E-Mail at Scale Using Neural Machine Translation},
year = {2022},
isbn = {9781450384902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464458.3464462},
doi = {10.1145/3464458.3464462},
abstract = {Advances in deep learning, specifically in Neural Machine Translation (NMT) [1, 20, 28], have opened up new application opportunities for generating human-readable text by machines. In this paper, we discuss one such application – the generation of spearphishing e-mail at scale – that has ramifications for cybersecurity. To this end, the ability to characterize text generated by people and by machines is important, and specific characteristics of text could be used as differentiating features. For example, we could evaluate whether the generated text is grammatically well-formed or semantically relevant to some topic. However, existing metrics for evaluating NMT models do not adequately address these needs. As a result, we propose new metrics that capture both well-formedness and topical relevance to evaluate the quality of text. We show that the proposed metrics corroborate the findings of existing metrics and provide additional insights. Although the metrics consistently indicate that text generated by state-of-the-art NMT models possess grammatical structure, the machine-generated text differ in performance with respect to the metrics when compared to human-generated text. By using these new metrics, we assessed whether the automatically-generated content would pass a spam/spearphishing filter [6]. The filter performs poorly in distinguishing between human-generated and machine-generated text indicating the robustness of existing models.},
booktitle = {Proceedings of the 2019 Workshop on DYnamic and Novel Advances in Machine Learning and Intelligent Cyber Security},
articleno = {4},
numpages = {11},
keywords = {neural machine translation, phishing, metrics, machine learning-driven cyber offense, machine learning-based defense and response},
location = {San Juan, PR, USA},
series = {DYNAMICS '19}
}

@inproceedings{10.1145/3495018.3495510,
author = {Sun, Wei},
title = {Integration of Machine Translation and Manual Translation in Translation Practice Based on Artificial Intelligence and Big Data Technology},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495510},
doi = {10.1145/3495018.3495510},
abstract = {In recent years, thanks to the development of big data and information technology, the emergence of new algorithms, the improvement of computer performance, and the wide application of artificial intelligence, machine translation technology has made great progress and has won acknowledgement by the translation industry and customers as an important part of the efficient translation process. However, the quality of version produced by machine translation still cannot reach the level of professional translators in their manual task. Compared with the huge translation market demand, the limited number of translators can be described as a drop in the bucket. The use of machine translation engines in multilingual organizations and multinational companies has opened up new ways for translators, since machine translation has an unparalleled advantage over human translation. If machine translation and human translation can be perfectly combined, a multiplier effect with be achieved with half the effort. In order to achieve a balance between translation quality and translation efficiency, and give full play to the advantages of human-computer interaction, pre-editing and post-editing have become the translation implementation methods actively adopted by the translation service industry.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1895–1897},
numpages = {3},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/3373267,
author = {Che, Wanjin and Yu, Zhengtao and Yu, Zhiqiang and Wen, Yonghua and Guo, Junjun},
title = {Towards Integrated Classification Lexicon for Handling Unknown Words in Chinese-Vietnamese Neural Machine Translation},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373267},
doi = {10.1145/3373267},
abstract = {In Neural Machine Translation (NMT), due to the limitations of the vocabulary, unknown words cannot be translated properly, which brings suboptimal performance of the translation system. For resource-scarce NMT that have small-scale training corpus, the effect is amplified. The traditional approach of amplifying the scale of the corpus is not applicable, because the parallel corpus is difficult to obtain in a resource-scarce setting; however, it is easy to obtain and utilize external knowledge, bilingual lexicon, and other resources. Therefore, we propose classification lexicon approach for processing unknown words in the Chinese-Vietnamese NMT task. Specifically, three types of unknown Chinese-Vietnamese words are classified and their corresponding classification lexicon are constructed by word alignment, Wikipedia extraction, and rule-based methods, respectively. After translation, the unknown words are restored by lexicon for post-processing. Experiment results on Chinese-Vietnamese, English-Vietnamese, and Mongolian-Chinese translations show that our approach significantly improves the accuracy and the performance of NMT especially in a resource-scarce setting.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {42},
numpages = {17},
keywords = {resource-scarce, unknown words, Neural machine translation, classification lexicon}
}

@inproceedings{10.1145/3405755.3406124,
author = {Lee, Minha},
title = {Speech Acts Redux: Beyond Request-Response Interactions},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406124},
doi = {10.1145/3405755.3406124},
abstract = {We communicate to (1) express how we feel, (2) share observations about the world, (3) commit to future acts, (4) request others to do things, and (5) change the state of the world according to pragmatics. Of these categories, today's conversational interfaces like Siri and Alexa are mainly designed to fulfill our imperatives, i.e., to respond to our requests on command. Yet, could future conversational interfaces go beyond request-response interactions? One way forward is to consider what conversational interactions allow us to do with language. Not only can we send request to CUIs, but we can also share our emotions, attitudes, beliefs, and promises as speech acts---acts we regularly perform with other humans. To open up pragmatics as an under-investigated design space for conversational technologies, I elaborate on what pragmatics and affective pragmatics are and give examples involving conversational agents. As a theoretical contribution, I provide a taxonomy from pragmatics and affective pragmatics to move beyond request-response interactions. The aim is to extend our conversational experiences with technology to cover the full spectrum of everyday speech acts. Our words can change the world; expressions to CUIs can also do so.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {13},
numpages = {10},
keywords = {Conversational interactions, Affective Pragmatics, Pragmatics, Conversational user interfaces, Interaction paradigms},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3446132.3446405,
author = {Duanzhu, Sangjie and Zhang, Rui and Jia, Cairang},
title = {Bidirectional Boost: On Improving Tibetan-Chinese Neural Machine Translation With Back-Translation and Self-Learning},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446405},
doi = {10.1145/3446132.3446405},
abstract = {Despite the remarkable success of Neural Machine Translation system, such challenges as its drawback in low-resourced conditions persist. In recent years, working mechanism of exploiting either one or both source and target side monolingual data within the Neural Machine Translation framework gained much attention in the field. Among many supervised and unsupervised proposals, back translation is increasingly seen as one of the most promising methods to improve low-resource NMT performance. Regardless of its simplicity, the effectiveness of back translation is highly dependent on performance of the backward model which is initially trained on available parallel data. To address the dilemma of back translation practices in low resource scenarios, we propose to employ target-side monolingual data to improve both backward and forward models by step-wise adoption of self-learning and back translation, which we refer to as Bidirectional Boost.Our experiments on a Tibetan-Chinese translation task attested the proposed approach with a result of producing 3.1 and 8.2 BLEU scores, respectively, both on forward and backward models over vanilla Transformers trained on genuine parallel data under supervised settings.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {78},
numpages = {6},
keywords = {Self-learning, Tibetan-Chinese, Back-translation, Low-resource, Neural Machine Translation},
location = {Sanya, China},
series = {ACAI '20}
}

@article{10.1145/3574130,
author = {Chauhan, Shweta and Shet, Jayashree Premkumar and Beram, Shehab Mohamed and Jagota, Vishal and Dighriri, Mohammed and Ahmad, Mohd Wazih and Hossain, Md Shamim and Rizwan, Ali},
title = {Rule Based Fuzzy Computing Approach on Self-Supervised Sentiment Polarity Classification with Word Sense Disambiguation in Machine Translation for Hindi Language},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3574130},
doi = {10.1145/3574130},
abstract = {With increasing globalization, communication among people of diverse cultural backgrounds is also taking place to a very large extent in the present era. Issues like language diversity in various parts of the world can lead to hindrance in communication. The usage of social media and user-generated material has grown at an exponential rate and existing supervised sentiment polarity classification techniques need labelling for the training dataset. In this study, two problems have been analyzed. First, sentiment analysis of the Twitter dataset and sense disambiguation of morphologically rich Hindi language. A rule-based fuzzy logics-based system for self-supervised sentiment classification was used to compute and analyze the self-supervised or completely unsupervised sentiment categorization of a social-media dataset using three types of lexicons.&nbsp; The combination of fuzzy with three different types of lexicons gives sentiment analysis a new path. The unsupervised fuzzy rules integrate the fuzziness of both negative as well as positive scores, and fuzzy logic-based systems can cope with ambiguity and vagueness. The fuzzy-system uses an unsupervised/self-supervised fuzzy rule-based technique to identify text using natural language processing (NLP) and sense of word. We compared the results of fuzzy rule based self-supervised sentiment classification by using three types of lexicons on five different datasets, with unsupervised as well as supervised sentiment classification techniques. Second, using cross-lingual sense embedding rather than cross-lingual word embedding resolves the ambiguity issue. The word sense embeddings are produced for the source languages to learn multiple or various senses of the words. Different evaluation metrics depict an improved performance for English-Hindi language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {153},
numpages = {21},
keywords = {fuzzy sets, Sentiment analysis, unsupervised sentiment classification, self-learning, lexicon}
}

@article{10.1109/TASLP.2019.2925973,
author = {Bai, Xuefeng and Cao, Hailong and Chen, Kehai and Zhao, Tiejun},
title = {A Bilingual Adversarial Autoencoder for Unsupervised Bilingual Lexicon Induction},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2925973},
doi = {10.1109/TASLP.2019.2925973},
abstract = {Unsupervised bilingual lexicon induction aims to generate bilingual lexicons without any cross-lingual signals. Successfully solving this problem would benefit many downstream tasks, such as unsupervised machine translation and transfer learning. In this work, we propose an unsupervised framework, named bilingual adversarial autoencoder, which automatically generates bilingual lexicon for a pair of languages from their monolingual word embeddings. In contrast to existing frameworks which learn a direct cross-lingual mapping of word embeddings from the source language to the target language, we train two autoencoders jointly to transform the source and the target monolingual word embeddings into a shared embedding space, where a word and its translation are close to each other. In this way, we capture the cross-lingual features of word embeddings from different languages and use them to induce bilingual lexicons. By conducting extensive experiments across eight language pairs, we demonstrate that the proposed method significantly outperforms the existing adversarial methods and even achieves best-published results across most language pairs.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1639–1648},
numpages = {10}
}

@inproceedings{10.1145/3485832.3485892,
author = {Zhang, Zhaohe (John) and Yang, Edwin and Fang, Song},
title = {CommanderGabble: A Universal Attack Against ASR Systems Leveraging Fast Speech},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485892},
doi = {10.1145/3485832.3485892},
abstract = {Automatic Speech Recognition (ASR) systems are widely used in various online transcription services and personal digital assistants. Emerging lines of research have demonstrated that ASR systems are vulnerable to hidden voice commands, i.e., audio that can be recognized by ASRs but not by humans. Such attacks, however, often either highly depend on white-box knowledge of a specific machine learning model or require special hardware to construct the adversarial audio. This paper proposes a new model-agnostic and easily-constructed attack, called CommanderGabble, which uses fast speech to camouflage voice commands. Both humans and ASR systems often misinterpret fast speech, and such misinterpretation can be exploited to launch hidden voice command attacks. Specifically, by carefully manipulating the phonetic structure of a target voice command, ASRs can be caused to derive a hidden meaning from the manipulated, high-speed version. We implement the discovered attacks both over-the-wire and over-the-air, and conduct a suite of experiments to demonstrate their efficacy against 7 practical ASR systems. Our experimental results show that the over-the-wire attacks can disguise as many as 96 out of 100 tested voice commands into adversarial ones, and that the over-the-air attacks are consistently successful for all 18 chosen commands in multiple real-world scenarios.},
booktitle = {Annual Computer Security Applications Conference},
pages = {720–731},
numpages = {12},
keywords = {ASR misinterpretation, adversarial audio, syllabification},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@inproceedings{10.1145/3372278.3390717,
author = {Han, Yifeng and Li, Lin and Zhang, Jianwei},
title = {A Coordinated Representation Learning Enhanced Multimodal Machine Translation Approach with Multi-Attention},
year = {2020},
isbn = {9781450370875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372278.3390717},
doi = {10.1145/3372278.3390717},
abstract = {In recent years, the application of machine translation has become more and more widely. Currently, the neural multimodal translation models have made attractive progress, which combines images into deep learning networks, such as Transformer and RNN. When considering images in translation models, they directly apply gate structure or image attention to introduce image feature to enhance the translation effect. We argue that it may mismatch the text and image features since they are in different semantic space. In this paper, we propose a coordinated representation learning enhanced multimodal machine translation approach with multimodal attention. Our approach accepts the text data and its relevant image data as the input. The image features are fed into the decoder side of the basic Transformer model. Moreover, the Coordinated Representation Learning is utilized to map the different text and image modal features into their semantic representations. The mapped representations are linearly related in a shared semantic space. Finally, the sum of the image and text representations, called Coordinated Visual-Semantic Representation (CVSR), will be sent to a Multimodal Attention Layer (MAL) in our Transformer based translation approach. Experimental results show that our approach achieves the state-of-art performance on the public Multi30k dataset.},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {571–577},
numpages = {7},
keywords = {data fusion, transformer, multi-head attention, multimodal translation},
location = {Dublin, Ireland},
series = {ICMR '20}
}

@inproceedings{10.1145/3290605.3300461,
author = {Herbig, Nico and Pal, Santanu and van Genabith, Josef and Kr\"{u}ger, Antonio},
title = {Multi-Modal Approaches for Post-Editing Machine Translation},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300461},
doi = {10.1145/3290605.3300461},
abstract = {Current advances in machine translation increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and improves quality. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Our results of an elicitation study with professional translators indicate that a combination of pen, touch, and speech could well support common PE tasks, and received high subjective ratings by our participants. Therefore, we argue that future translation environment research should focus more strongly on these modalities in addition to mouse- and keyboard-based approaches. On the other hand, eye tracking and gesture modalities seem less important. An additional interview regarding interface design revealed that most translators would also see value in automatically receiving additional resources when a high cognitive load is detected during PE.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {multi-modality, post-editing, computer-aided translation},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/3568674,
author = {Kchaou, Sam\'{e}h and Boujelbane, Rahma and Hadrich, Lamia},
title = {Hybrid Pipeline for Building Arabic Tunisian Dialect-Standard Arabic Neural Machine Translation Model from Scratch},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3568674},
doi = {10.1145/3568674},
abstract = {Deep Learning is one of the most promising technologies compared to other methods in the context of machine translation. It has been proven to achieve impressive results on large amounts of parallel data for well-endowed languages. Nevertheless, for low-resource languages such as the Arabic Dialects, Deep Learning models failed due to the lack of available parallel corpora. In this article, we present a method to create a parallel corpus to build an effective NMT model able to translate into MSA, Tunisian Dialect texts present in social networks. For this, we propose a set of data augmentation methods aiming to increase the size of the state-of-the-art parallel corpus. By evaluating the impact of this step, we noticed that it has effectively boosted both the size and the quality of the corpus. Then, using the resulted corpus, we compare the effectiveness of CNN, RNN and transformers models to translate Tunisian Dialect into MSA. Experiments show that a better translation is achieved by the transformer model with a BLEU score of 60 vs., respectively, 33.36 and 53.98 with RNN and CNN models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {85},
numpages = {21},
keywords = {Arabic Tunisian Dialect, data augmentation, Neural Machine Translation, Modern Standard Arabic}
}

@article{10.1145/3623270,
author = {Gong, Yan and Cheng, Li},
title = {Research on the Application of Translation Parallel Corpus in Interpretation Teaching},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3623270},
doi = {10.1145/3623270},
abstract = {Large and organized sets of translated texts between languages are called parallel translation corpora (PTLs). Even though data-driven learning can generate insights from massive datasets and create more tailored learning experiences, it has gained in popularity. There are however a few problems with this strategy, such as poor data quality, privacy concerns, inability to scale, a lack of clear explanations, and high costs. To provide high-quality output, machine translation algorithms are generally trained utilizing parallel corpora generated by human translators. The written word can be deciphered from one language to another through translation. The spoken word can be conveyed from one language to another through translation. Based on the study of real samples, machine translation from corpus linguistics uses its translations to create its translations. Statistical approaches are only one of the many ways a corpus may be used. Translated texts from two or more languages are called parallel corpora. With the emergence of data-driven learning (DDL) in translation training and language instruction, they are becoming increasingly popular in translation and contrastive research. While working as a professional translator, you're likely to encounter a wide range of challenges. These are lexical-semantic, grammar, syntactic, rhetorical, practical, and cultural difficulties. There are limitations to the number of possible translations that dictionaries can provide and difficulty in doing a thorough search. When it comes to translating, contemporary technologies bring up a whole new world of possibilities thanks to the sheer volume of data and the speed at which it is available. Students of translation can benefit from this study's innovative way of employing PTL-DDL, which can help them improve the quality and speed of their translations. In addition, the parallel corpora's sample sentences are readily available, making it easier to choose the best translations from a pool of translation candidates. Because of these characteristics, the approach is well-suited to creating active or encoding dictionaries.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {DDL, Translation, Machine translation, Corpus, Parallel, Student}
}

@inproceedings{10.1145/3568199.3568215,
author = {Wu, Jie and Feng, Ben and Sun, Yanan},
title = {Genetic Algorithm-Based Transformer Architecture Design for Neural Machine Translation},
year = {2023},
isbn = {9781450397551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568199.3568215},
doi = {10.1145/3568199.3568215},
abstract = {A Great progress of Neural Machine Translation (NMT) tasks has been achieved by the transformer models in recent years, which is largely owing to the careful design of multi-head attention and feed-forward neural network layers in its encoder-decoder architecture. However, these layers are often manually designed by expertise, makes it time-consuming and hard to explore potentially promising ordering patterns. In this paper, an automatic transformer architecture design algorithm based on genetic algorithm is proposed to evolve the optimal transformer architecture for NMT tasks. Particularly, a novel gene encoding strategy is developed in the proposed algorithm to effectively enable transformer architectures to have various layer ordering patterns and hyper-parameters, and then the effective genetic operators are designed to perform the efficient evolutionary search for finding optimal architecture. To validate the effectiveness of the proposed algorithm, the experiments are conducted on a widely used machine translation benchmark, and the result shows that the model automatically searched by the proposed algorithm outperforms the vanilla transformer under different model sizes.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Machine Intelligence},
pages = {98–104},
numpages = {7},
keywords = {Genetic algorithm, Transformer},
location = {Hangzhou, China},
series = {MLMI '22}
}

@inproceedings{10.1109/ICSE-Companion.2019.00131,
author = {Zheng, Wujie and Wang, Wenyu and Liu, Dian and Zhang, Changrong and Zeng, Qinsong and Deng, Yuetang and Yang, Wei and He, Pinjia and Xie, Tao},
title = {Testing Untestable Neural Machine Translation: An Industrial Case},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00131},
doi = {10.1109/ICSE-Companion.2019.00131},
abstract = {Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {314–315},
numpages = {2},
keywords = {in-vivo testing, AI quality assurance, neural machine translation},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3427478.3427482,
author = {Das, Mithun and Mathew, Binny and Saha, Punyajoy and Goyal, Pawan and Mukherjee, Animesh},
title = {Hate Speech in Online Social Media},
year = {2020},
issue_date = {Autumn 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2020},
number = {Autumn},
issn = {1931-1745},
url = {https://doi.org/10.1145/3427478.3427482},
doi = {10.1145/3427478.3427482},
abstract = {Social media platforms like Twitter, Gab, Facebook are available in the market to billions1 of users. These platforms allow users to share their ideas and opinions instantly almost with no cost. These have already been utilized by bad actors in the society to cause damage. Scenarios like the Rohingya genocide in Myanmar, anti-Muslim mob violence in Sri Lanka, and the Pittsburgh synagogue shooting can be linked to these platforms. Recently, hate speech is considered to be one of the major issues poisoning the online social media environment. To keep these platforms healthy there is a need to understand how these hateful content spread, how hateful users behave and finally, what could be an effective way to mitigate hate speech. In this article, we look at the recent advances and issues surrounding hate speech in online social media. We take three different perspectives - analysis &amp; spread, detection, and mitigation.},
journal = {SIGWEB Newsl.},
month = {nov},
articleno = {4},
numpages = {8}
}

@inproceedings{10.1145/3482632.3487424,
author = {Li, Shan},
title = {Application of Machine-Aided Translation System Based on Multilingual Parallel Corpus in Japanese Translation of Traditional Culture},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487424},
doi = {10.1145/3482632.3487424},
abstract = {Understanding and expression are two basic elements in translation. In the process of expression, how to provide authentic translation is a major problem for translators, especially those who are not native speakers of the target language. With the accelerating process of global integration and the expanding exchanges between China and Japan, many linguists and translation agencies are increasingly calling for the establishment of a Sino-Japanese translation corpus. For this reason, some scholars and institutions in China have also conducted research one after another. This paper puts forward an application method of machine-aided translation system based on multilingual parallel corpus in Japanese translation of traditional culture. Parallel argumentative database provides translators with the retrieval of vocabulary, phrases and even sentence structure, which can help translators' secret translation. However, it still has some shortcomings, which can be remedied if it is combined with dictionaries. In the teaching of Chinese-Japanese translation, we should fully consider the characteristics of Chinese-Japanese language and culture, and effectively use the theory of functional compensation to guide translation practice.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2339–2342},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3421515.3421522,
author = {Aneja, Sandhya and Nur Afikah Bte Abdul Mazid, Siti and Aneja, Nagender},
title = {Neural Machine Translation Model for University Email Application},
year = {2020},
isbn = {9781450388627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421515.3421522},
doi = {10.1145/3421515.3421522},
abstract = {Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation (NMT) model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for ML → EN (Malay to English) and EN → ML (English to Malay) translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of English to Malay of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.},
booktitle = {Proceedings of the 2020 2nd Symposium on Signal Processing Systems},
pages = {74–79},
numpages = {6},
keywords = {Malay Language, Gated Recurrent Unit, Machine Translation, Recurrent Neural Network Model},
location = {Guangdong, China},
series = {SSPS '20}
}

@article{10.1145/3486677,
author = {Zhu, Shaolin and Mi, Chenggang and Li, Tianqi and Yang, Yong and Xu, Chun},
title = {Unsupervised Parallel Sentences of Machine Translation for Asian Language Pairs},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3486677},
doi = {10.1145/3486677},
abstract = {Parallel sentence pairs play a very important role in many natural language processing tasks, especially cross-lingual tasks such as machine translation. So far, many Asian language pairs lack bilingual parallel sentences. As collecting bilingual parallel data is very time-consuming and difficult, it is very important for many low-resource Asian language pairs. While existing methods have shown encouraging results, they rely on bilingual data seriously or have some drawbacks in an unsupervised situation. To address these issues, we propose a new unsupervised similarity calculation and dynamic selection metric to obtain parallel sentence pairs in an unsupervised situation. First, our method maps bilingual word embedding by postdoc adversarial training, which rotates the source space to match the target without parallel data. Then, we introduce a new cross-domain similarity adaption to obtain parallel sentence pairs. Experimental results on real-world datasets show that our model can obtain better accuracy and recall on mining parallel sentence pairs. We also show that the extracted bilingual sentence corpora can significantly improve the performance of neural machine translation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {64},
numpages = {14},
keywords = {Parallel data, adversarial training, unsupervised method, machine translation}
}

@inproceedings{10.1109/ASE51524.2021.9678715,
author = {Ji, Pin and Feng, Yang and Liu, Jia and Zhao, Zhihong and Xu, Baowen},
title = {Automated Testing for Machine Translation via Constituency Invariance},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678715},
doi = {10.1109/ASE51524.2021.9678715},
abstract = {With the development of deep neural networks, machine translation has achieved significant progress and integrated with people's daily lives to assist in various tasks. However, machine translators, which are essentially one kind of software, also suffer from software defects. Translation errors might cause misunderstanding or even lead to marketing blunders, and political crisis. Thus, almost all translation service providers have feedback channels of incorrect translations to collect training data and improve product performance. Inspired by the syntax structure analysis, we introduce the constituency invariance, which reflects the structural similarity between a simple sentence and sentences derived from it, to test machine translators. We implement it into an automated tool CIT to detect translation errors by checking the constituency invariance relation between the translation results. CIT adopts constituency parse trees to represent the syntactic structures of sentences and employs an efficient data augmentation method to derive multiple new sentences based on one sentence. To validate CIT, we experiment with three widely-used machine translators, i.e., Bing Microsoft Translator, Google Translate, and Youdao Translator. With 600 seed sentences as input, CIT detects 2212, 1910, and 1590 translation errors with around 77% precision. We have submitted detected errors to the development teams. Until we submit this paper, Google, Bing, and Youdao have fixed 15.4%, 32.0%, 14.3% of reported errors, respectively.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {468–479},
numpages = {12},
keywords = {constituency invariance, machine translation testing, metamorphic testing},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1162/coli_a_00353,
author = {Gamallo, Pablo and Sotelo, Susana and Pichel, Jos\'{e} Ramom and Artetxe, Mikel},
title = {Contextualized Translations of Phrasal Verbs with Distributional Compositional Semantics and Monolingual Corpora},
year = {2019},
issue_date = {September 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00353},
doi = {10.1162/coli_a_00353},
abstract = {This article describes a compositional distributional method to generate contextualized senses of words and identify their appropriate translations in the target language using monolingual corpora. Word translation is modeled in the same way as contextualization of word meaning, but in a bilingual vector space. The contextualization of meaning is carried out by means of distributional composition within a structured vector space with syntactic dependencies, and the bilingual space is created by means of transfer rules and a bilingual dictionary. A phrase in the source language, consisting of a head and a dependent, is translated into the target language by selecting both the nearest neighbor of the head given the dependent, and the nearest neighbor of the dependent given the head. This process is expanded to larger phrases by means of incremental composition. Experiments were performed on English and Spanish monolingual corpora in order to translate phrasal verbs in context. A new bilingual data set to evaluate strategies aimed at translating phrasal verbs in restricted syntactic domains has been created and released.},
journal = {Comput. Linguist.},
month = {sep},
pages = {395–421},
numpages = {27}
}

@inproceedings{10.1145/3442536.3442544,
author = {Do, Quang-Minh and Zeng, Kungan and Paik, Incheon},
title = {Resolving Lexical Ambiguity in English-Japanese Neural Machine Translation},
year = {2021},
isbn = {9781450388832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442536.3442544},
doi = {10.1145/3442536.3442544},
abstract = {Although Lexical ambiguity, i.e., the presence of two or more meanings for a single word, is an inherent and challenging problem for machine translation systems. Even though the use of recurrent neural networks (RNN) and attention mechanisms are expected to solve this problem, machine translation systems are not always able to correctly translate lexically ambiguous sentences. In this work, we attempt to resolve the problem of lexical ambiguity in English-Japanese neural machine translation systems by combining a pretrained Bidirectional Encoder Representations from Transformer (BERT) language model that can produce contextualized word embeddings and a Transformer translation model, which is a state-of-the-art architecture for the machine translation task. These two proposed architectures have been shown to be more effective in translating ambiguous sentences than a vanilla Transformer model and the Google Translate system. Furthermore, one of the proposed models, the TransformerBERT-WE, achieves a higher BLEU score compared to the vanilla Transformer model in terms of general translation, which is concrete proof that the use of contextualized word embeddings from BERT can not only solve the problem of lexical ambiguity, but also boosts the translation quality in general.},
booktitle = {Proceedings of the 2020 3rd Artificial Intelligence and Cloud Computing Conference},
pages = {46–51},
numpages = {6},
keywords = {Machine translation, Lexical ambiguity, BLEU, Embedding, BERT},
location = {Kyoto, Japan},
series = {AICCC '20}
}

@inproceedings{10.1145/3358331.3358366,
author = {Shi, Yan and Shi, Chunrang and Zhou, Zehua},
title = {Error Types of Machine Translation of Popular Science Text},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358366},
doi = {10.1145/3358331.3358366},
abstract = {This paper summarizes and categorizes the common error types of machine translation of popular science text, which includes mistranslation of technical terms, omission, over-translation, mistranslation of clauses, mistranslation of attributive sequences, and mistranslation caused by rigidly converting English special sentence structures. The categorization of these common errors in machine translation might be helpful as a reference for machine translation program improvement.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {35},
numpages = {4},
keywords = {post-editing, Machine translation, translation quality},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3471621.3471855,
author = {Zhang, Yangyong and Arora, Sunpreet and Shirvanian, Maliheh and Huang, Jianwei and Gu, Guofei},
title = {Practical Speech Re-Use Prevention in Voice-Driven Services},
year = {2021},
isbn = {9781450390583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3471621.3471855},
doi = {10.1145/3471621.3471855},
abstract = {Voice-driven services (VDS) are being used in a variety of applications ranging from smart home control to payments using digital assistants. The input to such services is often captured via an open voice channel, e.g., using a microphone, in an unsupervised setting. One of the key operational security requirements in such setting is the freshness of the input speech. We present AEOLUS, a security overlay that proactively embeds a dynamic acoustic nonce at the time of user interaction, and detects the presence of the embedded nonce in the recorded speech to ensure freshness. We demonstrate that acoustic nonce can (i) be reliably embedded and retrieved, and (ii) be non-disruptive (and even imperceptible) to a VDS user. Optimal parameters (acoustic nonce’s operating frequency, amplitude, and bitrate) are determined for (i) and (ii) from a practical perspective. Experimental results show that AEOLUS yields 0.5% FRR at 0% FAR for speech re-use prevention upto a distance of 4 meters in three real-world environments with different background noise levels. We also conduct a user study with 120 participants, which shows that the acoustic nonce does not degrade overall user experience for 94.16% of speech samples, on average, in these environments. AEOLUS can therefore be used in practice to prevent speech re-use and ensure the freshness of speech input.},
booktitle = {Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {282–295},
numpages = {14},
keywords = {nonce embedding, voice-driven service, replay attacks, voice assistant security},
location = {San Sebastian, Spain},
series = {RAID '21}
}

@inproceedings{10.1145/3560905.3568297,
author = {Pasandi, Hannaneh B. and Pasandi, Haniyeh B.},
title = {Evaluation of ASR Systems for Conversational Speech: A Linguistic Perspective},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568297},
doi = {10.1145/3560905.3568297},
abstract = {Automatic speech recognition (ASR) meets more informal and free-form input data as voice user interfaces and conversational agents such as the voice assistants such as Alexa, Google Home, etc., gain popularity. Conversational speech is both the most difficult and environmentally relevant sort of data for speech recognition. In this paper, we take a linguistic perspective, and take the French language as a case study toward disambiguation of the French homophones. Our contribution aims to provide more insight into human speech transcription accuracy in conditions to reproduce those of state-of-the-art ASR systems, although in a much focused situation. We investigate a case study involving the most common errors encountered in the automatic transcription of French language.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {962–965},
numpages = {4},
keywords = {ASR systems, homophones, voice assistant, natural language processing},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@inproceedings{10.1145/3570991.3571068,
author = {Maheshwari, Ayush and Ravindran, Ajay and Subramanian, Venkatapathy and Ramakrishnan, Ganesh},
title = {UDAAN - Machine Learning Based Post-Editing Tool for Document Translation},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571068},
doi = {10.1145/3570991.3571068},
abstract = {We introduce UDAAN, an open-source post-editing tool that can reduce manual editing efforts to quickly produce publishable-standard documents in several Indic languages. UDAAN has an end-to-end Machine Translation (MT) plus post-editing pipeline wherein users can upload a document to obtain raw MT output. Further, users can edit the raw translations using our tool. UDAAN offers several advantages: i. Domain-aware, vocabulary-based lexical constrained MT. ii. source-target and target-target lexicon suggestions for users. Replacements are based on the source and target texts’ lexicon alignment. iii. Translation suggestions are based on logs created during user interaction. iv. Source-target sentence alignment visualisation that reduces the cognitive load of users during editing. v. Translated outputs from our tool are available in multiple formats: docs, latex, and PDF. We also provide the facility to use around 100 in-domain dictionaries for lexicon-aware machine translation. Although we limit our experiments to English-to-Hindi translation, our tool is independent of the source and target languages. Experimental results based on the usage of the tools and users’ feedback show that our tool speeds up the translation time by approximately a factor of three compared to the baseline method of translating documents from scratch. Our tool is available for both Windows and Linux platforms. The tool is open-source under MIT license, and the source code can be accessed from our website, https://www.udaanproject.org. Demonstration and tutorial videos for various features of our tool can be accessed here. Our MT pipeline can be accessed at https://udaaniitb.aicte-india.org/udaan/translate/.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {263–267},
numpages = {5},
keywords = {document translation, machine translation, post-editing software},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@article{10.1109/TASLP.2018.2855968,
author = {Wu, Shuangzhi and Zhang, Dongdong and Zhang, Zhirui and Yang, Nan and Li, Mu and Zhou, Ming},
title = {Dependency-to-Dependency Neural Machine Translation},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2855968},
doi = {10.1109/TASLP.2018.2855968},
abstract = {Recent research has proven that syntactic knowledge is effective to improve the performance of neural machine translation NMT. Most previous work focuses on leveraging either source or target syntax in the recurrent neural network RNN based encoder–decoder model. In this paper, we simultaneously use both source and target dependency tree to improve the NMT model. First, we propose a simple but effective syntax-aware encoder to incorporate source dependency tree into NMT. The new encoder enriches each source state with dependence relations from the tree. Then, we propose a novel sequence-to-dependence framework. In this framework, the target translation and its corresponding dependence tree are jointly constructed and modeled. During decoding, the tree structure is used as context to facilitate word generations. Finally, we extend the sequence-to-dependence framework with the syntax-aware encoder to build a dependence-NMT model and apply the dependence-based framework to the Transformer. Experimental results on several translation tasks show that both source and target dependence structures can improve the translation quality and their effects can be accumulated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2132–2141},
numpages = {10}
}

@inproceedings{10.1145/3360901.3364423,
author = {Moussallem, Diego and Ngonga Ngomo, Axel-Cyrille and Buitelaar, Paul and Arcan, Mihael},
title = {Utilizing Knowledge Graphs for Neural Machine Translation Augmentation},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364423},
doi = {10.1145/3360901.3364423},
abstract = {While neural networks have led to substantial progress in machine translation, their success depends heavily on large amounts of training data. However, parallel training corpora are not always readily available. Moreover, out-of-vocabulary words---mostly entities and terminological expressions---pose a difficult challenge to Neural Machine Translation systems. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as external knowledge injection. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to better translation quality. We investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. Additionally, we examine the effectiveness of our augmented models on domain-specific texts and ontologies. Our knowledge-graph-augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2015 and 2018 for the WMT English-German translation task.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {139–146},
numpages = {8},
keywords = {nlp, knowledge graphs, neural machine translation, linked data},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@article{10.1145/3448216,
author = {Li, Yachao and Jiang, Jing and Yangji, Jia and Ma, Ning},
title = {Finding Better Subwords for Tibetan Neural Machine Translation},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3448216},
doi = {10.1145/3448216},
abstract = {Subword segmentation plays an important role in Tibetan neural machine translation (NMT). The structure of Tibetan words consists of two levels. First, words consist of a sequence of syllables, and then a syllable consists of a sequence of characters. According to this special word structure, we propose two methods for Tibetan subword segmentation, namely syllable-based and character-based methods. The former generates subwords based on the Tibetan syllables, and the latter is based on Tibetan characters. In addition, we carry out experiments with these two subword segmentation methods on low-resource Tibetan-to-Chinese NMT, respectively. The experimental results show that both of them can improve translation performance, in which the subword segmentation based on character sequences can achieve better results. Overall, our proposed character-based subword segmentation is more simple and effective. Moreover, it can achieve better experimental results without paying much attention to the linguistic features of Tibetan.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {24},
numpages = {11},
keywords = {low resource, Neural machine translation, Tibetan, subword}
}

@inproceedings{10.1145/3462244.3479893,
author = {Rueben, Matthew and Syed, Mohammad and London, Emily and Camarena, Mark and Shin, Eunsook and Zhang, Yulun and Wang, Timothy S. and Groechel, Thomas R. and Lee, Rhianna and Matari\'{c}, Maja J.},
title = {Long-Term, in-the-Wild Study of Feedback about Speech Intelligibility for K-12 Students Attending Class via a Telepresence Robot},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479893},
doi = {10.1145/3462244.3479893},
abstract = {Telepresence robots offer presence, embodiment, and mobility to remote users, making them promising options for homebound K-12 students. It is difficult, however, for robot operators to know how well they are being heard in remote and noisy classroom environments. One solution is to estimate the operator’s speech intelligibility to their listeners in order to provide feedback about it to the operator. This work contributes the first evaluation of a speech intelligibility feedback system for homebound K-12 students attending class remotely. In our four long-term, in-the-wild deployments we found that students speak at different volumes instead of adjusting the robot’s volume, and that detailed audio calibration and network latency feedback are needed. We also contribute the first findings about the types and frequencies of multimodal comprehension cues given to homebound students by listeners in the classroom. By annotating and categorizing over 700 cues, we found that the most common cue modalities were conversation turn timing and verbal content. Conversation turn timing cues occurred more frequently overall, whereas verbal content cues contained more information and might be the most frequent modality for negative cues. Our work provides recommendations for telepresence systems that could intervene to ensure that remote users are being heard.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {567–576},
numpages = {10},
keywords = {spoken dialogue systems, K-12 education, mobile remote presence, telepresence},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3388767.3407339,
author = {Edwards, Pif and Landreth, Chris and Pop\l{}awski, Mateusz and Malinowski, Robert and Watling, Sarah and Fiume, Eugene and Singh, Karan},
title = {JALI-Driven Expressive Facial Animation and Multilingual Speech in Cyberpunk 2077},
year = {2020},
isbn = {9781450379717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388767.3407339},
doi = {10.1145/3388767.3407339},
abstract = {Cyberpunk 2077 is a highly anticipated massive open-world video game, with a complex, branching narrative. This talk details new research and innovative workflow contributions, developed by jali, toward the generation of an unprecedented number of hours of realistic, expressive speech animation in ten languages, often with multiple languages interleaved within individual sentences. The speech animation workflow is largely automatic but remains under animator control, using a combination of audio and tagged text transcripts. We use insights from anatomy, perception, and the psycho-linguistic literature to develop independent and combined language models that drive procedural animation of the mouth and paralingual (speech supportive non-verbal expression) motion of the neck, brows and eyes. Directorial tags in the speech transcript further enable the integration of performance capture driven facial emotion. The entire workflow is animator-centric, allowing efficient key-frame customization and editing of the resulting facial animation on any typical facs-like face rig. The talk will focus equally on technical contributions and its integration and creative use within the animation pipeline of the highly anticipated aaa game title: Cyberpunk 2077.},
booktitle = {ACM SIGGRAPH 2020 Talks},
articleno = {60},
numpages = {2},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@inproceedings{10.1145/3428690.3429153,
author = {Slimi, Anwer and Hamroun, Mohamed and Zrigui, Mounir and Nicolas, Henri},
title = {Emotion Recognition from Speech Using Spectrograms and Shallow Neural Networks},
year = {2021},
isbn = {9781450389242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428690.3429153},
doi = {10.1145/3428690.3429153},
abstract = {Emotions add meaning to the conversation between individuals and allow us to better understand each other. Besides, Human-Computer interaction has produced significant changes in recent years to satisfy the requirements and responsibilities of clients. From this angle, it would be perfect for machines to automatically recognize human feelings in order to improve communication and interaction both parts. Some of the works in this field are based on hand-crafted features and others are based on Deep Learning (DL) models. In this article, we will propose a SER (Speech Emotion Recognition) system in which we will combine the power of DL models in self pattern recognition together with the ability of working on small databases.},
booktitle = {Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {35–39},
numpages = {5},
keywords = {spectrograms, neural networks, emotion classification},
location = {Chiang Mai, Thailand},
series = {MoMM '20}
}

@inproceedings{10.1145/3510858.3510865,
author = {Xiao, Ting},
title = {Research on Overcoming about Language Barriers of AI Machine Translation Replacing Interpreting under Information Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510865},
doi = {10.1145/3510858.3510865},
abstract = {Today, as new technologies continue to iterate, it seems that everything has been given the AI soul. However, despite the rapid development of artificial intelligence, there are still many shortcomings, especially in terms of interpreting The purpose of this paper is to analyze the language barriers that AI machine translation needs to overcome to replace interpreting. Through the literature review method, case analysis method, experience summary method, comparative analysis and other research methods, this paper proposes the language barriers that AI machine translation needs to overcome to analyze. The reason why these obstacles are difficult to break through. By analyzing the impact of AI machine translation on the interpreting industry, some suggestions were given to interpreters, and through questionnaires, people's attitudes towards AI machine translation replacing interpreters were understood. The research results show that the difficulty of AI machine translation to replace interpreting is mainly due to the subjectivity of language, lack of understanding of humor and other emotions in big data, and difficulty in accurately recognizing speech. 38% believe that the main obstacle is the lack of emotion in AI machine translation, 28% think that the main obstacle is the subjectivity of language, and 25% think that the main obstacle is the problem of language recognition. Changes in the working environment of interpreters have put forward new and higher requirements on the capabilities and qualities of interpreters, and they need to learn and master advanced translation technologies to substantially improve the quality and efficiency of interpreting.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {12–20},
numpages = {9},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.5555/3314872.3314916,
author = {Kim, Yonghae and Kim, Hyesoon},
title = {Translating CUDA to OpenCL for Hardware Generation Using Neural Machine Translation},
year = {2019},
isbn = {9781728114361},
publisher = {IEEE Press},
abstract = {Hardware generation from high-level languages like C/C++ has been one of the dreams of software and hardware engineers for decades. Several high-level synthesis (HLS) or domain-specific languages (DSLs) have been developed to reduce the gap between high-level languages and hardware descriptive languages. However, each language tends to target some specific applications or there is a big learning curve in learning DSLs, which ends up having many program languages and tool chains. To address these challenges, we propose the use of a source-to-source translation to pick and choose which framework to use so that the hardware designer chooses the best target HLS/DSL that can be synthesized to the best performing hardware. In this work, we present source-to-source translation between CUDA to OpenCL using NMT, which we call PLNMT. The contribution of our work is that it develops techniques to generate training inputs. To generate a training dataset, we extract CUDA API usages from CUDA examples and write corresponding OpenCL API usages. With a pair of API usages acquired, we construct API usage trees that helps users find unseen usages from new samples and easily add them to a training input. Our initial results show that we can translate many applications from benchmarks such as CUDA SDK, polybench-gpu, and Rodinia. Furthermore, we show that translated kernel code from CUDA applications can be run in the OpenCL FPGA framework, which implies a new direction of HLS.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {285–286},
numpages = {2},
keywords = {Neural Machine Translation, Program Translator, High-level Synthesis},
location = {Washington, DC, USA},
series = {CGO 2019}
}

@article{10.1109/TASLP.2019.2921423,
author = {Wang, Yijun and Xia, Yingce and Zhao, Li and Bian, Jiang and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
title = {Semi-Supervised Neural Machine Translation via Marginal Distribution Estimation},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921423},
doi = {10.1109/TASLP.2019.2921423},
abstract = {Neural machine translation NMT heavily relies on parallel bilingual corpora for training. Since large-scale, high-quality parallel corpora are usually costly to collect, it is appealing to exploit monolingual corpora to improve NMT. Inspired by the law of total probability, which connects the probability of a given target-side monolingual sentence to the conditional probability of translating from a source sentence to the target one, we propose to explicitly exploit this connection and help the training procedure of NMT models using monolingual data. The key technical challenge of this approach is that there are exponentially many source sentences for a target monolingual sentence while computing the sum of the conditional probability given each possible source sentence. We address this challenge by leveraging the reverse translation model target-to-source translation model to sample several mostly likely source-side sentences and avoid enumerating all possible candidate source sentences. Then we propose two different methods to leverage the law of total probability, including marginal distribution regularization and likelihood maximization of monolingual corpora. Experiment results on English$rightarrow$French and German$rightarrow$English tasks demonstrate that our methods achieve significant improvement over several strong baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1564–1576},
numpages = {13}
}

@article{10.1145/3442695,
author = {Al-Thanyyan, Suha S. and Azmi, Aqil M.},
title = {Automated Text Simplification: A Survey},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442695},
doi = {10.1145/3442695},
abstract = {Text simplification (TS) reduces the complexity of the text to improve its readability and understandability, while possibly retaining its original information content. Over time, TS has become an essential tool in helping those with low literacy levels, non-native learners, and those struggling with various types of reading comprehension problems. In addition, it is used in a preprocessing stage to enhance other NLP tasks. This survey presents an extensive study of current research studies in the field of TS, as well as covering resources, corpora, and evaluation methods that have been used in those studies.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {43},
numpages = {36},
keywords = {syntactic simplification, lexical simplification, survey, Text simplification, monolingual machine translation}
}

@inproceedings{10.1145/3510858.3510898,
author = {Tong, Qiang},
title = {Optimization of English Machine Translation Algorithm Based on Internet Information Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510898},
doi = {10.1145/3510858.3510898},
abstract = {Machine translation English technology uses computers to translate one natural language into another, and quickly convert between different languages. The translation efficiency of English automatic translation technology is low. In order to meet the needs of real-time application rather than people's expectations, the process of generating correct translation in English translation system is inseparable from the participation of translators. In this case, researchers turn their attention from automatic English translation to computer-aided English translation. The system provides translators with useful translation and auxiliary tools to help them review the translation. The interaction between the machine translation English system and the translator can guide the computer to decode and improve the quality of the output translation, so as to realize the combination of the high efficiency of the English translation system and the high accuracy of the translator. The current basic IMT method takes Internet information technology as an example to expand the scope of candidate translation elements, and designs an interactive interface based on the translator's cognitive translation process to improve the post-translation process. This paper introduces the research situation and subject source of relevant researchers in the field of interactive machine translation English in detail. The research results show that this method is effective. More computer experts and scholars also study machine translation English system and devote themselves to the research of Internet machine translation. Compared with the traditional translation methods, it improves the work efficiency.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {100–104},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3313808.3313811,
author = {Cota, Emilio G. and Carloni, Luca P.},
title = {Cross-ISA Machine Instrumentation Using Fast and Scalable Dynamic Binary Translation},
year = {2019},
isbn = {9781450360203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313808.3313811},
doi = {10.1145/3313808.3313811},
abstract = {The rise in instruction set architecture (ISA) diversity and the growing adoption of virtual machines are driving a need for fast, scalable, full-system, cross-ISA emulation and instrumentation tools. Unfortunately, achieving high performance for these cross-ISA tools is challenging due to dynamic binary translation (DBT) overhead and the complexity of instrumenting full-system emulators. In this paper we improve cross-ISA emulation and instrumentation performance through three novel techniques. First, we increase floating point (FP) emulation performance by observing that most FP operations can be correctly emulated by surrounding the use of the host FP unit with a minimal amount of non-FP code. Second, we introduce the design of a translator with a shared code cache that scales for multi-core guests, even when they generate translated code in parallel at a high rate. Third, we present an ISA-agnostic instrumentation layer that can instrument guest operations that occur outside of the DBT’s intermediate representation (IR), which are common in full-system emulators. We implement our approach in Qelt, a high-performance cross-ISA machine emulator and instrumentation tool based on QEMU. Our results show that Qelt scales to 32 cores when emulating a guest machine used for parallel compilation, which demonstrates scalable code translation. Furthermore, experiments based on SPEC06 show that Qelt (1) outperforms QEMU as a full-system cross-ISA machine emulator by 1.76\texttimes{}/2.18\texttimes{} for integer/FP workloads, (2) outperforms state-of-the-art, cross-ISA, full-system instrumentation tools by 1.5\texttimes{}-3\texttimes{}, and (3) can match the performance of Pin, a state-of-the-art, same-ISA DBI tool, when used for complex instrumentation such as cache simulation.},
booktitle = {Proceedings of the 15th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {74–87},
numpages = {14},
keywords = {Dynamic Binary Translation, Scalability, Binary Instrumentation, Floating Point, Machine Emulation},
location = {Providence, RI, USA},
series = {VEE 2019}
}

@inproceedings{10.1145/3430984.3431026,
author = {Philip, Jerin and Siripragada, Shashank and Namboodiri, Vinay P. and Jawahar, C. V.},
title = {Revisiting Low Resource Status of Indian Languages in Machine Translation},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431026},
doi = {10.1145/3430984.3431026},
abstract = {Indian language machine translation performance is hampered due to the lack of large scale multi-lingual sentence aligned corpora and robust benchmarks. Through this paper, we provide and analyse an automated framework to obtain such a corpus for Indian language neural machine translation (NMT) systems. Our pipeline consists of a baseline NMT system, a retrieval module, and an alignment module that is used to work with publicly available websites such as press releases by the government. The main contribution towards this effort is to obtain an incremental method that uses the above pipeline to iteratively improve the size of the corpus as well as improve each of the components of our system. Through our work, we also evaluate the design choices such as the choice of pivoting language and the effect of iterative incremental increase in corpus size. Our work in addition to providing an automated framework also results in generating a relatively larger corpus as compared to existing corpora that are available for Indian languages. This corpus helps us obtain substantially improved results on the publicly available WAT evaluation benchmark and other standard evaluation benchmarks.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
pages = {178–187},
numpages = {10},
keywords = {machine translation, parallel corpus, information retrieval},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@article{10.1145/3527664,
author = {Chen, Qi and Kwong, Oi Yee and Li, Yinqiao and Xiao, Tong and Zhu, Jingbo},
title = {Coarse-to-Fine Output Predictions for Efficient Decoding in Neural Machine Translation},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3527664},
doi = {10.1145/3527664},
abstract = {Neural Machine Translation (NMT) systems are undesirably slow as the decoder often has to compute probability distributions over large target vocabularies. In this work, we propose a coarse-to-fine approach to reduce the complexity of the decoding process, using only the information of the weight matrix in the Softmax layer. The large target vocabulary is first trimmed to a small candidate set in the coarse-grained phase, and from this candidate set the final top-k results are generated in the fine-grained phase. Tested on an RNN-based NMT system and a Transformer-based NMT system separately, our GPU-friendly method achieved a significant speed-up without harming the translation quality.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {135},
numpages = {12},
keywords = {softmax optimization, Neural networks, acceleration, machine translation}
}

@article{10.1145/3365916,
author = {Kim, Hyun and Na, Seung-Hoon},
title = {Uniformly Interpolated Balancing for Robust Prediction in Translation Quality Estimation: A Case Study of English-Korean Translation},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3365916},
doi = {10.1145/3365916},
abstract = {There has been growing interest among researchers in quality estimation (QE), which attempts to automatically predict the quality of machine translation (MT) outputs. Most existing works on QE are based on supervised approaches using quality-annotated training data. However, QE training data quality scores readily become imbalanced or skewed: QE data are mostly composed of high translation quality sentence pairs but the data lack low translation quality sentence pairs. The use of imbalanced data with an induced quality estimator tends to produce biased translation quality scores with “high” translation quality scores assigned even to poorly translated sentences. To address the data imbalance, this article proposes a simple, efficient procedure called uniformly interpolated balancing to construct more balanced QE training data by inserting greater uniformness to training data. The proposed uniformly interpolated balancing procedure is based on the preparation of two different types of manually annotated QE data: (1) default skewed data and (2) near-uniform data. First, we obtain default skewed data in a naive manner without considering the imbalance by manually annotating qualities on MT outputs. Second, we obtain near-uniform data in a selective manner by manually annotating a subset only, which is selected from the automatically quality-estimated sentence pairs. Finally, we create uniformly interpolated balanced data by combining these two types of data, where one half originates from the default skewed data and the other half originates from the near-uniform data. We expect that uniformly interpolated balancing reflects the intrinsic skewness of the true quality distribution and manages the imbalance problem. Experimental results on an English-Korean quality estimation task show that the proposed uniformly interpolated balancing leads to robustness on both skewed and uniformly distributed quality test sets when compared to the test sets of other non-balanced datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {37},
numpages = {27},
keywords = {Translation quality estimation, uniformly interpolated balancing, imbalanced data, Predictor-Estimator}
}

@inproceedings{10.1145/3342558.3345423,
author = {Lins, Rafael Dueire and Oliveira, Hilario and Cabral, Luciano and Batista, Jamilson and Tenorio, Bruno and Salcedo, Diego A. and Ferreira, Rafael and Lima, Rinaldo and de Fran\c{c}a Pereira e Silva, Gabriel and Simske, Steven J.},
title = {The CNN-Corpus in Spanish: A Large Corpus for Extractive Text Summarization in the Spanish Language},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345423},
doi = {10.1145/3342558.3345423},
abstract = {This paper details the development and features of the CNN-corpus in Spanish, possibly the largest test corpus for single document extractive text summarization in the Spanish language. Its current version encompasses 1,117 well-written texts in Spanish, each of them has an abstractive and an extractive summary. The development methodology adopted allows good-quality qualitative and quantitative assessments of summarization strategies for tools developed in the Spanish language.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {38},
numpages = {4},
keywords = {Extractive Summarization, Spanish, CNN Corpus, Multi-language Summarization, Single-document Summarization},
location = {Berlin, Germany},
series = {DocEng '19}
}

@article{10.1145/3580495,
author = {Jha, Piyush and Kumar, Rashi and Sahula, Vineet},
title = {Filtering and Extended Vocabulary Based Translation for Low-Resource Language Pair of Sanskrit-Hindi},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3580495},
doi = {10.1145/3580495},
abstract = {Neural Machine Translation (NMT) is widely employed for language translation tasks because it performs better than the conventional statistical and phrase-based approaches. However, NMT techniques involve challenges, such as requiring a large and clean corpus of parallel data and the inability to deal with rare words. They need to be faster for real-time applications. More work needs to be done using NMT to address the challenges in translating Sanskrit, one of the oldest and rich languages known to the world, with its morphological richness and limited multilingual parallel corpus. There is usually no similar data between a language pair; hence, no application exists so far that can translate Sanskrit to/from other languages. This study presents an in-depth analysis to address these challenges with the help of a low-resource Sanskrit-Hindi language pair. We employ a novel training corpus filtering with extended vocabulary in a zero-shot transformer architecture. The structure of the Sanskrit language is thoroughly investigated to justify the use of each step. Furthermore, the proposed method is analyzed based on variations in sentence length and also applied to a high-resource language pair in order to demonstrate its efficacy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {120},
numpages = {15},
keywords = {neural machine translation, filtering, Hindi, low resource, Sanskrit, Zero-shot, transformers, extended vocabulary}
}

@article{10.1145/3588569,
author = {Wang, Zhiguo and Na, Hongwei},
title = {Multimedia Technology Based Interactive Translation Learning for Students},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588569},
doi = {10.1145/3588569},
abstract = {Multimedia technology incorporates into the educational arena to translate traditional educational material into interactive digital mode. This has permitted teachers into the learning environment to design and integrate interactive multimedia learning. Many problems towards giving more attention to students facing teacher's multimedia levels are uneven, and their integration significance, bringing enormous curriculum learning strategies, is not entirely apparent. This research introduces multimedia network interpretation teaching using machine learning (MNIT-ML)in multimedia education design to enhance and strengthen the traditional teaching process and promote various modern approaches to transmit knowledge towards students.Allocation of learning resources (ALR) framework is mainly to enlarge the use and utilization of materials for learning activities from impressed resources into recordings, video content, motion graphics, and other forms of resources. The purpose of the Allocation of Learning Resources (ALR) framework is to aid teachers in making sound decisions about how to distribute available educational materials. Its goal is to guide teachers in making smart choices about how to use limited resources like time, money, and technology in order to improve students' educational achievements.The ALR framework was picked because it offers a methodical strategy for selecting choices that is founded in educational research and best practices. Resource allocation is a key aspect in influencing student outcomes, and efficient allocation can help guarantee that all students have access to the tools they need to succeed.There are a number of alternative frameworks that might direct studies on the distribution of educational resources. The Technological Acceptance Model (TAM) is a common tool for analysing what factors lead to widespread implementation of educational technology.The goal of other frameworks like the Universal Design for Learning (UDL) framework is to help educators create lessons and methods that are inclusive of students of all backgrounds and abilities.The final decision on which framework to use will be determined by the nature of the research issues and the setting in which they are being investigated. In order to make educated decisions, it is crucial to pick a framework that is appropriate for the research issues at hand and that gives a systematic approach based on established educational best practices. The digital promoting resources for teaching (DPRT) method creates a real environment for students to learn, focusing on enhancing training to make students feel the standard language translation skills. The simulation analysis is performed based on security 94.6%, the performance of 95.9%, and privacy, proving the proposed framework's reliability overall ratio of 93.4%.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {Multimedia Technology, Learning, Interactive Translation}
}

@inproceedings{10.1145/3512527.3531386,
author = {Peng, Ru and Zeng, Yawen and Zhao, Junbo},
title = {HybridVocab: Towards Multi-Modal Machine Translation via Multi-Aspect Alignment},
year = {2022},
isbn = {9781450392389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512527.3531386},
doi = {10.1145/3512527.3531386},
abstract = {Multi-modal machine translation (MMT) aims to augment the linguistic machine translation frameworks by incorporating aligned vision information. As the core research challenge for MMT, how to fuse the image information and further align it with the bilingual data remains critical. Existing works have either focused on a methodological alignment in the space of bilingual text or emphasized the combination of the one-sided text and given image. In this work, we entertain the possibility of a triplet alignment, among the source and target text together with the image instance. In particular, we propose Multi-aspect AlignmenT (MAT) model that augments the MMT tasks to three sub-tasks --- namely cross-language translation alignment, cross-modal captioning alignment and multi-modal hybrid alignment tasks. Core to this model consists of a hybrid vocabulary which compiles the visually depictable entity (nouns) occurrence on both sides of the text as well as the detected object labels appearing in the images. Through this sub-task, we postulate that MAT manages to further align the modalities by casting three instances into a shared domain, as compared against previously proposed methods. Extensive experiments and analyses demonstrate the superiority of our approaches, which achieve several state-of-the-art results on two benchmark datasets of the MMT task.},
booktitle = {Proceedings of the 2022 International Conference on Multimedia Retrieval},
pages = {380–388},
numpages = {9},
keywords = {hybrid vocabulary, multi-aspect alignment, multi-modal machine translation},
location = {Newark, NJ, USA},
series = {ICMR '22}
}

@article{10.1145/3589640,
author = {Wang, Zhiguo and Ma, Chunxiao},
title = {Research on Korean Translation in the Context of Epidemic Prevention and Control},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3589640},
doi = {10.1145/3589640},
abstract = {An emergency like COVID-19 requires a theoretical framework for policy implementation that involves public and private sector collaborations. After policy failures, new institutions have formed that trigger PPP's later, allowing the incumbent administration to continue in office longer. It focuses on novel approaches to dealing with pandemics. The present administration put these rules in place to keep COVID-19 under control. When it comes to Real Time - polymerase chain reaction (RT-PCR) testing, South Korea's government and corporations partnered to swiftly raise the quantity of testing in the country. Models of policy change are shown to be dynamic, cyclical, and recursive. During the COVID-19 outbreak in South Korea, an empirical content research was conducted. Even though South Korea's leader was at risk of losing public support to the point where impeachment was mentioned as a possible option, he dramatically reversed public mood to win general elections by a wide margin in April 2020, while the pandemic scenario persisted. To win reelection, democratic administrations are under more pressure to effectively perform crisis management when faced with a crisis. As a result, they are under even more pressure to immediately mobilize public and private resources. The emergency use authorization (EUA) protocol for test kits is an example of "leapfrogging actors" – up-and-coming innovators – who helped turn a pandemic tragedy into a possibility for sustained leadership and for them. The results based on infected premises culling rate ratio is 82.3%, number of measles cases report is 86.4%, spread and epidemic ratio is 84.2%, important of epidemiology is 89.35%, transmission potential of COVID-19 is 91.24% and illustration of epidemic control is 92.45. The results based on infected premises culling rate ratio is 82.3%, number of measles cases report is 86.4%, spread and epidemic ratio is 84.2%, important of epidemiology is 89.35%, transmission potential of COVID-19 is 91.24% and illustration of epidemic control is 92.45.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {Korean, machine translation, speaking, epidemics}
}

@inproceedings{10.1145/3594806.3596567,
author = {Balaskas, Georgios and Papadopoulos, Homer and Loisel, Quentin and Pappa, Dimitra and Efthymoglou, George and Chastin, Sebastien},
title = {An End-to-End System for Transcription, Translation, and Summarization to Support the Co-Creation Process. A Health CASCADE Study.},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596567},
doi = {10.1145/3594806.3596567},
abstract = {This paper presents a web service and a deep learning (DL) pipeline that has been developed and implemented as part of the MSCA Health CASCADE project. The purpose of the web service is to provide support, streamline, and enable participatory methods, such as co-creation, in the public health domain. The DL pipeline assists with translation, transcription, speaker diarization, relation extraction, and summarization of audio recordings. This is achieved by removing the need for time-consuming tasks, such as translating and transcribing audio recordings. Additional value is created by extracting implicit relations from the transcribed text and identifying patterns, key themes, and trends. Finally, providing summaries of the transcripts creates a sense of ownership that can improve stakeholder retention in participatory methods.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {625–631},
numpages = {7},
keywords = {relation extraction, transcription, co-creation, speech-to-text translation, natural language processing, dialogue summarization, public health, speaker diarization},
location = {Corfu, Greece},
series = {PETRA '23}
}

@article{10.1109/TASLP.2021.3138719,
author = {Zhao, Yuting and Komachi, Mamoru and Kajiwara, Tomoyuki and Chu, Chenhui},
title = {Word-Region Alignment-Guided Multimodal Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138719},
doi = {10.1109/TASLP.2021.3138719},
abstract = {We propose word-region alignment-guided multimodal neural machine translation (MNMT), a novel model for MNMT that links the semantic correlation between textual and visual modalities using word-region alignment (WRA). Existing studies on MNMT have mainly focused on the effect of integrating visual and textual modalities. However, they do not leverage the semantic relevance between the two modalities. We advance the semantic correlation between textual and visual modalities in MNMT by incorporating WRA as a bridge. This proposal has been implemented on two mainstream architectures of neural machine translation (NMT): the recurrent neural network (RNN) and the transformer. Experiments on two public benchmarks, English–German and English–French translation tasks using the Multi30k dataset and English–Japanese translation tasks using the Flickr30kEnt-JP dataset prove that our model has a significant improvement with respect to the competitive baselines across different evaluation metrics and outperforms most of the existing MNMT models. For example, 1.0 BLEU scores are improved for the English–German task and 1.1 BLEU scores are improved for the English–French task on the Multi30k test2016 set; and 0.7 BLEU scores are improved for the English–Japanese task on the Flickr30kEnt-JP test set. Further analysis demonstrates that our model can achieve better translation performance by integrating WRA, leading to better visual information use.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {244–259},
numpages = {16}
}

@inproceedings{10.1145/3587103.3594160,
author = {Russell, Se\'{a}n and Alaofi, Suad and Alshaigy, Bedour},
title = {Translate Together: Managed Translation and Peer-Review},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594160},
doi = {10.1145/3587103.3594160},
abstract = {This paper describes a tool for managing peer-reviewed translation activities within CS1 classes. The tool is designed to help students develop a better understanding of terms within class by writing and reviewing translations of those terms and their descriptions in their native language. The peer-review of the translations means that lecturers do not need to share the language of their students.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {595–596},
numpages = {2},
keywords = {ESL, translation, peer review},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3362789.3362878,
author = {Buend\'{\i}a, F\'{e}lix and Gayoso-Cabada, Joaquin and Juanes-M\'{e}ndez, Juan-Antonio and Mart\'{\i}n-Izquierdo, Manuela and Sierra, Jos\'{e}-Luis},
title = {Cataloguing Spanish Medical Reports with UMLS Terms},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362878},
doi = {10.1145/3362789.3362878},
abstract = {UMLS (Unified Medical Language System) is one of the most comprehensive terminological resources for the medical domain. Thus, the provision of instruments to assist in the cataloguing of medical reports with UMLS is of utmost relevance, especially when these reports are written in unstructured free-text natural language. For this purpose, it is possible to use tools that, like MetaMap, enable the automatic annotation of clinical texts with UMLS terms. However, these tools typically work on reports written in English, which seriously hinders their applicability to other languages. In this paper, we describe an approach to mitigate these shortcomings, which pipelines state-of-the-art language translation services with automatic mapping tools. We demonstrate the feasibility of the approach by combining Google Translate with MetaMap and by using the resulting pipeline to catalog, with UMLS, a representative set of Spanish-written X-Ray Thorax reports corresponding to images taken from the Indiana Chest X-ray radiology corpus. The resulting cataloguing is not significantly different, in quality, from that obtained through the direct application of MetaMap on a similar set of reports written in English and selected from this Indiana Chest X-ray corpus.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {423–430},
numpages = {8},
keywords = {Medical knowledge, MetaMap, UMLS, Clinical Reports, Google Translate},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@article{10.5555/3447051.3447054,
author = {Garcia, Caitlin and Jayaweera, Kaylee-Anna and Vinlove, Quinn and Gado, Kris and Mache, Jens and Weiss, Richard},
title = {Using Sentiment Analysis to Highlight the Discrepancy between High and Low Resource Language Translations},
year = {2020},
issue_date = {October 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {36},
number = {1},
issn = {1937-4771},
abstract = {Given the enormous cost of cybercrimes each year, many cybersecurity researchers are working to create an automated threat detection system, especially one that can accurately crawl non-English forums and markets. While text classification techniques involving sentiment analysis are fairly successful in English settings, these efforts are ultimately depreciated in non-English platforms. Translation efforts fail to acknowledge the semantic qualities of different languages, especially languages with relatively few bodies of text corpora. To fix these shortcomings and improve current threat detection techniques, it is important to first understand by what degree current methods are failing. In this preliminary study, we highlight the discrepancies of translation quality across three different languages with varying degrees of resourcefulness.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {21–25},
numpages = {5}
}

@inproceedings{10.1145/3330430.3333622,
author = {Pardos, Zachary A. and Chau, Hung and Zhao, Haocheng},
title = {Data-Assistive Course-to-Course Articulation Using Machine Translation},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333622},
doi = {10.1145/3330430.3333622},
abstract = {Higher education at scale, such as in the California public post-secondary system, has promoted upward socioeconomic mobility by supporting student transfer from 2-year community colleges to 4-year degree granting universities. Among the barriers to transfer is earning enough credit at 2-year institutions that qualify for the transfer credit required by 4-year degree programs. Defining which course at one institution will count as credit for an equivalent course at another institution is called course articulation, and it is an intractable task when attempting to manually articulate every set of courses at every institution with one another. In this paper, we present a methodology towards making tractable this process of defining and maintaining articulations by leveraging the information contained within historic enrollment patterns and course catalog descriptions. We provide a proof-of-concept analysis using data from a 4-year and 2-year institution to predict articulation pairs between them, produced from machine translation models and validated by a set of 65 institutionally pre-established course-to-course articulations. Finally, we create a report of proposed articulations for consumption by the institutions and close with a discussion of limitations and the challenges to adoption.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {22},
numpages = {10},
keywords = {credit mobility, course-to-course articulation, enrollment data, machine translation, Higher education},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@article{10.1145/3369869,
author = {Plaza-Del-Arco, Flor-Miriam and Molina-Gonz\'{a}lez, M. Dolores and Ure\~{n}a-L\'{o}pez, L. Alfonso and Mart\'{\i}n-Valdivia, M. Teresa},
title = {Detecting Misogyny and Xenophobia in Spanish Tweets Using Language Technologies},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3369869},
doi = {10.1145/3369869},
abstract = {Today, misogyny and xenophobia are some of the most important social problems. With the increase in the use of social media, this feeling of hatred toward women and immigrants can be more easily expressed, and therefore it can have harmful effects on social media users. For this reason, it is important to develop systems capable of detecting hateful comments automatically. In this article, we analyze the hate speech in Spanish tweets against women and immigrants conducting classification experiments using different approaches. Moreover, we create appropriate language resources for hate speech detection in Spanish.},
journal = {ACM Trans. Internet Technol.},
month = {mar},
articleno = {12},
numpages = {19},
keywords = {xenophobia detection, machine learning, classifier ensemble, Misogyny detection, social media, hate speech classification, text mining, lexicon}
}

@article{10.1145/3616867,
author = {Safdar, Kamal and Nisar, Shibli and Iqbal, Waseem and Ahmad, Awais and Bangash, Yawar Abbas},
title = {Demographical Based Sentiment Analysis for Detection of Hate Speech Tweets for Low Resource Language},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616867},
doi = {10.1145/3616867},
abstract = {Advancement in IT and communication technology provides the opportunity for social media users to communicate their ideas and thoughts across the globe within no time as well big data promulgated in a result of the communication process itself has immense challenges. Recently, the provision of freedom of speech has witnessed immense promulgation of offensive and hate speech content on the internet aimed the basic human rights violation. The detection of abusive content on social media for rich resource language has become a hot area for researchers in the recent past. However, low-resource languages are underprivileged due to the non-availability of large corpus and its complexity to understand. The proposed methodology mainly has two parts. One is to detect abusive content and the other is to have a demographical analysis of the Indigenously developed dataset. The process starts with the development of a unique unlabeled Urdu dataset of 0.2 M from Twitter through a web scrapper tool named snscraper. The dataset is collected against the 36 districts of Punjab from Pakistan and from the duration 2018- Apr 2022. The dataset is labeled into three target classes Neutral, Offensive, and Hate Speech. After data cleaning, the feature extraction process is achieved with the help of traditional techniques such as Bow and tf-idf with the combination of word and char n-gram and word embedding word2Vec. The dataset is trained on both machine learning algorithms SVM and Logistic regression and deep learning techniques Long Short Term Memory (LSTM) and Convolutional Neural Networks (CNN). The best F score achieved through LSTM on this dataset is 64 and accuracy is 93 through CNN algorithms. A Choropleth map is used for visualization of the dataset distributed among 36 districts of Punjab and a time series plot for time analysis covers five years duration from 2018-Apr to 22.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
keywords = {snscrapper, Choropleth map, Word Embedding, CNN, LSTM}
}

@inproceedings{10.1145/3564858.3564873,
author = {Zou, Benjin and Wei, Zhiqiao},
title = {Application and Development of Intelligent Translation Technology},
year = {2022},
isbn = {9781450396721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564858.3564873},
doi = {10.1145/3564858.3564873},
abstract = {Intelligent translation is the product of the combination of artificial intelligence technology and machine translation technology, and is the product of the development of artificial intelligence, which greatly enhances the accuracy of machine translation. Its application will be greatly expanded, not only as an intelligent auxiliary tool for human translation. It can also be used in voice services, robotics, etc. Although there are still certain limitations such as insufficient training corpus, insufficient recognition of context, lack of creativity, etc. In the future, with the continuous expansion of artificial intelligence, it will continue to overcome its weaknesses in all aspects and eventually replace human translation.},
booktitle = {Proceedings of the 5th International Conference on Information Management and Management Science},
pages = {85–89},
numpages = {5},
keywords = {Intelligent Translation, Machine Translation, Artificial Intelligence},
location = {Chengdu, China},
series = {IMMS '22}
}

@article{10.1145/3469655,
author = {Eddine, Meftah Mohammed Charaf},
title = {A New Concept of Electronic Text Based on Semantic Coding System for Machine Translation},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3469655},
doi = {10.1145/3469655},
abstract = {In the field of machine translation of texts, the ambiguity in both lexical (dictionary) and structural aspects is still one of the difficult problems. Researchers in this field use different approaches, the most important of which is machine learning in its various types. The goal of the approach that we propose in this article is to define a new concept of electronic text, which makes the electronic text free from any lexical or structural ambiguity. We used a semantic coding system that relies on attaching the original electronic text (via the text editor interface) with the meanings intended by the author. The author defines the meaning desired for each word that can be a source of ambiguity. The proposed approach in this article can be used with any type of electronic text (text processing applications, web pages, email text, etc.). Thanks to the approach that we propose and through the experiments that we have conducted using it, we can obtain a very high accuracy rate. We can say that the problem of lexical and structural ambiguity can be completely solved. With this new concept of electronic text, the text file contains not only the text but also with it the true sense of the exact meaning intended by the writer in the form of symbols. These semantic symbols are used during machine translation to obtain a translated text completely free of any lexical and structural ambiguity.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {16},
numpages = {16},
keywords = {Semantic, code, text, word, dictionary ambiguity, structural ambiguity}
}

@article{10.1109/TASLP.2018.2845111,
author = {Huang, Jizhou and Sun, Yaming and Zhang, Wei and Wang, Haifeng and Liu, Ting},
title = {Entity Highlight Generation as Statistical and Neural Machine Translation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2845111},
doi = {10.1109/TASLP.2018.2845111},
abstract = {Entity highlight refers to a short, concise, and characteristic description for an entity, which can be applied to various applications. In this article, we study the problem of automatically generating entity highlights from the descriptive sentences of entities. Specifically, we develop two computational approaches, one is inspired by the statistical machine translation SMT and another is a sequence-to-sequence learning Seq2Seq approach, which has been successfully applied in neural machine translation and neural summarization. In the Seq2Seq approach, we use attention mechanism, copy mechanism, and coverage mechanism. To generate entity-specific highlights, we also incorporate entity name into the Seq2Seq model to guide the decoding process. We automatically collect large-scale instances as training data without any manual annotation, and ask annotators to create a test set. We compare with several strong baseline methods, and evaluate the approaches with both automatic evaluation and manual evaluation. Experimental results show that the entity enhanced Seq2Seq model with attention, copy, and coverage mechanisms significantly outperforms all other approaches in terms of multiple evaluation metrics.1},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1860–1872},
numpages = {13}
}

@inproceedings{10.1145/3503162.3503171,
author = {Sharma, Prawaal and Goyal, Navneet},
title = {Zero-Shot Reductive Paraphrasing for Digitally Semi-Literate},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503171},
doi = {10.1145/3503162.3503171},
abstract = {People in developing countries with restricted schooling, face hurdles with their digital enablement. Constrained education creates issues with comprehensibility of information on internet and other digital platforms. Most content on digital platforms use enriched vocabulary for more accomplished users and hence does not seem to be very useful for digitally semi-literate users. Artificial Intelligence (AI), Information Retrieval (IR) and Natural Language Processing (NLP) can be applied for text simplification to bridge the digital divide and empower these users. In this paper we propose to achieve reductive paraphrasing using Neural Machine Translation (NMT) framework along with encoder-decoder model and Gated Recurring Unit (GRU). Our approach combines Zero-shot Learning (ZSL) using multi-pivot method to execute our experiment. We have considered English as the base language and three pivot languages (French, German and Spanish) to verify our claims. We have designed the simplified vocabulary from movies for younger audience. It has been observed that using the approach as described in our paper, an improvement of an average of 25% in ease of comprehension can be achieved.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {91–98},
numpages = {8},
keywords = {Reductive Paraphrasing, Human Computer Interaction, Text simplification, Machine Translation},
location = {Virtual Event, India},
series = {FIRE '21}
}

@article{10.1145/3377851,
author = {Li, Yachao and Li, Junhui and Zhang, Min and Li, Yixin and Zou, Peng},
title = {Improving Neural Machine Translation with Linear Interpolation of a Short-Path Unit},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3377851},
doi = {10.1145/3377851},
abstract = {In neural machine translation (NMT), the source and target words are at the two ends of a large deep neural network, normally mediated by a series of non-linear activations. The problem with such consequent non-linear activations is that they significantly decrease the magnitude of the gradient in a deep neural network, and thus gradually loosen the interaction between source words and their translations. As a result, a source word may be incorrectly translated into a target word out of its translational equivalents. In this article, we propose short-path units (SPUs) to strengthen the association of source and target words by allowing information flow over adjacent layers effectively via linear interpolation. In particular, we enrich three critical NMT components with SPUs: (1) an enriched encoding model with SPU, which interpolates source word embeddings linearly into source annotations; (2) an enriched decoding model with SPU, which enables the source context linearly flow to target-side hidden states; and (3) an enriched output model with SPU, which further allows linear interpolation of target-side hidden states into output states. Experimentation on Chinese-to-English, English-to-German, and low-resource Tibetan-to-Chinese translation tasks demonstrates that the linear interpolation of SPUs significantly improves the overall translation quality by 1.88, 1.43, and 3.75 BLEU, respectively. Moreover, detailed analysis shows that our approaches much strengthen the association of source and target words. From the preceding, we can see that our proposed model is effective both in rich- and low-resource scenarios.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {feb},
articleno = {44},
numpages = {16},
keywords = {neural networks, Neural machine translation, short-path units, low resource}
}

@inproceedings{10.1145/3418994.3419000,
author = {Pham, Viet and Nguyen, Long H. B. and Dinh, Dien},
title = {Semantic Convolutional Neural Machine Translation Using AMR for English-Vietnamese},
year = {2020},
isbn = {9781450377379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418994.3419000},
doi = {10.1145/3418994.3419000},
abstract = {Semantic representation can help in enforcing meaning preservation and handling data sparsity of neural machine translation models. This paper presents an extension of the convolutional neural machine translation model to incorporate Abstract Meaning Representation as a kind of semantic representation to reduce language ambiguity or alleviate data sparseness problems. Evaluating on translating from English to Vietnamese with a low resource setting in the domain of TED talks, we obtain promising results in terms of both perplexity reductions and improved BLEU scores over the baseline method.},
booktitle = {Proceedings of the 2020 International Conference on Computer Communication and Information Systems},
pages = {52–56},
numpages = {5},
keywords = {Abstract Meaning Representation, Convolutional Neural Networks, Neural Networks},
location = {Ho Chi Minh City, Viet Nam},
series = {CCCIS 2020}
}

@inproceedings{10.1145/3493700.3493740,
author = {Gupta, Kamal Kumar and Kumari, Divya and Chennabasavraj, Soumya and Garera, Nikesh and Ekbal, Asif},
title = {ReviewMT: Sentiment Preserved E-Commerce Review Translation System},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493740},
doi = {10.1145/3493700.3493740},
abstract = {We present a demonstration of our English–to–Indian languages (Hindi, Bengali, Gujarati, Marathi, Punjabi) product review translation system, ReviewMT1. The system is based on the neural machine translation (NMT) model which is used to translate the product reviews written by the users on the e-commerce platform, Flipkart. The objective of our translation system is to translate the user reviews, provide an interface to the translators/users to post-edit the output, rate the quality and to retrain the model from the corrected samples. The retraining using the post-edited samples provides better learning to the NMT model so that it does not repeat the same mistake during translation. ReviewMT also provides an additional functionality called ‘sentiment-preserved machine translation’ which preserves the sentiment of the source sentence into the target sentence during the translation. This is important as Machine Translation (MT) systems often fail to preserve different stylistic and pragmatic properties of the source text (e.g. sentiment, emotion, gender traits, etc.) to the target, especially in a low-resource scenario.},
booktitle = {5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {275–279},
numpages = {5},
keywords = {sentiment preserved machine translation, neural machine translation, noisy text translation, review translation},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.1145/3457682.3457744,
author = {Zhi, Xiu and Wang, Siriguleng},
title = {Research on the Application of BERT in Mongolian-Chinese Neural Machine Translation},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457744},
doi = {10.1145/3457682.3457744},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {404–409},
numpages = {6},
keywords = {BERT, Mongolian-Chinese neural machine translation, Transformer},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1145/3512937,
author = {Zhang, Yongle and Asamoah Owusu, Dennis and Carpuat, Marine and Gao, Ge},
title = {Facilitating Global Team Meetings Between Language-Based Subgroups: When and How Can Machine Translation Help?},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512937},
doi = {10.1145/3512937},
abstract = {Global teams frequently consist of language-based subgroups who put together complementary information to achieve common goals. Previous research outlines a two-step work communication flow in these teams. There are team meetings using a required common language (i.e., English); in preparation for those meetings, people have subgroup conversations in their native languages. Work communication at team meetings is often less effective than in subgroup conversations. In the current study, we investigate the idea of leveraging machine translation (MT) to facilitate global team meetings. We hypothesize that exchanging subgroup conversation logs before a team meeting offers contextual information that benefits teamwork at the meeting. MT can translate these logs, which enables comprehension at a low cost. To test our hypothesis, we conducted a between-subjects experiment where twenty quartets of participants performed a personnel selection task. Each quartet included two English native speakers (NS) and two non-native speakers (NNS) whose native language was Mandarin. All participants began the task with subgroup conversations in their native languages, then proceeded to team meetings in English. We manipulated the exchange of subgroup conversation logs prior to team meetings: with MT-mediated exchanges versus without. Analysis of participants' subjective experience, task performance, and depth of discussions as reflected through their conversational moves jointly indicates that team meeting quality improved when there were MT-mediated exchanges of subgroup conversation logs as opposed to no exchanges. We conclude with reflections on when and how MT could be applied to enhance global teamwork across a language barrier.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {90},
numpages = {26},
keywords = {shared context, global teams, language choice, machine translation}
}

@article{10.1145/3340544,
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
title = {An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3340544},
doi = {10.1145/3340544},
abstract = {Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {19},
numpages = {29},
keywords = {Neural machine translation, bug-fixes}
}

@inproceedings{10.1145/3442381.3450034,
author = {Xu, Chang and Wang, Jun and Tang, Yuqing and Guzm\'{a}n, Francisco and Rubinstein, Benjamin I. P. and Cohn, Trevor},
title = {A Targeted Attack on Black-Box Neural Machine Translation with Parallel Data Poisoning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450034},
doi = {10.1145/3442381.3450034},
abstract = {As modern neural machine translation (NMT) systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary (e.g., secured commercial systems). In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data. We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system’s training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train &amp; fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data (tens of millions), the attacks are still successful (over 50% success rate) under surprisingly low poisoning budgets (e.g., 0.006%). Lastly, we discuss potential defences to counter such attacks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3638–3650},
numpages = {13},
keywords = {black-box attacks, data poisoning, neural machine translation},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3383583.3398605,
author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Nguyen, Nhu-Van and Coustaty, Mickael and Doucet, Antoine},
title = {Neural Machine Translation with BERT for Post-OCR Error Detection and Correction},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398605},
doi = {10.1145/3383583.3398605},
abstract = {The quality of OCR has a direct impact on information access, and an indirect impact on the performance of natural language processing applications, making fine-grained (e.g., semantic) information access even harder. This work proposes a novel post-OCR approach based on a contextual language model and neural machine translation, aiming to improve the quality of OCRed text by detecting and rectifying erroneous tokens. This new technique obtains results comparable to the best-performing approaches on English datasets of the competition on post-OCR text correction in ICDAR 2017/2019.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {333–336},
numpages = {4},
keywords = {BERT, neural machine translation, post-ocr processing},
location = {Virtual Event, China},
series = {JCDL '20}
}

@article{10.1145/3624740,
author = {Ding, Zishuo and Tang, Yiming and Cheng, Xiaoyu and Li, Heng and Shang, Weiyi},
title = {LoGenText-Plus: Improving Neural Machine Translation-Based Logging Texts Generation with Syntactic Templates},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624740},
doi = {10.1145/3624740},
abstract = {Developers insert logging statements in the source code to collect important runtime information about software systems. The textual descriptions in logging statements (i.e., logging texts) are printed during system executions and exposed to multiple stakeholders including developers, operators, users, and regulatory authorities. Writing proper logging texts is an important but often challenging task for developers. Prior studies find that developers spend significant efforts modifying their logging texts. However, despite extensive research on automated logging suggestions, research on suggesting logging texts rarely exists. To fill this knowledge gap, we first propose LoGenText, reported in our conference paper (Ding et&nbsp;al., 2022), an automated approach that uses neural machine translation models to generate logging texts by translating the related source code into short textual descriptions. LoGenText takes the preceding source code of a logging text as the input and considers other context information such as the location of the logging statement, to automatically generate the logging text. The LoGenText’s evaluation on 10 open-source projects indicates that the approach is promising for automatic logging text generation and significantly outperforms the state-of-the-art approach. Furthermore, we extend LoGenText to LoGenText-Plus by incorporating the syntactic templates of the logging texts. Different from LoGenText, LoGenText-Plus decomposes the logging text generation process into two stages. LoGenText-Plus first adopts a neural machine translation model to generate the syntactic template of the target logging text. Then LoGenText-Plus feeds the source code and the generated template as the input to another neural machine translation model for logging text generation. We also evaluate LoGenText-Plus on the same 10 projects and observe that it outperforms LoGenText on nine of them. According to a human evaluation from developers’ perspectives, the logging texts generated by LoGenText-Plus have a higher quality than those generated by LoGenText and the prior baseline approach. By manually examining the generated logging texts, we then identify five aspects that can serve as guidance for writing or generating good logging texts. Our work is an important step towards the automated generation of logging statements, which can potentially save developers’ efforts and improve the quality of software logging. Our findings shed light on research opportunities that leverage advances in neural machine translation techniques for automated generation and suggestion of logging statements.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
keywords = {software logging, neural machine translation, logging text}
}

@inproceedings{10.1145/3292500.3330710,
author = {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
title = {Unsupervised Clinical Language Translation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330710},
doi = {10.1145/3292500.3330710},
abstract = {As patients' access to their doctors' clinical notes becomes common, translating professional, clinical jargon to layperson-understandable language is essential to improve patient-clinician communication. Such translation yields better clinical outcomes by enhancing patients' understanding of their own health conditions, and thus improving patients' involvement in their own care. Existing research has used dictionary-based word replacement or definition insertion to approach the need. However, these methods are limited by expert curation, which is hard to scale and has trouble generalizing to unseen datasets that do not share an overlapping vocabulary. In contrast, we approach the clinical word and sentence translation problem in a completely unsupervised manner. We show that a framework using representation learning, bilingual dictionary induction and statistical machine translation yields the best precision at 10 of 0.827 on professional-to-consumer word translation, and mean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and layperson readability, respectively, on sentence translation. Our fully-unsupervised strategy overcomes the curation problem, and the clinically meaningful evaluation reduces biases from inappropriate evaluators, which are critical in clinical machine learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3121–3131},
numpages = {11},
keywords = {representation learning, consumer health, machine translation, unsupervised learning},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1109/TASLP.2018.2875794,
author = {Ferrer, Luciana and Nandwana, Mahesh Kumar and McLaren, Mitchell and Castan, Diego and Lawson, Aaron},
title = {Toward Fail-Safe Speaker Recognition: Trial-Based Calibration With a Reject Option},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2875794},
doi = {10.1109/TASLP.2018.2875794},
abstract = {The output scores of most of the speaker recognition systems are not directly interpretable as stand-alone values. For this reason, a calibration step is usually performed on the scores to convert them into proper likelihood ratios, which have a clear probabilistic interpretation. The standard calibration approach transforms the system scores using a linear function trained using data selected to closely match the evaluation conditions. This selection, though, is not feasible when the evaluation conditions are unknown. In previous work, we proposed a calibration approach for this scenario called trial-based calibration TBC. TBC trains a separate calibration model for each test trial using data that is dynamically selected from a candidate training set to match the conditions of the trial. In this work, we extend the TBC method, proposing: 1 a new similarity metric for selecting training data that result in significant gains over the one proposed in the original work; 2 a new option that enables the system to reject a trial when not enough matched data are available for training the calibration model; and 3 the use of regularization to improve the robustness of the calibration models trained for each trial. We test the proposed algorithms on a development set composed of several conditions and on the Federal Bureau of Investigation multi-condition speaker recognition dataset, and we demonstrate that the proposed approach reduces calibration loss to values close to 0 for most of the conditions when matched calibration data are available for selection, and that it can reject most of the trials for which relevant calibration data are unavailable.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {140–153},
numpages = {14}
}

@article{10.1162/coli_a_00348,
author = {Eriguchi, Akiko and Hashimoto, Kazuma and Tsuruoka, Yoshimasa},
title = {Incorporating Source-Side Phrase Structures into Neural Machine Translation},
year = {2019},
issue_date = {June 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00348},
doi = {10.1162/coli_a_00348},
abstract = {Neural machine translation NMT has shown great success as a new alternative to the traditional Statistical Machine Translation model in multiple languages. Early NMT models are based on sequence-to-sequence learning that encodes a sequence of source words into a vector space and generates another sequence of target words from the vector. In those NMT models, sentences are simply treated as sequences of words without any internal structure. In this article, we focus on the role of the syntactic structure of source sentences and propose a novel end-to-end syntactic NMT model, which we call a tree-to-sequence NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our proposed model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. We have empirically compared the proposed model with sequence-to-sequence models in various settings on Chinese-to-Japanese and English-to-Japanese translation tasks. Our experimental results suggest that the use of syntactic structure can be beneficial when the training data set is small, but is not as effective as using a bi-directional encoder. As the size of training data set increases, the benefits of using a syntactic tree tends to diminish.},
journal = {Comput. Linguist.},
month = {jun},
pages = {267–292},
numpages = {26}
}

@article{10.1109/TASLP.2022.3153257,
author = {Tan, Zhixing and Yang, Zeyuan and Zhang, Meng and Liu, Qun and Sun, Maosong and Liu, Yang},
title = {Dynamic Multi-Branch Layers for On-Device Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153257},
doi = {10.1109/TASLP.2022.3153257},
abstract = {With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications, such as neural machine translation (NMT), from cloud to mobile devices. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.5 times faster with the same number of parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {958–967},
numpages = {10}
}

@article{10.1109/TASLP.2018.2883740,
author = {Zhang, Jiajun and Zhao, Yang and Li, Haoran and Zong, Chengqing},
title = {Attention With Sparsity Regularization for Neural Machine Translation and Summarization},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2883740},
doi = {10.1109/TASLP.2018.2883740},
abstract = {The attention mechanism has become the de facto standard component in neural sequence to sequence tasks, such as machine translation and abstractive summarization. It dynamically determines which parts in the input sentence should be focused on when generating each word in the output sequence. Ideally, only few relevant input words should be attended to at each decoding time step and the attention weight distribution should be sparse and sharp. However, previous methods have no good mechanism to control this attention weight distribution. In this paper, we propose a sparse attention model in which a sparsity regularization term is designed to augment the objective function. We explore two kinds of regularizations: $L_{infty }$-norm regularization and minimum entropy regularization, both of which aim to sharpen the attention weight distribution. Extensive experiments on both neural machine translation and abstractive summarization demonstrate that our proposed sparse attention model can substantially outperform the strong baselines. And the detailed analyses reveal that the final attention distribution indeed becomes sparse and sharp.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {507–518},
numpages = {12}
}

@inproceedings{10.1145/3377812.3382162,
author = {Gupta, Shashij},
title = {Machine Translation Testing via Pathological Invariance},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382162},
doi = {10.1145/3377812.3382162},
abstract = {Due to the rapid development of deep neural networks, in recent years, machine translation software has been widely adopted in people's daily lives, such as communicating with foreigners or understanding political news from the neighbouring countries. However, machine translation software could return incorrect translations because of the complexity of the underlying network. To address this problem, we introduce a novel methodology called PaInv for validating machine translation software. Our key insight is that sentences of different meanings should not have the same translation (i.e., pathological invariance). Specifically, PaInv generates syntactically similar but semantically different sentences by replacing one word in the sentence and filter out unsuitable sentences based on both syntactic and semantic information. We have applied PaInv to Google Translate using 200 English sentences as input with three language settings: English→Hindi, English→Chinese, and English→German. PaInv can accurately find 331 pathological invariants in total, revealing more than 100 translation errors.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {107–109},
numpages = {3},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3495018.3495407,
author = {He, Xinyu},
title = {Evaluation of Machine Translation Quality Based on Neural Network and Its Application on Foreign Language Education},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495407},
doi = {10.1145/3495018.3495407},
abstract = {The form of communication at home and abroad requires translators to constantly develop new professional skills and language skills to meet the needs of translation, At present, neural network translation has become the mainstream of machine translation technology. Based on this, this paper analyzes the quality of machine translation based on neural network and its impact on foreign language education. Through the combination of questionnaire survey and experimental comparison, this paper studies the use and response of machine translation in students' daily learning through questionnaire survey the differences between traditional machine translation and neural network-based machine translation are analyzed based on the combination of questionnaire and experiment. From the data analysis, we can see that the accuracy and efficiency of neural network machine translation are improved. When the vocabulary is 500, the accuracy is increased by 8%, when the time is increased by 1 s, when the vocabulary is 1000, the accuracy is increased by 11%, when the time is increased by 2.3S, 1500 words The accuracy of convergence time increased by 9%, the time increased by 3.4s, the accuracy of 2000 vocabulary increased by 9%, and the time increased by 4.5s.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1395–1399},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3485447.3511940,
author = {Perevalov, Aleksandr and Both, Andreas and Diefenbach, Dennis and Ngonga Ngomo, Axel-Cyrille},
title = {Can Machine Translation Be a Reasonable Alternative for Multilingual Question Answering Systems over Knowledge Graphs?},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511940},
doi = {10.1145/3485447.3511940},
abstract = {Providing access to information is the main and most important purpose of the Web. However, despite available easy-to-use tools (e.g., search engines, chatbots, question answering) the accessibility is typically limited by the capability of using the English language. This excludes a huge amount of people. In this work, we discuss Knowledge Graph Question Answering (KGQA) systems that aim at providing natural language access to data stored in Knowledge Graphs (KG). While several KGQA systems have been proposed, only very few have dealt with a language other than English. In this work, we follow our research agenda of enabling speakers of any language to access the knowledge stored in KGs. Because of the lack of native support for many languages, we use machine translation (MT) tools to evaluate KGQA systems regarding questions in languages that are unsupported by a KGQA system. In total, our evaluation is based on 8 different languages (including some that never were evaluated before). For the intensive evaluation, we extend the QALD-9 dataset for KGQA with Wikidata queries and high-quality translations. The extension was done in a crowdsourcing manner by native speakers of the different languages. By using multiple KGQA systems for the evaluation, we were enabled to investigate and answer the main research question: “Can MT be an alternative for multilingual KGQA systems?”. The evaluation results demonstrated that the monolingual KGQA systems can be effectively ported to the new languages with MT tools.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {977–986},
numpages = {10},
keywords = {question answering dataset, knowledge graph question answering, machine translation, multilingual question answering},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3617372,
author = {Banik, Debajyoty and Paul, Rahul and Rathore, Rajkumar Singh and Jhaveri, Rutvij H.},
title = {Improving Access to Medical Information for Multilingual Patients Using Pipelined Ensemble Average Based Machine Translation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617372},
doi = {10.1145/3617372},
abstract = {Machine translation has shown potential in improving access to medical information and healthcare services for multilingual patients. This research aims to enhance machine translation accuracy in the medical field, specifically for translating from Hindi to English. The study introduces a new approach that dynamically allocates decoding parameters using regression models, overcoming the limitations of fixed parameters in the decoder. A comprehensive dataset is created to address limited data availability, enabling regression models to predict optimal pruning parameters. The main motivation for the study is the introduction of a regression method for optimizing pruning parameters, which is a novel approach in this context. The proposed approach outperforms existing methods, achieving improved translation accuracy. Standard metrics such as the BLEU score are used to evaluate translations. Ensemble average and pipeline approaches further enhance performance. The improved performance of the proposed models can be attributed to the ensemble of diverse models (Extra Trees, LightGBM, XGBoost, and Random Forest) that employ various techniques to reduce overfitting, enhance prediction accuracy, and improve translation by correcting prediction errors. The study contributes to facilitating the translation and sharing of medical literature, promoting collaboration and knowledge exchange across languages. The research demonstrates the effectiveness of the regression method for optimizing pruning parameters in machine translation, leading to improved translation accuracy in the medical field. The proposed models offer promising results, paving the way for enhanced machine translation systems and promoting collaboration and knowledge exchange in the medical domain. The source code is available at https://huggingface.co/debajyoty/statistical-regression-Based-MT/tree/main/Statistical-Regression-SMT.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
keywords = {Beam Search Algorithm, Regression;, Threshold Pruning, Statistical Machine Translation, RMSE, Decoder, Histogram Pruning, Machine Learning}
}

@inproceedings{10.1145/3411763.3451837,
author = {Zhang, Yongle and Asamoah Owusu, Dennis and Gong, Emily and Chopra, Shaan and Carpuat, Marine and Gao, Ge},
title = {Leveraging Machine Translation to Support Distributed Teamwork Between Language-Based Subgroups: The Effects of Automated Keyword Tagging},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451837},
doi = {10.1145/3411763.3451837},
abstract = {Modern teamwork often happens between subgroups located in different countries. Members of the same subgroup prefer to communicate in their native language for efficiency, which increases the coordination cost between subgroups. The current study extends previous HCI literature that explores the effects of machine translation (MT) on crosslingual teamwork. We investigated whether automated keyword tagging would assist people's comprehension of imperfect MT outputs and, therefore, enhance the quality of communication between subgroups. We conducted an online experiment where twenty teams performed a collaborative task. Each team consisted of two native English speakers and two native Mandarin speakers. We provided MT support that enabled participants to read all subgroups’ discussions in English before team meetings, but in two forms: with vs. without automated keyword tagging. We found MT with automated keyword tagging affected people's interaction with the translated materials, but it did not enhance translation comprehensibility in the context of teamwork.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {381},
numpages = {6},
keywords = {Machine translation (MT), Multilingual teamwork, Automated keyword extraction},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3424953.3426536,
author = {Reis, Luana Silva and de Ara\'{u}jo, Tiago Maritan Ugulino and Aguiar, Yuska Paola Costa and Lima, Manuella Aschoff Cavalcanti Brand\~{a}o},
title = {Evaluating Machine Translation Systems for Brazilian Sign Language in the Treatment of Critical Grammatical Aspects},
year = {2020},
isbn = {9781450381727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424953.3426536},
doi = {10.1145/3424953.3426536},
abstract = {Machine translation tools for sign languages are developed with the aim of reducing barriers to access to information for deaf people. However, these technologies have some limitations related to the difficulty of dealing with some specific grammatical aspects of sign languages, which can negatively influence the experience of deaf users. In order to analyze this problem, in this study, we assess the translation of Brazilian Portuguese (Pt-br) content into Brazilian Sign Language (Libras) by three machine translators: HandTalk, Ryben\'{a} and VLibras. More specifically, we conducted an experiment with 30 Brazilian human interpreters that evaluate the treatment of 7 specific grammatical aspects in the applications. As a result, we observed significant important limitation in the treatment of homonyms, adverbs (negation, mode and intensity), directional verbs, and phrases (interrogative and exclamatory) in the translations performed by these applications, indicating the need for them to improve the treatment of these grammatical aspects.},
booktitle = {Proceedings of the 19th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {46},
numpages = {6},
keywords = {deaf user, machine translation, accessibility, quality of translation, brazilian sign language (Libras)},
location = {Diamantina, Brazil},
series = {IHC '20}
}

@inproceedings{10.1145/3482632.3483145,
author = {Yang, Fan},
title = {Analysis of English Machine Translation Standards of Professional Terms under Big Data Context Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483145},
doi = {10.1145/3482632.3483145},
abstract = {In recent years, with the development of science and technology, information technology has also been widely used in various industries. In today's era of big data, great changes have taken place in translation, from the traditional manual translation at the beginning to the current machine translation. The advantage of machine translation lies in its fast translation speed and low cost, but certain errors will also occur in the translated translation. The translation errors of professional terms account for a larger proportion, which greatly affects the quality of the translation. At the same time, professional terminology is the core knowledge in the article, which will directly affect people's understanding of technical information in the professional field. The purpose of this article is to study the English machine translation standards of professional terms in the context of big data. This article focuses on the English machine translation of professional terms, and analyzes the English machine translation standards of professional terms in the context of big data based on relevant research at home and abroad. This article will analyze its professional terminology translation standards from all aspects, summarize and summarize the translation standards studied by various scholars, analyze the factors that affect translation standards, and discover various characteristics of translation standards. At the same time, in order to solve the shortcomings of machine translation in the translation of professional terminology in the past, this article attempts to summarize a set of objective and comprehensive English machine translation system evaluation standards for professional terminology, so as to improve its use efficiency. The experimental results show that from the ranking results, the highest comprehensive evaluation score is candidate translation 1, and the weight is as high as 27%. In the analysis of the translation results, information characteristics are added to reorder the candidate translations, and the most suitable context and the best translation are selected.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1332–1336},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3503161.3548425,
author = {Mikulowski, Dariusz},
title = {Support for Teaching Mathematics of the Blind by Sighted Tutors Through Multisensual Access to Formulas with Braille Converters and Speech},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548425},
doi = {10.1145/3503161.3548425},
abstract = {Nowadays, teaching various subjects at school is successfully supported by information and remote technologies such as Google Class, Moodle and others. Nevertheless, students with special needs such as the visually impaired (BVI) face incredible barriers to using such remote technologies, especially with learning mathematics or physics. The main problem is that BVI uses different tools and techniques than their sighted peers, i.e., a different way of working with mathematical expressions or a lack of the possibility to edit graphics. Traditional methods such as the Brailler, figure models or cubarithms are still used. Another challenge is that there are entirely different systems of presenting formulas in different countries, so-called Braille mathematical notations. To overcome these barriers, we propose universal tools to assist sighted teachers and BVI students in remote training math using a multimodal form of editing of mathematical formulas. It consists of the simultaneous combination of three forms of presentation of math formulas in graphical form for the teacher, intelligent reading through speech synthesis and Braille mathematical notation for BVI. It is possible thanks to the use of intelligent converters between formats such as MathML, intelligent text and Braille and dedicated editors that allow for creating math documents by students and teachers.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1552–1560},
numpages = {9},
keywords = {math formula converters., multimodal access, teaching math, blind students, braille math notation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2020.3036783,
author = {Chai, Li and Du, Jun and Liu, Qing-Feng and Lee, Chin-Hui},
title = {A Cross-Entropy-Guided Measure (CEGM) for Assessing Speech Recognition Performance and Optimizing DNN-Based Speech Enhancement},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3036783},
doi = {10.1109/TASLP.2020.3036783},
abstract = {A new cross-entropy-guided measure (CEGM) is proposed to indirectly assess accuracies of automatic speech recognition (ASR) of degraded speech with a speech enhancement front-end and without directly performing ASR experiments. The proposed CEGM is calculated in three steps, namely: (1) a low-level representations via feature extraction, (2) a high-level nonlinear mapping using an acoustic model, and (3) a final CEGM calculation between the high-level representations of clean and enhanced speech. Specifically, state posterior probabilities from outputs of conventional hybrid acoustic model of the target ASR system are adopted as the high-level representations and a cross-entropy criterion is used to calculate the CEGM. Due to CEGM's differentiability, it can also be used to replace the conventional minimum mean squared error (MMSE) criterion as an objective function for deep neural network (DNN)-based speech enhancement. Therefore, the front-end enhancement model can be optimized towards improving the accuracies of the back-end ASR system. Experiments on single-channel CHiME-4 Challenge show that CEGM yields consistently the highest correlations with word error rate (WER) which is often costly to calculate, and achieves the most accurate assessment of ASR performance when compared to the perceptual evaluation metrics commonly used for assessing speech enhancement performance. Furthermore, CEGM-optimized speech enhancement could effectively reduce the WER on the CHiME-4 real test set when compared to unprocessed noisy speech and enhanced speech obtained with MMSE-optimized enhancement for ASR systems with fixed multi-condition acoustic models in various deep architectures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {106–117},
numpages = {12}
}

@article{10.1145/3230638,
author = {Huang, Guoping and Zhang, Jiajun and Zhou, Yu and Zong, Chengqing},
title = {Input Method for Human Translators: A Novel Approach to Integrate Machine Translation Effectively and Imperceptibly},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3230638},
doi = {10.1145/3230638},
abstract = {Computer-aided translation (CAT) systems are the most popular tool for helping human translators efficiently perform language translation. To further improve the translation efficiency, there is an increasing interest in applying machine translation (MT) technology to upgrade CAT. To thoroughly integrate MT into CAT systems, in this article, we propose a novel approach: a new input method that makes full use of the knowledge adopted by MT systems, such as translation rules, decoding hypotheses, and n-best translation lists. The proposed input method contains two parts: a phrase generation model, allowing human translators to type target sentences quickly, and an n-gram prediction model, helping users choose perfect MT fragments smoothly. In addition, to tune the underlying MT system to generate the input method preferable results, we design a new evaluation metric for the MT system. The proposed input method integrates MT effectively and imperceptibly, and it is particularly suitable for many target languages with complex characters, such as Chinese and Japanese. The extensive experiments demonstrate that our method saves more than 23% in time and over 42% in keystrokes, and it also improves the translation quality by more than 5 absolute BLEU scores compared with the strong baseline, i.e., post-editing using Google Pinyin.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
articleno = {4},
numpages = {22},
keywords = {evaluation metric, input method, Machine translation, computer-aided translation}
}

@inproceedings{10.1145/3436369.3437429,
author = {Chen, Linjie and Wang, Jianzong and Huang, Zhangcheng and Xiao, Jing},
title = {An Approach for Neural Machine Translation with Graph Attention Network},
year = {2021},
isbn = {9781450387835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436369.3437429},
doi = {10.1145/3436369.3437429},
abstract = {The achievement of Neural Machine Translation (NMT) drew the attention of the professionals in recent years. The translation quality outperforms traditional methods such as Statistical Machine Translation. However, for the document-level machine translation tasks, the research shows that there are still some problems with the Machine Translation model, such as the loss of the original document theme and lack of fluency.The improvement of translation quality is the aim of implying this research, and we concentrate on designing an NMT network that can capture the contextual information from the other sentences in the article. This paper proposed a competitive model named CG-Transformer that effectively converts the context information to vectors, and generates an accurate translation of target language based on the Graph Attention mechanism. This method enables the model to learn the context information during the training process. Due to the lack of a document-level parallel corpora, the strategy of two-step training is applied. The evaluation results of Chinese to English translation have proved that our context-aware model increases the BLEU scores by 3.77% and 3.29% compared with the sentence-based model.},
booktitle = {Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition},
pages = {472–478},
numpages = {7},
keywords = {Document-level Translation, Graph Neural Network, Graph Attention},
location = {Xiamen, China},
series = {ICCPR '20}
}

@inbook{10.1145/3477322.3477329,
author = {Aylett, Matthew P. and Clark, Leigh and Cowan, Benjamin R. and Torre, Ilaria},
title = {Building and Designing Expressive Speech Synthesis},
year = {2021},
isbn = {9781450387200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3477322.3477329},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior, Cognition},
pages = {173–212},
numpages = {40}
}

@inproceedings{10.1145/3468891.3468915,
author = {Chen, Xi and Bromuri, Stefano and van Eekelen, Marko},
title = {Neural Machine Translation for Harmonized System Codes Prediction},
year = {2021},
isbn = {9781450389402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468891.3468915},
doi = {10.1145/3468891.3468915},
abstract = {The harmonized system codes (HS codes) are used worldwide to categorize products in international shipments. In its basic form HS codes come in 6 digit format, subdivided hierarchically into groups of two digits (chapters, headings and subheadings). When shipping products, it is mandatory to specify a HS code for the purpose of producing a custom declaration. Currently the process is mostly carried out by human experts who take a decision on the HS code to be assigned to a shipment depending on the item description provided by the shipper. As such the process is time consuming and prone to errors due to generic, incomplete or non-interpretable descriptions. The objective of this research is to automate the classification of HS codes in order to increase productivity to cope with extra volume in the custom classification area. For the purpose of testing the developed models, we used an anonymized data set of shipments provided by DHL. The main contribution of this paper is we applied a deep learning model which have not been tried on tackling the HS code classification problem: an attention-based neural machine translation (NMT) model with integration of hierarchical loss. The model can classify around 29% percentage of the dataset where the model's accuracy can reach 85%.},
booktitle = {Proceedings of the 2021 6th International Conference on Machine Learning Technologies},
pages = {158–163},
numpages = {6},
keywords = {automatic classification, hs codes, shipments, line items, logistics},
location = {Jeju Island, Republic of Korea},
series = {ICMLT '21}
}

@article{10.1145/3265752,
author = {Le, Ngoc Tan and Sadat, Fatiha and Menard, Lucie and Dinh, Dien},
title = {Low-Resource Machine Transliteration Using Recurrent Neural Networks},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3265752},
doi = {10.1145/3265752},
abstract = {Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These models are based on initial alignments between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network--based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pretrained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. Evaluation and experiments involving French and Vietnamese showed that with only a small bilingual pronunciation dictionary available for training the transliteration models, promising results were obtained with a large increase in BLEU scores and a reduction in Translation Error Rate (TER) and Phoneme Error Rate (PER). Moreover, we compared our proposed neural network--based transliteration approach with a statistical one.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {13},
numpages = {14},
keywords = {low-resource language, French-Vietnamese, recurrent neural networks, alignment, Machine transliteration, grapheme-to-phoneme, embeddings}
}

@inproceedings{10.1145/3368567.3368579,
author = {Mahata, Sainik Kumar and Mandal, Soumil and Das, Dipankar and Bandyopadhyay, Sivaji},
title = {Code-Mixed to Monolingual Translation Framework},
year = {2019},
isbn = {9781450377508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368567.3368579},
doi = {10.1145/3368567.3368579},
abstract = {The use of multilingualism among the new generation is widespread in the form of code-mixed data on social media, and therefore a robust translation system is required for catering to the novice and monolingual users. In this work, we present a translation framework that uses a translation-transliteration strategy for translating code-mixed data into their equivalent monolingual instances. One of the goals of this work is to translate a code-mixed source (written in Roman script) to a Bengali target (written in Devanagari script), where the source may contain English, along with transliterated Bengali. Finally, to convert the output to a more readable form, it is reordered using a target language model. The decisive advantage of the proposed framework is that it does not require a code-mixed to monolingual parallel corpus for training and decoding. On testing the framework, it achieved BLEU and TER scores of 16.47 and 55.45, respectively. Since the proposed framework deals with various sub-modules, we dive deeper into the importance of each of them, analyze the errors and finally, discuss some improvement strategies.},
booktitle = {Proceedings of the 11th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {30–35},
numpages = {6},
keywords = {Neural Network, Machine Translation, Code-Mixing, Transliteration},
location = {Kolkata, India},
series = {FIRE '19}
}

@inproceedings{10.1145/3462462.3468879,
author = {Makrynioti, Nantia and Ley-Wild, Ruy and Vassalos, Vasilis},
title = {Machine Learning in SQL by Translation to TensorFlow},
year = {2021},
isbn = {9781450384865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462462.3468879},
doi = {10.1145/3462462.3468879},
abstract = {We present sql4ml, a framework for expressing machine learning (ML) algorithms in a relational database management system (RDBMS). The user writes the objective function of an ML model as a SQL query, then sql4ml translates the query into an equivalent TensorFlow (TF) graph, which can be automatically differentiated and optimized to learn the model weights. Sql4ml makes the database a unified programming environment for feature engineering, learning/inference, and evaluating models. The proposed approach is more expressive than using ready-made ML algorithms, but abstracts away the details of the training process. We present the architecture of sql4ml and describe the method for translating an objective function in SQL to a TensorFlow representation. We show how recent ideas from Factorized ML [7] can be leveraged to efficiently move data between a database and an ML framework. Finally, we present experimental results regarding both the proposed translation and the optimization techniques for data transfer. Our results show that translation time is negligible compared to time for data processing, and that the optimization techniques achieve up to 50% improvement in the export runtime and up to 85% decrease in the size of the exported data.},
booktitle = {Proceedings of the Fifth Workshop on Data Management for End-To-End Machine Learning},
articleno = {2},
numpages = {11},
keywords = {SQL, TensorFlow, mathematical optimization problems, RDBMS},
location = {Virtual Event, China},
series = {DEEM '21}
}

@inproceedings{10.1145/3482632.3483147,
author = {Li, Hanhui},
title = {The Establishment of Machine Translation Bilingual Corpus Based on Artificial Intelligence and Big Data Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483147},
doi = {10.1145/3482632.3483147},
abstract = {With the rapid development of artificial intelligence and big data technology, all areas of society have gradually begun to use big data technology. Machine translation is a very popular and challenging research content in the field of natural language processing. From the idea of machine translation to the integration of syntactic information into machine translation, scholars at home and abroad have designed many formal models and algorithms for machine translation. They have made positive contributions to the research of machine translation in natural language processing and have made them increasingly mature. This article uses artificial intelligence big data technology to establish a machine translation bilingual prediction database. The content of the experiment is to conduct translation research on Chinese and English bilinguals through the machine translation bilingual prediction database. The main test is to test the accuracy of the number of sentences under different retrieval structures. Perform experimental tests on the recall rate, and analyze the data based on the test, and draw relevant conclusions.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1342–1346},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1109/TASLP.2021.3120592,
author = {Lu, Ziyao and Li, Xiang and Liu, Yang and Zhou, Chulun and Cui, Jianwei and Wang, Bin and Zhang, Min and Su, Jinsong},
title = {Exploring Multi-Stage Information Interactions for Multi-Source Neural Machine Translation},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120592},
doi = {10.1109/TASLP.2021.3120592},
abstract = {Existing studies for multi-source neural machine translation (NMT) either separately model different source sentences or resort to the conventional single-source NMT by simply concatenating all source sentences. However, there exist two drawbacks in these approaches. First, they ignore the explicit word-level semantic interactions between source sentences, which have been shown effective in the embeddings of multilingual texts. Second, multiple source sentences are simultaneously encoded by an NMT model, which is unable to fully exploit the semantic information of each source sentence. In this paper, we explore multi-stage information interactions for multi-source NMT. Specifically, we first propose a multi-source NMT model that performs information interactions at the encoding stage. Its encoder contains multiple semantic interaction layers, each of which sequentially consists of (1) monolingual semantic interaction sub-layer, which is based on the self-attention mechanism and used to learn word-level monolingual contextual representations of source sentences, and (2) cross-lingual semantic interaction sub-layer, which leverages word alignments to perform fine-grained semantic transitions among hidden states of different source sentences. Furthermore, at the training stage, we introduce a mutual distillation based training framework, where single-source models and ours perform information interactions. Such framework can fully exploit the semantic information of each source sentence to enhance our model. Extensive experimental results on the WMT14 English-German-French dataset show our method exhibits significant improvements upon competitive baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {562–570},
numpages = {9}
}

@inproceedings{10.1145/3482632.3484009,
author = {Zhang, Xiaohui},
title = {Research on Machine Translation and Computer Aided Translation Based on Cloud Computing},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484009},
doi = {10.1145/3482632.3484009},
abstract = {New breakthroughs have not been made in the principle and technology of machine translation, and high-quality automatic translation has not yet been realized. The core technology of computer-aided translation is translation memory technology. Although computer-aided translation has made considerable progress, the translation memory technology has not yet made a decisive breakthrough. Aiming at the problem that the translation time of machine-aided translation system is relatively long at present, the design of machine-aided translation system based on cloud computing is proposed. A new machine-aided translation system is designed by referring to the cloud computing model. The hardware of the system is divided into four layers: user layer, service layer, computing layer and storage layer. The storage structure, translation structure and retrieval structure are designed respectively. After that, the software and hardware of the system are designed. The test results show that the translation time of the machine-assisted translation system based on cloud computing is shorter than that of the traditional machine translation system.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1644–1648},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3450613.3456825,
author = {Wilschut, Thomas and van der Velde, Maarten and Sense, Florian and Fountas, Zafeirios and van Rijn, Hedderik},
title = {Translating a Typing-Based Adaptive Learning Model to Speech-Based L2 Vocabulary Learning},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3456825},
doi = {10.1145/3450613.3456825},
abstract = {Memorising vocabulary is an important aspect of formal foreign language learning. Advances in cognitive psychology have led to the development of adaptive learning systems that make vocabulary learning more efficient. These computer-based systems measure learning performance in real time to create optimal study strategies for individual learners. While such adaptive learning systems have been successfully applied to written word learning, they have thus far seen little application in spoken word learning. Here we present a system for adaptive, speech-based word learning. We show that it is possible to improve the efficiency of speech-based learning systems by applying a modified adaptive model that was originally developed for typing-based word learning. This finding contributes to a better understanding of the memory processes involved in speech-based word learning. Furthermore, our work provides a basis for the development of language learning applications that use real-time pronunciation assessment software to score the accuracy of the learner’s pronunciations. Speech-based learning applications are educationally relevant because they focus on what may be the most important aspect of language learning: to practice speech.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {245–250},
numpages = {6},
keywords = {speech, adaptive learning, memory, vocabulary learning, pronunciation},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@inproceedings{10.1109/MSR.2019.00021,
author = {Rahman, Musfiqur and Rigby, Peter C and Palani, Dharani and Nguyen, Tien},
title = {Cleaning StackOverflow for Machine Translation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00021},
doi = {10.1109/MSR.2019.00021},
abstract = {Generating source code API sequences from an English query using Machine Translation (MT) has gained much interest in recent years. For any kind of MT, the model needs to be trained on a parallel corpus. In this paper we clean StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts. We contrast three data cleaning approaches: standard NLP, title only, and software task extraction. We evaluate the quality of the each corpus for MT. To provide indicators of how useful each corpus will be for machine translation, we provide researchers with measurements of the corpus size, percentage of unique tokens, and per-word maximum likelihood alignment entropy. We have used these corpus cleaning approaches to translate between English and Code [22, 23], to compare existing SMT approaches from word mapping to neural networks [24], and to re-examine the "natural software" hypothesis [29]. After cleaning and aligning the data, we create a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements. This model provides a basis for the success of using StackOverflow for search and other tasks in the software engineering literature and paves the way for MT. Our scripts and corpora are publicly available on GitHub [1] as well as at https://search.datacite.org/works/10.5281/zenodo.2558551.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {79–83},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1145/3342351,
author = {Srivastava, Jyoti and Sanyal, Sudip and Srivastava, Ashish Kumar},
title = {An Automatic and a Machine-Assisted Method to Clean Bilingual Corpus},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342351},
doi = {10.1145/3342351},
abstract = {Two different methods of corpus cleaning are presented in this article. One is a machine-assisted technique, which is good to clean small-sized parallel corpus, and the other is an automatic method, which is suitable for cleaning large-sized parallel corpus. A baseline SMT (MOSES) system is used to evaluate these methods. The machine-assisted technique used two features: word alignment and length of the source and target language sentence. These features are used to detect mistranslations in the corpus, which are then handled by a human translator. Experiments of this method are conducted on the English-to-Indian Language Machine Translation (EILMT) corpus (English-Hindi). The Bilingual Evaluation Understudy (BLEU) score is improved by 0.47% for the clean corpus. Automatic method of corpus cleaning uses a combination of two features. One feature is length of source and target language sentence and the second feature is Viterbi alignment score generated by Hidden Markov Model for each sentence pair. Two different threshold values are used for these two features. These values are decided by using a small-sized manually annotated parallel corpus of 206 sentence pairs. Experiments of this method are conducted on the HindEnCorp corpus, released in the workshop of the Association of Computational Linguistics (ACL 2014). The BLEU score is improved by 0.6% on clean corpus. A comparison of the two methods is also presented on EILMT corpus.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {oct},
articleno = {13},
numpages = {19},
keywords = {Bilingual Corpus Cleaning, Statistical Machine Translation}
}

@article{10.1145/3312573,
author = {Yu, Hui and Xu, Weizhi and Lin, Shouxun and Liu, Qun},
title = {Machine Translation Evaluation Metric Based on Dependency Parsing Model},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3312573},
doi = {10.1145/3312573},
abstract = {Most of the syntax-based metrics obtain the similarity by comparing the sub-structures extracted from the trees of hypothesis and reference. These sub-structures cannot represent all the information in the trees because their lengths are limited. To sufficiently use the reference syntax information, a new automatic evaluation metric is proposed based on the dependency parsing model. First, a dependency parsing model is trained using the reference dependency tree for each sentence. Then, the hypothesis is parsed by this dependency parsing model and the corresponding hypothesis dependency tree is generated. The quality of hypothesis can be judged by the quality of the hypothesis dependency tree. Unigram F-score is included in the new metric so that lexicon similarity is obtained. According to experimental results, the proposed metric can perform better than METEOR and BLEU on system level and get comparable results with METEOR on sentence level. To further improve the performance, we also propose a combined metric which gets the best performance on the sentence level and on the system level.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {44},
numpages = {15},
keywords = {dependency parsing model, Automatic evaluation metric, machine translation}
}

@article{10.1145/3519386,
author = {Feng, Hailin and Xie, Shuxuan and Wei, Wei and Lv, Haibin and Lv, Zhihan},
title = {Deep Learning in Computational Linguistics for Chinese Language Translation},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3519386},
doi = {10.1145/3519386},
abstract = {Applying artificial intelligence to Chinese language translation in computational linguistics is of practical significance for economic boosts and cultural exchanges. In the present work, the bi-directional long short-term memory (BiLSTM) network is employed to extract Chinese text features regarding the overlapping semantic roles in Chinese language translation and hard-to-converge training of high-dimensional text word vectors in text classification during translation. In addition, AlexNet is optimized to extract the local features of the text and meanwhile update and learn network parameters in the deep network. Then, the attention mechanism is introduced to build a forecasting algorithm of Chinese language translation based on BiLSTM and improved AlexNet. Last, the forecasting algorithm is simulated to validate its performance. Some state-of-the-art algorithms are selected for a comparative experiment, including long short-term memory, regions with convolutional neural network features, AlexNet, and support vector machine. Results demonstrate that the forecasting algorithm proposed here can achieve a feature identification accuracy of 90.55%, at least an improvement of 4.24% over other algorithms. In addition, it provides an area under the curve of above 90%, a training duration of about 54.21 seconds, and a test duration of about 19.07 seconds. Regarding the performance of Chinese language translation, the algorithm proposed here provides a bilingual evaluation understudy (BLEU) value of 28.21 on the training set, with a performance gain ratio reaching 111.55%; on the test set, its BLEU reaches 40.45, with a performance gain ratio of 129.80%. Hence, this forecasting algorithm is notably superior to other algorithms, which can enhance the machine translation performance. Through experiments, the Chinese language translation algorithm constructed here improves translation performance while ensuring a high correct identification rate, providing experimental references for the later intelligent development of Chinese language translation in computational linguistics.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {74},
numpages = {20},
keywords = {BiLSTM, bilingual evaluation understudy (BLEU), Chinese language translation, computational linguistics, deep learning}
}

@inproceedings{10.1145/3469595.3469624,
author = {Cuadra, Andrea and Goedicke, David and Zamfirescu-Pereira, J.D.},
title = {Democratizing Design and Fabrication Using Speech: Exploring Co-Design with a Voice Assistant},
year = {2021},
isbn = {9781450389983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469595.3469624},
doi = {10.1145/3469595.3469624},
abstract = {Recent advances in artificial intelligence (AI) and voice interfaces have made possible the digital reincarnation of the draftsperson and craftsperson, capable of designing and producing custom work “to spec” for a reasonable cost. We present findings from an experiment using a Wizard-of-Oz-backed recreation of a voice assistant-based designer; participants describe a holiday ornament of their own imagination to the voice assistant, which elicits details and offers design choices before ultimately “creating” the ornament for the participant. For 8 out of 16 participants, a video channel allows the participant to see the designed object as it is iteratively designed and refined. We offer observations about the nature of the interactions between participants and the voice assistant, as well as quantitative measures of participant-reported cognitive load and predicted and actual satisfaction and accuracy of reproduction. Participants report greater satisfaction with their ornaments and experience reduced cognitive load if they can see them being designed; their expectations are higher if they cannot. 15 of 16 participants would use a voice assistant again for a design task.},
booktitle = {Proceedings of the 3rd Conference on Conversational User Interfaces},
articleno = {29},
numpages = {8},
keywords = {Wizard-of-Oz, CMC, Voice assistants},
location = {Bilbao (online), Spain},
series = {CUI '21}
}

@inproceedings{10.1145/3394486.3403335,
author = {Yabe, Takahiro and Tsubouchi, Kota and Shimizu, Toru and Sekimoto, Yoshihide and Ukkusuri, Satish V.},
title = {Unsupervised Translation via Hierarchical Anchoring: Functional Mapping of Places across Cities},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403335},
doi = {10.1145/3394486.3403335},
abstract = {Unsupervised translation has become a popular task in natural language processing (NLP) due to difficulties in collecting large scale parallel datasets. In the urban computing field, place embeddings generated using human mobility patterns via recurrent neural networks are used to understand the functionality of urban areas. Translating place embeddings across cities allow us to transfer knowledge across cities, which may be used for various downstream tasks such as planning new store locations. Despite such advances, current methods fail to translate place embeddings across domains with different scales (e.g. Tokyo to Niigata), due to the straightforward adoption of neural machine translation (NMT) methods from NLP, where vocabulary sizes are similar across languages. We refer to this issue as the domain imbalance problem in unsupervised translation tasks. We address this problem by proposing an unsupervised translation method that translates embeddings by exploiting common hierarchical structures that exist across imbalanced domains. The effectiveness of our method is tested using place embeddings generated from mobile phone data in 6 Japanese cities of heterogeneous sizes. Validation using landuse data clarify that using hierarchical anchors improves the translation accuracy across imbalanced domains. Our method is agnostic to input data type, thus could be applied to unsupervised translation tasks in various fields in addition to linguistics and urban computing.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2841–2851},
numpages = {11},
keywords = {human mobility, mobile phone data, embeddings, neural machine translation, hierarchical structures},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1145/3557894,
author = {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng, Michael},
title = {Improving Readability for Automatic Speech Recognition Transcription},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3557894},
doi = {10.1145/3557894},
abstract = {Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {142},
numpages = {23},
keywords = {post-processing for readability, Automatic speech recognition, pre-trained model, data synthesis}
}

@inproceedings{10.1145/3442188.3445907,
author = {Cho, Won Ik and Kim, Jiwon and Yang, Jaeyeong and Kim, Nam Soo},
title = {Towards Cross-Lingual Generalization of Translation Gender Bias},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445907},
doi = {10.1145/3442188.3445907},
abstract = {Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {449–457},
numpages = {9},
keywords = {cross-linguality, evaluation, gender bias, machine translation},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/3479645.3479703,
author = {Puspitaningrum, Diyah},
title = {A Study of English-Indonesian Neural Machine Translation with Attention (Seq2Seq, ConvSeq2Seq, RNN, and MHA): A Comparative Study of NMT on English-Indonesian},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479703},
doi = {10.1145/3479645.3479703},
abstract = {In recent years, Neural Machine Translation (NMT) with attention mechanisms has emerged in research and industry. This study discusses the essentials of NMT (Seq2Seq, Convolutional Seq2Seq (ConvSeq2Seq), Recurrent Neural Networks (RNN), and Multi-Head Attention (MHA)) while implemented in formal passages in English-Indonesian and Indonesian-English. The experimental results for ConvSeq2Seq achieve up to 38.99 BLEU sentence scores, 43.23 BLEU corpus scores, and 39.48 GLEU corpus scores over the Seq2Seq English-Indonesian. For Indonesian-English, the results for ConvSeq2Seq achieved as follows: up to 42.59 BLEU sentence scores, 42.91 BLEU corpus scores, 41.05 GLEU corpus scores, and 1356.65 WER scores over RNN and MHA. Thus, while ConvSeq2Seq tends to be the supremacy, this literature also describes the combination of architectures and specific fine-tuning strategies as a discussion.},
booktitle = {Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology},
pages = {271–280},
numpages = {10},
keywords = {RNN, Seq2Seq, multi-head attention, convolutional Seq2Seq},
location = {Malang, Indonesia},
series = {SIET '21}
}

@article{10.1109/TASLP.2020.3042001,
author = {Kawara, Yuki and Chu, Chenhui and Arase, Yuki},
title = {Preordering Encoding on Transformer for Translation},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3042001},
doi = {10.1109/TASLP.2020.3042001},
abstract = {The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel <italic>preordering encoding</italic> that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese–English and WMT 2015 English–German, English–Czech, and English–Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by <inline-formula><tex-math notation="LaTeX">${text{1.34}}$</tex-math></inline-formula> points in the Japanese–to–English task, <inline-formula><tex-math notation="LaTeX">${text{2.19}}$</tex-math></inline-formula> points in the English–to–German task, <inline-formula><tex-math notation="LaTeX">${text{0.15}}$</tex-math></inline-formula> points in the Czech–to–English task, and <inline-formula><tex-math notation="LaTeX">${text {1.48}}$</tex-math></inline-formula> points in the English–to–Russian task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {644–655},
numpages = {12}
}

@inproceedings{10.5555/3382225.3382357,
author = {Oh, Jaehoon and Lee, Injung and Seonwoo, Yeon and Sung, Simin and Kwon, Ilbong and Lee, Jae-Gil},
title = {TED Talk Recommender Using Speech Transcripts},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Nowadays, online video platforms mostly recommend related videos by analyzing user-driven data such as viewing patterns, rather than the content of the videos. However, content is more important than any other element when videos aim to deliver knowledge. Therefore, we have developed a web application which recommends related TED lecture videos to the users, considering the content of the videos from the transcripts. TED Talk Recommender constructs a network for recommending videos that are similar content-wise and providing a user interface. Our demo system is available at http://dmserver6.kaist.ac.kr:24673/.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {598–600},
numpages = {3},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/3405755.3406143,
author = {Simpson, James},
title = {Are CUIs Just GUIs with Speech Bubbles?},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406143},
doi = {10.1145/3405755.3406143},
abstract = {Conversational User Interfaces (CUIs) have become ubiquitous in recent years from mobile assistants like Apple's Siri to standalone products such as the Amazon Echo. However, as we move from established models of Human Computer Interaction (HCI), namely the Graphical User Interface (GUIs) prevalent for several decades to more dynamic, socially aware forms of interaction with technology, namely CUIs, we need to fundamentally rethink how we conduct user research. Simply applying existing methodologies to these new interaction methods will not likely tackle the unique challenges posed by them.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {23},
numpages = {3},
keywords = {evaluation, Conversational interface, user experience, study, usability},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3406367,
author = {Giabbanelli, Philippe},
title = {Session Details: Keynote Speech},
year = {2020},
isbn = {9781450375924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406367},
doi = {10.1145/3406367},
booktitle = {Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
location = {Miami, FL, Spain},
series = {SIGSIM-PADS '20}
}

@inproceedings{10.1145/3406363,
author = {Taylor, Simon},
title = {Session Details: Keynote Speech},
year = {2020},
isbn = {9781450375924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406363},
doi = {10.1145/3406363},
booktitle = {Proceedings of the 2020 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
location = {Miami, FL, Spain},
series = {SIGSIM-PADS '20}
}

@article{10.1145/3378414,
author = {Zarnoufi, Randa and Jaafar, Hamid and Abik, Mounia},
title = {Machine Normalization: Bringing Social Media Text from Non-Standard to Standard Form},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3378414},
doi = {10.1145/3378414},
abstract = {User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation--like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {49},
numpages = {30},
keywords = {automatic language identification, word sense disambiguation, code switching normalization, dialects, matrix language, social media, Text normalization, standard languages, multilingual vertical context}
}

@article{10.1109/TASLP.2022.3161160,
author = {Mrinalini, K. and Vijayalakshmi, P. and Nagarajan, T.},
title = {SBSim: A Sentence-BERT Similarity-Based Evaluation Metric for Indian Language Neural Machine Translation Systems},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3161160},
doi = {10.1109/TASLP.2022.3161160},
abstract = {Machine translation (MT) outputs are widely scored using automatic evaluation metrics and human evaluation scores. The automatic evaluation metrics are expected to be easily computable and a reflection of human evaluation. Traditional string-based metrics such as BLEU, ChrF++ scores, are widely used to evaluate MT systems, but fail to account for synonyms that appear in the state-of-the-art neural machine translation (NMT) systems, owing to their inability to evaluate paraphrases. While similarity-based metrics such as Yisi, BERTScore address this issue, these metrics need to be modified to better evaluate morphologically rich Indian languages such as, Tamil and Hindi. The current work proposes a novel and individual sentence-BERT based similarity (SBSim) metric, that makes use of a paraphrase-BERT model and sentence-level embedding to evaluate NMT outputs. The effectiveness of the BLEU, ChrF++, Yisi, BERTScore, and the proposed SBSim are evaluated on English-to-Tamil and English-to-Hindi NMT outputs. The sentence-level metric correlation of the proposed SBSim metric with respect to human scores is observed to outperform the existing metrics with a correlation of 0.9123 and 0.9052 for English-to-Tamil and English-to-Hindi NMT systems, respectively. Further, the average metric correlation of the SBSim metric is also observed to be the highest with a value of 0.9801 and 0.9836 for these NMT systems, respectively. The proposed metric is also evaluated on WMT2020 dataset and reports the highest correlation of 0.7129 with the human scores.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1396–1406},
numpages = {11}
}

@article{10.1145/3325887,
author = {Liu, Dayiheng and Yang, Kexin and Qu, Qian and Lv, Jiancheng},
title = {Ancient–Modern Chinese Translation with a New Large Training Dataset},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3325887},
doi = {10.1145/3325887},
abstract = {Ancient Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. However, the lack of large-scale parallel corpus limits the study of machine translation in ancient–modern Chinese. In this article, we propose an ancient–modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale ancient–modern Chinese parallel corpus that contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality ancient–modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {6},
numpages = {13},
keywords = {Ancient–Modern Chinese parallel corpus, bilingual text alignment, neural machine translation}
}

@inproceedings{10.1145/3495018.3495480,
author = {Zhang, Fan},
title = {Comparative Analysis and Future Development Research of Machine Translation and Human Translation Application in the Network Environment},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495480},
doi = {10.1145/3495018.3495480},
abstract = {The development of globalization, the construction of the One Belt One Road, and the continuous deepening of exchanges and cooperation between countries have caused the demand for translation to continue to increase. In today's network environment, relying solely on manual translation can no longer meet this demand, so more and more people have done research on machine translation. Since the emergence of machine translation, its fast translation speed and declining cost have been favored by the whole society. In order to make machine translation more usable by people, computational linguists have been working to improve the accuracy of machine translation. This article aims to study the comparative analysis and future development of the application of machine translation and manual translation in the network environment. It mainly adopts the literature research method, questionnaire survey method, quantitative analysis method and qualitative analysis method, and comprehensively collects and organizes the literature in this area. Classification, research and summary, provide important knowledge, perspective and methodology accumulation for this article. Experimental research shows that most teachers and students believe that the quality of human translation is better than that of machine translation, and that machine translation will not replace human translation in the future.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1764–1768},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1109/TASLP.2021.3074757,
author = {Zhang, Ya-Jie and Ling, Zhen-Hua},
title = {Extracting and Predicting Word-Level Style Variations for Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3074757},
doi = {10.1109/TASLP.2021.3074757},
abstract = {This paper proposes a speech synthesis method based on unsupervisedly-learned fine-grained style representations, named word-level style variations (WSVs), in order to improve the naturalness of synthetic speech. The whole model contains a WSV extractor and a WSV predictor. The WSV extractor is jointly trained with a sequence-to-sequence (Seq2seq) synthesizer and learns a WSV vector from the mel-spectrogram of each prosodic word in the training set by extending the global style token (GST) framework. In contrast to GST weights which describe the global styles of utterances, WSVs operate at word-level and are expected to describe local style properties, such as stresses. Besides, Gumbel softmax is adopted and the extracted WSVs are close to one-hot vectors which facilitate the subsequent prediction task. The WSV predictor is a deterministic model which generates the sequence of WSV vectors from input text using an autoregressive LSTM network. In addition to phonetic information, e.g., phoneme sequences, Bidirectional Encoder Representation from Transformers (BERT) model is employed by the predictor to obtain the semantic descriptions of input text for better predicting the latent speech representation, i.e., WSVs. The WSV predictor is trained by considering both the accuracy of WSV prediction and the distortion of mel-spectrograms recovered from the predicted WSVs. Experimental results show that our proposed method can achieve better naturalness of synthetic speech than baseline Tacotron2, text-predicted global style token (TP-GST) and BERT-Tacotron2 models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1582–1593},
numpages = {12}
}

@article{10.1109/TASLP.2021.3093823,
author = {Zhou, Xiao and Ling, Zhen-Hua and Dai, Li-Rong},
title = {UnitNet: A Sequence-to-Sequence Acoustic Model for Concatenative Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3093823},
doi = {10.1109/TASLP.2021.3093823},
abstract = {This paper presents UnitNet, a sequence-to-sequence (Seq2Seq) acoustic model for concatenative speech synthesis. Comparing with the Tacotron2 model for Seq2Seq speech synthesis, UnitNet utilizes the phone boundaries of training data and its decoder contains autoregressive structures at both phone and frame levels. This hierarchical architecture can not only extract embedding vectors for representing phone-sized units in the corpus but also measure the dependency among consecutive units, which makes the UnitNet model capable of guiding the selection of phone-sized units for concatenative speech synthesis. A byproduct of this model is that it can also be applied to statistical parametric speech synthesis (SPSS) and improve the robustness of Seq2Seq acoustic feature prediction since it adopts interpretable transition probability prediction rather than attention mechanism for frame-level alignment. Experimental results show that our UnitNet-based concatenative speech synthesis method not only outperforms the unit selection methods using hidden Markov models and Tacotron-based unit embeddings, but also achieves better naturalness and faster inference speed than the SPSS method using FastSpeech and Parallel WaveGAN. Besides, the UnitNet-based SPSS method makes fewer synthesis errors than Tacotron2 and FastSpeech without naturalness degradation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2643–2655},
numpages = {13}
}

@article{10.1109/TASLP.2021.3125142,
author = {Zhou, Yi and Tian, Xiaohai and Li, Haizhou},
title = {Language Agnostic Speaker Embedding for Cross-Lingual Personalized Speech Generation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3125142},
doi = {10.1109/TASLP.2021.3125142},
abstract = {Cross-lingual personalized speech generation seeks to synthesize a target speaker’s voice from only a few training samples that are in a different language. One popular technique is to condition a speech synthesizer on a speaker embedding, that characterizes the target speaker. Unfortunately, such a speaker embedding is usually affected by the language being spoken, which compromises the speaker similarity in cross-lingual personalized speech generation. In this paper, we propose a novel speaker encoding mechanism that learns a language agnostic speaker embedding to characterize speaker individuality. Specifically, we adopt an encoder-decoder architecture to disentangle the language information from speaker embeddings via multi-task learning. We conduct experiments on both voice conversion and text-to-speech synthesis between English and Mandarin that involve cross-lingual speech generation. All objective and subjective evaluations consistently confirm that the proposed speaker embedding is language agnostic, thus improving cross-lingual personalized speech generation in terms of speaker similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3427–3439},
numpages = {13}
}

@inproceedings{10.1145/3395027.3419588,
author = {Vale, Rafaella and Lins, Rafael Dueire and Ferreira, Rafael},
title = {An Assessment of Sentence Simplification Methods in Extractive Text Summarization},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419588},
doi = {10.1145/3395027.3419588},
abstract = {The unprecedented growth of textual content on the Web made essential the development of automatic or semi-automatic techniques to help people to find valuable information in such a huge heap of text data. Automatic text summarization is one of such techniques that is being pointed out as offering a viable solution in such a chaotic scenario. Extractive text summarization, in particular, selects a set of sentences from a text according to specific criteria. Strategies for extractive summarization can benefit from preprocessing techniques that emphasize the relevance or infor-mativeness of sentences with respect to the selection criteria. This paper tests such a hypothesis using sentence simplification methods. Four methods are used to simplify a corpus of news articles in English: a rule-based method, an optimization method, a supervised deep learning model and an unsupervised deep learning model. The simplified outputs are summarized using 14 sentence selection strategies. The combinations of simplification and summarization methods are compared with the baseline --- the summarized corpus without previous simplification --- with a quantitative analysis, which suggests sentence compression with restrictions and models learned from large parallel corpora tend to perform better and yield gains over summarization without prior simplification.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {9},
numpages = {9},
keywords = {extractive text summarization, sentence simplification, monolingual machine translation},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@article{10.1109/TASLP.2020.2997118,
author = {Xiang, Yang and Bao, Changchun},
title = {A Parallel-Data-Free Speech Enhancement Method Using Multi-Objective Learning Cycle-Consistent Generative Adversarial Network},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2997118},
doi = {10.1109/TASLP.2020.2997118},
abstract = {Recently, deep neural networks (DNNs) have become the mainstream strategy for speech enhancement task because it can achieve the higher speech quality and intelligibility than the traditional methods. However, these DNN-based methods always need a large number of parallel corpus consisting of clean speech and noise to produce noisy data for the training of the DNN in order to improve the generalization of the network. As a result, this implies that many noisy speech signals that are collected in real environment cannot be used to train the DNN because of the lack of corresponding clean speech and noise. Additionally, as we know, noise varies with the time and scenario, so we cannot obtain parallel speech and noise due to infinite noise data and some limited speech data. Thus, the network training with unparallel speech and noise data is essential for the generalization of the network. To address this problem, we propose a novel parallel-data-free speech enhancement method, in which the cycle-consistent generative adversarial network (CycleGAN) and multi-objective learning are employed. Our method is also able to make best use of the benefits of multi-objective learning. On the training stage, we utilize two different encoders to encode the features of clean speech and noisy speech, respectively. Then, two forward generators are immediately used to predict the ideal time-frequency (T-F) mask and log-power spectrum (LPS) of clean speech. Two inverse generators are applied to map the magnitude spectrum (MS) and LPS of noisy speech, respectively. In addition, four discriminators are used to distinguish the real speech features from the generated features. Two encoders, four generators and four discriminators are simultaneously trained by using adversarial, identity-mapping, latent similarity and cycle-consistent loss. On the test stage, we directly utilize the forward generators and encoders to acquire the enhanced speech. The experimental results indicate that the proposed approach is able to achieve the better speech enhancement performance than the reference methods. Moreover, the proposed method is also effective to improve speech quality and intelligibility when the networks are trained under the parallel data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1826–1838},
numpages = {13}
}

@inproceedings{10.1145/3565995.3566031,
author = {Barbay, J\'{e}r\'{e}my and Labarca-Rosenbluth, Camila and Pe\~{n}a-Haipas, Brandon},
title = {A Loggable Aid to Speech: A Research Proposal},
year = {2023},
isbn = {9781450398305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565995.3566031},
doi = {10.1145/3565995.3566031},
abstract = {Validating that non human animals can communicate with humans using Augmentative and Alternative Communication (AAC) requires extensive logging, and traditional techniques are costly in resources and time. We propose to implement 1) a configurable “communication board” application aimed at small non human animals able to use touch interfaces, which not only emits human words associated to each button, but also logs such interactions; 2) a hardware keyboard to extend the use of such an application to larger non human animals unable to use a touch screen, but able to use large keys and 3) a centralized back-end gathering the logs from various devices, facilitating their study by researchers. We propose to validate the usability of such prototype solutions with two monk parakeets parrots for the application, a dog and two cats for the keyboard (and application), and a researcher in comparative psychology for the website of the back-end.},
booktitle = {Proceedings of the Ninth International Conference on Animal-Computer Interaction},
articleno = {13},
numpages = {7},
keywords = {Animal Computer Interaction, Augmentative Interspecies Communication, Comparative Psychology, Digital Life Enrichment.},
location = {Newcastle-upon-Tyne, United Kingdom},
series = {ACI '22}
}

@inproceedings{10.1145/3405962.3405977,
author = {G\'{o}mez-Suta, Manuela and Echeverry-Correa, Juli\'{a}n D. and Soto-Mej\'{\i}a, Jos\'{e} A.},
title = {Semi-Automatic Extraction and Validation of Concepts in Ontology Learning from Texts in Spanish},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405977},
doi = {10.1145/3405962.3405977},
abstract = {The construction of ontologies from texts in Spanish is a challenge since this language lacks conceptual databases to validate abstract ontology structures as concepts and relations between them. The preceding generates the necessity of using manual evaluation by human experts; carrying high expenses that limit the calibration of algorithm parameters and large-scale evaluations. This document presents a proposal to evaluate abstract ontology structures through the task of semantic clustering of documents, without the expensive necessity of using manual evaluation or conceptual databases. The proposal is not only affordable but also applicable to model data and domains that lack structured knowledge resources. The experiments lead to the extraction and validation of the ontology structures from texts in Spanish regarding the domain of the Colombian armed conflict.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {7–16},
numpages = {10},
keywords = {Spanish, evaluation, concepts, Ontology learning},
location = {Biarritz, France},
series = {WIMS 2020}
}

@article{10.1109/TASLP.2020.2988423,
author = {Zhang, Wangyou and Chang, Xuankai and Qian, Yanmin and Watanabe, Shinji},
title = {Improving End-to-End Single-Channel Multi-Talker Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2988423},
doi = {10.1109/TASLP.2020.2988423},
abstract = {Although significant progress has been made in single-talker automatic speech recognition (ASR), there is still a large performance gap between multi-talker and single-talker speech recognition systems. In this article, we propose an enhanced end-to-end monaural multi-talker ASR architecture and training strategy to recognize the overlapped speech. The single-talker end-to-end model is extended to a multi-talker architecture with permutation invariant training (PIT). Several methods are designed to enhance the system performance, including speaker parallel attention, scheduled sampling, curriculum learning and knowledge distillation. More specifically, the speaker parallel attention extends the basic single shared attention module into multiple attention modules for each speaker, which can enhance the tracing and separation ability. Then the scheduled sampling and curriculum learning are proposed to make the model better optimized. Finally the knowledge distillation transfers the knowledge from an original single-speaker model to the current multi-speaker model in the proposed end-to-end multi-talker ASR structure. Our proposed architectures are evaluated and compared on the artificially mixed speech datasets generated from the WSJ0 reading corpus. The experiments demonstrate that our proposed architectures can significantly improve the multi-talker mixed speech recognition. The final system obtains more than 15% relative performance gains in both character error rate (CER) and word error rate (WER) compared to the basic end-to-end multi-talker ASR system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1385–1394},
numpages = {10}
}

@article{10.1109/TASLP.2021.3124420,
author = {Ding, Yi-Yang and Lin, Hao-Jian and Liu, Li-Juan and Ling, Zhen-Hua and Hu, Yu},
title = {Robustness of Speech Spoofing Detectors Against Adversarial Post-Processing of Voice Conversion},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3124420},
doi = {10.1109/TASLP.2021.3124420},
abstract = {With the development of speech synthesis and voice conversion techniques, the quality of artificially generated speech has been significantly improved and detecting such spoofing speech becomes crucial to practical applications, such as automatic speaker verification (ASV). State-of-the-art neural-network-based spoofing detection models can distinguish most artificial utterances from natural ones effectively in the latest ASVspoof 2019 evaluation. Motivated by recent progresses of adversarial example generation, this paper studies the robustness of neural-network-based speech spoofing detectors against adversarial attacks. To this end, an adversarial post-processing network (APN) is proposed which generates adversarial examples against a white-box anti-spoofing model by post-processing the speech waveforms produced by a baseline voice conversion system. Experimental results demonstrate the adversarial ability of our proposed APNs against the white-box anti-spoofing models which were used as the adversarial targets of APNs at the training stage. For example, the equal error rate (EER) of a fused detection model based on light convolution neural networks (LCNNs) increased from 0.278% to 12.743% under the white-box condition without degrading the subjective quality of converted speech. Furthermore, the trained APNs can also perform against the detectors with either unseen structures or unseen features by raising their EERs in our experiments. All these results indicate the threat of adversarial speech generation to the performance of state-of-the-art spoofing detection models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3415–3426},
numpages = {12}
}

@inproceedings{10.1145/3313831.3376310,
author = {Reitmaier, Thomas and Robinson, Simon and Pearson, Jennifer and Kalarikalayil Raju, Dani and Jones, Matt},
title = {An Honest Conversation: Transparently Combining Machine and Human Speech Assistance in Public Spaces},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376310},
doi = {10.1145/3313831.3376310},
abstract = {There is widespread concern over the ways speech assistant providers currently use humans to listen to users' queries without their knowledge. We report two iterations of the TalkBack smart speaker, which transparently combines machine and human assistance. In the first, we created a prototype to investigate whether people would choose to forward their questions to a human answerer if the machine was unable to help. Longitudinal deployment revealed that most users would do so when given the explicit choice. In the second iteration we extended the prototype to draw upon spoken answers from previous deployments, combining machine efficiency with human richness. Deployment of this second iteration shows that this corpus can help provide relevant, human-created instant responses. We distil lessons learned for those developing conversational agents or other AI-infused systems about how to appropriately enlist human-in-the-loop information services to benefit users, task workers and system performance.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {conversational agents, public space interaction, emergent users, speech appliances},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1109/TASLP.2020.3030495,
author = {Sun, Xingwei and Gao, Ze-Feng and Lu, Zhong-Yi and Li, Junfeng and Yan, Yonghong},
title = {A Model Compression Method With Matrix Product Operators for Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3030495},
doi = {10.1109/TASLP.2020.3030495},
abstract = {The deep neural network (DNN) based speech enhancement approaches have achieved promising performance. However, the number of parameters involved in these methods is usually enormous for the real applications of speech enhancement on the device with the limited resources. This seriously restricts the applications. To deal with this issue, model compression techniques are being widely studied. In this paper, we propose a model compression method based on matrix product operators (MPO) to substantially reduce the number of parameters in DNN models for speech enhancement. In this method, the weight matrices in the linear transformations of neural network model are replaced by the MPO decomposition format before training. In experiment, this process is applied to the causal neural network models, such as the feedforward multilayer perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM models with/without compression are then utilized to estimate the ideal ratio mask for monaural speech enhancement. The experimental results show that our proposed MPO-based method outperforms the widely-used pruning method for speech enhancement under various compression rates, and further improvement can be achieved with respect to low compression rates. Our proposal provides an effective model compression method for speech enhancement, especially in cloud-free application.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2837–2847},
numpages = {11}
}

@article{10.1109/TASLP.2019.2937174,
author = {Wood, Sean U. N. and Stahl, Johannes K. W. and Mowlaee, Pejman},
title = {Binaural Codebook-Based Speech Enhancement With Atomic Speech Presence Probability},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2937174},
doi = {10.1109/TASLP.2019.2937174},
abstract = {In this work, we present a universal codebook-based speech enhancement framework that relies on a single codebook to encode both speech and noise components. The atomic speech presence probability ASPP is defined as the probability that a given codebook atom encodes speech at a given point in time. We develop ASPP estimators based on binaural cues including the interaural phase and level difference IPD and ILD, the interaural coherence magnitude ICM, as well as a combined version leveraging the full interaural transfer function ITF. We evaluate the performance of the resulting ASPP-based speech enhancement algorithms on binaural mixtures of reverberant speech and real-world noise. The proposed approach improves both objective speech quality and intelligibility over a wide range of input SNR, as measured with PESQ and binaural STOI metrics, outperforming two binaural speech enhancement benchmark methods. We show that the proposed ITF-based ASPP approach achieves a good balance of the trade-off between binaural noise reduction and binaural cue preservation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2150–2161},
numpages = {12}
}

@inproceedings{10.1145/3330482.3330491,
author = {Aulia, Nofa and Budi, Indra},
title = {Hate Speech Detection on Indonesian Long Text Documents Using Machine Learning Approach},
year = {2019},
isbn = {9781450361064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330482.3330491},
doi = {10.1145/3330482.3330491},
abstract = {Due to the growth of hate speech on social media in recent years, it is important to understand this issue. An automatic hate speech detection system is needed to help to counter this problem. There have been many studies on detecting hate speech in short documents like Twitter data. But to our knowledge, research on long documents is rare, we suppose that the difficulty is increasing due to the possibility of the message of the text may be hidden. In this research, we explore in detecting hate speech on Indonesian long documents using machine learning approach. We build a new Indonesian hate speech dataset from Facebook. The experiment showed that the best performance obtained by Support Vector Machine (SVM) as its classifier algorithm using TF-IDF, char quad-gram, word unigram, and lexicon features that yield f1-score of 85%.},
booktitle = {Proceedings of the 2019 5th International Conference on Computing and Artificial Intelligence},
pages = {164–169},
numpages = {6},
keywords = {hate speech detection, machine learning, SVM, long documents},
location = {Bali, Indonesia},
series = {ICCAI '19}
}

@inproceedings{10.1145/3548636.3548647,
author = {Thai, Lam H. and Nguyen, Long H. B. and Dinh, Dien},
title = {Improve the Automatic Transliteration from N\^{o}m Scripts into Vietnamese National Scripts by Integrating Sino – Vietnamese Knowledge into Statistical Machine Translation},
year = {2022},
isbn = {9781450396820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548636.3548647},
doi = {10.1145/3548636.3548647},
abstract = {N\^{o}m scripts (chundefined N\^{o}m) are Vietnamese ancient scripts that were popularly used in Vietnam from the 10th century to the early 20th century. Nowadays, some automatic transliteration from N\^{o}m scripts (NS) into Vietnamese National scripts (chundefined Quundefinedc ngundefined - QN) systems were developed to help modern Vietnamese people acquire many valuable lessons and knowledge from previous generations through preserving the Sino-Nom heritage. However, these systems have still not performed well in many domains, except for Literature. Our research continues to employ Statistical Machine Translation (SMT) but expands the dataset up to 10 domains. Furthermore, we also focus on analyzing the impact of Chinese scripts with Sino-Vietnamese readings on N\^{o}m script – National script and then integrating this knowledge into our transliteration model. Our experimental results show that our approach helps the model reach 94.04 BLEU score, dramatically increasing by 8.63 BLEU score in the genealogical domain and 0.31 BLEU score in the general model.},
booktitle = {Proceedings of the 4th International Conference on Information Technology and Computer Communications},
pages = {69–77},
numpages = {9},
location = {Guangzhou, China},
series = {ITCC '22}
}

@article{10.1109/TASLP.2020.3039929,
author = {Edraki, Amin and Chan, Wai-Yip and Jensen, Jesper and Fogerty, Daniel},
title = {Speech Intelligibility Prediction Using Spectro-Temporal Modulation Analysis},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3039929},
doi = {10.1109/TASLP.2020.3039929},
abstract = {Spectro-temporal modulations are believed to mediate the analysis of speech sounds in the human primary auditory cortex. Inspired by humans’ robustness in comprehending speech in challenging acoustic environments, we propose an intrusive speech intelligibility prediction (SIP) algorithm, wSTMI, for normal-hearing listeners based on spectro-temporal modulation analysis (STMA) of the clean and degraded speech signals. In the STMA, each of 55 modulation frequency channels contributes an intermediate intelligibility measure. A sparse linear model with parameters optimized using Lasso regression results in combining the intermediate measures of 8 of the most salient channels for SIP. In comparison with a suite of 10 SIP algorithms, wSTMI performs consistently well across 13 datasets, which together cover degradation conditions including modulated noise, noise reduction processing, reverberation, near-end listening enhancement, and speech interruption. We show that the optimized parameters of wSTMI may be interpreted in terms of modulation transfer functions of the human auditory system. Thus, the proposed approach offers evidence affirming previous studies of the perceptual characteristics underlying speech signal intelligibility.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {210–225},
numpages = {16}
}

@article{10.1145/3588769,
author = {Chen, Yuanyuan},
title = {Intelligent English Language Translation And Grammar Learning Based On Internet Of Things Technology},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588769},
doi = {10.1145/3588769},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr}
}

@article{10.1109/TASLP.2022.3230453,
author = {Prakash, Anusha and Murthy, Hema A.},
title = {Exploring the Role of Language Families for Building Indic Speech Synthesisers},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3230453},
doi = {10.1109/TASLP.2022.3230453},
abstract = {Building end-to-end speech synthesisers for Indian languages is challenging, given the lack of adequate clean training data and multiple grapheme representations across languages. This work explores the importance of training multilingual and multi-speaker text-to-speech (TTS) systems based on <italic>language families</italic>. The objective is to exploit the phonotactic properties of language families, where small amounts of accurately transcribed data across languages can be pooled together to train TTS systems. These systems can then be adapted to new languages belonging to the same family in extremely low-resource scenarios. TTS systems are trained separately for Indo-Aryan and Dravidian language families, and their performance is compared to that of a combined Indo-Aryan+Dravidian voice. We also investigate the amount of training data required for a language in a multilingual setting. Same-family and cross-family synthesis and adaptation to unseen languages are analysed. The analyses show that language family-wise training of Indic systems is the way forward for the Indian subcontinent, where a large number of languages are spoken.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {734–747},
numpages = {14}
}

@inproceedings{10.1145/3529570.3529602,
author = {Cheng, Pengyu and Ling, Zhenhua},
title = {Speaker Adaption with Intuitive Prosodic Features for Statistical Parametric Speech Synthesis},
year = {2022},
isbn = {9781450395809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529570.3529602},
doi = {10.1145/3529570.3529602},
abstract = {In this paper, we propose a method of speaker adaption with intuitive prosodic features for statistical parametric speech synthesis. The intuitive prosodic features employed in this method include pitch, pitch range, speech rate and energy considering that they are directly related with the overall prosodic characteristics of different speakers. The intuitive prosodic features are extracted at utterance-level or speaker-level, and are further integrated into the existing speaker-encoding-based and speaker-embedding-based adaptation frameworks respectively. The acoustic models are sequence-to-sequence ones based on Tacotron2. Intuitive prosodic features are concatenated with text encoder outputs and speaker vectors for decoding acoustic features. Experimental results have demonstrated that our proposed methods can achieve better objective and subjective performance than the baseline methods without intuitive prosodic features. Besides, the proposed speaker adaption method with utterance-level prosodic features has achieved the best similarity of synthetic speech among all compared methods.},
booktitle = {Proceedings of the 6th International Conference on Digital Signal Processing},
pages = {187–193},
numpages = {7},
keywords = {speaker adaption, intuitive prosodic features, speech synthesis, Tacotron2, speaker representation},
location = {Chengdu, China},
series = {ICDSP '22}
}

@inproceedings{10.1145/3534678.3539188,
author = {Li, Zhuliu and Wang, Yiming and Yan, Xiao and Meng, Weizhi and Li, Yanen and Yang, Jaewon},
title = {TaxoTrans: Taxonomy-Guided Entity Translation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539188},
doi = {10.1145/3534678.3539188},
abstract = {Taxonomies describe the definitions of entities, entities' attributes and the relations among the entities, and thus play an important role in building a knowledge graph. In this paper, we tackle the task of taxonomy entity translation, which is to translate the names of taxonomy entities in a source language to a target language. The translations then can be utilized to build a knowledge graph in the target language. Despite its importance, taxonomy entity translation remains a hard problem for AI models due to two major challenges. One challenge is understanding the semantic context in very short entity names. Another challenge is having deep understanding for the domain where the knowledge graph is built. We present TaxoTrans, a novel method for taxonomy entity translation that can capture the context in entity names and the domain knowledge in taxonomy. To achieve this, TaxoTrans creates a heterogeneous graph to connect entities, and formulates the entity name translation problem as link prediction in the heterogeneous graph: given a pair of entity names across two languages, TaxoTrans applies a graph neural network to determine whether they form a translation pair or not. Because of this graph, TaxoTrans can capture both the semantic context and the domain knowledge. Our offline experiments on LinkedIn's skill and title taxonomies show that by modeling semantic information and domain knowledge in the heterogeneous graph, TaxoTrans outperforms the state-of-the-art translation methods by ∼10%. Human annotation and A/B test results further demonstrate that the accurately translated entities significantly improves user engagements and advertising revenue at LinkedIn.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3279–3287},
numpages = {9},
keywords = {knowledge graph, entity translation, taxonomy, graph neural network},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3387940.3391541,
author = {Zhang, Jie M.},
title = {Automatic Improvement of Machine Translation Using Mutamorphic Relation: Invited Talk Paper},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391541},
doi = {10.1145/3387940.3391541},
abstract = {This paper introduces Mutamorphic Relation for Machine Learning Testing. Mutamorphic Relation combines data mutation and metamorphic relations as test oracles for machine learning systems. These oracles can help achieve fully automatic testing as well as automatic repair of the machine learning models.The paper takes TransRepair as an example to show the effectiveness of Mutamorphic Relation in automatically testing and improving machine translators, TransRepair detects inconsistency bugs without access to human oracles. It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Manual inspection indicates that the translations repaired by TransRepair improve consistency in 87% of cases (degrading it in 2%), and that the repairs of have better translation acceptability in 27% of the cases (worse in 8%).},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {425–426},
numpages = {2},
keywords = {mutation testing, metamorphic testing, mutamorphic relation},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3582099.3582101,
author = {Gonzales, Joshua Andre Huertas and Gustilo, J-Adrielle Enriquez and Nituda, Glenn Michael Vequilla and Adlaon, Kristine Mae Monteza},
title = {Developing a Hybrid Neural Network for Part-Of-Speech Tagging and Named Entity Recognition},
year = {2023},
isbn = {9781450398749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582099.3582101},
doi = {10.1145/3582099.3582101},
abstract = {Enabling our computers to understand human languages is very important as we move towards the 4th to 5th industrial revolution. A lot of efforts are already made to fast track this development most especially for highly resourced languages such as English, French, German, among others. The most notable open-source NLP tool built is the Natural Language toolkit, a collection of modules and corpora that provides researchers in Natural Language Processing (NLP) with extremely useful tools and resources. In the Philippines, several researchers have contributed to this advancement mostly for the Filipino language. In this paper, we introduce the design architecture of a hybrid neural network model that combines the best component features of the existing architectures for the POS Tagging and NER tasks. We also present initial experiment results.},
booktitle = {Proceedings of the 2022 5th Artificial Intelligence and Cloud Computing Conference},
pages = {7–13},
numpages = {7},
keywords = {Natural Language Processing, Cebuano, NER, POS Tagging, Neural Networks},
location = {Osaka, Japan},
series = {AICCC '22}
}

@inproceedings{10.1145/3517428.3544805,
author = {Fontana de Vargas, Mauricio and Dai, Jiamin and Moffatt, Karyn},
title = {AAC with Automated Vocabulary from Photographs: Insights from School and Speech-Language Therapy Settings},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544805},
doi = {10.1145/3517428.3544805},
abstract = {Traditional symbol-based AAC devices impose meta-linguistic and memory demands on individuals with complex communication needs and hinder conversation partners from stimulating symbolic language in meaningful moments. This work presents a prototype application that generates situation-specific communication boards formed by a combination of descriptive, narrative, and semantic related words and phrases inferred automatically from photographs. Through semi-structured interviews with AAC professionals, we investigate how this prototype was used to support communication and language learning in naturalistic school and therapy settings. We find that the immediacy of vocabulary reduces conversation partners’ workload, opens up opportunities for AAC stimulation, and facilitates symbolic understanding and sentence construction. We contribute a nuanced understanding of how vocabularies generated automatically from photographs can support individuals with complex communication needs in using and learning symbolic AAC, offering insights into the design of automatic vocabulary generation methods and interfaces to better support various scenarios of use and goals.},
booktitle = {Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {23},
numpages = {18},
keywords = {Augmentative and Alternative Communication, just-in-time, automatic, autism},
location = {Athens, Greece},
series = {ASSETS '22}
}

@article{10.1145/3591471,
author = {Trujillo-Romero, Felipe and Garc\'{\i}a-Bautista, Gibran},
title = {Mexican Sign Language Corpus: Towards an Automatic Translator},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3591471},
doi = {10.1145/3591471},
abstract = {The development of the Sign Language Corpus has been motivated by its great utility and application to various purposes and research areas. However, some countries do not have their own Sign Language Corpus. Developing a corpus thereby benefits the community of people with speech disabilities in diverse areas such as education. Thus, the motivation to develop this work is to present an advance toward constructing an RGB-D corpus of Mexican Sign Language captured by a Kinect sensor. A total of 90,000 samples of 570 words and 30 phrases interpreted by 150 people who commonly use Mexican Sign Language were collected. Of the participants, 86 were women and 64 were men, aged between 12 and 60 years old. The Mexican Sign Language Corpus was recorded by signers from three different regions of the south of Mexico. The constructed corpus contains depth, color, point clouds, and human skeleton positions. Six hundred of the most used words were selected from 17 semantic fields, considering the variability in the movement of both hands. After training a neural network, the performance developed by the recognition system was 98.62%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {212},
numpages = {24},
keywords = {sign recognition system, corpus, Mexican Sign Language, Applied linguistics}
}

@article{10.1109/TASLP.2019.2941592,
author = {Pfeifenberger, Lukas and Zohrer, Matthias and Pernkopf, Franz},
title = {Eigenvector-Based Speech Mask Estimation for Multi-Channel Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2941592},
doi = {10.1109/TASLP.2019.2941592},
abstract = {We present the Eigennet architecture for estimating a gain mask from noisy, multi-channel microphone observations. While existing mask estimators use magnitude features, our system also exploits the spatial information embedded in the phase of the data. The mask is used to obtain the Minimum Variance Distortionless Response MVDR and Generalized Eigenvalue GEV beamformers. We also derive the Phase Aware Normalization PAN postfilter, which corrects both magnitude and phase distortions caused by the GEV. Further, we demonstrate the properties of our eigenvector features, and compare their performance with three state-of-the-art reference systems. We report their performance in terms of SNR improvement and Word Error Rate WER using Google and Kaldi Speech-to-Text API. Experiments are performed on the WSJ0 and CHiME4 corpora, where competitive performance in both WER and SNR is achieved.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2162–2172},
numpages = {11}
}

@article{10.1109/TASLP.2021.3074012,
author = {Zhang, Chen and Lee, Grandee and D’Haro, Luis Fernando and Li, Haizhou},
title = {D-Score: Holistic Dialogue Evaluation Without Reference},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3074012},
doi = {10.1109/TASLP.2021.3074012},
abstract = {In artistic gymnastics, difficulty score or D-score is used for judging performance. Starting from zero, an athlete earns points from different aspects such as composition requirement, difficulty, and connection between moves. The final score is a composition of the quality of various performance indicators. Similarly, when evaluating dialogue responses, human judges generally follow a number of criteria, among which language fluency, context coherence, logical consistency, and semantic appropriateness are on top of the agenda. In this paper, we propose an automatic dialogue evaluation framework called D-score that resembles the way gymnastics is evaluated. Following the four human judging criteria above, we devise a range of evaluation tasks and model them under a multi-task learning framework. The proposed framework, without relying on any human-written reference, learns to appreciate the overall quality of human-human conversations through a representation that is shared by all tasks without over-fitting to individual task domain. We evaluate D-score by performing comprehensive correlation analyses with human judgement on three dialogue evaluation datasets, among which two are from past DSTC series, and benchmark against state-of-the-art baselines. D-score not only outperforms the best baseline by a large margin in terms of system-level Spearman correlation but also represents an important step towards explainable dialogue scoring.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {2502–2516},
numpages = {15}
}

@inproceedings{10.1145/3549206.3549251,
author = {Yadavalli, Aditya and Jain, Shelly and Mirishkar, Ganesh and Vuppala, Anil Kumar},
title = {Investigation of Subword-Based Bilingual Automatic Speech Recognition for Indian Languages},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549251},
doi = {10.1145/3549206.3549251},
abstract = {Modelling Indian languages is difficult as the number of word forms they present is high. In such cases, prior work has proposed using subwords instead of words as the basic units to model the language. This provides the potential to cover more word forms. Additionally, previous work revealed that all Indian languages have several phonetic similarities. Consequently, multilingual acoustic models have been proposed to counter the data scarcity issue most Indian language datasets have. However, these models use monolingual language models (LM). They do not exploit the common basic tokens that certain Indian languages have. This motivated us to implement a bilingual subword-based Automatic Speech Recognition (ASR) system for Hindi and Marathi. Further, we try a combination of word-based monolingual LM with bilingual acoustic model to examine the reason for degradation in word-based multilingual ASRs. Our experiments show that multilingual subword-based ASR models outperform their word-based counterparts by upto 9.77% and 26.35% relative Character Error Rate (CER) in the case of Hindi and Marathi respectively.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {234–241},
numpages = {8},
keywords = {Speech Recognition, Language Modelling, Multilingual NLP, Subword Analysis, Low-Resource},
location = {Noida, India},
series = {IC3-2022}
}

@article{10.1145/3594631,
author = {Chakrabarty, Abhisek and Dabre, Raj and Ding, Chenchen and Utiyama, Masao and Sumita, Eiichiro},
title = {Low-Resource Multilingual Neural Translation Using Linguistic Feature-Based Relevance Mechanisms},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3594631},
doi = {10.1145/3594631},
abstract = {This article investigates approaches to effectively harness source-side linguistic features for low-resource multilingual neural machine translation (MNMT). Previous works focus on using various features of a word such as lemma, part-of-speech tag, dependency label, and so on, to improve translation quality in a low-resource scenario. However, these studies deal with bilingual translation and do not focus on using features in multilingual training setups. Our work focuses on this particular point and experiments with low-resource multilingual models incorporating source-side linguistic features. Although techniques for integrating features into an NMT model such as concatenation and feature relevance perform quite well in bilingual settings, they do not work well in multilingual settings. To remedy this, we propose the use of dummy features and language indicator features in MNMT models. Experiments are conducted on English to Asian language translation on a multilingual, multi-parallel corpus spanning English and eight Asian languages where for each language pair, the training data size does not exceed 20,000 parallel sentences. After establishing strong bilingual baselines using feature relevance mechanisms and multilingual baselines without any features, we show that our proposed dummy features and language indicator features, in combination with feature relevance mechanisms, yield significant improvements in BLEU points for all language pairs. We then analyze our models from the perspectives of model sizes, the impact of individual linguistic features, validation perplexity computed during training, visualization of the activations of the relevance mechanisms, and exhaustive tuning of hyperparameters. We also report preliminary results for multilingual multi-way models using linguistic features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {191},
numpages = {36},
keywords = {neural networks, Linguistic features, morphology}
}

@article{10.1109/TASLP.2020.2986877,
author = {T, Lavanya and T, Nagarajan and P, Vijayalakshmi},
title = {Multi-Level Single-Channel Speech Enhancement Using a Unified Framework for Estimating Magnitude and Phase Spectra},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986877},
doi = {10.1109/TASLP.2020.2986877},
abstract = {Speech enhancement algorithms aim to improve the quality and intelligibility of noise corrupted speech, through spectral or temporal modifications. Most of the existing speech enhancement algorithms achieve this by modifying the magnitude spectrum alone, while keeping the phase spectrum intact. In the current work, both phase and magnitude spectra are modified to enhance noisy speech using a multi-level speech enhancement technique. Proposed phase compensation (PC) function achieves first-level enhancement by modifying the phase spectrum alone. Second-level enhancement performs energy redistribution in the phase compensated speech signal to make weak speech and non-speech regions highly contrastive. Energy redistribution from the energy-rich voiced to the weak unvoiced regions is carried out using adaptive power law transformation (APLT) technique by optimizing the parameters with a total energy constraint employing particle swarm optimization algorithm. Log MMSE technique with a novel speech presence uncertainty (SPU) estimation method is proposed for third-level enhancement. The compensated phase spectrum and the magnitude spectrum estimated using log MMSE, with proposed SPU estimation (log MMSE + proposed SPU), are used to reconstruct the enhanced speech signal. The proposed speech enhancement technique is compared with recent speech enhancement techniques that estimate both magnitude and phase, for various noise levels (−5 to +5&nbsp;dB), in terms of objective and subjective measures. It is observed that the proposed technique improves signal quality and maintains or improves intelligibility under stationary, and non-stationary noise conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1315–1327},
numpages = {13}
}

@article{10.1109/TASLP.2022.3145307,
author = {Mingote, Victoria and Miguel, Antonio and Ribas, Dayana and Ortega, Alfonso and Lleida, Eduardo},
title = {ADCF Loss Function for Deep Metric Learning in End-to-End Text-Dependent Speaker Verification Systems},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145307},
doi = {10.1109/TASLP.2022.3145307},
abstract = {Metric learning approaches have widely expanded to the training of Speaker Verification (SV) systems based on Deep Neural Networks (DNNs), by using a loss function more consistent with the evaluation process than the traditional identification losses. However, these methods do not consider the performance measure and can involve high computational cost, for example, the need for a careful pair or triplet data selection. This paper proposes the approximated Detection Cost Function (aDCF) loss, which is a loss function based on the measure of the decision errors in SV systems, namely the False Rejection Rate (FRR) and the False Acceptance Rate (FAR). With aDCF loss as the training objective function, the end-to-end system learns how to minimize decision errors. Furthermore, we replace the typical linear layer as the last layer of DNN by a cosine distance layer, which reduces the difference between the metric in the training process and the metric during evaluation. aDCF loss function was evaluated in RSR2015-Part I and RSR2015-Part II datasets for text-dependent speaker verification. The system trained with aDCF loss outperforms all the state-of-the-art functions employed in this paper in both parts of the database.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {772–784},
numpages = {13}
}

@article{10.1145/3618111,
author = {Dong, Jun},
title = {Transfer Learning-Based Neural Machine Translation for Low-Resource Languages},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3618111},
doi = {10.1145/3618111},
abstract = {College of Liberal Arts, Ludong University, Yantai 264025, ChinaChinese Lexicography Research Center, Yantai 264025, ChinaNeural Machine Translation (NMT) improves readability by augmenting sentence suggestions based on the precise likelihood of the words. The word suggestions are trained using learning paradigms through repeated translations, word searches, and user inputs. However, the challenging process is the implication of NMT for low-resource language wherein the chances of false suggestions/ word substitutions are high. So, this article proposes a Likelihood-based Machine Translation Model (LMTM) for low-resource languages. The model uses word frequency and potential substitutions from less-known sentences to identify sentences with high precision. This is achieved through a combination of recurrence and substitutions using transfer learning. The identified high-likelihood words are used for sentence augmentation, and the entire set of words from the generated sentence updates the learning paradigm. The model suggests the highest likelihood words for NMT, preventing sentence falsification and ensuring accurate translations. The proposed model increases likelihood by 9.15%, correctness by 7.46%, and substitutions by 7.7%, respectively. It reduces falsification and time complexity by 9.33% and 8.52%, respectively. Overall, the LMTM improves translation quality for low-resource languages.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {NMT, Word Suggestions, Transfer Learning, Likelihood}
}

@article{10.1109/TASLP.2021.3129335,
author = {Zhang, Zhuohuang and Xu, Yong and Yu, Meng and Zhang, Shi-Xiong and Chen, Lianwu and Williamson, Donald S. and Yu, Dong},
title = {Multi-Channel Multi-Frame ADL-MVDR for Target Speech Separation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3129335},
doi = {10.1109/TASLP.2021.3129335},
abstract = {Many purely neural network based speech separation approaches have been proposed to improve objective assessment scores, but they often introduce nonlinear distortions that are harmful to modern automatic speech recognition (ASR) systems. Minimum variance distortionless response (MVDR) filters are often adopted to remove nonlinear distortions, however, conventional neural mask-based MVDR systems still result in relatively high levels of residual noise. Moreover, the matrix inverse involved in the MVDR solution is sometimes numerically unstable during joint training with neural networks. In this study, we propose a multi-channel multi-frame (MCMF) all deep learning (ADL)-MVDR approach for target speech separation, which extends our preliminary multi-channel ADL-MVDR approach. The proposed MCMF ADL-MVDR system addresses linear and nonlinear distortions. Spatio-temporal cross correlations are also fully utilized in the proposed approach. The proposed systems are evaluated using a Mandarin audio-visual corpus and are compared with several state-of-the-art approaches. Experimental results demonstrate the superiority of our proposed systems under different scenarios and across several objective evaluation metrics, including ASR performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3526–3540},
numpages = {15}
}

@inproceedings{10.1145/3343031.3351004,
author = {Wang, Yaxing and Gonzalez-Garcia, Abel and van de Weijer, Joost and Herranz, Luis},
title = {SDIT: Scalable and Diverse Cross-Domain Image Translation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351004},
doi = {10.1145/3343031.3351004},
abstract = {Recently, image-to-image translation research has witnessed remarkable progress. Although current approaches successfully generate diverse outputs or perform scalable image transfer, these properties have not been combined into a single method. To address this limitation, we propose SDIT: Scalable and Diverse image-to-image translation. These properties are combined into a single generator. The diversity is determined by a latent variable which is randomly sampled from a normal distribution. The scalability is obtained by conditioning the network on the domain attributes. Additionally, we also exploit an attention mechanism that permits the generator to focus on the domain-specific attribute. We empirically demonstrate the performance of the proposed method on face mapping and other datasets beyond faces.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1267–1276},
numpages = {10},
keywords = {image translation, generative adversarial networks, image generation},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3527173,
author = {Huang, Xin},
title = {On Teaching Mode of MTI Translation Workshop Based on IPT Corpus for Tibetan Areas of China},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3527173},
doi = {10.1145/3527173},
abstract = {With the technological turn of applied research in translation, increasing attention has been paid to the teaching of translation technology. This article addresses two important questions in this regard: how to independently develop Master of Translation and Interpreting (MTI) translation teaching resources with ethnic minority characteristics and how to use information technology to carry out Tibet-related computer-assisted translation (CAT) teaching. This article discusses the background, structure, and functions of the International Publicity Translation Corpus (IPT Corpus) for Tibetan Areas of China through empirical research, combining theory with practice, and validates the translation teaching mode through case study to better train translators and interpreters working on content related to Tibetan culture. Through teaching practice since 2017, the MTI translation workshop based on the IPT Corpus has proven to be an effective teaching mode that is worthy of further improvement and extension.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jan},
articleno = {31},
numpages = {16},
keywords = {IPT Corpus, workshop, MTI translation teaching, Translation technology}
}

@inproceedings{10.1145/3510858.3510981,
author = {Tu, Xiangying},
title = {An Intelligent Fuzzy Decision Tree Algorithm for English Machine Translation},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510981},
doi = {10.1145/3510858.3510981},
abstract = {Natural language processing is a method in computer science to obtain and analyze meaning from human language and interact with human in an intelligent way. The translation results of decision tree have advantages for data distribution, but the distribution refinement can not be effectively improved. This paper analyzes the factors that affect the Machine Translation quality by means of the theory of fuzzy multi decision fuzzy analysis. In order to let more people know more about fuzzy decision analysis method of Machine Translation, this paper elaborates the process of making sure the influencing factors to establish the fuzzy comprehensive evaluation method of Machine Translation, so as to establish decision tree model of English Machine Translation based on a fuzzy multi-decision analysis. The solving process of the model is also described in detail, and the results of evaluation are analyzed quantitatively, and the rationality of the Machine Translation is obtained.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {425–428},
numpages = {4},
location = {Changsha, China},
series = {ICASIT 2021}
}

@article{10.1109/TASLP.2022.3233238,
author = {Ghorbani, Shahram and Hansen, John H.L.},
title = {Domain Expansion for End-to-End Speech Recognition: Applications for Accent/Dialect Speech},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3233238},
doi = {10.1109/TASLP.2022.3233238},
abstract = {Training Automatic Speech Recognition (ASR) systems with sequentially incoming data from alternate domains is an essential milestone in order to reach human intelligibility level in speech recognition. The main challenge of sequential learning is that current adaptation techniques result in significant performance degradation for previously-seen domains. To mitigate the catastrophic forgetting problem, this study proposes effective domain expansion techniques for two scenarios: 1) where only new domain data is available, and 2) where both prior and new domain data are available. We examine the efficacy of the approaches through experiments on adapting a model trained with native English to different English accents. For the first scenario, we study several existing and proposed regularization-based approaches to mitigate performance loss of initial data. The experiments demonstrate the superior performance of our proposed Soft KL-Divergence (SKLD)-Model Averaging (MA) approach. In this approach, SKLD first alleviates the forgetting problem during adaptation; next, MA makes the final efficient compromise between the two domains by averaging parameters of the initial and adapted models. For the second scenario, we explore several rehearsal-based approaches, which leverage initial data to maintain the original model performance. We propose Gradient Averaging (GA) as well as an approach which operates by averaging gradients computed for both initial and new domains. Experiments demonstrate that GA outperforms retraining and specifically designed continual learning approaches, such as Averaged Gradient Episodic Memory (AGEM). Moreover, GA significantly improves computational costs over the complete retraining approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {762–774},
numpages = {13}
}

@article{10.1145/3511806,
author = {Ahmadi, Sina and Hassani, Hossein and Jaff, Daban Q.},
title = {Leveraging Multilingual News Websites for Building a Kurdish Parallel Corpus},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511806},
doi = {10.1145/3511806},
abstract = {Machine translation has been a major motivation of development in natural language processing. Despite the burgeoning achievements in creating more efficient machine translation systems, thanks to deep learning methods, parallel corpora have remained indispensable for progress in the field. In an attempt to create parallel corpora for the Kurdish language, in this article, we describe our approach in retrieving potentially alignable news articles from multi-language websites and manually align them across dialects and languages based on lexical similarity and transliteration of scripts. We present a corpus containing 12,327 translation pairs in the two major dialects of Kurdish, Sorani and Kurmanji. We also provide 1,797 and 650 translation pairs in English-Kurmanji and English-Sorani. The corpus is publicly available under the CC BY-NC-SA 4.0 license.1},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {99},
numpages = {11},
keywords = {machine translation, less-resourced languages, Kurdish, natural language processing, Parallel corpus}
}

@inproceedings{10.1145/3351095.3375666,
author = {Kaeser-Chen, Christine and Dubois, Elizabeth and Sch\"{u}\"{u}r, Friederike and Moss, Emanuel},
title = {Positionality-Aware Machine Learning: Translation Tutorial},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375666},
doi = {10.1145/3351095.3375666},
abstract = {Positionality is a person's unique and always partial view of the world which is shaped by social and political contexts. Machine Learning (ML) systems have positionality, too, as a consequence of the choices we make when we develop ML systems. Being positionality-aware is key for ML practitioners to acknowledge and embrace the necessary choices embedded in ML by its creators.When groups form a shared view of the world, or group positionality, they have the power to embed and institutionalize their unique perspectives in artifacts such as standards and ontologies. For example, the international standard for reporting diseases and health conditions (International Classification of Diseases, ICD) is shaped by a distinctly medical, European and North American perspective. It dictates how we collect data, and limits what questions we can ask of data and what ML systems we can develop. Researchers struggle to study the effects of social factors on health outcomes because of what the ICD renders legible (usually in medicalized terms) and what it renders invisible (usually social contexts) in data. The ICD, as with all information infrastructures, promotes and propagates the perspective(s) of its creators. Over time, it establishes what counts as "truth".Positionality, and how it embeds itself in standards, ontologies, and data collection, is the root for bias in our data and algorithms. Every perspective has its limits - there is no view from nowhere. Without an awareness of positionality, the current debate on bias in machine learning is quite limited: adding more data to the set cannot remove bias. Instead, we propose positionality-aware ML, a new workflow focused on continuous evaluation and improvement of the fit between the positionality embedded in ML systems and the scenarios within which it is deployed.To demonstrate how to uncover positionality in standards, ontologies, data, and ML systems, we discuss recent work on online harassment of Canadian journalists and politicians on Twitter. Using legal definitions of hate speech and harassment, Twitter's community standards, and insight from interviews with journalists and politicians, we created standards and annotation guidelines for labeling the intensity of harassment in tweets. We then hand labeled a sample of data and through this process identified instances where positionality impacts choices about how many categories of harassment should exist, how to label boundary cases, and how to interpret messy data. We take three perspectives---technical, systems, socio-technical---that when combined illuminate areas of tension which serve as a signal of misalignment between the positionality embedded in the ML system and the deployment context. We demonstrate how the concept of positionality allows us to delineate sets of use cases that may not be suited for automated, ML solutions. Finally, we discuss strategies for developing positionality-aware ML systems, which embed a positionality appropriate for the application context, and continuously evolve to maintain this contextual fit, with an emphasis on the need for of democratic, egalitarian dialogues between knowledge-producing groups.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {704},
numpages = {1},
keywords = {positionality, artificial intelligence, categories},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{10.1145/3585088.3589369,
author = {Lamichhane, Dev Raj and Read, Janet and Mackenzie, Scott},
title = {When Children Chat with Machine Translated Text: Problems, Possibilities, Potential},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589369},
doi = {10.1145/3585088.3589369},
abstract = {Two cross-lingual (Nepalese and English) letter exchanges took place between school children from Nepal and England, using Digipal; an Android chatting application. Digipal uses Google Translate to enable children to read and reply in their native language. In two studies we analysed the errors made and the effect of errors on children’s understanding and on the flow of conversation. We found that errors of input negatively affected translation, although this can be reduced through initial grammar cleaning. We highlight features of children’s text that cause errors in translation whilst showing how children worked with and around these errors. Errors sometimes added humour and contributed to continuing the conversations.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {198–209},
numpages = {12},
keywords = {Text Entry, Machine Translation, Child Computer Interaction, Errors},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@article{10.1109/TASLP.2021.3126947,
author = {Shifas, Muhammed P.V. and Zoril\u{a}, C\u{a}t\u{a}lin and Stylianou, Yannis},
title = {End-to-End Neural Based Modification of Noisy Speech for Speech-in-Noise Intelligibility Improvement},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126947},
doi = {10.1109/TASLP.2021.3126947},
abstract = {Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {162–173},
numpages = {12}
}

@article{10.1145/3581771,
author = {Peres, Florent and Ghazel, Mohamed},
title = {A Proven Translation from a UML State Machine Subset to Timed Automata},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1539-9087},
url = {https://doi.org/10.1145/3581771},
doi = {10.1145/3581771},
abstract = {Although UML state machines constitute a convenient modeling formalism that is widely used in many applications, the lack of formal semantics impedes carrying out some automatic processing such as formal verification for instance. In this paper, we aim to achieve a proven translation from a subset of UML state machines to timed automata. A generic abstract syntax is defined for state machines, which allows us to specify state machines as a tree-like structure, thus explicitly illustrating the hierarchical relationships within the model. Based on this syntax, a formal asynchronous semantics for state machines and systems of state machines is established. Additionally, the semantics of timed automata is specified. Then, a translation relation from the considered set of state machines to timed automata is defined and a strong equivalence relation, namely a timed bisimulation between the source and target models, is formally proven. The proof is carried out inductively while considering continuous (time) and discrete transitions separately. Such a proof allows us to feature a strong similitude between these models.},
note = {Just Accepted},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
keywords = {Model Transformation, Bisimulation Equivalence., Timed Automata, UML State machines}
}

@inproceedings{10.1145/3207677.3277980,
author = {Zhang, Yanchen},
title = {The Roles of Translation Teachers in Computer-Assisted Translation Teaching},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277980},
doi = {10.1145/3207677.3277980},
abstract = {This paper1 focuses on the roles of translation teacher in computer-assisted translation teaching. The core quality of translation teachers determines not only the personal development prospects of teachers, but also the professional competence and level of translation major students. As a member of the majority of teachers, the roles of the core literacy of translation teachers is not only the need for the international competition, but also an important part of the reform of the new era of translation teachers to meet the current implementation of our country. As the leading force for the training of international translators, the core literacy of translation teachers will have a decisive influence on the quality of the training of international translators. At the same time, computer is widely used in translation. In order to meet the needs of training international translation talents, translation teachers should play different roles in computer-assisted translation teaching.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {133},
numpages = {5},
keywords = {Roles, computer-assisted translation teaching, translation teacher},
location = {Hohhot, China},
series = {CSAE '18}
}

@article{10.1145/3442629,
author = {Novoa, Jos\'{e} and Mahu, Rodrigo and Wuth, Jorge and Escudero, Juan Pablo and Fredes, Josu\'{e} and Yoma, N\'{e}stor Becerra},
title = {Automatic Speech Recognition for Indoor HRI Scenarios},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
url = {https://doi.org/10.1145/3442629},
doi = {10.1145/3442629},
abstract = {This article presents a stand-alone automatic speech recognition system that accounts for listener movement, time-varying reverberation effects, environmental noise, and user position information for beamforming approaches in an HRI setting. We raise the importance of replacing the classical black-box integration of automatic speech recognition technology in HRI applications with the incorporation of the acoustic environment representation and modeling, and of the target source direction. Test data were recorded on a real robot under various moving conditions. For addressing the time-varying acoustic channel problem and incorporating environmental effect during training, clean speech samples were passed through estimated static channel responses and noise was added. Beamforming is investigated regarding oracle source tracking using, for instance, image processing. The proposed strategy is interesting for the robotics community, because it allows the development of voice-based HRI with limited training data and without relying on third-party technologies or Internet access eliminating the need to upload data to the cloud. In our mobile HRI scenario, the resulting speech recognition engine provided an average word error rate that is at least 19% and 34% lower than publicly available speech recognition APIs with the playback (i.e., loudspeaker) and human testing modalities, respectively.},
journal = {J. Hum.-Robot Interact.},
month = {mar},
articleno = {17},
numpages = {30},
keywords = {indoor environments, Beamforming, time-varying acoustic channel, DNN-HMM, ASR}
}

@inproceedings{10.1145/3302505.3312594,
author = {Mondol, Md Abu Sayeed and Stankovic, John A.},
title = {Neural Sensor Translation: Poster Abstract},
year = {2019},
isbn = {9781450362832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302505.3312594},
doi = {10.1145/3302505.3312594},
abstract = {Neural Machine Translation (NMT), a neural network based approach for language translation, has been shown to be more effective than traditional approaches like statistical machine translation. Inspired by NMT, we propose Neural Sensor Translation (NST), a process of translating data sequences from one or a set of sensors to another using neural networks. NST is a data-driven approach for creating virtual sensors that have useful applications in sensor networks and internet of things. We present a neural network based approach for sensor translation and demonstrate the potential of the approach using sensor data from a home.},
booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
pages = {279–280},
numpages = {2},
keywords = {sensor translation, virtual sensor, internet of things},
location = {Montreal, Quebec, Canada},
series = {IoTDI '19}
}

@inproceedings{10.1145/3324884.3416587,
author = {Ding, Yangruibo and Ray, Baishakhi and Devanbu, Premkumar and Hellendoorn, Vincent J.},
title = {Patching as Translation: The Data and the Metaphor},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416587},
doi = {10.1145/3324884.3416587},
abstract = {Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that "software patching is like language translation". We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as "proof-of-concept" tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {275–286},
numpages = {12},
keywords = {neural machine translation, automated program repair, sequence-to-sequence model, big code},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3395035.3425184,
author = {van der Klis, Anika and Adriaans, Frans and Han, Mengru and Kager, Ren\'{e}},
title = {Automatic Recognition of Target Words in Infant-Directed Speech},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425184},
doi = {10.1145/3395035.3425184},
abstract = {This study assesses the performance of a state-of-the-art automatic speech recognition (ASR) system at extracting target words in two different speech registers: infant-directed speech (IDS) and adult-directed speech (ADS). We used the Kaldi-NL ASR-service, developed by the Dutch Foundation of Open Speech Technology. The results indicate that the accuracy of the tool is much lower in IDS than in ADS. There are differences between IDS and ADS which negatively affect the performance of the existing ASR system. Therefore, new tools need to be developed for the automatic annotation of IDS. Nevertheless, the ASR system can already find more than half of the target words, which is promising.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {522},
numpages = {1},
keywords = {infant-directed speech, speech registers, automatic speech recognition, keyword extraction},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3383972.3384024,
author = {Qu, Yili and Deng, Chufu and Su, Wanqi and Wang, Ying and Lu, Yutong and Chen, Zhiguang},
title = {Multimodal Brain MRI Translation Focused on Lesions},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384024},
doi = {10.1145/3383972.3384024},
abstract = {Registered multimodal images are lacking in many medical image processing tasks. To obtain sufficient registered multimodal data, in this paper, we propose a new unsupervised scheme for medical image translation based on cycle generative adversarial networks (CycleGAN), which can generate registered multimodal from single modality and retain the lesion information. We improve parameter initialization method, upsampling method and loss items to speed up model training and improve translation quality. Compared with previous studies that focus only on the overall quality of translation, we attach more importance to the lesions information in medical images, so we propose a method for the preservation of lesions information in the translation process. We perform a series of multimodal translation experiments on the BRATS2015 dataset, verify the effect of each of our improvements as well as the consistency of the lesions information between translation images and original images. And we also verify the effectiveness and availability of the lesions information in translation images.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {352–359},
numpages = {8},
keywords = {multimodal, medical images, image translation, brain MRI, GAN},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@article{10.1145/3571742,
author = {Sethi, Nandini and Dev, Amita and Bansal, Poonam and Sharma, Deepak Kumar and Gupta, Deepak},
title = {Hybridization Based Machine Translations for Low-Resource Language with Language Divergence},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3571742},
doi = {10.1145/3571742},
abstract = {A hybridised form of direct and rule-based language processing is used in this paper to present a Machine translation system from Sanskrit to Hindi. The divergence between Sanskrit and Hindi is also discussed in this paper, along with a proposition for how to handle it. Sanskrit-Hindi bilingual dictionaries, Grammatical Sanskrit corpus and a Sanskrit analyses rule base, have all been used in the projected system. The projected system's ability to access data from various data vocabularies and rule bases utilised in the system expansion has been improved by the usage of Elasticsearch technique. Additionally, a novel technique that builds a parse tree from the parsing table is presented in this paper. The system processes the input Sanskrit sentence using the parsing approach and the Context Free Grammar in normal form for Sanskrit language processing. No standard Sanskrit-Hindi Grammatical corpora available for Machine Translation which is designed and developed in the proposed work. The specific language sentence is produced using the Grammatical corpora and bilingual dictionaries. The proposed system achieved a Bilingual Evaluation Understudy (BLEU) score of 51.6 percent after being tested using Python's natural language toolkit API. The proposed system performs better than current systems when compared to cutting-edge systems, according to the comparison.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
keywords = {Tagger, Corpora, Elasticsearch, Natural language processing, Direct translation, Sanskrit grammar}
}

@article{10.1109/TASLP.2018.2877465,
author = {Nakashika, Toru and Takaki, Shinji and Yamagishi, Junichi},
title = {Complex-Valued Restricted Boltzmann Machine for Speaker-Dependent Speech Parameterization From Complex Spectra},
year = {2019},
issue_date = {February 2019},
publisher = {IEEE Press},
volume = {27},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2877465},
doi = {10.1109/TASLP.2018.2877465},
abstract = {This paper describes a novel energy-based probabilistic distribution that represents complex-valued data and explains how to apply it to direct feature extraction from complex-valued spectra. The proposed model, the complex-valued restricted Boltzmann machine CRBM, is designed to deal with complex-valued visible units as an extension of the well-known restricted Boltzmann machine RBM. Like the RBM, the CRBM learns the relationships between visible and hidden units without having connections between units in the same layer, which dramatically improves training efficiency by using Gibbs sampling or contrastive divergence. Another important characteristic is that the CRBM also has connections between real and imaginary parts of each of the complex-valued visible units that help represent the data distribution in the complex domain. In speech signal processing, classification and generation features are often based on amplitude spectra e.g., MFCC, cepstra, and mel-cepstra even if they are calculated from complex spectra, and they ignore phase information. In contrast, the proposed feature extractor using the CRBM directly encodes the complex spectra or another complex-valued representation of the complex spectra into binary-valued latent features hidden units. Since the visible-hidden connections are undirected, we can also recover decode the complex spectra from the latent features directly. Our speech representation experiments demonstrated that the CRBM outperformed other speech representation methods, such as methods using a conventional RBM, a mel-log spectrum approximate decoder, etc.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {244–254},
numpages = {11}
}

@article{10.1109/TASLP.2022.3196879,
author = {Novitasari, Sashi and Sakti, Sakriani and Nakamura, Satoshi},
title = {A Machine Speech Chain Approach for Dynamically Adaptive Lombard TTS in Static and Dynamic Noise Environments},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3196879},
doi = {10.1109/TASLP.2022.3196879},
abstract = {Recent end-to-end text-to-speech synthesis (TTS) systems have successfully synthesized high-quality speech. However, TTS speech intelligibility degrades in noisy environments because most of these systems were not designed to handle noisy environments. Several works attempted to address this problem by using offline fine-tuning to adapt their TTS to noisy conditions. Unlike machines, humans never perform offline fine-tuning. Instead, they speak with the Lombard effect in noisy places, where they dynamically adjust their vocal effort to improve the audibility of their speech. This ability is supported by the speech chain mechanism, which involves auditory feedback passing from speech perception to speech production. This paper proposes an alternative approach to TTS in noisy environments that is closer to the human Lombard effect. Specifically, we implement Lombard TTS in a machine speech chain framework to synthesize speech with dynamic adaptation. Our TTS performs adaptation by generating speech utterances based on the auditory feedback that consists of the automatic speech recognition (ASR) loss as the speech intelligibility measure and the speech-to-noise ratio (SNR) prediction as power measurement. Two versions of TTS are investigated: non-incremental TTS with utterance-level feedback and incremental TTS (ITTS) with short-term feedback to reduce the delay without significant performance loss. Furthermore, we evaluate the TTS systems in both static and dynamic noise conditions. Our experimental results show that auditory feedback enhanced the TTS speech intelligibility in noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2673–2688},
numpages = {16}
}

@inproceedings{10.1145/3544549.3583942,
author = {Oppong, Abigail},
title = {Building a Participatory Data Design Approach to Examine Gender Bias in English-Twi Machine Translation},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583942},
doi = {10.1145/3544549.3583942},
abstract = {This project attempts to build a data-design approach to examine the detection and mitigation of gender bias in an English– Twi machine translation. This project makes use of an open-source data English-Akuapem Twi parallel corpus. Training the dataset on a sequence-to sequence model reveals a stereotypical bias in the machine translation system associated with a high-status profession like 'engineering.' The quality of translation for sentences associated with 'he' is higher than 'she.' Hence, a proposed data design approach is made to help examine bias in the dataset. The quality of the translation of sentences associated with either 'he’ or 'she’ improved and got either identical bleu scores or just a little difference in the bleu scores.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {564},
numpages = {6},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3383972.3384029,
author = {He, Jianrong and Wang, Xin'an and Zhang, Xing and Wang, Bo and Li, Qiuping and Qiu, Changpei},
title = {Unvoiced Speech Recognition Algorithm Based on Myoelectric Signal},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384029},
doi = {10.1145/3383972.3384029},
abstract = {This paper presented our studies of automatic speech recognition based on myoelectric signal in the application of Chinese pronounce recognition. Facial myoelectric signal records the articulatory apparatus and thus allows us to recognize spoken words even in silence. In this way, communication is not prone to ambient noise and can be used for patients with language difficulties. Activity sections were segment from original data by moving average method combined with a threshold comparison. According to MES characteristic, the coefficients of time domain and frequency domain, wavelet energy and Mel-frequency cepstral coefficients are selected from the original data for speech recognition. The principal component analysis (PCA) method was applied to reduce dimension and generate a 24-dimensional feature vector. We examined different classifiers with optimized parameters and found that the support vector machine classifier performs the best among all. Our final system achieved a 90.04% accuracy rate on a 10 Chinese digit words task. Therefore, this approach can bring muscle speech recognition within a powerful potential in the application of silent speech.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {450–456},
numpages = {7},
keywords = {Mel Frequency Cepstral Coefficient (MFCC), support vector machine (SVM), feature extraction, segmentation, myoelectric signal(MES), Speech recognition},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@article{10.1145/3586995,
author = {Qiao, Chengche and Huang, Xiaodong},
title = {Enhancing Translation Competence via Flipping the EFL Translation Classroom in China},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3586995},
doi = {10.1145/3586995},
abstract = {The ultimate goal of translation teaching is to cultivate the students’ translation competence. Inspired by the PACTE (PACTE, 2003) about the definition of translation competence, we further break down the translation competences into five sub-competences with more explicit teaching goals: lexical variety, grammatical correctness, stylistic appropriateness, fluency and coherence competence, which is vital for the cultivation of translation competence of engineering students. Then, an experimental method is used to compare the outcomes of students enrolled in a flipped and a traditional version of a EFL (English as a Foreign Language) Translation Course at a university in China. This empirical study aims to determine the effectiveness of flipped classroom model (FCM) in translation class. 60 undergraduates majoring in science and technology are involved in this research. Pre- and post- tests are used as instruments and data are analyzed using descriptive and inferential statistics. Quantitative data analysis reveals that comparing with traditional classroom flipped classroom can promote the cultivation of lexical variety, stylistic appropriateness, fluency and coherence competence. Considering the positive results obtained in the study, the article points out the use of the flipped approach as an example of good practice for cultivating the translation competence in EFL translation course.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {Flipped Classroom, Translation Competence, English as a Foreign Language, Tertiary Education, English Learning}
}

@article{10.1109/TASLP.2020.2982297,
author = {Fernando, Tharindu and Sridharan, Sridha and McLaren, Mitchell and Priyasad, Darshana and Denman, Simon and Fookes, Clinton},
title = {Temporarily-Aware Context Modeling Using Generative Adversarial Networks for Speech Activity Detection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982297},
doi = {10.1109/TASLP.2020.2982297},
abstract = {This paper presents a novel framework for Speech Activity Detection (SAD). Inspired by the recent success of multi-task learning approaches in the speech processing domain, we propose a novel joint learning framework for SAD. We utilise generative adversarial networks to automatically learn a loss function for joint prediction of the frame-wise speech/ non-speech classifications together with the next audio segment. In order to exploit the temporal relationships within the input signal, we propose a temporal discriminator which aims to ensure that the predicted signal is temporally consistent. We evaluate the proposed framework on multiple public benchmarks, including NIST OpenSAT’ 17, AMI Meeting and HAVIC, where we demonstrate its capability to outperform state-of-the-art SAD approaches. Furthermore, our cross-database evaluations demonstrate the robustness of the proposed approach across different languages, accents, and acoustic environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1159–1169},
numpages = {11}
}

@article{10.1145/3522576,
author = {Cao, Weiran},
title = {Image Semantic Analysis in Visual Media Art Using Machine Learning and Neural Machine Translation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3522576},
doi = {10.1145/3522576},
abstract = {The current archaeological work in China has the problems of high cost, large material consumption, more attention on human detection and long time-consuming. It is urgent to use modern high-precision detection technology for auxiliary work. This exploration will also use the semantic recognition system based on deep learning and neural network for the recognition of oracle bone inscriptions in archaeology. Therefore, combined with the concept of multimedia semantic recognition and analysis, a unified real-time target detection semantic analysis model named You Only Look Once (YOLOv2) is established based on the deep convolutional neural network under deep learning in the field of machine learning, to test the semantic analysis of oracle bone inscriptions. Moreover, Faster Region-Convolutional Neural Network (Faster R-CNN) and traditional YOLO models are selected to conduct the controlled experiments. A YOLOv2 recognition system is established based on Diffusion-Convolutional Neural Networks (DCNN) under deep learning. First, the concept and performance of DCNN are studied. Next, the basic information of oracle bone inscriptions is analyzed. A recognition system based on DCNN is established. On the premise that the three models can identify different directions of the same oracle bone inscriptions sample, the detection accuracy and detection loss value of YOLOv2 are better than those of the other two models, the detection accuracy is as high as 0.90, and the loss value is less than 0.10. Therefore, it is considered that this YOLOv2 semantic analysis model can be applied in oracle bone inscriptions and other archaeological work, which improves the work efficiency and simplifies the human work items for the domestic archaeological work. This semantic analysis model is of great help to the pattern recognition of cultural relics in archaeological work, and can help professionals analyze the meaning of patterns faster when there are massive similar oracle bone inscriptions.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {deep learning, semantic analysis, archaeological work, convolutional neural network, YOLOv2 model}
}

@article{10.5555/3586589.3586657,
author = {Zhu, Wei and Qiu, Qiang and Calderbank, Robert and Sapiro, Guillermo and Cheng, Xiuyuan},
title = {Scaling-Translation-Equivariant Networks with Decomposed Convolutional Filters},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Encoding the scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many computer vision tasks especially when dealing with multiscale inputs. We study, in this paper, a scaling-translation-equivariant (ST -equivariant) CNN with joint convolutions across the space and the scaling group, which is shown to be both sufficient and necessary to achieve equivariance for the regular representation of the scaling-translation group ST. To reduce the model complexity and computational burden, we decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation, a property which is theoretically analyzed and empirically verified. Numerical experiments demonstrate that the proposed scaling-translation-equivariant network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {68},
numpages = {45},
keywords = {convolutional neural network (CNN), deformation robust equivariant representation, scaling-translation-equivariant, decomposed convolutional filters, computer vision}
}

@article{10.1145/3477498,
author = {Ananthanarayana, Tejaswini and Srivastava, Priyanshu and Chintha, Akash and Santha, Akhil and Landy, Brian and Panaro, Joseph and Webster, Andre and Kotecha, Nikunj and Sah, Shagan and Sarchet, Thomastine and Ptucha, Raymond and Nwogu, Ifeoma},
title = {Deep Learning Methods for Sign Language Translation},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/3477498},
doi = {10.1145/3477498},
abstract = {Many sign languages are bona fide natural languages with grammatical rules and lexicons hence can benefit from machine translation methods. Similarly, since sign language is a visual-spatial language, it can also benefit from computer vision methods for encoding it. With the advent of deep learning methods in recent years, significant advances have been made in natural language processing (specifically neural machine translation) and in computer vision methods (specifically image and video captioning). Researchers have therefore begun expanding these learning methods to sign language understanding. Sign language interpretation is especially challenging, because it involves a continuous visual-spatial modality where meaning is often derived based on context.The focus of this article, therefore, is to examine various deep learning–based methods for encoding sign language as inputs, and to analyze the efficacy of several machine translation methods, over three different sign language datasets. The goal is to determine which combinations are sufficiently robust for sign language translation without any gloss-based information.To understand the role of the different input features, we perform ablation studies over the model architectures (input features + neural translation models) for improved continuous sign language translation. These input features include body and finger joints, facial points, as well as vector representations/embeddings from convolutional neural networks. The machine translation models explored include several baseline sequence-to-sequence approaches, more complex and challenging networks using attention, reinforcement learning, and the transformer model. We implement the translation methods over multiple sign languages—German (GSL), American (ASL), and Chinese sign languages (CSL). From our analysis, the transformer model combined with input embeddings from ResNet50 or pose-based landmark features outperformed all the other sequence-to-sequence models by achieving higher BLEU2-BLEU4 scores when applied to the controlled and constrained GSL benchmark dataset. These combinations also showed significant promise on the other less controlled ASL and CSL datasets.},
journal = {ACM Trans. Access. Comput.},
month = {oct},
articleno = {22},
numpages = {30},
keywords = {deep learning, accessibility, Deaf and Hard-of-Hearing, Sign language translation, attention, transformer, sequence modeling}
}

@inproceedings{10.1145/3600211.3604655,
author = {Lammerts, Philippe and Lippmann, Philip and Hsu, Yen-Chia and Casati, Fabio and Yang, Jie},
title = {How Do You Feel? Measuring User-Perceived Value for Rejecting Machine Decisions in Hate Speech Detection},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604655},
doi = {10.1145/3600211.3604655},
abstract = {Hate speech moderation remains a challenging task for social media platforms. Human-AI collaborative systems offer the potential to combine the strengths of humans’ reliability and the scalability of machine learning to tackle this issue effectively. While methods for task handover in human-AI collaboration exist that consider the costs of incorrect predictions, insufficient attention has been paid to accurately estimating these costs. In this work, we propose a value-sensitive rejection mechanism that automatically rejects machine decisions for human moderation based on users’ value perceptions regarding machine decisions. We conduct a crowdsourced survey study with 160 participants to evaluate their perception of correct and incorrect machine decisions in the domain of hate speech detection, as well as occurrences where the system rejects making a prediction. Here, we introduce Magnitude Estimation, an unbounded scale, as the preferred method for measuring user (dis)agreement with machine decisions. Our results show that Magnitude Estimation can provide a reliable measurement of participants’ perception of machine decisions. By integrating user-perceived value into human-AI collaboration, we further show that it can guide us in 1) determining when to accept or reject machine decisions to obtain the optimal total value a model can deliver and 2) selecting better classification models as compared to the more widely used target of model accuracy.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {834–844},
numpages = {11},
keywords = {value-sensitive machine learning, hate speech, machine confidence, rejection, crowdsourcing, human-in-the-loop},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3495018.3495378,
author = {Xie, Shengliang},
title = {A Comparative Study on the Quality of English-Chinese Machine Translation in the Era of Artificial Intelligence},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495378},
doi = {10.1145/3495018.3495378},
abstract = {By combing the status quo of the research on the quality of machine translation, this paper evaluates the translation quality under the guidance of the traditional translation standards of faithfulness, expressiveness and elegance. It makes a qualitative and quantitative analysis of the current mainstream online machine translation systems such as Google translation and Baidu translation, compares their differences, analyzes the translation quality, summarizes and discusses the common typical errors and their laws, in order to improve machine translation and its quality.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1261–1264},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3573428.3573448,
author = {Liu, Junyan},
title = {The Pre-Ordering Model for Statistical Machine Translation of Enhancing the N-Best Syntactic Knowledge},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573448},
doi = {10.1145/3573428.3573448},
abstract = {Syntactic heterogeneity between source and target languages has an important impact on the performance of Statistical Machine Translation (SMT). On the basis of phrase-based Chinese-English SMT, a method of source language pre-ordering based on N-best syntactic knowledge enhancement is proposed. First, the source language input sentences are analyzed by N-best Syntax, and the high reliability sub-tree structure is obtained by calculating statistical probability. Two optimization strategies are used to optimize the initial rule set: rule deduction and rule probability threshold control mechanism. Second, the source language phrase translation table is used as a constraint to control the sequence between phrases. Finally, the syntax analysis tree of the source-side sentences is pre-ordered. The experimental results of Chinese-English SMT based on the NIST 2005 and 2008 test data sets show that comparing to the baseline system, the BLEU score of automatic evaluation criterion of the N-best syntactic knowledge-enhanced SMT pre-ordering method increased by 0.68 and 0.83 respectively.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {109–113},
numpages = {5},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3416921.3416931,
author = {Alashwal, Hany and Lucman, Juwayni},
title = {Utilizing Cost-Sensitive Machine Learning Classifiers to Identify Compounds That Inhibit Alzheimer's APP Translation},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416931},
doi = {10.1145/3416921.3416931},
abstract = {Virtual screening of bioassay data can be of immense benefit to identify compounds which can assist in restricting the production of amyloid beta peptides (Aβ), observed in Alzheimer patients, by inhibiting the translation of amyloid precursor protein (APP). Machine learning classifiers can be adopted on the dataset to investigate those compounds. The ratio of the active molecules that achieve the goal of inhibiting APP, nonetheless, is minimal compared to their inactive counterparts. The imbalance between the two classes is handled by introducing cost-sensitivity to reweight the training instances depending on the misclassification cost allotted to each class. The paper shows the performance of cost-sensitive classifiers (Random Forest, Naive Bayes, and Logistic Regression classifier) to spot the minority (active) molecules from the majority (inactive) classes and shows their evaluation metrics. Sensitivity, specificity, False Negative rate, ROC area, and accuracy are evaluated while keeping the False Positive rate at 20.6%. The aim of the study is to investigate the most reliable classifier for the bioassay data and to explore the ideal misclassification cost. Random Forest classifier was the most robust model compared to Naive Bayes and Logistic Regression Classifiers. Moreover, each classifier had a different optimal misclassification cost.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {113–117},
numpages = {5},
keywords = {Random Forest, Cost Sensitivity, Alzheimer's Disease, Logistic Regression, Classification, Naive Bayes, Primary Screen Bioassay},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@inproceedings{10.1145/3387940.3391484,
author = {Lee, Dickson T. S. and Zhou, Zhi Quan and Tse, T. H.},
title = {Metamorphic Robustness Testing of Google Translate},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391484},
doi = {10.1145/3387940.3391484},
abstract = {Current research on the testing of machine translation software mainly focuses on functional correctness for valid, well-formed inputs. By contrast, robustness testing, which involves the ability of the software to handle erroneous or unanticipated inputs, is often overlooked. In this paper, we propose to address this important shortcoming. Using the metamorphic robustness testing approach, we compare the translations of original inputs with those of follow-up inputs having different categories of minor typos. Our empirical results reveal a lack of robustness in Google Translate, thereby opening a new research direction for the quality assurance of neural machine translators.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {388–395},
numpages = {8},
keywords = {Metamorphic testing, MT4MT, Metamorphic robustness testing, Oracle problem, Robustness testing, Machine translation},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3578741.3578784,
author = {Li, Runkun and Wang, Shichao},
title = {Experimental Study on the Validity of AES System in College Translation Teaching},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578784},
doi = {10.1145/3578741.3578784},
abstract = {Abstract: Considerable attention has been directed to employing computer-aided automated tools in language skills learning and teaching but few has been developed and put to use in translation education. This study seeks to investigate if Automated Writing Evaluation tools can be adapted for translation assessment and training. 80 English translation program undergraduates who are in their first-year professional training are recruited for this study and their eight translation courseworks are assessed by Pigai.org, an open-source automated writing evaluation and feedback platform and twice by human markers, before and after self-revision based on suggestions from Pigai.org. The results demonstrate Pigai.org scores can reliably reflect overall translation quality for general translation but less so for literary and technical translations. Almost consistently higher scores awarded to second submissions confirm the benefits students gain from platform feedback and autonomous learning process. Also revealed by the study is how the positive effect is limited when it comes to minimizing mistranslation, commanding appropriate register, genre and stylistic features and affording cultural and topical knowledge and considerations. While the majority of the trainee translators manage to produce better translation, the bottom tier sees no big changes, exposing the limitation of autonomous learning with CALL tools. Possible rationale and factors that may contribute to the validity and usefulness of automated writing evaluation and feedback are discussed and suggestions for effective use of AES tools for translation education conclude the study.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {207–214},
numpages = {8},
keywords = {AES system, translation quality assessment, college translation teaching},
location = {Sanya, China},
series = {MLNLP '22}
}

@inbook{10.1145/3477495.3531921,
author = {Villatoro-Tello, Esa\'{u} and Madikeri, Srikanth and Motlicek, Petr and Ganapathiraju, Aravind and Ivanov, Alexei V.},
title = {Expanded Lattice Embeddings for Spoken Document Retrieval on Informal Meetings},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531921},
abstract = {In this paper, we evaluate different alternatives to process richer forms of Automatic Speech Recognition (ASR) output based on lattice expansion algorithms for Spoken Document Retrieval (SDR). Typically, SDR systems employ ASR transcripts to index and retrieve relevant documents. However, ASR errors negatively affect the retrieval performance. Multiple alternative hypotheses can also be used to augment the input to document retrieval to compensate for the erroneous one-best hypothesis. In Weighted Finite State Transducer-based ASR systems, using the n-best output (i.e. the top "n'' scoring hypotheses) for the retrieval task is common, since they can easily be fed to a traditional Information Retrieval (IR) pipeline. However, the n-best hypotheses are terribly redundant, and do not sufficiently encapsulate the richness of the ASR output, which is represented as an acyclic directed graph called the lattice. In particular, we utilize the lattice's constrained minimum path cover to generate a minimum set of hypotheses that serve as input to the reranking phase of IR. The novelty of our proposed approach is the incorporation of the lattice as an input for neural reranking by considering a set of hypotheses that represents every arc in the lattice. The obtained hypotheses are encoded through sentence embeddings using BERT-based models, namely SBERT and RoBERTa, and the final ranking of the retrieved segments is obtained with a max-pooling operation over the computed scores among the input query and the hypotheses set. We present our evaluation on the publicly available AMI meeting corpus. Our results indicate that the proposed use of hypotheses from the expanded lattice improves the SDR performance significantly over the n-best ASR output.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2669–2674},
numpages = {6}
}

@inproceedings{10.1145/3411109.3411123,
author = {Draghici, Alexandra and Abe\ss{}er, Jakob and Lukashevich, Hanna},
title = {A Study on Spoken Language Identification Using Deep Neural Networks},
year = {2020},
isbn = {9781450375634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411109.3411123},
doi = {10.1145/3411109.3411123},
abstract = {In this paper, we investigate a previously proposed algorithm for spoken language identification based on convolutional neural networks and convolutional recurrent neural networks. We improve the algorithm by modifying the training strategy to ensure equal class distribution and efficient memory usage. We successfully replicate previous experimental findings using a modified set of languages. Our findings confirm that both a convolutional neural network as well as convolutional recurrent neural networks are capable to learn language-specific patterns in mel spectrogram representations of speech recordings.},
booktitle = {Proceedings of the 15th International Audio Mostly Conference},
pages = {253–256},
numpages = {4},
keywords = {convolutional recurrent neural networks, spoken language identification, convolutional neural networks, speech recognition},
location = {Graz, Austria},
series = {AM '20}
}

@inproceedings{10.1145/3428757.3429971,
author = {Radzikowski, Kacper and Yoshie, Osamu and Nowak, Robert},
title = {Support Software for Automatic Speech Recognition Systems Targeted for Non-Native Speech},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429971},
doi = {10.1145/3428757.3429971},
abstract = {Nowadays automatic speech recognition (ASR) systems can achieve higher and higher accuracy rates depending on the methodology applied and datasets used. The rate decreases significantly when the ASR system is being used with a non-native speaker of the language to be recognized. The main reason for this is specific pronunciation and accent features related to the mother tongue of that speaker, which influence the pronunciation. At the same time, an extremely limited volume of labeled non-native speech datasets makes it difficult to train, from the ground up, sufficiently accurate ASR systems for non-native speakers.In this research we address the problem and its influence on the accuracy of ASR systems, using the style transfer methodology. We designed a pipeline for modifying the speech of a non-native speaker so that it more closely resembles the native speech. This paper covers experiments for accent modification using different setups and different approaches, including neural style transfer and autoencoder. The experiments were conducted on English language pronounced by Japanese speakers (UME-ERJ dataset). The results show that there is a significant relative improvement in terms of the speech recognition accuracy. Our methodology reduces the necessity of training new algorithms for non-native speech (thus overcoming the obstacle related to the data scarcity) and can be used as a wrapper for any existing ASR system. The modification can be performed in real time, before a sample is passed into the speech recognition system itself.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {55–61},
numpages = {7},
keywords = {machine learning, deep learning, neural network, style transfer, speech recognition, non-native speaker},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1109/TASLP.2019.2940662,
author = {Tu, Yan-Hui and Du, Jun and Lee, Chin-Hui},
title = {Speech Enhancement Based on Teacher–Student Deep Learning Using Improved Speech Presence Probability for Noise-Robust Speech Recognition},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2940662},
doi = {10.1109/TASLP.2019.2940662},
abstract = {In this paper, we propose a novel teacher-student learning framework for the preprocessing of a speech recognizer, leveraging the online noise tracking capabilities of improved minima controlled recursive averaging IMCRA and deep learning of nonlinear interactions between speech and noise. First, a teacher model with deep architectures is built to learn the target of ideal ratio masks IRMs using simulated training pairs of clean and noisy speech data. Next, a student model is trained to learn an improved speech presence probability by incorporating the estimated IRMs from the teacher model into the IMCRA approach. The student model can be compactly designed in a causal processing mode having no latency with the guidance of a complex and noncausal teacher model. Moreover, the clean speech requirement, which is difficult to meet in real-world adverse environments, can be relaxed for training the student model, implying that noisy speech data can be directly used to adapt the regression-based enhancement model to further improve speech recognition accuracies for noisy speech collected in such conditions. Experiments on the CHiME-4 challenge task show that our best student model with bidirectional gated recurrent units BGRUs can achieve a relative word error rate WER reduction of 18.85% for the real test set when compared to unprocessed system without acoustic model retraining. However, the traditional teacher model degrades the performance of the unprocessed system in this case. In addition, the student model with a deep neural network DNN in causal mode having no latency yields a relative WER reduction of 7.94% over the unprocessed system with 670 times less computing cycles when compared to the BGRU-equipped student model. Finally, the conventional speech enhancement and IRM-based deep learning method destroyed the ASR performance when the recognition system became more powerful. While our proposed approach could still improve the ASR performance even in the more powerful recognition system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2080–2091},
numpages = {12}
}

@article{10.1109/TASLP.2019.2950099,
author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi and King, Simon and Tokuda, Keiichi},
title = {A Vector Quantized Variational Autoencoder (VQ-VAE) Autoregressive Neural &lt;inline-Formula&gt;&lt;tex-Math Notation="LaTeX"&gt;$F_0$&lt;/Tex-Math&gt;&lt;/Inline-Formula&gt; Model for Statistical Parametric Speech Synthesis},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950099},
doi = {10.1109/TASLP.2019.2950099},
abstract = {Recurrent neural networks (RNNs) can predict fundamental frequency (F0) for statistical parametric speech synthesis systems, given linguistic features as input. However, these models assume conditional independence between consecutive <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> values, given the RNN state. In a previous study, we proposed autoregressive (AR) neural <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> models to capture the causal dependency of successive <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> values. In subjective evaluations, a deep AR model (DAR) outperformed an RNN. Here, we propose a Vector Quantized Variational Autoencoder (VQ-VAE) neural <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> model that is both more efficient and more interpretable than the DAR. This model has two stages: one uses the VQ-VAE framework to learn a latent code for the <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> contour of each linguistic unit, and other learns to map from linguistic features to latent codes. In contrast to the DAR and RNN, which process the input linguistic features frame-by-frame, the new model converts one linguistic feature vector into one latent code for each linguistic unit. The new model achieves better objective scores than the DAR, has a smaller memory footprint and is computationally faster. Visualization of the latent codes for phones and moras reveals that each latent code represents an <inline-formula><tex-math notation="LaTeX">$F_0$</tex-math></inline-formula> shape for a linguistic unit.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {157–170},
numpages = {14}
}

@inproceedings{10.1145/3379173.3393707,
author = {Benitez-Garcia, Gibran and Shimoda, Wataru and Yanai, Keiji},
title = {Style Image Retrieval for Improving Material Translation Using Neural Style Transfer},
year = {2020},
isbn = {9781450371377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379173.3393707},
doi = {10.1145/3379173.3393707},
abstract = {In this paper, we propose a CNN-feature-based image retrieval method to find the ideal style image that better translates the material of an object. An ideal style image must share semantic information with the content image, while containing distinctive characteristics of the desired material. Therefore, we first refine the search by selecting the most discriminative images from the target material. Subsequently, our search process focuses on the object semantics by removing the style information using instance normalization whitening. Thus, the search is performed using the normalized CNN features. In order to translate materials to object regions, we combine semantic segmentation with neural style transfer. We segment objects from the content image by using a weakly supervised segmentation method, and transfer the material of the retrieved style image to the segmented areas. We demonstrate quantitatively and qualitatively that by using ideal style images, the results of the conventional neural style transfer are significantly improved, overcoming state-of-the-art approaches, such as WCT, MUNIT, and StarGAN.},
booktitle = {Proceedings of the 2020 Joint Workshop on Multimedia Artworks Analysis and Attractiveness Computing in Multimedia},
pages = {1–6},
numpages = {6},
keywords = {material translation, instance normalization, neural style transfer, style image retrieval},
location = {Dublin, Ireland},
series = {MMArt-ACM '20}
}

@article{10.1145/3465621,
author = {Farahi, Behnaz},
title = {Can the Subaltern Speak? Feminism in Robotic Mask Design},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3465621},
doi = {10.1145/3465621},
abstract = {This paper presents AI-controlled robotic masks intended to empower women and allow them to communicate with one another. These are inspired by the historical masks worn by the Bandari women from southern Iran. Legend has it that these masks were developed during Portuguese colonial rule as a way of protecting the wearer from the gaze of slave masters looking for attractive women. In this project two robotic masks seemingly begin to develop their own language to communicate with each other, blinking their eyelashes in rapid succession using AI-generated Morse code. This project draws upon a Facebook experiment where two AI bots apparently began to develop their own language. It also draws upon an incident when an American soldier used his eyes to blink and spell out the word "TORTURE" using Morse code during his captivity in Vietnam, as well as stories of women using code to report domestic abuse during the COVID-19 lockdown. The aim is to sow anxiety within the patriarchal system where the "wink" of the sexual predator is subverted into a language to protect women from the advances of a predator. The project bridges AI, interactive design, and critical thinking (Figure 1).},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {aug},
articleno = {18},
numpages = {11},
keywords = {Communication, Feminism, Robotics, Design, Machine Learning, Mask}
}

@inproceedings{10.1145/3443279.3443313,
author = {Xu, Jiahua and Matta, Kaveen and Islam, Shaiful and N\"{u}rnberger, Andreas},
title = {German Speech Recognition System Using DeepSpeech},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443313},
doi = {10.1145/3443279.3443313},
abstract = {Speech recognition focus on the translation of speech from an audio format to a text. Popular models are available for the English language as open source in the domain of voice/speech recognition; however, German language open models and training schemes are rather rare. An end-to-end real-time German speech-to-text system based on multiple German language datasets is worthy of more attention and further investigation. In this paper, we combined multiple German datasets on the market and optimizes the Deep-speech for training a real-time German speech-to-text model. A GUI is also proposed for functionality demonstration. Our model performs considerably well compared to other state-of-the-art since we utilized noisy data to replicate real-life scenarios. We released our fully trained German model along with its parameter configurations to promote the diversification of the open-source model for the German language.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {102–106},
numpages = {5},
keywords = {natural language processing, neural networks, speech-to-text, Deep learning},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@article{10.1109/TASLP.2020.3036237,
author = {Yousefi, Midia and Hansen, John H. L.},
title = {Block-Based High Performance CNN Architectures for Frame-Level Overlapping Speech Detection},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3036237},
doi = {10.1109/TASLP.2020.3036237},
abstract = {Speech technology systems such as Automatic Speech Recognition (ASR), speaker diarization, speaker recognition, and speech synthesis have advanced significantly by the emergence of deep learning techniques. However, none of these voice-enabled systems perform well in natural environmental circumstances, specifically in situations where one or more potential interfering talkers are involved. Therefore, overlapping speech detection has become an important front-end triage step for speech technology applications. This is crucial for large-scale datsets where manual labeling in not possible. A block-based CNN architecture is proposed to address modeling overlapping speech in audio streams with frames as short as 25 ms. The proposed architecture is robust to both: (i) shifts in distribution of network activations due to the change in network parameters during training, (ii) local variations from the input features caused by feature extraction, environmental noise, or room interference. We also investigate the effect of alternate input features including spectral magnitude, MFCC, MFB, and pyknogram on both computational time and classification performance. Evaluation is performed on simulated overlapping speech signals based on the GRID corpus. The experimental results highlight the capability of the proposed system in detecting overlapping speech frames with 90.5% accuracy, 93.5% precision, 92.7% recall, and 92.8% Fscore on same gender overlapped speech. For opposite gender cases, the network scores exceed 95% in all the classification metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {28–40},
numpages = {13}
}

@inproceedings{10.1145/3510858.3511425,
author = {Liu, Nan},
title = {Intelligent English Automatic Translation System Based on Computer Corpus},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511425},
doi = {10.1145/3510858.3511425},
abstract = {Machine translation system, also known as automatic translation system, generally refers to a relatively complex process of automatically by computer, and generally refers to the translation of sentences or full texts in this process. At present, systems are generally divided into two categories: one is statistical-based machine translation systems, and the other is case-based machine translation systems. Corpus-based machine translation system involves many disciplines, among which linguistics, mathematics and computer science are the most basic. Multi-disciplinary cooperation constitutes a rapidly developing machine translation system based on corpus. The rapid development of machine translation system has accelerated the cultural, economic and other exchanges between different countries. Looking at the current development situation, we can see that the rapid development of corpus linguistics and computer linguistics has greatly improved the performance of today's machine translation system.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {925–929},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3287624.3287717,
author = {Li, Qin and Zhang, Xiaofan and Xiong, JinJun and Hwu, Wen-mei and Chen, Deming},
title = {Implementing Neural Machine Translation with Bi-Directional GRU and Attention Mechanism on FPGAs Using HLS},
year = {2019},
isbn = {9781450360074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287624.3287717},
doi = {10.1145/3287624.3287717},
abstract = {Neural machine translation (NMT) is a popular topic in Natural Language Processing which uses deep neural networks (DNNs) for translation from source to targeted languages. With the emerging technologies, such as bidirectional Gated Recurrent Units (GRU), attention mechanisms, and beam-search algorithms, NMT can deliver improved translation quality compared to the conventional statistics-based methods, especially for translating long sentences. However, higher translation quality means more complicated models, higher computation/memory demands, and longer translation time, which causes difficulties for practical use. In this paper, we propose a design methodology for implementing the inference of a real-life NMT (with the problem size = 172 GFLOP) on FPGA for improved run time latency and energy efficiency. We use High-Level Synthesis (HLS) to build high-performance parameterized IPs for handling the most basic operations (multiply-accumulations) and construct these IPs to accelerate the matrix-vector multiplication (MVM) kernels, which are frequently used in NMT. Also, we perform a design space exploration by considering both computation resources and memory access bandwidth when utilizing the hardware parallelism in the model and generate the best parameter configurations of the proposed IPs. Accordingly, we propose a novel hybrid parallel structure for accelerating the NMT with affordable resource overhead for the targeted FPGA. Our design is demonstrated on a Xilinx VCU118 with overall performance at 7.16 GFLOPS.},
booktitle = {Proceedings of the 24th Asia and South Pacific Design Automation Conference},
pages = {693–698},
numpages = {6},
location = {Tokyo, Japan},
series = {ASPDAC '19}
}

@article{10.1145/3555171,
author = {Zhang, Peng and Guan, Zhengqing and Liu, Baoxi and Ding, Xianghua (Sharon) and Lu, Tun and Gu, Hansu and Gu, Ning},
title = {Building User-Oriented Personalized Machine Translator Based on User-Generated Textual Content},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555171},
doi = {10.1145/3555171},
abstract = {Machine Translation (MT) has been a very useful tool to assist multilingual communication and collaboration. In recent years, by taking advantage of the exciting developments of neural networks and deep learning, the accuracy and speed of machine translation have been continuously improved. However, most machine translation methods and systems are data-driven. They tend to select a consensus response represented in training data, while a user's preferred linguistic style, which is important for translation comprehension and user experience, is ignored. For this problem, we aim to build a user-oriented personalized machine translation model in this paper. The model aims to learn each user's linguistic style from the textual content that is generated by her/him (User-Generated Textual Content, UGTC) in social media context and generate personalized translation results utilizing several state-of-the-art deep learning techniques like Transformer and pre-training. We also implemented a user-oriented personalized machine translator using Weibo as a case of the source of UGTC to provide a systematical implementation scheme of a user-oriented personalized machine translation system based on our model. The translator was evaluated by automatic evaluation in combination with human evaluation. The results suggest that our model can generate more personalized, natural and lively translation results and enhance the comprehensibility of translation results, which makes its generations more preferred by users versus general translation results.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {280},
numpages = {26},
keywords = {personalized, Weibo, linguistic style, user-generated textual content, machine translation}
}

@inproceedings{10.1145/3284179.3284335,
author = {S\'{a}nchez-Holgado, Patricia and Arcila-Calder\'{o}n, Carlos},
title = {Towards the Study of Sentiment in the Public Opinion of Science in Spanish},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284335},
doi = {10.1145/3284179.3284335},
abstract = {Every day millions of short messages that show opinions, information and contents of all kinds move around the networks. The analysis of this large volume of data is possible thanks to computer techniques. The sentiments of the messages can provide observations on the acceptance of topics, social trends or currents of opinion. Therefore, this research is part of a project that addresses the creation of a prototype for the analysis of the sentiment of messages on scientific topics on Twitter using supervised machine learning algorithms. These methods require having a large set of data labeled (corpus), to train the model in the best possible way. The detailed process of creating this corpus is the objective of this dissertation. The ultimate goal of the project is to create a function that is able to predict what the value of an input element would be after having been trained with the sentiment classifier. The first results of the classifier show a reliability around 70% in the tested algorithms and from them you can extract adjusted classifications in real time connected to the Twitter Streaming API.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {963–970},
numpages = {8},
keywords = {social media, sentiment analysis, machine learning, Twitter, big data, Spanish, Science communication},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3462244.3479926,
author = {Tutul, Abdullah Aman and Nirjhar, Ehsanul Haque and Chaspari, Theodora},
title = {Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479926},
doi = {10.1145/3462244.3479926},
abstract = {Trust is a key element in the development of effective collaborative relationships between humans and increasingly complex artificial intelligence (AI) systems. Here, we examine trust in AI in the context of a human-AI partnership that involves a joint decision making task for estimating levels of public speaking anxiety based on speech signals. The AI system is comprised of an explainable machine learning (ML) algorithm, that takes acoustic characteristics as input and outputs the estimate of public speaking anxiety levels, a local explanation about the most important features that contributed to the decision of each speech sample, and a global explanation about the most important features for the data overall. We analyze interactions between AI and human annotators with background in psychological sciences, and measure trust over time via the annotators’ agreement with the AI model and the annotators’ self-reports. We further examine factors of trust that are related to the characteristics of the human annotator and the ML algorithm. Results indicate that trust in AI depends on the openness level of the annotator and the importance level of input features. Findings from this study can provide guidelines to designing solutions that properly calibrate human trust in AI in collaborative human-AI tasks.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {288–296},
numpages = {9},
keywords = {human-AI interaction, speech, public speaking anxiety, Trustworthy AI},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@article{10.1109/TASLP.2019.2944348,
author = {Sekiguchi, Kouhei and Bando, Yoshiaki and Nugraha, Aditya Arie and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Semi-Supervised Multichannel Speech Enhancement With a Deep Speech Prior},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2944348},
doi = {10.1109/TASLP.2019.2944348},
abstract = {This paper describes a semi-supervised multichannel speech enhancement method that uses clean speech data for prior training. Although multichannel nonnegative matrix factorization (MNMF) and its constrained variant called independent low-rank matrix analysis (ILRMA) have successfully been used for unsupervised speech enhancement, the low-rank assumption on the power spectral densities (PSDs) of all sources (speech and noise) does not hold in reality. To solve this problem, we replace a low-rank speech model with a deep generative speech model, i.e., formulate a probabilistic model of noisy speech by integrating a deep speech model, a low-rank noise model, and a full-rank or rank-1 model of spatial characteristics of speech and noise. The deep speech model is trained from clean speech data in an unsupervised auto-encoding variational Bayesian manner. Given multichannel noisy speech spectra, the full-rank or rank-1 spatial covariance matrices and PSDs of speech and noise are estimated in an unsupervised maximum-likelihood manner. Experimental results showed that the full-rank version of the proposed method was significantly better than MNMF, ILRMA, and the rank-1 version. We confirmed that the initialization-sensitivity and local-optimum problems of MNMF with many spatial parameters can be solved by incorporating the precise speech model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2197–2212},
numpages = {16}
}

@inproceedings{10.1145/3310986.3311007,
author = {Iqbal, Muhamad and Adnan, Risman and Widyanto, M. Rahmat and Basaruddin, T.},
title = {Evaluation Study of Unsupervised Face-to-Face Translation Using Generative Adversarial Networks},
year = {2019},
isbn = {9781450366120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310986.3311007},
doi = {10.1145/3310986.3311007},
abstract = {Cross-domain image-to-image translation provides mechanism to capture special characteristics of one image collection and convert into other image collection with different representations. Recent research on generative learning have produced powerful image-to-image translation methods in supervised setting, where paired training datasets are available. However, collecting paired training data is difficult, expensive and required manual authoring. We present an evaluation study of recent unsupervised Generative Adversarial Network (GAN) models that can learn to translate a facial image from a source domain X to a target domain Y without paired labeled training dataset. Each GAN model is trained on the same facial image dataset and comparable hyperparameters. We report a comparison result using same GAN model evaluation metrics.},
booktitle = {Proceedings of the 3rd International Conference on Machine Learning and Soft Computing},
pages = {226–231},
numpages = {6},
keywords = {Generative adversarial network, image-to-image translation, model evaluation, facial image},
location = {Da Lat, Viet Nam},
series = {ICMLSC '19}
}

@article{10.1109/TASLP.2020.2997584,
author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
title = {Corrections to “Machine Speech Chain”},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2997584},
doi = {10.1109/TASLP.2020.2997584},
abstract = {Presents corrections to author information for the above named paper.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1706},
numpages = {1}
}

@article{10.5555/3546258.3546487,
author = {Biscione, Valerio and Bowers, Jeffrey S.},
title = {Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {When seeing a new object, humans can immediately recognize it across different retinal locations: the internal object representation is invariant to translation. It is commonly believed that Convolutional Neural Networks (CNNs) are architecturally invariant to translation thanks to the convolution and/or pooling operations they are endowed with. In fact, several studies have found that these networks systematically fail to recognise new objects on untrained locations. In this work, we test a wide variety of CNNs architectures showing how, apart from DenseNet-121, none of the models tested was architecturally invariant to translation. Nevertheless, all of them could learn to be invariant to translation. We show how this can be achieved by pretraining on ImageNet, and it is sometimes possible with much simpler data sets when all the items are fully translated across the input canvas. At the same time, this invariance can be disrupted by further training due to catastrophic forgetting/interference. These experiments show how pretraining a network on an environment with the right 'latent' characteristics (a more naturalistic environment) can result in the network learning deep perceptual rules which would dramatically improve subsequent generalization.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {229},
numpages = {28},
keywords = {internal representation, equivariance, translation invariance, convolutional neural networks}
}

@inproceedings{10.1145/3492547.3492651,
author = {Rakhimova, Diana and Karyukin, Vladislav and Karibayeva, Aidana and Turarbek, Assem and Turganbayeva, Aliya},
title = {The Development of the Light Post-Editing Module for English-Kazakh Translation},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492651},
doi = {10.1145/3492547.3492651},
abstract = {Applied intelligent systems play an important role in the modern world. One of their tasks is machine translation (MT) from one language into another one. MT allows people to freely communicate despite language barriers. This new technology is a special step in helping to understand what a companion speaks, or writes to you. Automatic post-editing is the task of correcting errors present in texts as a result of machine translation. Since MT cannot always give the desired result, it becomes necessary to edit the translation. The drawbacks of the translation have to be eliminated by post-editing. This need for post-editing is largely determined by the quality of machine translation: low-quality translation leaves a lot of room for post-editing, and high-quality and human translations require minimal text editing. This work describes the development of the light post-editing module for the English-Kazakh language pairs. The neural network model is trained on pairs mt, pe and triplets src, mt, pe using the OpenNMT framework. Then the results of the BLEU metrics mt - pe and mt - ape are compared, and a conclusion about the quality of post-editing is made.},
booktitle = {The 7th International Conference on Engineering &amp; MIS 2021},
articleno = {69},
numpages = {5},
keywords = {Kazakh, English, light post-editing, CSE-model, post-editing},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@article{10.1145/3517196,
author = {Xiang, Yueting and Zeng, Chengang and Sun, Kaiyang and Tang, Kunzhi},
title = {Design of Intelligent Decision System via Computer Aided Translation Software Evaluation under the Background of Internet of Things},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517196},
doi = {10.1145/3517196},
abstract = {The identification and classification of professional terms of machine translation are studied in this work, to improve the accuracy and professionalism of computer aided translation (CAT) software. Firstly, the current situation and related fields of machine translation are analyzed to summarize the difficulties and shortcomings in machine translation. Secondly, the concept of term is introduced to conduct targeted research on the imbalance problem of terminology classification and recognition in machine translation. Thirdly, a term recognition model based on integrated recognition method is proposed. Finally, the classification accuracy and recall rate of the model are verified using the method of confusion matrix in experiments. The results demonstrate that in comparison of the recall rate, classification accuracy, and f value in different fields, the classification accuracy of network terms by the hybrid method combining the over-sampling method and under-sampling method is the highest of 77%, that of sports terms is the lowest of 71%, and that of economic terms is 74%. Among the recall rate, accuracy rate and f value, the recall rate is the highest, reaching more than 80%, especially for economic terms of 91%. The combination of over-sampling and under-sampling performs better than the under-sampling with playback and under-sampling without playback in terms of term recognition and classification in different fields. Through the classification results before and after integration, it is obvious that the integration of each base classifier not only effectively improves the classification accuracy of terms, but also greatly improves the recall rate. This term recognition model can help CAT software in improving the recognition accuracy of term translation, which has certain practical effects and provides reference for research in related fields.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
keywords = {intelligent decision, term recognition, computer aided translation, machine translation}
}

@inproceedings{10.1145/3397481.3450656,
author = {Weisz, Justin D. and Muller, Michael and Houde, Stephanie and Richards, John and Ross, Steven I. and Martinez, Fernando and Agarwal, Mayank and Talamadupula, Kartik},
title = {Perfection Not Required? Human-AI Partnerships in Code Translation},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450656},
doi = {10.1145/3397481.3450656},
abstract = {Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency. New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another. The artifacts produced in this way may contain imperfections, such as compilation or logical errors. We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors. Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task of translating source code from one language to another. Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system’s outputs would be established, and future opportunities for generative AI in application modernization. Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.},
booktitle = {26th International Conference on Intelligent User Interfaces},
pages = {402–412},
numpages = {11},
keywords = {imperfect AI, NMT, neural machine translation, code translation, generative AI, application modernization},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1145/3453933.3454022,
author = {Engelke, Alexis and Okwieka, Dominik and Schulz, Martin},
title = {Efficient LLVM-Based Dynamic Binary Translation},
year = {2021},
isbn = {9781450383943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453933.3454022},
doi = {10.1145/3453933.3454022},
abstract = {Emulation of other or newer processor architectures is necessary for a wide variety of use cases, from ensuring compatibility to offering a vehicle for computer architecture research. This problem is usually approached using dynamic binary translation, where machine code is translated, on the fly, to the host architecture during program execution. Existing systems, like QEMU, usually focus on translation performance rather than the overall program execution, and extensions, like HQEMU, are limited by their underlying implementation. Conversely, performance-focused systems are typically used for binary instrumentation. E.g., DynamoRIO reuses original instructions where possible, while Instrew utilizes the LLVM compiler infrastructure, but only supports same-architecture code generation. In this short paper, we generalize Instrew to support different guest and host architectures by refactoring the lifter and by implementing target-independent optimizations to re-use host hardware features for emulated code. We demonstrate this flexibility by adding support for RISC-V as guest architecture and AArch64 as host architecture. Our performance results on SPEC CPU2017 show significant improvements compared to QEMU, HQEMU as well as the original Instrew.},
booktitle = {Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {165–171},
numpages = {7},
keywords = {Optimization, RISC-V, Architecture Simulation, Dynamic Binary Translation, LLVM},
location = {Virtual, USA},
series = {VEE 2021}
}

@inproceedings{10.1145/3544549.3585667,
author = {Seaborn, Katie and Kim, Yeongdae},
title = {“I'm” Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585667},
doi = {10.1145/3544549.3585667},
abstract = {As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {168},
numpages = {6},
keywords = {Feminist HCI, Translation, Gender bias, Natural language processing, Data sets},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3369318.3369330,
author = {Herrera-Camacho, Abel and Z\'{u}\~{n}iga-Sainos, Adri\'{a}n and Sierra-Mart\'{\i}nez, Gerardo and Trangol-Curipe, Jos\'{e} and Mota-Montoya, Margarita and Jarqu\'{\i}n-Casas, Adonay},
title = {Design and Testing of a Corpus for Forensic Speaker Recognition Using MFCC, GMM and MLE},
year = {2020},
isbn = {9781450371483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369318.3369330},
doi = {10.1145/3369318.3369330},
abstract = {The importance of applying speaker recognition systems in forensic environments has increased in this century. One reason is the use of audio recordings as evidence in trials of every kind. This article presents a voice corpus design with this use in mind, based in recordings from speakers of the Spanish language dialect used in Central Mexico, distinct from any other corpus currently in use. For its evaluation, we used a Mel frequency Cepstral coefficient with a Gaussian Mixture Models for parametrization, and a Maximum Likelihood Estimation approach for classification. Results show an accuracy of more than 93% identification of the speaker at any condition, proving a good recognition model.},
booktitle = {Proceedings of the 2019 International Conference on Video, Signal and Image Processing},
pages = {105–110},
numpages = {6},
keywords = {Mel frequency cepstral coefficient, Speech corpus, Forensic speaker recognition, Gaussian mixture model, Maximum likelihood estimation},
location = {Wuhan, China},
series = {VSIP '19}
}

@article{10.1109/TASLP.2022.3190736,
author = {Ferrer, Luciana and Castan, Diego and McLaren, Mitchell and Lawson, Aaron},
title = {A Discriminative Hierarchical PLDA-Based Model for Spoken Language Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190736},
doi = {10.1109/TASLP.2022.3190736},
abstract = {Spoken language recognition (SLR) refers to the automatic process used to determine the language present in a speech sample. SLR is an important task in its own right, for example, as a tool to analyze or categorize large amounts of multi-lingual data. Further, it is also an essential tool for selecting downstream applications in a work flow, for example, to chose appropriate speech recognition or machine translation models. SLR systems are usually composed of two stages, one where an embedding representing the audio sample is extracted and a second one which computes the final scores for each language. In this work, we approach the SLR task as a detection problem and implement the second stage as a probabilistic linear discriminant analysis (PLDA) model. We show that discriminative training of the PLDA parameters gives large gains with respect to the usual generative training. Further, we propose a novel hierarchical approach where two PLDA models are trained, one to generate scores for clusters of highly-related languages and a second one to generate scores conditional to each cluster. The final language detection scores are computed as a combination of these two sets of scores. The complete model is trained discriminatively to optimize a cross-entropy objective. We show that this hierarchical approach consistently outperforms the non-hierarchical one for detection of highly related languages, in many cases by large margins. We train our systems on a collection of datasets including over 100 languages, and test them both on matched and mismatched conditions, showing that the gains are robust to condition mismatch.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2396–2410},
numpages = {15}
}

@article{10.1145/3610887,
author = {Testa, Brian and Xiao, Yi and Sharma, Harshit and Gump, Avery and Salekin, Asif},
title = {Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610887},
doi = {10.1145/3610887},
abstract = {Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have been widely adopted due to their seamless integration with smart home devices and the Internet of Things (IoT) technologies. These VA services raise privacy concerns, especially due to their access to our speech. This work considers one such use case: the unaccountable and unauthorized surveillance of a user's emotion via speech emotion recognition (SER). This paper presents DARE-GP, a solution that creates additive noise to mask users' emotional information while preserving the transcription-relevant portions of their speech. DARE-GP does this by using a constrained genetic programming approach to learn the spectral frequency traits that depict target users' emotional content, and then generating a universal adversarial audio perturbation that provides this privacy protection. Unlike existing works, DARE-GP provides: a) real-time protection of previously unheard utterances, b) against previously unseen black-box SER classifiers, c) while protecting speech transcription, and d) does so in a realistic, acoustic environment. Further, this evasion is robust against defenses employed by a knowledgeable adversary. The evaluations in this work culminate with acoustic evaluations against two off-the-shelf commercial smart speakers using a small-form-factor (raspberry pi) integrated with a wake-word system to evaluate the efficacy of its real-world, real-time deployment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {126},
numpages = {30},
keywords = {Smart Speakers, Adversarial Machine Learning}
}

@article{10.1145/3595377,
author = {Taj, Soonh and Mujtaba, Ghulam and Daudpota, Sher Muhammad and Mughal, Muhammad Hussain},
title = {Urdu Speech Emotion Recognition: A Systematic Literature Review},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3595377},
doi = {10.1145/3595377},
abstract = {Research on Speech Emotion Recognition is becoming more mature day by day, and a lot of research is being carried out on Speech Emotion Recognition in resource-rich languages like English, German, French, and Chinese. Urdu is among the top 10 languages spoken worldwide. Despite its importance, few studies have worked on Urdu Speech emotion as Urdu is recognized as a resource-poor language. The Urdu language lacks publicly available datasets, and for this reason, few researchers have worked on Urdu Speech Emotion Recognition. To the best of our knowledge, no review has been found on Urdu Speech Emotion recognition. This study is the first systematic literature review on Urdu Speech Emotion Recognition, and the primary goal of this study is to provide a detailed analysis of the literature on Urdu Speech Emotion Recognition which includes the datasets, features, pre-processing, approaches, performance metrics, and validation methods used for Urdu Speech Emotion Recognition. This study also highlights the challenges and future directions for Urdu Speech Emotion Recognition.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {186},
numpages = {33},
keywords = {machine learning, low resource language, deep learning, Speech Emotion Recognition, Urdu Speech}
}

@inproceedings{10.1145/3589572.3589593,
author = {Ahmed, Tasnim and Munir, Ahnaf and Ahmed, Sabbir and Hasan, Md. Bakhtiar and Reza, Md. Taslim and Kabir, Md. Hasanul},
title = {Structure-Enhanced Translation from PET to CT Modality with Paired GANs},
year = {2023},
isbn = {9781450399531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589572.3589593},
doi = {10.1145/3589572.3589593},
abstract = {Computed Tomography (CT) images play a crucial role in medical diagnosis and treatment planning. However, acquiring CT images can be difficult in certain scenarios, such as patients inability to undergo radiation exposure or unavailability of CT scanner. An alternative solution can be generating CT images from other imaging modalities. In this work, we propose a medical image translation pipeline for generating high-quality CT images from Positron Emission Tomography (PET) images using a Pix2Pix Generative Adversarial Network (GAN), which are effective in image translation tasks. However, traditional GAN loss functions often fail to capture the structural similarity between generated and target image. To alleviate this issue, we introduce a Multi-Scale Structural Similarity Index Measure (MS-SSIM) loss in addition to the GAN loss to ensure that the generated images preserve the anatomical structures and patterns present in the real CT images. Experiments on the ‘QIN-Breast’ dataset demonstrate that our proposed architecture achieves a Peak Signal-to-Noise Ratio (PSNR) of 17.70 dB and a Structural Similarity Index Measure (SSIM) of 42.51% in the region of interest.},
booktitle = {Proceedings of the 2023 6th International Conference on Machine Vision and Applications},
pages = {142–146},
numpages = {5},
keywords = {Medical Image Translation, PET to CT, Breast Cancer Treatment, QIN-Breast, GAN, Medical Imaging},
location = {Singapore, Singapore},
series = {ICMVA '23}
}

@inproceedings{10.1145/3406601.3406648,
author = {Chotirat, Saranlita and Meesad, Phayung},
title = {Effects of Part-of-Speech on Thai Sentence Classification to Wh-Question Categories Using Machine Learning Approach},
year = {2020},
isbn = {9781450377591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406601.3406648},
doi = {10.1145/3406601.3406648},
abstract = {In the last decade, question classification is a strong signal for answer selection and help to find the structure of question sentences from sentences. For this paper, we evaluated the proposed pre-processing method for classifying the simple sentence to wh-question categories ("What", "When", "Who", "Where", and "How") on Thai texts by considering Part-of-Speech tagging (POS). The performances are evaluated using classification accuracy obtained from traditional classification models including Na\"{\i}ve Bayes, Logistic Regression, Support Vector Machine, K-Nearest Neighbors, and neural networks which employs Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). We compared traditional models and neural networks; the experimental results showed that the result of the neural networks models better than the traditional model. The accuracy of the proposed model using the LSTM model with pre-trained word embedding is improved with an average F1 of 79.60%.},
booktitle = {Proceedings of the 11th International Conference on Advances in Information Technology},
articleno = {43},
numpages = {5},
keywords = {Question classification, Natural language processing, Sentences classification, Part-of-Speech tagging},
location = {Bangkok, Thailand},
series = {IAIT2020}
}

@inproceedings{10.1145/3491102.3502050,
author = {Xu, Ying and Vigil, Valery and Bustamante, Andres S. and Warschauer, Mark},
title = {“Elinor’s Talking to Me!”:Integrating Conversational AI into Children’s Narrative Science Programming},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502050},
doi = {10.1145/3491102.3502050},
abstract = {Video programs are important, accessible educational resources for young children, especially those from an under-resourced backgrounds. These programs’ potential can be amplified if children are allowed to socially interact with media characters during their video watching. This paper presents the design and empirical investigation of interactive science-focused videos in which the main character, powered by a conversational agent, engaged in contingent conversation with children by asking children questions and providing responsive feedback. We found that children actively interacted with the media character in the conversational videos and their parents spontaneously provided support in the process. We also found that the children who watched the conversational video performed better in the immediate, episode-specific science assessment compared to their peers who watched the broadcast, non-interactive version of the same episode. Several design implications are discussed for using conversational technologies to better support child active learning and parent involvement in video watching.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {166},
numpages = {16},
keywords = {science learning, conversational agents, children, Conversational AI, educational media},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1109/TASLP.2022.3180684,
author = {Bhati, Saurabhchand and Villalba, Jes\'{u}s and \.{Z}elasko, Piotr and Moro-Velazquez, Laureano and Dehak, Najim},
title = {Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3180684},
doi = {10.1109/TASLP.2022.3180684},
abstract = {Typically, unsupervised segmentation of speech into the phone- and wordlike units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We lower the feature extraction rate from the typical 100 Hz to 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2002–2014},
numpages = {13}
}

@inproceedings{10.1145/3307650.3322223,
author = {Yan, Zi and Lustig, Daniel and Nellans, David and Bhattacharjee, Abhishek},
title = {Translation Ranger: Operating System Support for Contiguity-Aware TLBs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322223},
doi = {10.1145/3307650.3322223},
abstract = {Virtual memory (VM) eases programming effort but can suffer from high address translation overheads. Architects have traditionally coped by increasing Translation Lookaside Buffer (TLB) capacity; this approach, however, requires considerable hardware resources. One promising alternative is to rely on software-generated translation contiguity to compress page translation encodings within the TLB. To enable this, operating systems (OSes) have to assign spatially-adjacent groups of physical frames to contiguous groups of virtual pages, as doing so allows compression or coalescing of these contiguous translations in hardware. Unfortunately, modern OSes do not currently guarantee translation contiguity in many real-world scenarios; as systems remain online for long periods of time, their memory can and does become fragmented.We propose Translation Ranger, an OS service that recovers lost translation contiguity even where previous contiguity-generation proposals struggle with memory fragmentation. Translation Ranger increases contiguity by actively coalescing scattered physical frames into contiguous regions and can be leveraged by any contiguity-aware TLB without requiring changes to applications. We implement and evaluate Translation Ranger in Linux on real hardware and find that it generates contiguous memory regions 40\texttimes{} larger than the Linux default configuration, permitting TLB coverage of 120GB memory with typically no more than 128 contiguous translation regions. This is achieved with less than 2% run time overhead, a number that is outweighed by the TLB coverage improvements that Translation Ranger provides.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {698–710},
numpages = {13},
keywords = {heterogeneous memory management, translation lookaside buffers, operating system, memory defragmentation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3460569.3460589,
author = {Song, Jiehu and Lin, Zuoquan},
title = {Neural Machine Translating from XML to RDF},
year = {2021},
isbn = {9781450389464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460569.3460589},
doi = {10.1145/3460569.3460589},
booktitle = {Proceedings of the 2021 6th International Conference on Mathematics and Artificial Intelligence},
pages = {130–136},
numpages = {7},
keywords = {XML, Neural Machine Translation, RDF, Knowledge Graph},
location = {Chengdu, China},
series = {ICMAI '21}
}

@inproceedings{10.1145/3462244.3479883,
author = {Hussen Abdelaziz, Ahmed and Kumar, Anushree Prasanna and Seivwright, Chloe and Fanelli, Gabriele and Binder, Justin and Stylianou, Yannis and Kajareker, Sachin},
title = {Audiovisual Speech Synthesis Using Tacotron2},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479883},
doi = {10.1145/3462244.3479883},
abstract = {Audiovisual speech synthesis involves synthesizing a talking face while maximizing the coherency of the acoustic and visual speech. To solve this problem, we propose using AVTacotron2, which is an end-to-end text-to-audiovisual speech synthesizer based on the Tacotron2 architecture. AVTacotron2 converts a sequence of phonemes into a sequence of acoustic features and the corresponding controllers of a face model. The output acoustic features are passed through a WaveRNN model to reconstruct the speech waveform. The speech waveform and the output facial controllers are used to generate the corresponding video of the talking face. As a baseline, we use a modular system, where acoustic speech is synthesized from text using the traditional Tacotron2. The reconstructed acoustic speech is then used to drive the controls of the face model using an independently trained audio-to-facial-animation neural network. We further condition both the end-to-end and modular approaches on emotion embeddings that encode the required prosody to generate emotional audiovisual speech. A comprehensive analysis shows that the end-to-end system is able to synthesize close to human-like audiovisual speech with mean opinion scores (MOS) of 4.1, which is the same MOS obtained on the ground truth generated from professionally recorded videos.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {503–511},
numpages = {9},
keywords = {blendshape coefficients, emotional speech synthesis, speech synthesis, Audiovisual speech, Tacotron2},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@article{10.1145/3585002,
author = {Li, Zhansheng and Xu, Yangyang and Zhao, Nanxuan and Zhou, Yang and Liu, Yongtuo and Lin, Dahua and He, Shengfeng},
title = {Parsing-Conditioned Anime Translation: A New Dataset and Method},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3585002},
doi = {10.1145/3585002},
abstract = {Anime is an abstract art form that is substantially different from the human portrait, leading to a challenging misaligned image translation problem that is beyond the capability of existing methods. This can be boiled down to a highly ambiguous unconstrained translation between two domains. To this end, we design a new anime translation framework by deriving the prior knowledge of a pre-trained StyleGAN model. We introduce disentangled encoders to separately embed structure and appearance information into the same latent code, governed by four tailored losses. Moreover, we develop a FaceBank aggregation method that leverages the generated data of the StyleGAN, anchoring the prediction to produce in-domain animes. To empower our model and promote the research of anime translation, we propose the first anime portrait parsing dataset, Danbooru-Parsing, containing 4,921 densely labeled images across 17 classes. This dataset connects the face semantics with appearances, enabling our new constrained translation setting. We further show the editability of our results, and extend our method to manga images, by generating the first manga parsing pseudo data. Extensive experiments demonstrate the values of our new dataset and method, resulting in the first feasible solution on anime translation.},
journal = {ACM Trans. Graph.},
month = {apr},
articleno = {30},
numpages = {14},
keywords = {image-to-image translation, image editing, Generative adversarial networks}
}

@inproceedings{10.1145/3474085.3475577,
author = {Gan, Shiwei and Yin, Yafeng and Jiang, Zhiwei and Xie, Lei and Lu, Sanglu},
title = {Skeleton-Aware Neural Sign Language Translation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475577},
doi = {10.1145/3474085.3475577},
abstract = {As an essential communication way for deaf-mutes, sign languages are expressed by human actions. To distinguish human actions for sign language understanding, the skeleton which contains position information of human pose can provide an important cue, since different actions usually correspond to different poses/skeletons. However, skeleton has not been fully studied for Sign Language Translation (SLT), especially for end-to-end SLT. Therefore, in this paper, we propose a novel end-to-end Skeleton-Aware neural Network (SANet) for video-based SLT. Specifically, to achieve end-to-end SLT, we design a self-contained branch for skeleton extraction. To efficiently guide the feature extraction from video with skeletons, we concatenate the skeleton channel and RGB channels of each frame for feature extraction. To distinguish the importance of clips, we construct a skeleton-based Graph Convolutional Network (GCN) for feature scaling, i.e., giving importance weight for each clip. The scaled features of each clip are then sent to a decoder module to generate spoken language. In our SANet, a joint training strategy is designed to optimize skeleton extraction and sign language translation jointly. Experimental results on two large scale SLT datasets demonstrate the effectiveness of our approach, which outperforms the state-of-the-art methods. Our code is available at https://github.com/SignLanguageCode/SANet.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4353–4361},
numpages = {9},
keywords = {skeleton, sign language translation, neural network},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.5555/3546258.3546298,
author = {Bakhtin, Anton and Deng, Yuntian and Gross, Sam and Ott, Myle and Ranzato, Marc'Aurelio and Szlam, Arthur},
title = {Residual Energy-Based Models for Text},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Current large-scale auto-regressive language models (Radford et al., 2019; Liu et al., 2018; Graves, 2013) display impressive fluency and can generate convincing text. In this work we start by asking the question: Can the generations of these models be reliably distinguished from real text by statistical discriminators? We find experimentally that the answer is affirmative when we have access to the training data for the model, and guardedly affirmative even if we do not.This suggests that the auto-regressive models can be improved by incorporating the (globally normalized) discriminators into the generative process. We give a formalism for this using the Energy-Based Model framework, and show that it indeed improves the results of the generative models, measured both in terms of perplexity and in terms of human evaluation.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {40},
numpages = {41},
keywords = {text generation, energy-based models, importance sampling, real/fake discrimination, negative sampling, generalization}
}

@inproceedings{10.1145/3321408.3326670,
author = {Chen, XiaoFeng and Wang, Hao and Xiang, Wei},
title = {Implementation of Tibetan-Chinese Translation Platform Based on LSTM Algorithm},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3326670},
doi = {10.1145/3321408.3326670},
abstract = {With the rapid economic development and increasingly frequent language exchanges in Tibet, the traditional statistical machine translation methods are faced with problems such as lack of data and over-fitting of training, resulting in poor translation quality. Combined with the current development of natural language processing (NLP), the LSTM algorithm based on Google's TensorFlow framework is proposed to realize the Tibetan and Chinese neural machine translation method. In the preprocessing stage of corpus, word segmentation module is constructed for tibetan-chinese bilingual parallel corpus by using the combination algorithm of gated circulatory neural network (GRU) and conditional random field (CRF). In the model construction stage, the LSTM method is used to construct the model. The experimental results show that the short and long time memory networks have good translation effects.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {142},
numpages = {5},
keywords = {GRU-CRF algorithm, LSTM networks, bilingual parallel corpus},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3482632.3482750,
author = {Cui, Gaili},
title = {Design of Translation Accuracy Correction Algorithm for English Translation Software Based on Rough Set},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482750},
doi = {10.1145/3482632.3482750},
abstract = {With the development of modern information technology, network teaching is becoming more and more popular, which provides a strong technical support for English teaching. Many English translation tools emerge as the times require. With the rapid development of computer software technology, the development and application of English translation software has attracted more and more attention. Machine translation is an important research field of natural language processing, which has a wide application prospect in today's information society and military departments. Traditional machine translation methods use pipeline successive operations to identify parts of speech and parse the original corpus, so as to obtain the syntactic structure of English language, which makes the error iterative transmission between translation tasks and the accuracy of structured examples lower, which leads to the lower accuracy of English language and literature translation. Based on rough set theory, this paper expounds the application of translation accuracy correction algorithm in English translation software.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {558–562},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1109/ICSE48619.2023.00063,
author = {Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur},
title = {Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00063},
doi = {10.1109/ICSE48619.2023.00063},
abstract = {Software bugs claim ≈ 50% of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {640–652},
numpages = {13},
keywords = {natural language processing, software maintenance, software engineering, bug explanation, transformers, software bug, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3395363.3397369,
author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
title = {CoCoNuT: Combining Context-Aware Neural Translation Models Using Ensemble for Program Repair},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397369},
doi = {10.1145/3395363.3397369},
abstract = {Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&amp;V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–114},
numpages = {14},
keywords = {Automated program repair, Deep Learning, AI and Software Engineering, Neural Machine Translation},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1109/TASLP.2019.2930914,
author = {Yang, Yan and Bao, Changchun},
title = {RS-CAE-Based AR-Wiener Filtering and Harmonic Recovery for Speech Enhancement},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2930914},
doi = {10.1109/TASLP.2019.2930914},
abstract = {By taking into account temporal correlation of speech feature. In this paper, a novel structure of convolutional Auto Encoder CAE was proposed. In this structure, the historical output of the CAE was fed into a CAE stack recurrently. We name this structure as Recurrent Stack Convolutional Auto Encoder RS-CAE. In the training stage, the training feature maps of the RS-CAE comprise of log power spectrum LPS of noisy speech and an additional feature map derived from the LPS of the enhanced speech in the history. In this way, the temporal correlation is incorporated as much as possible in the RS-CAE. The training target is a concatenated vector of auto-regressive AR model parameters of speech and noise. At online stage, the LPS of noisy speech and the LPS of the enhanced speech from the history make up input feature maps together. The outputs of the RS-CAE are the AR model parameters of speech and noise, which are used to construct the AR-Wiener filter. Because the estimated AR model parameters are not completely accurate and some harmonics may be lost in the enhanced speech, the codebook-based harmonic recovery technique was proposed to reconstruct harmonic structure of the enhanced speech. The test results confirmed that the proposed method achieved better performance compared with some existing approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {1752–1762},
numpages = {11}
}

@article{10.1109/TASLP.2022.3155286,
author = {Kothapally, Vinay and Hansen, John H. L.},
title = {SkipConvGAN: Monaural Speech Dereverberation Using Generative Adversarial Networks via Complex Time-Frequency Masking},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3155286},
doi = {10.1109/TASLP.2022.3155286},
abstract = {With the advancements in deep learning approaches, the performance of speech enhancing systems in the presence of background noise have shown significant improvements. However, improving the system’s robustness against reverberation is still a work in progress, as reverberation tends to cause loss of formant structure due to smearing effects in time and frequency. A wide range of deep learning-based systems either enhance the magnitude response and reuse the distorted phase or enhance complex spectrogram using a complex time-frequency mask. Though these approaches have demonstrated satisfactory performance, they do not directly address the lost formant structure caused by reverberation. We believe that retrieving the formant structure can help improve the efficiency of existing systems. In this study, we propose SkipConvGAN - an extension of our prior work SkipConvNet. The proposed system’s generator network tries to estimate an efficient complex time-frequency mask, while the discriminator network aids in driving the generator to restore the lost formant structure. We evaluate the performance of our proposed system on simulated and real recordings of reverberant speech from the single-channel task of the REVERB challenge corpus. The proposed system shows a consistent improvement across multiple room configurations over other deep learning-based generative adversarial frameworks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1600–1613},
numpages = {14}
}

@article{10.1145/3426239,
author = {Liu, Shuo and Gao, Mingliang and John, Vijay and Liu, Zheng and Blasch, Erik},
title = {Deep Learning Thermal Image Translation for Night Vision Perception},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3426239},
doi = {10.1145/3426239},
abstract = {Context enhancement is critical for the environmental perception in night vision applications, especially for the dark night situation without sufficient illumination. In this article, we propose a thermal image translation method, which can translate thermal/infrared (IR) images into color visible (VI) images, called IR2VI. The IR2VI consists of two cascaded steps: translation from nighttime thermal IR images to gray-scale visible images (GVI), which is called IR-GVI; and the translation from GVI to color visible images (CVI), which is known as GVI-CVI in this article. For the first step, we develop the Texture-Net, a novel unsupervised image translation neural network based on generative adversarial networks. Texture-Net can learn the intrinsic characteristics from the GVI and integrate them into the IR image. In comparison with the state-of-the-art unsupervised image translation methods, the proposed Texture-Net is able to address some common challenges, e.g., incorrect mapping and lack of fine details, with a structure connection module and a region-of-interest focal loss. For the second step, we investigated the state-of-the-art gray-scale image colorization methods and integrate the deep convolutional neural network into the IR2VI framework. The results of the comprehensive evaluation experiments demonstrate the effectiveness of the proposed IR2VI image translation method. This solution will contribute to the environmental perception and understanding in varied night vision applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {9},
numpages = {18},
keywords = {image translation, Context enhancement, night vision, generative adversarial network}
}

@article{10.1109/TASLP.2020.3045556,
author = {Wang, Peidong and Chen, Zhuo and Wang, DeLiang and Li, Jinyu and Gong, Yifan},
title = {Speaker Separation Using Speaker Inventories and Estimated Speech},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3045556},
doi = {10.1109/TASLP.2020.3045556},
abstract = {We propose speaker separation using speaker inventories and estimated speech (SSUSIES), a framework leveraging speaker profiles and estimated speech for speaker separation. SSUSIES contains two methods, speaker separation using speaker inventories (SSUSI) and speaker separation using estimated speech (SSUES). SSUSI performs speaker separation with the help of speaker inventory. By combining the advantages of permutation invariant training (PIT) and speech extraction, SSUSI significantly outperforms conventional approaches. SSUES is a widely applicable technique that can substantially improve speaker separation performance using the output of first-pass separation. We evaluate the models on both speaker separation and speech recognition metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {537–546},
numpages = {10}
}

@article{10.1109/TASLP.2018.2842159,
author = {Wang, DeLiang and Chen, Jitong},
title = {Supervised Speech Separation Based on Deep Learning: An Overview},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842159},
doi = {10.1109/TASLP.2018.2842159},
abstract = {Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement speech-nonspeech separation, speaker separation multitalker separation, and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1702–1726},
numpages = {25}
}

@article{10.1145/3564698,
author = {Liu, Yuzhi and Piccardi, Massimo},
title = {Topic-Based Unsupervised and Supervised Dictionary Induction},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564698},
doi = {10.1145/3564698},
abstract = {Word translation is a natural language processing task that provides translation between the words of a source and a target language. As a task, it reduces to the induction of a bilingual dictionary, which is typically performed by aligning word embeddings of the source language to word embeddings of the target language. To date, all the existing approaches have focused on performing a single, global alignment in word embedding space. However, semantic differences between the various languages, in addition to differences in the content of the corpora used for training the word embeddings, can hinder the effectiveness of such a global alignment. For this reason, in this article we propose conducting the alignment between the source and target embedding spaces by multiple mappings at topic level. The experimental results show that our approach has been able to achieve an average accuracy improvement of +3.30 percentage points over a state-of-the-art approach in unsupervised dictionary induction from languages as diverse as German, French, Italian, Spanish, Finnish, Turkish, and Chinese to English, and +3.95 points average improvement in supervised dictionary induction.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {77},
numpages = {21},
keywords = {Word translation, topic-based dictionary induction, word embedding alignment, dictionary induction, topic modeling}
}

@inproceedings{10.1145/3269206.3269262,
author = {Seki, Kazuhiro},
title = {Exploring Neural Translation Models for Cross-Lingual Text Similarity},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269262},
doi = {10.1145/3269206.3269262},
abstract = {This paper explores a neural network-based approach to computing similarity of two texts written in different languages. Such similarity can be useful for a variety of applications including cross-lingual information retrieval and cross-lingual text classification. To compute similarity, we focus on neural machine translation models and examine the utility of their intermediate states. Through experiments on an English-Japanese translation corpus, it is demonstrated that the intermediate states of input texts are indeed beneficial for computing cross-lingual text similarity, outperforming other approaches including a strong machine translation-based baseline.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1591–1594},
numpages = {4},
keywords = {distributed representation, sequence-to-sequence models, cross-lingual information retrieval},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1109/TASLP.2022.3205766,
author = {Yue, Zhengjun and Loweimi, Erfan and Christensen, Heidi and Barker, Jon and Cvetkovic, Zoran},
title = {Acoustic Modelling From Raw Source and Filter Components for Dysarthric Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3205766},
doi = {10.1109/TASLP.2022.3205766},
abstract = {Acoustic modelling for automatic dysarthric speech recognition (ADSR) is a challenging task. Data deficiency is a major problem and substantial differences between typical and dysarthric speech complicate the transfer learning. In this paper, we aim at building acoustic models using the raw magnitude spectra of the source and filter components for ADSR. The proposed multi-stream models consist of convolutional, recurrent and fully-connected layers allowing for pre-processing various information streams and fusing them at an optimal level of abstraction. We demonstrate that such a multi-stream processing leverages information encoded in the vocal tract and excitation components and leads to normalising nuisance factors such as speaker attributes and speaking style. This leads to a better handling of dysarthric speech that exhibits large inter- and intra-speaker variabilities and results in a notable performance gain. Furthermore, we analyse the learned convolutional filters and visualise the outputs of different layers after dimensionality reduction to demonstrate how the speaker-related attributes are normalised along the pipeline. We also compare the proposed multi-stream model with various systems based on MFCC, FBank, raw waveform and i-vector, and, study the training dynamics as well as usefulness of the feature normalisation and data augmentation via speed perturbation. On the widely used TORGO and UASpeech dysarthric speech corpora, the proposed approach leads to a competitive performance of up to 35.3% and 30.3% WERs for dysarthric speech, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2968–2980},
numpages = {13}
}

@article{10.1109/TASLP.2021.3052688,
author = {Wu, Xixin and Cao, Yuewen and Lu, Hui and Liu, Songxiang and Kang, Shiyin and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
title = {Exemplar-Based Emotive Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3052688},
doi = {10.1109/TASLP.2021.3052688},
abstract = {Expressive text-to-speech (E-TTS) synthesis is important for enhancing user experience in communication with machines using the speech modality. However, one of the challenges in E-TTS is the lack of a precise description of emotions. Previous categorical specifications may be insufficient for describing complex emotions. The dimensional specifications face the difficulty of ambiguity in annotation. This work advocates a new approach of describing emotive speech acoustics using spoken exemplars. We investigate methods to extract emotion descriptions from the input exemplar of emotive speech. The measures are combined to form two descriptors, based on capsule network (CapNet) and residual error network (RENet). The first is designed to consider the spatial information in the input exemplary spectrogram, and the latter is to capture the contrastive information between emotive acoustic expressions. Two different approaches are applied for conversion from the variable-length feature sequence to fixed-size description vector: (1) dynamic routing groups similar capsules to the output description; and (2) recurrent neural network's hidden states store the temporal information for the description. The two descriptors are integrated to a state-of-the-art sequence-to-sequence architecture to obtain an end-to-end architecture that is optimized as a whole towards the same goal of generating correct emotive speech. Experimental results on a public audiobook dataset demonstrate that the two exemplar-based approaches achieve significant performance improvement over the baseline system in both emotion similarity and speech quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {874–886},
numpages = {13}
}

@inproceedings{10.1145/3510858.3511006,
author = {Lu, Nan and Liu, Lei},
title = {Design and Implementation of Intelligent Translation System Based on Computer Algorithm Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511006},
doi = {10.1145/3510858.3511006},
abstract = {The important means and tools of human communication are language and writing. The biggest difficulty encountered in communication between people is often the language barrier. Accurate and rapid conversion between two natural languages has always been people's dream. However, training a professional translator not only requires deep language skills, but also a wealth of professional knowledge, and it takes a long time. With the development of the world gradually entering internationalization, the exponentially increasing content needs to be translated, and manual translation has not kept up with the development of society. In order to meet the society's need for translation and the need for communication between people, people are constantly working towards machine translation. Therefore, the purpose of this article is to study the design and implementation of an intelligent translation system based on computer algorithm technology. This article analyzes and studies the difficulties in the implementation of the translation system, and believes that the translation should be based on the usual simple sentences, with additional processing of large sentences: that is, the large sentences are broken down into several. The simple sentences that are syntactically and semantically equivalent are translated one by one, using computer algorithms to form long sentences based on the relationship between each sentence. This separates the processing of long sentences from the processing of simple sentences, so that both the structural features of long sentences and the structural features of simple sentences can be fully considered. Experimental results show that the accuracy of translation is about 90% after using this algorithm.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {549–553},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3340531.3411977,
author = {Zhao, Tianxiang and Tang, Xianfeng and Zhang, Xiang and Wang, Suhang},
title = {Semi-Supervised Graph-to-Graph Translation},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411977},
doi = {10.1145/3340531.3411977},
abstract = {Graph translation is very promising research direction and has awide range of potential real-world applications. Graph is a natural structure for representing relationship and interactions, and its translation can encode the intrinsic semantic changes of relation-ships in different scenarios. However, despite its seemingly wide possibilities, usage of graph translation so far is still quite limited.One important reason is the lack of high-quality paired dataset. For example, we can easily build graphs representing peoples? shared music tastes and those representing co-purchase behavior, but a well paired dataset is much more expensive to obtain. Therefore,in this work, we seek to provide a graph translation model in the semi-supervised scenario. This task is non-trivial, because graph translation involves changing the semantics in the form of link topology and node attributes, which is difficult to capture due to the combinatory nature and inter-dependencies. Furthermore, due to the high order of freedom in graph's composition, it is difficult to assure the generalization ability of trained models. These difficulties impose a tighter requirement for the exploitation of unpaired samples. Addressing them, we propose to construct a dual representation space, where transformation is performed explicitly to model the semantic transitions. Special encoder/decoder structures are designed, and auxiliary mutual information loss is also adopted to enforce the alignment of unpaired/paired examples. We evaluate the proposed method in three different datasets.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1863–1872},
numpages = {10},
keywords = {graph neural network, graph translation, semi-supervised learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1109/TASLP.2022.3224305,
author = {Wang, Mou and Chen, Junqi and Zhang, Xiao-Lei and Rahardja, Susanto},
title = {End-to-End Multi-Modal Speech Recognition on an Air and Bone Conducted Speech Corpus},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3224305},
doi = {10.1109/TASLP.2022.3224305},
abstract = {Automatic speech recognition (ASR) has been significantly improved in the past years. However, most robust ASR systems are based on air-conducted (AC) speech, and their performances in low signal-to-noise-ratio (SNR) conditions are not satisfactory. Bone-conducted (BC) speech is intrinsically insensitive to environmental noise, and therefore can be used as an auxiliary source for improving the performance of an ASR at low SNR. In this paper, we first develop a multi-modal Mandarin corpus, which contains air- and bone-conducted synchronized speech (ABCS). The multi-modal speeches are recorded with a headset equipped with both AC and BC microphones. To our knowledge, it is by far the largest corpus for conducting bone conduction ASR research. Then, we propose a multi-modal conformer ASR system based on a novel multi-modal transducer (MMT). The proposed system extracts semantic embeddings from the AC and BC speech signals by a conformer-based encoder and a transformer-based truncated decoder. The semantic embeddings of the two speech sources are fused dynamically with adaptive weights by the MMT module. Experimental results demonstrate the proposed multi-modal system outperforms single-modal systems with either AC or BC modality and multi-modal baseline system by a large margin at various SNR levels. It also shows the two modalities complement with each other, and our method can effectively utilize the complementary information of different sources.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {513–524},
numpages = {12}
}

@inproceedings{10.1145/3349341.3349513,
author = {Li, Fuxue and Zhang, Rui and Yan, Hong and Mu, Shujie and Jie, Longmei and Zhang, Jing},
title = {Forced Decoding Based Rule Extraction for Phrased-Based Translation},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349513},
doi = {10.1145/3349341.3349513},
abstract = {Statistical machine translation models typically learn phrase or syntactic translation rules extracted from parallel data with word alignments, but the approach suffers from a problem where some desirable rules are blocked due to word alignment mistakes. Many researchers focus on improving the translation rule extraction by correcting word alignment mistakes, while this paper presents a phrase-based forced decoding method to address this issue, and a rule-augmentation approach is proposed to improve the quality of translation based on the translation rules extracted by forced decoding method. Experiment on Chinese-English translation task demonstrates significant improvements over the baseline system.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {789–792},
numpages = {4},
keywords = {SMT, Forced decoding, Word alignment},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@article{10.1109/TASLP.2022.3145293,
author = {Lei, Yi and Yang, Shan and Wang, Xinsheng and Xie, Lei},
title = {MsEmoTTS: Multi-Scale Emotion Transfer, Prediction, and Control for Emotional Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145293},
doi = {10.1109/TASLP.2022.3145293},
abstract = {Expressive synthetic speech is essential for many human-computer interaction and audio broadcast scenarios, and thus synthesizing expressive speech has attracted much attention in recent years. Previous methods performed the expressive speech synthesis either with explicit labels or with a fixed-length style embedding extracted from reference audio, both of which can only learn an average style and thus ignores the multi-scale nature of speech prosody. In this paper, we propose MsEmoTTS, a multi-scale emotional speech synthesis framework, to model the emotion from different levels. Specifically, the proposed method is a typical attention-based sequence-to-sequence model and with proposed three modules, including global-level emotion presenting module (GM), utterance-level emotion presenting module (UM), and local-level emotion presenting module (LM), to model the global emotion category, utterance-level emotion variation, and syllable-level emotion strength, respectively. In addition to modeling the emotion from different levels, the proposed method also allows us to synthesize emotional speech in different ways, i.e., transferring the emotion from reference audio, predicting the emotion from input text, and controlling the emotion strength manually. Extensive experiments conducted on a Chinese emotional speech corpus demonstrate that the proposed method outperforms the compared reference audio-based and text-based emotional speech synthesis methods on the emotion transfer speech synthesis and text-based emotion prediction speech synthesis respectively. Besides, the experiments also show that the proposed method can control the emotion expressions flexibly. Detailed analysis shows the effectiveness of each module and the good design of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {853–864},
numpages = {12}
}

@inproceedings{10.1145/3508259.3508267,
author = {Komatsu, Rina and Yamazaki, Keisuke},
title = {Conditional Drive Environment Translation Using StarGAN with CBIN},
year = {2022},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508267},
doi = {10.1145/3508259.3508267},
abstract = {Automatic driving without human's control requires a lot of training to be able to adopt any situation. About situation, the environment at driving has the variety such as dark at night, wet road because of rain and the stack of snow. Preparing the drive image dataset with a lot of environment situations is the difficult task in collecting. This study tried solving the preparing problem by constructing deep learning model with limited data and translating single image to multi environment situations. For accomplishing this subject, we employed N domains translation model called StarGAN. In this paper, we investigated the StarGAN with enough conditional translation performance through comparing visualized results and FID score. We trained StarGANs including original to translate single image to 6 kinds of drive environment domain: daytime &amp; clear, daytime &amp; rainy, daytime &amp; snowy, night &amp; clear, night &amp; rainy and night &amp; snowy. Through experiments, we found the StarGAN employing “CBIN: Central Biasing Instance Normalization” and “AdaLIN: Adaptive Layer-Instance Normalization” at Generator, and “the adversarial loss of CAM Logit” at Discriminator could mark the lower FID score than original StarGAN.},
booktitle = {Proceedings of the 2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {53–61},
numpages = {9},
keywords = {StarGAN, Image-to-Image Translation, Image processing, Deep Learning, Visualization},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@article{10.1109/TASLP.2022.3182268,
author = {Ai, Yang and Ling, Zhen-Hua and Wu, Wei-Lu and Li, Ang},
title = {Denoising-and-Dereverberation Hierarchical Neural Vocoder for Statistical Parametric Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3182268},
doi = {10.1109/TASLP.2022.3182268},
abstract = {This paper presents a denoising and dereverberation hierarchical neural vocoder (DNR-HiNet) to convert noisy and reverberant acoustic features into clean speech waveforms. The DNR-HiNet vocoder is built by modifying the amplitude spectrum predictor (ASP) in the original HiNet vocoder. This modified denoising and dereverberation ASP (DNR-ASP) can predict clean log amplitude spectra from input degraded acoustic features. To achieve this, the DNR-ASP first predicts the log amplitude spectra of noisy and reverberant speech, the log amplitude spectra of additive noise and the room impulse response (RIR) and then performs initial denoising and dereverberation by signal processing algorithms. The initially processed log amplitude spectra are then enhanced by another neural network to obtain the final clean log amplitude spectra. We also introduce a bandwidth extension model and a frequency resolution extension model into the DNR-ASP to further improve its performance. Finally, a statistical parametric speech synthesis (SPSS) method with DNR-HiNet is proposed to deal with the situation that the quality of target speaker’s recordings is degraded by noise and reverberation. Experimental results indicate that the DNR-HiNet vocoder was able to generate denoised and dereverberated waveforms given noisy and reverberant acoustic features and outperformed the original HiNet vocoder and a few other neural vocoders. On speech enhancement tasks, its performance was competitive with several advanced speech enhancement methods. Furthermore, the SPSS method with DNR-HiNet achieved better quality of synthetic speech than the conventional approach which directly applied speech enhancement to the degraded adaptation data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2036–2048},
numpages = {13}
}

@inproceedings{10.1145/3366715.3366746,
author = {Xu, Yinxia},
title = {Improving Flash Translation Layer Performance by Using Log Block Mapping Scheme and Two-Level Buffer for Address Translation Information},
year = {2019},
isbn = {9781450362429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366715.3366746},
doi = {10.1145/3366715.3366746},
abstract = {In the era of big data, the requirement of mass storage and fast access of data makes solid state disk(SSD) based on NAND flash be widely used. However, increasing flash memory capacity imposes huge SRAM consumption for logical-physical translation table in a page-level flash translation layer(FTL). Existing FTL schemes selectively cache the on-demand address mappings to quicken the address translation, while keeping all address mappings in flash memory. But the page-level catching mechanism causes a certain degree of cache pollution. In this paper, we manage page-level address translation information at hybrid-level mapping scheme and use two-level buffer for map groups to decrease SRAM consumption while reducing the cache pollution. What's more, an efficient replacement policy is designed. We can increase the cache hit ratio and reduce the write backs of evicted dirty entries and decrease garbage collection operations by these means. The performance and lifetime of the flash memory is improved. Experimental results show that the proposed scheme increases cache hit ratio by up to 28% and decreases the average response time by up to 23% compared with the existing FTL schemes.},
booktitle = {Proceedings of the 2019 International Conference on Robotics Systems and Vehicle Technology},
pages = {171–176},
numpages = {6},
keywords = {Flash Translation Layer, Log Block Mapping, Two-level Buffer},
location = {Wuhan, China},
series = {RSVT '19}
}

@inproceedings{10.1145/3340531.3412169,
author = {Giachanou, Anastasia and Rosso, Paolo},
title = {The Battle Against Online Harmful Information: The Cases of Fake News and Hate Speech},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412169},
doi = {10.1145/3340531.3412169},
abstract = {Social media have given the opportunity to users to express their opinions online in a fast and easy way. The ease of generating content online and the anonymity that social media provide have increased the amount of harmful content that is published. This tutorial will focus on the topic of online harmful information. First, we will analyse and explain the different types of online harmful information with a particular focus on fake news and hate speech. In addition, we will explain the different computational approaches proposed in the literature for the detection of fake news and hate speech. Next, we will present details regarding the evaluation process, datasets and shared tasks and finally, we will discuss future directions in the field of online harmful information detection.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3503–3504},
numpages = {2},
keywords = {hate speech, online harmful information, fake news detection},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3372806.3372819,
author = {Umezaki, Naoto and Okubo, Takumi and Watanabe, Hideyuki and Katagiri, Shigeru and Ohsaki, Miho},
title = {Minimum Classification Error Training with Speech Synthesis-Based Regularization for Speech Recognition},
year = {2020},
isbn = {9781450372213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372806.3372819},
doi = {10.1145/3372806.3372819},
abstract = {To increase the utility of Regularization, which is a common framework for avoiding the underestimation of ideal Bayes error, for speech recognizer training, we propose a new classifier training concept that incorporates a regularization term that represents the speech synthesis ability of classifier parameters. To implement our new concept, we first introduce a speech recognizer that embeds Line Spectral Pairs-Conjugate Structure-Algebraic Code Excited Linear Prediction (LSP-CS-ACELP) in a Multi-Prototype State-Transition-Model (MP-STM) classifier, define a regularization term that represents the speech synthesis ability by the distance between a training sample and its nearest MP-STM word model, and formalize a new Minimum Classification Error (MCE) training method for jointly minimizing a conventional smooth classification error count loss and the newly defined regularization term. We evaluated the proposed training method in an isolated-word, closed-vocabulary, and speaker-independent speech recognition task whose Bayes error is estimated to be about 20% and found that our method successfully produced an estimate of Bayes error (about 18.4%) with a single training run over a training dataset without such data resampling as Cross-Validation or the assumptions of sample distribution. Moreover, we investigated the quality of the synthesized speech using LSP parameters derived from the trained prototypes and found that the quality of the Bayes error estimation is clearly supported by the speech synthesis ability preserved in the training.},
booktitle = {Proceedings of the 2019 2nd International Conference on Signal Processing and Machine Learning},
pages = {62–72},
numpages = {11},
keywords = {Line spectral pairs, Code-excited linear prediction, Minimum classification error training, Speech recognition},
location = {Hangzhou, China},
series = {SPML '19}
}

@article{10.1145/3533386,
author = {Canet Sola, Mar and Guljajeva, Varvara},
title = {Dream Painter: Exploring Creative Possibilities of AI-Aided Speech-to-Image Synthesis in the Interactive Art Context},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3533386},
doi = {10.1145/3533386},
abstract = {This paper describes an interactive robotic art installation Dream Painter by the artistic duo Varvara &amp; Mar that deploys artificial intelligence (AI), a KUKA industrial robot and interaction technology in order to offer the audience an artistic interpretation of their past dreams, which are then turned into a collective painting. The installation is composed of four larger parts: audience interaction design, AI-driven multicoloured drawing software, communication with an arm robot, and a kinetic part that is the automatic paper progression following each completed dream drawing. All these interconnected parts are orchestrated into an interactive and autonomous system in the form of an art installation that occupies two floors of a cultural centre. In the article, we document the technical and conceptual frameworks of the project, and the experience gained through the creation and exhibition of the interactive robotic art installation. In addition, the paper explores the creative potential of speech-to-AI-drawing transformation, which is a translation of different semiotic spaces performed by a robot as a method for audience interaction in the art exhibition context.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {sep},
articleno = {33},
numpages = {11},
keywords = {interactive art, human-centred interface, latent-space, co-creative AI, speech-to-image, robotic art}
}

@inproceedings{10.1145/3284179.3284319,
author = {Figuerola, Carlos G.},
title = {Applying Topic Modeling Techniques to Degraded Texts: Spanish Historical Press during the Transici\'{o}n (1977-1982)},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284319},
doi = {10.1145/3284179.3284319},
abstract = {Topic modeling techniques are applied in the field of Digital Humanities, specifically wit historical texts some often. However, digitizing documents often produces texts with poor readability. This is the case of the historical press, in which to the degrading of the support must be added the layout, the inclusion of advertisements, illustrations, etc. This paper describes the application of topic modeling to a specific Spanish newspaper with these difficulties; as well as the same application during the same period to another newspaper converted to text manually. The comparison of the results shows consistency between both newspapers},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {857–862},
numpages = {6},
keywords = {text tagging, document scanning, Historical press, OCR, topic modeling, LDA},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00017,
author = {Malyala, Aniketh and Zhou, Katelyn and Ray, Baishakhi and Chakraborty, Saikat},
title = {On ML-Based Program Translation: Perils and Promises},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00017},
doi = {10.1109/ICSE-NIER58687.2023.00017},
abstract = {With the advent of new and advanced programming languages, it becomes imperative to migrate legacy software to new programming languages. Unsupervised Machine Learning-based Program Translation could play an essential role in such migration, even without a sufficiently sizeable reliable corpus of parallel source code. However, these translators are far from perfect due to their statistical nature. This work investigates unsupervised program translators and where and why they fail. With in-depth error analysis of such failures, we have identified that the cases where such translators fail follow a few particular patterns. With this insight, we develop a rule-based program mutation engine, which pre-processes the input code if the input follows specific patterns and post-process the output if the output follows certain patterns. We show that our code processing tool, in conjunction with the program translator, can form a hybrid program translator and significantly improve the state-of-the-art. In the future, we envision an end-to-end program translation tool where programming domain knowledge can be embedded into an ML-based translation pipeline using pre- and post-processing steps.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {60–65},
numpages = {6},
keywords = {code generation, code translation, program transformation},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1145/3482632.3482753,
author = {Chai, Jinlian},
title = {Intelligent English Automatic Translation System Based on Artificial Intelligence Support Vector Machine},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482753},
doi = {10.1145/3482632.3482753},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {572–576},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1109/TASLP.2018.2886739,
author = {Gao, Jianqing and Du, Jun and Chen, Enhong},
title = {Mixed-Bandwidth Cross-Channel Speech Recognition via Joint Optimization of DNN-Based Bandwidth Expansion and Acoustic Modeling},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2886739},
doi = {10.1109/TASLP.2018.2886739},
abstract = {Automatic speech recognition ASR systems are often built using scene related speech data due to large variations of transmission channels and sampling rates in different scenarios. In this study, we propose a general framework that establishes a unified model for diversified speech data with different sampling rates and channels. The framework is a joint optimization of deep neural network DNN-based bandwidth expansion and acoustic modeling to exploit a large amount of diversified training data. First, we design two novel DNN architectures to map the acoustic features from narrowband to wideband speech through direct mapping and progressive mapping. The learning targets of the direct mapping DNN DNN-DM are the acoustic features extracted from speech with the largest bandwidth, while the acoustic features from speech with all the other bandwidths are used as input. A progressive stacking network PSN gradually maps the features from the low sampling rates to the highest sampling rate through the design of intermediate target layers via multitask training. Then, in addition to these bandwidth expansion networks, we investigate several joint training strategies for DNN-based acoustic models. Our experiments conducted on three diversified large-scale Mandarin speech datasets with different recording channels and sampling rates 6, 8, and 16&nbsp;kHz show that the proposed unified model using PSN for bandwidth expansion not only is a more flexible and compact design than conventional multiple acoustic models with each bandwidth for a specific sampling rate, but also yields consistent and significant improvements over bandwidth-dependent models with an average relative word error rate reduction of 6.2%, indicating that the proposed model can fully utilize the diversified cross-channel speech data with multiple bandwidths. Moreover, the proposed methods are verified to be robust on different realistic scenes and can be effectively extended to a long short-term memory framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {559–571},
numpages = {13}
}

@inproceedings{10.1145/3588155.3588166,
author = {Wu, Zhen and Wang, Guo},
title = {A Study on the Corpus Expansion Method of Neural Machine Translation Based on Reverse Transcription Grammar},
year = {2023},
isbn = {9781450399500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588155.3588166},
doi = {10.1145/3588155.3588166},
abstract = {The current conventional corpus expansion methods mainly obtain the core words by mining the similarity of utterances to achieve the expansion of the corpus, which is better than the lack of decoding and encoding processing of the corpus data, resulting in the poor expansion effect. In this regard, a neural translation corpus expansion method based on reverse transcription grammar is proposed. The CYK decoding algorithm is combined with the move-in-reduction decoding algorithm to encode and decode the corpus data, and the difference and product of the corpus elements are quantified to realize the matching of utterance information, and finally the three-level filtering mechanism is used to obtain the core words under different classes to realize the corpus expansion. In the experiment, the proposed method is validated for the expansion effect. The analysis of the experimental results shows that the proposed method expands the corpus with high bilingual evaluation substitution values and has a more desirable expansion effect.},
booktitle = {Proceedings of the 2023 5th Asia Pacific Information Technology Conference},
pages = {69–75},
numpages = {7},
location = {Ho Chi Minh City, Vietnam},
series = {APIT '23}
}

@inproceedings{10.1145/3297280.3297351,
author = {Franciscatto, Maria Helena and Lima, Jo\~{a}o Carlos Damasceno and Trois, Celio and Maran, Vin\'{\i}cius and Soares, M\'{a}rcia Keske and Rocha, Cristiano Cortez da},
title = {Applying Situation-Awareness for Recommending Phonological Processes in the Children's Speech},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297351},
doi = {10.1145/3297280.3297351},
abstract = {Situation-Awareness (SA) involves the correct interpretation of scenarios, allowing a system to respond to the observed environment in several domains. Speech therapy is an area where SA may provide benefits; however, the related literature generally is not concerned with identifying phonological processes (PPs) in pronunciation and their effects on the management of therapeutic tasks. An early identification of speech sound disorders allows the diagnosis and treatment of various pathologies and the reasoning about situations may aid clinical decision-making. So, in this paper, we present a novel method for predicting PPs, supporting speech therapists in the identification of speech disorders in children. Our approach uses SA tied to machine learning to first classify the correctness in the pronunciation of a set of target words. Then, a second instance of ML uses scores calculated from mispelled words to predict the PPs. The method was evaluated through a speech corpus containing over a thousand of audio files, collected from pronunciation assessments performed by speech-language pathologists with more than 1,000 children. Our results showed an average accuracy over 92.5% for classifying the pronunciations, and 92.2% for predicting the PPs.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {739–746},
numpages = {8},
keywords = {phonological processes, speech recognition, speech therapy, machine learning, situation awareness},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3411764.3445171,
author = {Zaheer, Nimra and Ahmad, Obaid Ullah and Ahmed, Ammar and Khan, Muhammad Shehryar and Shabbir, Mudassir},
title = {SEMOUR: A Scripted Emotional Speech Repository for Urdu},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445171},
doi = {10.1145/3411764.3445171},
abstract = {Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15,040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {12},
keywords = {speech dataset, Emotional speech, Urdu language, digital recording, speech emotion recognition, deep learning, machine learning, human annotation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3308561.3353774,
author = {Bragg, Danielle and Koller, Oscar and Bellard, Mary and Berke, Larwan and Boudreault, Patrick and Braffort, Annelies and Caselli, Naomi and Huenerfauth, Matt and Kacorri, Hernisa and Verhoef, Tessa and Vogler, Christian and Ringel Morris, Meredith},
title = {Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353774},
doi = {10.1145/3308561.3353774},
abstract = {Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {16–31},
numpages = {16},
keywords = {sign language, and generation, recognition, asl, translation},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{10.1145/3409964.3461814,
author = {Bender, Michael A. and Bhattacharjee, Abhishek and Conway, Alex and Farach-Colton, Mart\'{\i}n and Johnson, Rob and Kannan, Sudarsun and Kuszmaul, William and Mukherjee, Nirjhar and Porter, Don and Tagliavini, Guido and Vorobyeva, Janet and West, Evan},
title = {Paging and the Address-Translation Problem},
year = {2021},
isbn = {9781450380706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409964.3461814},
doi = {10.1145/3409964.3461814},
abstract = {The classical paging problem, introduced by Sleator and Tarjan in 1985, formalizes the problem of caching pages in RAM in order to minimize IOs. Their online formulation ignores the cost of address translation: programs refer to data via virtual addresses, and these must be translated into physical locations in RAM. Although the cost of an individual address translation is much smaller than that of an IO, every memory access involves an address translation, whereas IOs can be infrequent. In practice, one can spend money to avoid paging by over-provisioning RAM; in contrast, address translation is effectively unavoidable. Thus address-translation costs can sometimes dominate paging costs, and systems must simultaneously optimize both. To mitigate the cost of address translation, all modern CPUs have translation lookaside buffers (TLBs), which are hardware caches of common address translations. What makes TLBs interesting is that a single TLB entry can potentially encode the address translation for many addresses. This is typically achieved via the use of huge pages, which translate runs of contiguous virtual addresses to runs of contiguous physical addresses. Huge pages reduce TLB misses at the cost of increasing the IOs needed to maintain contiguity in RAM. This tradeoff between TLB misses and IOs suggests that the classical paging problem does not tell the full story. This paper introduces the Address-Translation Problem, which formalizes the problem of maintaining a TLB, a page table, and RAM in order to minimize the total cost of both TLB misses and IOs. We present an algorithm that achieves the benefits of huge pages for TLB misses without the downsides of huge pages for IOs.},
booktitle = {Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {105–117},
numpages = {13},
keywords = {address translation, hashing, virtual memory, paging, iceberg, tlb},
location = {Virtual Event, USA},
series = {SPAA '21}
}

@inproceedings{10.1145/3394171.3413656,
author = {Fu, Huiyuan and Yu, Ting and Wang, Xin and Ma, Huadong},
title = {Cross-Granularity Learning for Multi-Domain Image-to-Image Translation},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413656},
doi = {10.1145/3394171.3413656},
abstract = {Image translation across diverse domains has attracted more and more attention. Existing multi-domain image-to-image translation algorithms only learn the features of the complete image without considering specific features of local instances. To ensure the important instance to be more realistically translated, we propose a cross-granularity learning model for multi-domain image-to-image translation. We provide detailed procedures to capture the features of instances during the learning process, and specifically learn the relationship between style of the global image and the style of an instance on the image through the enforcing of the cross-granularity consistency. In our design, we only need one generator to perform the instance-aware multi-domain image translation. Our extensive experiments on several multi-domain image-to-image translation datasets show that our proposed method can achieve superior performance compared with the state-of-the-art approaches.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3099–3107},
numpages = {9},
keywords = {image translation, image generation, gan},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3378904.3378915,
author = {Agrawal, Tushar and Urolagin, Siddhaling},
title = {2-Way Arabic Sign Language Translator Using CNNLSTM Architecture and NLP},
year = {2020},
isbn = {9781450376839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378904.3378915},
doi = {10.1145/3378904.3378915},
abstract = {Over 466 million (5%) people across the world are suffering from hearing impairment, according to the World Health Organization. There is a great need to bridge the communication gap between the deaf and the general population. In our research work, recent developments such as Natural Language Processing (NLP) and Deep Learning Neural Network (DLNN) are utilized to bridge this gap. We developed a 2-way sign language translator for the Arabic language, which translates text to sign and vice versa. The NLP such as parsing, part of speech tagging, tokenization and translation are developed to achieve text to sign translation. The Convolutional Neural Network (CNN) along with Long Short-Term Memory (LSTM) is used to perform sign to text translation.},
booktitle = {Proceedings of the 2020 2nd International Conference on Big Data Engineering and Technology},
pages = {96–101},
numpages = {6},
keywords = {Machine Translation, Language translation, Natural Language Processing, Gesture recognition, Deep Learning},
location = {Singapore, China},
series = {BDET 2020}
}

@inproceedings{10.1145/3446804.3446851,
author = {Wang, Wenwen},
title = {Helper Function Inlining in Dynamic Binary Translation},
year = {2021},
isbn = {9781450383257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446804.3446851},
doi = {10.1145/3446804.3446851},
abstract = {Dynamic binary translation (DBT) is the cornerstone of many important applications. Yet, it takes a tremendous effort to develop and maintain a real-world DBT system. To mitigate the engineering effort, helper functions are frequently employed during the development of a DBT system. Though helper functions greatly facilitate the DBT development, their adoption incurs substantial performance overhead due to the helper function calls. To solve this problem, this paper presents a novel approach to inline helper functions in DBT systems. The proposed inlining approach addresses several unique technical challenges. As a result, the performance overhead introduced by helper function calls can be reduced, and meanwhile, the benefits of helper functions for DBT development are not lost. We have implemented a prototype based on the proposed inlining approach using a popular DBT system, QEMU. Experimental results on the benchmark programs from the SPEC CPU 2017 benchmark suite show that an average of 1.2x performance speedup can be achieved. Moreover, the translation overhead introduced by inlining helper functions is negligible.},
booktitle = {Proceedings of the 30th ACM SIGPLAN International Conference on Compiler Construction},
pages = {107–118},
numpages = {12},
keywords = {Binary translation, QEMU, Function inlining, Compiler optimization},
location = {Virtual, Republic of Korea},
series = {CC 2021}
}

@article{10.1145/3547647,
author = {Ho, Son and Protzenko, Jonathan},
title = {Aeneas: Rust Verification by Functional Translation},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {ICFP},
url = {https://doi.org/10.1145/3547647},
doi = {10.1145/3547647},
abstract = {We present Aeneas, a new verification toolchain for Rust programs based on a lightweight functional translation. We leverage Rust’s rich region-based type system to eliminate memory reasoning for a large class of Rust programs, as long as they do not rely on interior mutability or unsafe code. Doing so, we relieve the proof engineer of the burden of memory-based reasoning, allowing them to instead focus on functional properties of their code. The first contribution of Aeneas is a new approach to borrows and controlled aliasing. We propose a pure, functional semantics for LLBC, a Low-Level Borrow Calculus that captures a large subset of Rust programs. Our semantics is value-based, meaning there is no notion of memory, addresses or pointer arithmetic. Our semantics is also ownership-centric, meaning that we enforce soundness of borrows via a semantic criterion based on loans rather than through a syntactic type-based lifetime discipline. We claim that our semantics captures the essence of the borrow mechanism rather than its current implementation in the Rust compiler. The second contribution of Aeneas is a translation from LLBC to a pure lambda-calculus. This allows the user to reason about the original Rust program through the theorem prover of their choice, and fulfills our promise of enabling lightweight verification of Rust programs. To deal with the well-known technical difficulty of terminating a borrow, we rely on a novel approach, in which we approximate the borrow graph in the presence of function calls. This in turn allows us to perform the translation using a new technical device called backward functions. We implement our toolchain in a mixture of Rust and OCaml; our chief case study is a low-level, resizing hash table, for which we prove functional correctness, the first such result in Rust. Our evaluation shows significant gains of verification productivity for the programmer. This paper therefore establishes a new point in the design space of Rust verification toolchains, one that aims to verify Rust programs simply, and at scale. Rust goes to great lengths to enforce static control of aliasing; the proof engineer should not waste any time on memory reasoning when so much already comes “for free”!},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {116},
numpages = {31},
keywords = {verification, functional translation, Rust}
}

@inproceedings{10.1145/3311957.3359485,
author = {Bipat, Taryn and Davidson, Diana Victoria and Guadarrama, Melissa and Li, Nancy and Black, Ryder and McDonald, David W. and Zachry, Mark},
title = {How Does Editor Interaction Help Build the Spanish Wikipedia?},
year = {2019},
isbn = {9781450366922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311957.3359485},
doi = {10.1145/3311957.3359485},
abstract = {The English language Wikipedia is notable for its large number of articles. However, 288 other active language editions of Wikipedia have also developed through the intricate interactions of contributing editors. While the editor interactions in the English Wikipedia have been researched extensively, these other language editions remain understudied. To understand how editors currently come to consensus in article building in the Spanish language, a team of researchers has leveraged an existing English framework that depicts how power and policies play a role in mass collaboration. Using this English language framework, we are utilizing qualitative coding methods to build a unique model of the editor interactions on the Spanish language Wikipedia (ES). The results of this study will help contribute to a deeper understanding of how a framework in a different language edition of Wikipedia varies from the English. Our preliminary findings show that the power plays utilized in the Spanish Wikipedia talk pages differ in type and amount from power plays identified in the talk pages in the English language Wikipedia (EN), suggesting different forms of editor interactions and facilitation strategies across this multilingual platform.},
booktitle = {Conference Companion Publication of the 2019 on Computer Supported Cooperative Work and Social Computing},
pages = {156–160},
numpages = {5},
keywords = {wikipedia, qualitative, spanish, collaboration, language},
location = {Austin, TX, USA},
series = {CSCW '19}
}

@inproceedings{10.1145/3284179.3284337,
author = {Lin, Jia and Cheng, Lifen},
title = {The Relation between Cinema, Subtitled Series and Spanish Learning for Chinese Students of Spanish Language},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284337},
doi = {10.1145/3284179.3284337},
abstract = {This study investigates the relationship between watching subtitled Spanish films and series and Spanish learning for Chinese students. A survey was developed with 207 university students from 42 universities (Chinese and Spanish) who voluntary responds to a questionnaire. A pilot study was performed in order to test the correspondence validity of the two versions (Chinese and Spanish) of the same questionnaire. The results show that watching subtitled Spanish films and series has been preferred by the respondents and they do it frequently. In addition, they take it as an effective way to learn Spanish. Likewise, the results indicate that the respondents are not satisfied with the current situation of the film and television market with subtitling neither in China nor Spain. Therefore, they wish to have more access to Spanish films and series resources in China and more subtitled version in Spain.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {971–977},
numpages = {7},
keywords = {ELE, multimedia resources, subtitling, Chinese students, Spanish films and series, enjoyment},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@article{10.1109/TASLP.2021.3120586,
author = {Wu, Xixin and Cao, Yuewen and Lu, Hui and Liu, Songxiang and Wang, Disong and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
title = {Speech Emotion Recognition Using Sequential Capsule Networks},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120586},
doi = {10.1109/TASLP.2021.3120586},
abstract = {Speech emotion recognition (SER) is an indispensable part of fluid human-machine interaction and attracts lots of research attentions. Recent work on SER has successfully applied convolutional neural networks (CNNs) to learn feature representations from speech spectrograms. However, the fundamental problem of CNNs is that the spatial information in spectrograms is lost, which includes positional and relationship information of low-level features, such as pitch and formant frequencies. We propose a novel architecture of sequential capsule networks (CapNets) by leveraging the advantange of CapNets that spatial information can be preserved in capsules and passed to upper capsule layers via dynamic routing. Also, the dynamic routing algorithm provides an effective alternative to pooling or storing recurrent hidden states for obtaining utterance-level features from the sequential capsule outputs. To further improve the model's ability to capture contextual information, we introduce a recurrent connection to the sequential structure. The experimental comparison of the proposed systems and previously published systems using CNNs and recurrent neural networks (RNNs) based on the IEMOCAP corpus demonstrates the effectiveness of the proposed sequential CapNets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {3280–3291},
numpages = {12}
}

@inproceedings{10.1145/3503161.3547830,
author = {Tang, Shengeng and Hong, Richang and Guo, Dan and Wang, Meng},
title = {Gloss Semantic-Enhanced Network with Online Back-Translation for Sign Language Production},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547830},
doi = {10.1145/3503161.3547830},
abstract = {Sign Language Production (SLP) aims to generate the visual appearance of sign language according to the spoken language, in which a key procedure is to translate sign Gloss to Pose (G2P). Existing G2P methods mainly focus on regression prediction of posture coordinates, namely closely fitting the ground truth. In this paper, we provide a new viewpoint: a Gloss semantic-Enhanced Network is proposed with Online Back-Translation (GEN-OBT) for G2P in the SLP task. Specifically, GEN-OBT consists of a gloss encoder, a pose decoder, and an online reverse gloss decoder. In the gloss encoder based on the transformer, we design a learnable gloss token without any prior knowledge of gloss, to explore the global contextual dependency of the entire gloss sequence. During sign pose generation, the gloss token is aggregated onto the existing generated poses as gloss guidance. Then, the aggregated features are interacted with the entire gloss embedding vectors to generate the next pose. Furthermore, we design a CTC-based reverse decoder to convert the generated poses backward into glosses, which guarantees the semantic consistency during the processes of gloss-to-pose and pose-to-gloss. Extensive experiments on the challenging PHOENIX14T benchmark demonstrate that the proposed GEN-OBT outperforms the state-of-the-art models. Visualization results further validate the interpretability of our method.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5630–5638},
numpages = {9},
keywords = {sign language production, gloss semantic enhancement, online back-translation, deep learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3543321.3543366,
author = {Pham, Anh Tuan},
title = {Investigating Factors Affecting ESL Students in Translation in Higher Education},
year = {2022},
isbn = {9781450396417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543321.3543366},
doi = {10.1145/3543321.3543366},
abstract = {Many language students find it difficult to translate English texts into another language as required since many books are now written in English language, so researches have examined some major factors that influence students’ translating process. A mixed method of quantitative and qualitative approach was employed to examine ESL students’ perception on grammar, vocabulary and prior knowledge factors in their translation. The participants were 116 English language students in online translation classes at a private university, under the employment of a 5-point Likert scale questionnaire and a semi-structured interview to gain deep understanding into factors affecting them in translation and possible solutions to achieve the best translation. The results revealed that vocabulary was the most influential factor. Prior knowledge was the second consideration as medical knowledge was most contributed while grammar was not significantly appreciated by the participants. It is to suggest that ESL students should learn a wide range of vocabulary and teachers should provide a good input of vocabulary to help students perform better in translation. Also, gaining more knowledge in different fields of study such as medical, technological and cultural knowledge was more considerable.},
booktitle = {Proceedings of the 7th International Conference on Distance Education and Learning},
pages = {273–278},
numpages = {6},
keywords = {perception, affecting, factors, translation, ESL students},
location = {Beijing, China},
series = {ICDEL '22}
}

@inproceedings{10.1145/3397271.3401035,
author = {Bonab, Hamed and Sarwar, Sheikh Muhammad and Allan, James},
title = {Training Effective Neural CLIR by Bridging the Translation Gap},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401035},
doi = {10.1145/3397271.3401035},
abstract = {We introduce Smart Shuffling, a cross-lingual embedding (CLE) method that draws from statistical word alignment approaches to leverage dictionaries, producing dense representations that are significantly more effective for cross-language information retrieval (CLIR) than prior CLE methods. This work is motivated by the observation that although neural approaches are successful for monolingual IR, they are less effective in the cross-lingual setting. We hypothesize that neural CLIR fails because typical cross-lingual embeddings "translate" query terms into related terms -- i.e., terms that appear in a similar context -- in addition to or sometimes rather than synonyms in the target language. Adding related terms to a query (i.e., query expansion) can be valuable for retrieval, but must be mitigated by also focusing on the starting query. We find that prior neural CLIR models are unable to bridge the translation gap, apparently producing queries that drift from the intent of the source query.We conduct extrinsic evaluations of a range of CLE methods using CLIR performance, compare them to neural and statistical machine translation systems trained on the same translation data, and show a significant gap in effectiveness. Our experiments on standard CLIR collections across four languages indicate that Smart Shuffling fills the translation gap and provides significantly improved semantic matching quality. Having such a representation allows us to exploit deep neural (re-)ranking methods for the CLIR task, leading to substantial improvement with up to 21% gain in MAP, approaching human translation performance. Evaluations on bilingual lexicon induction show a comparable improvement.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {9–18},
numpages = {10},
keywords = {neural clir, cross-lingual word embedding, cross-lingual information retrieval, translation gap},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1109/TASLP.2021.3129360,
author = {Lu, Xugang and Shen, Peng and Tsao, Yu and Kawai, Hisashi},
title = {Coupling a Generative Model With a Discriminative Learning Framework for Speaker Verification},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3129360},
doi = {10.1109/TASLP.2021.3129360},
abstract = {The task of speaker verification (SV) is to decide whether an utterance is spoken by a target or an imposter speaker. In most studies of SV, a log-likelihood ratio (LLR) score is estimated based on a generative probability model on speaker features, and compared with a threshold for making a decision. However, the generative model usually focuses on individual feature distributions, does not have the discriminative feature selection ability, and is easy to be distracted by nuisance features. The SV, as a hypothesis test, could be formulated as a binary discrimination task where neural network based discriminative learning could be applied. In discriminative learning, the nuisance features could be removed with the help of label supervision. However, discriminative learning pays more attention to classification boundaries, and is prone to overfitting to a training set which may result in bad generalization on a test set. In this paper, we propose a hybrid learning framework, i.e., coupling a joint Bayesian (JB) generative model structure and parameters with a neural discriminative learning framework for SV. In the hybrid framework, a two-branch Siamese neural network is built with dense layers that are coupled with factorized affine transforms as used in the JB model. The LLR score estimation in the JB model is formulated according to the distance metric in the discriminative learning framework. By initializing the two-branch neural network with the generatively learned model parameters of the JB model, we further train the model parameters with the pairwise samples as a binary discrimination task. Moreover, a direct evaluation metric (DEM) in SV based on minimum empirical Bayes risk (EBR) is designed and integrated as an objective function in the discriminative learning. We carried out SV experiments on Speakers in the wild (SITW) and Voxceleb. Experimental results showed that our proposed model improved the performance with a large margin compared with state of the art models for SV.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3631–3641},
numpages = {11}
}

@article{10.1145/3597456,
author = {Qiu, Shi},
title = {Construction of English Speech Recognition Model by Fusing CNN and Random Deep Factorization TDNN},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3597456},
doi = {10.1145/3597456},
abstract = {In current society, speech recognition can perform a variety of functions, such as completing voice commands, enabling speech processing, spoken language translation and facilitating communication. Therefore, the study of speech recognition technology is of high value. However, current speech recognition techniques focus on among clearly expressed spoken words, which poses great challenges for recognition with spoken pronunciation or dialect pronunciation. Some scholars currently use a model combining time-delay neural networks and long and short-term memory networks to build speech recognition systems, but the performance in acoustic recognition is poor. Therefore, the study proposes a convolutional neural network (CNN), time-delay neural network (TDNN) and output-gate projected Gated recurrent by analyzing the deep neural network unit (OPGRU) combined with a composite English speech recognition model. The model can optimize the acoustic model after the introduction of CNN, and the model can accurately recognize pronunciation features and make the model have a wider recognition range. The proposed composite model is compared with the Word error rate (Wer) and runtime metrics in the Mozilla Common Voice dataset. The Wer result of the composite model is 23.42% and the running time is 1418 s. The Wer result of the composite model is 24.61% and the running time is 1385 s. Compared with the TDNN-OPGRU model, the Wer of the composite model decreases by 1.19% but the running time increases by 33 s. The accuracy of the composite model is higher than that of the TDNN-OPGRU model. From a comprehensive consideration, the speech recognition model accuracy has higher priority, so the composite model proposed in the study has better performance.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
keywords = {Time-delay neural network, Acoustic model, Convolutional neural network, Threshold control network, Speech recognition}
}

@inproceedings{10.1145/3568923.3568933,
author = {Yang, Chaoying},
title = {The Application of Artificial Intelligence in Translation Teaching},
year = {2023},
isbn = {9781450397230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568923.3568933},
doi = {10.1145/3568923.3568933},
abstract = {The purpose of this paper is to analyze the concept of AI translation and the current situation of its application in college teaching. Taking the interpretation teaching as an example, this paper introduces the specific methods of applying AI technology in translation teaching from three aspects: pre-class preparation, classroom teaching and after-class practice, and tries to provide a way to change the traditional teaching methods and teaching mode, fully mobilize students’ subjective initiative, strongly stimulate students’ curiosity and their thirst for knowledge, and make students’ have more innovative ability and spirit.},
booktitle = {Proceedings of the 4th International Conference on Intelligent Science and Technology},
pages = {56–60},
numpages = {5},
keywords = {Development prospect, Artificial intelligence, Big data era, Translation teaching},
location = {Harbin, China},
series = {ICIST '22}
}

@inproceedings{10.1145/3472301.3484343,
author = {da Cunha Silva, Andr\'{e} Luiz and de S\'{a}, Tatiane Milit\~{a}o and Diniz, Ruan Sousa and Ferreira, Simone B. Leal and Siqueira, Sean W. M. and Bourguignon, Saulo Cabral},
title = {When Just Ok, is Not Ok: An Experimental Study through Sequential Chronological Cuts, with Prescriptive and Semantic Analyzes on the Dynamic Translation by VLibras Avatar},
year = {2021},
isbn = {9781450386173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472301.3484343},
doi = {10.1145/3472301.3484343},
abstract = {Internet sites expose their content primarily in a textual form. Accessibility solutions that use avatars to translate from Portuguese into sign language aim to reduce accessibility barriers for deaf people who are literate only in Libras or have Portuguese as a second language. This research aimed to identify whether the VLibras tool, widely adopted in governmental websites, is an effective accessibility solution in dynamic translation into sign language. It was an applied qualitative and exploratory research involving a bibliographic survey and support from expert interpreters. An experimental study was carried out through sequential chronological cuts, where prescriptive and semantic analyzes were applied. Evidence was found that there is no translation for Libras in the dynamic translation process carried out by the VLibras avatar, but only a transposition of part of the Libras lexicon into the Portuguese morphosyntactic structure. Difficulties of simultaneous translation in large and more complex texts were noticed, resulting in excessive pauses and dactylology for words that have a sign registered in the Libras basic dictionary. Using HCI concepts to evaluate the dynamic translation to Libras by VLibras avatar expands the existing theoretical discussion. It also contributes to minimizing communication problems caused by the discrepancy between the original message and the automatic translation, practical applicability of this study.},
booktitle = {Proceedings of the XX Brazilian Symposium on Human Factors in Computing Systems},
articleno = {56},
numpages = {12},
keywords = {Libras, dynamic translation, accessibility, VLibras, deaf},
location = {Virtual Event, Brazil},
series = {IHC '21}
}

@inproceedings{10.1145/3463945.3469057,
author = {Li, Tingting and Zhao, Huan and Wang, Song and Huang, Jing},
title = {Style-Guided Image-to-Image Translation for Multiple Domains},
year = {2021},
isbn = {9781450385305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463945.3469057},
doi = {10.1145/3463945.3469057},
abstract = {The cross-domain image translation has drawn more and more attention. It aims to translate images from a source domain into target domains, such that images can appear in multiple styles. The most popular approaches are using encoders to extract style features from the source domain and then pushing them into a generator to produce new images. However, these methods usually only suit for two domains translation, and present low diversity in multiple domains since the extracted features are roughly used as input for the generator, instead of making full use of them. In this paper, we design a novel loss function, style-guided diversity loss (Sd loss), which utilizes the extracted style features to encourage our model exploring the image space and discovering diverse images. It is proved theoretically that the proposed loss is better than the diversity sensitive loss in the state-of-the-art approaches. In addition, qualitative and quantitative experiments demonstrate the superiority of the proposed approach against several state-of-the-art approaches in terms of the quality and the diversity of translated images.},
booktitle = {Proceedings of the 2021 Workshop on Multi-Modal Pre-Training for Multimedia Understanding},
pages = {28–36},
numpages = {9},
keywords = {quality, generative adversarial network (GAN), multiple domains, diversity, style features, image-to-image translation},
location = {Taipei, Taiwan},
series = {MMPT '21}
}

@inproceedings{10.1145/3474085.3475507,
author = {Kumar, Abhishek and Braud, Tristan and Lee, Lik Hang and Hui, Pan},
title = {Theophany: Multimodal Speech Augmentation in Instantaneous Privacy Channels},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475507},
doi = {10.1145/3474085.3475507},
abstract = {Many factors affect speech intelligibility in face-to-face conversations. These factors lead conversation participants to speak louder and more distinctively, exposing the content to potential eavesdroppers. To address these issues, we introduce Theophany, a privacy-preserving framework for augmenting speech. Theophany establishes ad-hoc social networks between conversation participants to exchange contextual information, improving speech intelligibility in real-time. At the core of Theophany, we develop the first privacy perception model that assesses the privacy risk of a face-to-face conversation based on its topic, location, and participants. This framework allows to develop any privacy-preserving application for face-to-face conversation. We implement the framework within a prototype system that augments the speaker's speech with real-life subtitles to overcome the loss of contextual cues brought by mask-wearing and social distancing during the COVID-19 pandemic. We evaluate Theophany through a user survey and a user study on 53 and 17 participants, respectively. Theophany's privacy predictions match the participants' privacy preferences with an accuracy of 71.26%. Users considered Theophany to be useful to protect their privacy (3.88/5), easy to use (4.71/5), and enjoyable to use (4.24/5). We also raise the question of demographic and individual differences in the design of privacy-preserving solutions.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2056–2064},
numpages = {9},
keywords = {user privacy, multi-modal speech augmentation, augmented reality, human augmentation, speech intelligibility, assistive technology},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3503161.3548379,
author = {Fu, Szu-Wei and Fan, Yaran and Hosseinkashi, Yasaman and Gupchup, Jayant and Cutler, Ross},
title = {Improving Meeting Inclusiveness Using Speech Interruption Analysis},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548379},
doi = {10.1145/3503161.3548379},
abstract = {Meetings are a pervasive method of communication within all types of companies and organizations, and using remote collaboration systems to conduct meetings has increased dramatically since the COVID-19 pandemic. However, not all meetings are inclusive, especially in terms of the participation rates among attendees. In a recent large-scale survey conducted at Microsoft, the top suggestion given by meeting participants for improving inclusiveness is to improve the ability of remote participants to interrupt and acquire the floor during meetings. We show that the use of the virtual raise hand (VRH) feature can lead to an increase in predicted meeting inclusiveness at Microsoft. One challenge is that VRH is used in less than $1%$ of all meetings. In order to drive adoption of its usage to improve inclusiveness (and participation), we present a machine learning-based system that predicts when a meeting participant attempts to obtain the floor, but fails to interrupt (termed a 'failed interruption'). This prediction can be used to nudge the user to raise their virtual hand within the meeting. We believe this is the first failed speech interruption detector, and the performance on a realistic test set has an area under curve (AUC) of 0.95 with a true positive rate (TPR) of 50% at a false positive rate (FPR) of 1%. To our knowledge, this is also the first dataset of interruption categories (including the failed interruption category) for remote meetings. Finally, we believe this is the first such system designed to improve meeting inclusiveness through speech interruption analysis and active intervention.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {887–895},
numpages = {9},
keywords = {meeting inclusiveness, remote collaboration, machine learning, speech interruption analysis},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2021.3091805,
author = {Liu, Shansong and Geng, Mengzhe and Hu, Shoukang and Xie, Xurong and Cui, Mingyu and Yu, Jianwei and Liu, Xunying and Meng, Helen},
title = {Recent Progress in the CUHK Dysarthric Speech Recognition System},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3091805},
doi = {10.1109/TASLP.2021.3091805},
abstract = {Despite the rapid progress of automatic speech recognition (ASR) technologies in the past few decades, recognition of disordered speech remains a highly challenging task to date. Disordered speech presents a wide spectrum of challenges to current data intensive deep neural networks (DNNs) based ASR technologies that predominantly target normal speech. This paper presents recent research efforts at the Chinese University of Hong Kong (CUHK) to improve the performance of disordered speech recognition systems on the largest publicly available UASpeech dysarthric speech corpus. A set of novel modelling techniques including neural architectural search, data augmentation using spectra-temporal perturbation, model based speaker adaptation and cross-domain generation of visual features within an audio-visual speech recognition (AVSR) system framework were employed to address the above challenges. The combination of these techniques produced the lowest published word error rate (WER) of 25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER reduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric speech recognition system featuring a 6-way DNN system combination and cross adaptation of out-of-domain normal speech data trained systems. Bayesian model adaptation further allows rapid adaptation to individual dysarthric speakers to be performed using as little as 3.06 seconds of speech. The efficacy of these techniques were further demonstrated on a CUDYS Cantonese dysarthric speech recognition task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2267–2281},
numpages = {15}
}

@inproceedings{10.1145/3511430.3511471,
author = {Ganesha},
title = {Simulink to NuSMV Model Translation Challenges},
year = {2022},
isbn = {9781450396189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511430.3511471},
doi = {10.1145/3511430.3511471},
abstract = {Safety critical systems must be formally verified to prove their correctness in order to avoid the loss of human life or a huge investment. Simulink is widely used in the industry to model safety-critical systems. NuSMV is the model checker which can be used to verify the models. To verify the designed models, developers need to be familiar with formal verification tools, languages, and syntax. The automatic translation of created models into the model checker’s input language aids in the reduction of development time and expense. Model checking performed during the design phase detects system flaws early in the software development process. When numerous model verification tools are available, it is beneficial to implement an intermediate representation of the translated model. This paper discusses challenges faced in translating Simulink-based models into input language of a model checker.},
booktitle = {15th Innovations in Software Engineering Conference},
articleno = {32},
numpages = {2},
keywords = {Simulation, Simulink, Modeling, Translation, NuSMV},
location = {Gandhinagar, India},
series = {ISEC 2022}
}

@inproceedings{10.1145/3490035.3490284,
author = {Gupta, Anchit and Khan, Faizan Farooq and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C. V.},
title = {Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490284},
doi = {10.1145/3490035.3490284},
abstract = {This paper proposes a video editor based on OpenShot with several state-of-the-art facial video editing algorithms as added functionalities. Our editor provides an easy-to-use interface to apply modern lip-syncing algorithms interactively. Apart from lip-syncing, the editor also uses audio and facial re-enactment to generate expressive talking faces. The manual control improves the overall experience of video editing without missing out on the benefits of modern synthetic video generation algorithms. This control enables us to lip-sync complex dubbed movie scenes, interviews, television shows, and other visual content. Furthermore, our editor provides features that automatically translate lectures from spoken content, lip-sync of the professor, and background content like slides. While doing so, we also tackle the critical aspect of synchronizing background content with the translated speech. We qualitatively evaluate the usefulness of the proposed editor by conducting human evaluations. Our evaluations show a clear improvement in the efficiency of using human editors and an improved video generation quality.We attach demo videos with the supplementary material clearly explaining the tool and also showcasing multiple results.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {25},
numpages = {9},
keywords = {speech-to-speech translation, video editing, lip-sync, talking head generation, human in the loop},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.1145/3394171.3413953,
author = {Han, Yu and Yang, Shuai and Wang, Wenjing and Liu, Jiaying},
title = {From Design Draft to Real Attire: Unaligned Fashion Image Translation},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413953},
doi = {10.1145/3394171.3413953},
abstract = {Fashion manipulation has attracted growing interest due to its great application value, which inspires many researches towards fashion images. However, little attention has been paid to fashion design draft. In this paper, we study a new unaligned translation problem between design drafts and real fashion items, whose main challenge lies in the huge misalignment between the two modalities. We first collect paired design drafts and real fashion item images without pixel-wise alignment. To solve the misalignment problem, our main idea is to train a sampling network to adaptively adjust the input to an intermediate state with structure alignment to the output. Moreover, built upon the sampling network, we present design draft to real fashion item translation network (D2RNet), where two separate translation streams that focus on texture and shape, respectively, are combined tactfully to get both benefits. D2RNet is able to generate realistic garments with both texture and shape consistency to their design drafts. We show that this idea can be effectively applied to the reverse translation problem and present R2DNet accordingly. Extensive experiments on unaligned fashion design translation demonstrate the superiority of our method over state-of-the-art methods. Our project website is available at: https://victoriahy.github.io/MM2020/.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1533–1541},
numpages = {9},
keywords = {bidirectional translation, fashion design, unaligned data, image-to-image translation},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3474198.3478152,
author = {Cheng, Siyuan and Zhang, Dongya and Yin, Didi},
title = {A DenseNet-GRU Technology for Chinese Speech Emotion Recognition},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478152},
doi = {10.1145/3474198.3478152},
abstract = {The speech plays an important role in the transmission of human emotions. Voice emotion recognition has become an important part of human-machine systems, especially in specific systems that require high real-time and accuracy. In order to improve the accuracy and real-time performance of speech emotion recognition, people have done a lot of work in speech emotion recognition. This paper proposes Chinese speech emotion recognition based on DenseNet-GRU neural network. In the original Mel cepstrum coefficients feature extraction process, the selection of sampling points during the frame and window operation is performed, and the feature map size is adjusted to improve the recognition efficiency. At the same time, the DenseNet-GRU neural network structure is used for Chinese speech emotion recognition. The core is to use GRU (gated recurrent unit) to extract the association relationship between different speech emotions, and then obtain the feature changes between different speech emotions, the recognition accuracy rate is improved. The accuracy of the test set on CASIA speech emotion data reached 80%, and the model parameters were few. At the same time, white Gaussian noise enhancement is performed on the data before the neural network training, which can effectively alleviate the model overfitting.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {112},
numpages = {7},
keywords = {DenseNet – GRU, Chinese speech emotion recognition, White Gaussian Noise enhancement, Mel frequency Cepstral coefficient},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@inproceedings{10.1145/3510003.3510140,
author = {Sun, Weisong and Fang, Chunrong and Chen, Yuchen and Tao, Guanhong and Han, Tingxu and Zhang, Quanjun},
title = {Code Search Based on Context-Aware Code Translation},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510140},
doi = {10.1145/3510003.3510140},
abstract = {Code search is a widely used technique by developers during software development. It provides semantically similar implementations from a large code corpus to developers based on their queries. Existing techniques leverage deep learning models to construct embedding representations for code snippets and queries, respectively. Features such as abstract syntactic trees, control flow graphs, etc., are commonly employed for representing the semantics of code snippets. However, the same structure of these features does not necessarily denote the same semantics of code snippets, and vice versa. In addition, these techniques utilize multiple different word mapping functions that map query words/code tokens to embedding representations. This causes diverged embeddings of the same word/token in queries and code snippets. We propose a novel context-aware code translation technique that translates code snippets into natural language descriptions (called translations). The code translation is conducted on machine instructions, where the context information is collected by simulating the execution of instructions. We further design a shared word mapping function using one single vocabulary for generating embeddings for both translations and queries. We evaluate the effectiveness of our technique, called TranCS, on the CodeSearchNet corpus with 1,000 queries. Experimental results show that TranCS significantly outperforms state-of-the-art techniques by 49.31% to 66.50% in terms of MRR (mean reciprocal rank).},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {388–400},
numpages = {13},
keywords = {code search, deep learning, code translation},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3482632.3482726,
author = {Wei, Haiyan},
title = {Research on Teaching Mode of Japanese Translation Course Based on Computer Aided Translation},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482726},
doi = {10.1145/3482632.3482726},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {444–448},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3290607.3313046,
author = {Nishimura, Ayano and Itoh, Takayuki},
title = {Interactive Lyric Translation System: Implementation and Experiments},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313046},
doi = {10.1145/3290607.3313046},
abstract = {We can listen to music from many different countries due to the evolution of the Internet. However, understanding lyrics written in foreign languages is still difficult, even though many international songs have been translated. In this paper, we propose an interactive lyric translation system and describe its implementation. Users can modify lyrics by selecting a sample lyric from candidate translations and then freely edit the lyric using the proposed system. The proposed system also allows users to listen to their translation by applying singing voice synthesizer and search for related words. We conducted experiments with 12 participants to compare the lyric translation by the proposed system to manual lyric translation. The translation using the proposed system had better evaluation results comparing to the manual translation.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {user interface, music, lyric translations},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@article{10.1109/TASLP.2022.3196168,
author = {Bu, Suliang and Zhao, Yunxin and Zhao, Tuo and Wang, Shaojun and Han, Mei},
title = {Modeling Speech Structure to Improve T-F Masks for Speech Enhancement and Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3196168},
doi = {10.1109/TASLP.2022.3196168},
abstract = {Time-frequency (TF) masks are widely used in speech enhancement (SE). However, accurately estimating TF masks from noisy speech remains a challenge to both statistical or neural network (NN) approaches. Statistical model based mask estimation usually depends on a good parameter initialization, while NN-based method relies on setting proper and stable learning targets. To address these issues, we propose to extract TF speech structure from clean speech and partition noisy speech spectrogram into mutually exclusive regions. We investigate modeling clean speech by utterance-specific narrowband complex Gaussian mixture models to derive the regions, and using the region targets to supervise the training of UNet++, a high-performance NN, for predicting regions from noisy speech. For multichannel SE, we consider two scenarios of using speech regions: 1) integrating the regions with TF masks by constraining the mask values or the model parameter updates, and 2) using the predicted regions in place of TF masks. For single-channel SE, we consider using the region targets to improve TF mask targets. Furthermore, we propose to use UNet++ for TF mask estimation. Our experiment results on speech recognition (CHiME-3) and SE (CHiME-3 and LibriSpeech) have demonstrated the effectiveness of our proposed approach of modeling speech region structure to improve TF masks for speech recognition and enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2705–2715},
numpages = {11}
}

@inproceedings{10.1145/3297097.3297103,
author = {Sun, Jiabin and Zhang, Weimin and Liu, Zhaohui},
title = {Translation Stiffness Calculation for Serial Robots},
year = {2018},
isbn = {9781450365840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297097.3297103},
doi = {10.1145/3297097.3297103},
abstract = {The advantages of serial robots in flexibility, reach area and cost are considerable, while the application in many industrial fields is limited by the shortage of stiffness. Translation stiffness is a part of Cartesian stiffness, it determines the displacement of robot's end-effector when the torque is negligible. This paper proposes a calculation method of translation stiffness in the effective working space of serial robot based on traditional kinematic model and joint stiffness. With groups of measuring data of robot's end-effector displacements and corresponding external forces in different configurations, a L2-norm minimization function is used to identify joint stiffness of robot, and configurations are chosen efficiently through Design of Experiment(DoE) method. Translation stiffness of other configurations are measured to validate the calculation method.},
booktitle = {Proceedings of the 4th International Conference on Robotics and Artificial Intelligence},
pages = {87–91},
numpages = {5},
keywords = {Design of Experiment, Serial robot, Translation Stiffness},
location = {Guangzhou, China},
series = {ICRAI '18}
}

@inproceedings{10.1145/3242969.3264976,
author = {Sterpu, George},
title = {Large Vocabulary Continuous Audio-Visual Speech Recognition},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3264976},
doi = {10.1145/3242969.3264976},
abstract = {We like to conversate with other people using both sounds and visuals, as our perception of speech is bimodal. Essentially echoing the same speech structure, we manage to integrate the two modalities and often understand the message better than with the eyes closed. In this work we would like to learn more about the visual nature of speech, coined lip-reading, and to make use of it towards better automatic speech recognition systems. Recent developments in the Machine Learning area, together with the release of suitable audio-visual datasets aimed at large vocabulary continuous speech recognition, have led to a renewal of the lip-reading topic, and allow us to address the recurring question of how to better integrate visual and acoustic speech.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {538–541},
numpages = {4},
keywords = {audio-visual fusion, lip-reading, audio-visual speech recognition},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@inproceedings{10.1145/3366030.3366083,
author = {Radzikowski, Kacper and Forc, Mateusz and Wang, Le and Yoshie, Osamu and Nowak, Robert},
title = {Accent Neutralization for Speech Recognition of Non-Native Speakers},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366083},
doi = {10.1145/3366030.3366083},
abstract = {These days, automatic speech recognition (ASR) systems achieve higher and higher accuracy rates. The score drops significantly, in case when the ASR system is being used with a non-native speaker of the language to be recognized. The main reason is specific pronunciation and accent features. A limited volume of labeled non-native speech datasets makes it difficult to train new ASR systems for non-native speakers.In our research, we tried tackling the problem and its influence on the accuracy of ASR systems, using the style transfer methodology. We designed a pipeline for modifying the speech of a non-native speaker, so that it resembles the native speech to a higher extent. Our methodology can be used as a wrapper for any existing ASR system, which reduces the necessity of training new algorithms for non-native speech. The modification can be thus performed before passing the data forward to the speech recognition system itself.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {136–141},
numpages = {6},
keywords = {speech recognition, machine learning, deep learning, non-native speaker, neural network, style transfer},
location = {Munich, Germany},
series = {iiWAS2019}
}

@article{10.1145/3579854,
author = {Chen, Dongwei and Tong, Dong and Yang, Chun and Yi, Jiangfang and Cheng, Xu},
title = {FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3579854},
doi = {10.1145/3579854},
abstract = {Page-based virtual memory relies on TLBs to accelerate the address translation. Nowadays, the gap between application workloads and the capacity of TLB continues to grow, bringing many costly TLB misses and making the TLB a performance bottleneck. Previous studies seek to narrow the gap by exploiting the contiguity of physical pages. One promising solution is to group pages that are both virtually and physically contiguous into a memory range. Recording range translations can greatly increase the TLB reach, but ranges are also hard to index because they have arbitrary bounds. The processor has to compare against all the boundaries to determine which range an address falls in, which restricts the usage of memory ranges. In this article, we propose a tagged-pointer-based scheme, FlexPointer, to solve the range indexing problem. The core insight of FlexPointer is that large memory objects are rare, so we can create memory ranges based on such objects and assign each of them a unique ID. With the range ID integrated into pointers, we can index the range TLB with IDs and greatly simplify its structure. Moreover, because the ID is stored in the unused bits of a pointer and is not manipulated by the address generation, we can shift the range lookup to an earlier stage, working in parallel with the address generation. According to our trace-based simulation results, FlexPointer can reduce nearly all the L1 TLB misses, and page walks for a variety of memory-intensive workloads. Compared with a 4K-page baseline system, FlexPointer shows a 14% performance improvement on average and up to 2.8x speedup in the best case. For other workloads, FlexPointer shows no performance degradation.},
journal = {ACM Trans. Archit. Code Optim.},
month = {mar},
articleno = {30},
numpages = {24},
keywords = {Tagged pointer, TLB reach, address translation}
}

@inproceedings{10.1145/3490099.3511157,
author = {Weisz, Justin D. and Muller, Michael and Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Agarwal, Mayank and Talamadupula, Kartik and Richards, John T.},
title = {Better Together? An Evaluation of AI-Supported Code Translation},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511157},
doi = {10.1145/3490099.3511157},
abstract = {Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {369–391},
numpages = {23},
keywords = {generative AI, Code translation, imperfect AI, human-AI co-creation},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3489088.3489121,
author = {Zilvan, Vicky and Heryana, Ana and Yuliani, Asri Rizki and Krisnandi, Dikdik and Yuwana, R. Sandra and Pardede, Hilman F},
title = {Front-End Based Robust Speech Recognition Methods: A Review},
year = {2022},
isbn = {9781450385244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489088.3489121},
doi = {10.1145/3489088.3489121},
abstract = {Improving the robustness of speech recognition against environmental noise for speech recognition has been a long research problem. Even with many developments in the field, it remains a notable challenge. A plethora of methods have been proposed to deal with these problems. Some of them deal with noise on the signal levels at the front-end of speech recognition while others adapt the model at the back-end to the noisy conditions. In the past, applying various signal processing techniques has been dominant for front-end-based models, while model adaptations are often applied at the back-end. Currently, data-driven based such as machine learning and deep learning have been increasingly more popular approaches. Both supervised and unsupervised methods have been employed for this task. In this study, our main objective is to review various challenges of solving the robustness issues of speech recognition against environmental noise. We focus on front-end-based methods since their improvements are more tractable for deep learning-based speech recognition. Furthermore, we also summarize and describe various techniques for robust speech recognition and the trends for future methods with the emphasis on data-driven-based methods. Finally, we discuss the advantages and disadvantages of these methods and report several important results in the field.},
booktitle = {Proceedings of the 2021 International Conference on Computer, Control, Informatics and Its Applications},
pages = {136–140},
numpages = {5},
keywords = {front-end, speech masking, robust speech recognition, feature enhancement, speech enhancement},
location = {Virtual/online conference, Indonesia},
series = {IC3INA '21}
}

@article{10.1109/TASLP.2019.2935803,
author = {Chai, Li and Du, Jun and Liu, Qing-Feng and Lee, Chin-Hui},
title = {Using Generalized Gaussian Distributions to Improve Regression Error Modeling for Deep Learning-Based Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935803},
doi = {10.1109/TASLP.2019.2935803},
abstract = {From a statistical perspective, the conventional minimum mean squared error MMSE criterion can be considered as the maximum likelihood ML solution under an assumed homoscedastic Gaussian error model. However, in this paper, a statistical analysis reveals the super-Gaussian and heteroscedastic properties of the prediction errors in nonlinear regression deep neural network DNN-based speech enhancement when estimating clean log-power spectral LPS components at DNN outputs with noisy LPS features in DNN input vectors. Accordingly, we propose treating all dimensions of the prediction error vector as statistically independent random variables and model them with generalized Gaussian distributions GGDs. Then, the objective function with the GGD error model is derived according to the ML criterion. Experiments on the TIMIT corpus corrupted by simulated additive noises show consistent improvements of our proposed DNN framework over the conventional DNN framework in terms of various objective quality measures under 14 unseen noise types evaluated and at various signal-to-noise ratio levels. Furthermore, the ML optimization objective with GGD outperforms the conventional MMSE criterion, achieving improved generalization and robustness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {1919–1931},
numpages = {13}
}

@inproceedings{10.1145/3491101.3519611,
author = {Pandey, Laxmi and Arif, Ahmed Sabbir},
title = {Effects of Speaking Rate on Speech and Silent Speech Recognition},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519611},
doi = {10.1145/3491101.3519611},
abstract = {Speaking rate or the speed at which a person speaks is a fundamental user characteristic. This work investigates the rate in which users speak when interacting with speech and silent speech-based methods. Results revealed that native users speak about 8% faster than non-native users, but both groups slow down at comparable rates (34–40%) when interacting with these methods, mostly to increase their accuracy rates. A follow-up experiment confirms that slowing down does improve the accuracy of these methods. Both methods yield the best accuracy rates when speaking at 0.75x of the actual speaking rate. A post-hoc error analysis revealed that speech and silent speech methods and native and non-native speakers are susceptible to different types of errors.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {231},
numpages = {8},
keywords = {recognition, silent speech, Speech, speaking rate},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3240508.3240671,
author = {Wang, Shuo and Guo, Dan and Zhou, Wen-gang and Zha, Zheng-Jun and Wang, Meng},
title = {Connectionist Temporal Fusion for Sign Language Translation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240671},
doi = {10.1145/3240508.3240671},
abstract = {Continuous sign language translation (CSLT) is a weakly supervised problem aiming at translating vision-based videos into natural languages under complicated sign linguistics, where the ordered words in a sentence label have no exact boundary of each sign action in the video. This paper proposes a hybrid deep architecture which consists of a temporal convolution module (TCOV), a bidirectional gated recurrent unit module (BGRU), and a fusion layer module (FL) to address the CSLT problem. TCOV captures short-term temporal transition on adjacent clip features (local pattern), while BGRU keeps the long-term context transition across temporal dimension (global pattern). FL concatenates the feature embedding of TCOV and BGRU to learn their complementary relationship (mutual pattern). Thus we propose a joint connectionist temporal fusion (CTF) mechanism to utilize the merit of each module. The proposed joint CTC loss optimization and deep classification score-based decoding fusion strategy are designed to boost performance. With only once training, our model under the CTC constraints achieves comparable performance to other existing methods with multiple EM iterations. Experiments are tested and verified on a benchmark, i.e. the RWTH-PHOENIX-Weather dataset, which demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1483–1491},
numpages = {9},
keywords = {temporal cov, sign language translation, fusion, ctc, bgru},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3404716.3404717,
author = {Lu, Qidong and Li, Yingying and Qin, Zhiliang and Liu, Xiaowei and Xie, Yun},
title = {Speech Recognition Using EfficientNet},
year = {2020},
isbn = {9781450377485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404716.3404717},
doi = {10.1145/3404716.3404717},
abstract = {Aiming at the problem of speech recognition, this paper trains the latest convolutional network model EfficientNet with better performance to improve its recognition accuracy. To improve the generalization ability of the model, data augmentation operations such as noise addition, speed adjustment, pitch change are performed on the original speech waveforms. Because timedomain waveform can hardly describe the essential characteristics of an audio clip, Mel power spectrogram and its incremental characteristics are utilized as two-dimensional network inputs to accurately display the internal information of the signal. The experimental results show that method proposed in this paper can achieve high word classification accuracy according to the confusion matrix and accuracy variation curve of verification set on Google speech commands dataset.},
booktitle = {Proceedings of the 2020 5th International Conference on Multimedia Systems and Signal Processing},
pages = {64–68},
numpages = {5},
keywords = {Data augmentation, Speech commands dataset, EfficientNet, Mel power spectrogram, Speech recognition},
location = {Chengdu, China},
series = {ICMSSP '20}
}

@inproceedings{10.5555/3382225.3382239,
author = {Albadi, Nuha and Kurdi, Maram and Mishra, Shivakant},
title = {Are They Our Brothers? Analysis and Detection of Religious Hate Speech in the Arabic Twittersphere},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Religious hate speech in the Arabic Twittersphere is a notable problem that requires developing automated tools to detect messages that use inflammatory sectarian language to promote hatred and violence against people on the basis of religious affiliation. Distinguishing hate speech from other profane and vulgar language is quite a challenging task that requires deep linguistic analysis. The richness of the Arabic morphology and the limited available resources for the Arabic language make this task even more challenging. To the best of our knowledge, this paper is the first to address the problem of identifying speech promoting religious hatred in the Arabic Twitter. In this work, we describe how we created the first publicly available Arabic dataset annotated for the task of religious hate speech detection and the first Arabic lexicon consisting of terms commonly found in religious discussions along with scores representing their polarity and strength. We then developed various classification models using lexicon-based, n-gram-based, and deep-learning-based approaches. A detailed comparison of the performance of different models on a completely new unseen dataset is then presented. We find that a simple Recurrent Neural Network (RNN) architecture with Gated Recurrent Units (GRU) and pre-trained word embeddings can adequately detect religious hate speech with 0.84 Area Under the Receiver Operating Characteristic curve (AUROC).},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {69–76},
numpages = {8},
keywords = {religious hate speech, social media mining, text analytics, Arabic NLP, Twitter, cyberhate, online radicalization},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1145/3532862,
author = {Cui, Jinhua and Shinde, Shweta and Sen, Satyaki and Saxena, Prateek and Yuan, Pinghai},
title = {Dynamic Binary Translation for SGX Enclaves},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3532862},
doi = {10.1145/3532862},
abstract = {Enclaves, such as those enabled by Intel SGX, offer a hardware primitive for shielding user-level applications from the OS. While enclaves are a useful starting point, code running in the enclave requires additional checks whenever control or data is transferred to/from the untrusted OS. The enclave-OS interface on SGX, however, can be extremely large if we wish to run existing unmodified binaries inside enclaves. This article presents Ratel, a dynamic binary translation engine running inside SGX enclaves on Linux. Ratel offers complete interposition, the ability to interpose on all executed instructions in the enclave and monitor all interactions with the OS. Instruction-level interposition offers a general foundation for implementing a large variety of inline security monitors in thefuture.We take a principled approach in explaining why complete interposition on SGX is challenging. We draw attention to five design decisions in SGX that create fundamental trade-offs between performance and ensuring complete interposition, and we explain how to resolve them in the favor of complete interposition. To illustrate the utility of the Ratel framework, we present the first attempt to offer binary compatibility with existing software on SGX. We report that Ratel offers binary compatibility with over 200 programs we tested, including micro-benchmarks and real applications, such as Linux shell utilities. Runtimes for two programming languages, namely, Python and R, tested with standard benchmarks work out-of-the-box on Ratel without any specialized handling.},
journal = {ACM Trans. Priv. Secur.},
month = {jul},
articleno = {32},
numpages = {40},
keywords = {instrumentation, complete interposition, TEEs, compatibility, lift and shift, SGX design restrictions, trusted computing, dynamorio, porting, Trusted execution environments, enclaves, dynamic binary translation}
}

@article{10.1109/TASLP.2020.3039930,
author = {Zhang, Jie and Chen, Huawei and Dai, Li-Rong and Hendriks, Richard Christian},
title = {A Study on Reference Microphone Selection for Multi-Microphone Speech Enhancement},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3039930},
doi = {10.1109/TASLP.2020.3039930},
abstract = {Multi-microphone speech enhancement methods typically require a reference position with respect to which the target signal is estimated. Often, this reference position is arbitrarily chosen as one of the reference microphones. However, it has been shown that the choice of the reference microphone can have a significant impact on the final noise reduction performance. In this paper, we therefore theoretically analyze the impact of selecting a reference on the noise reduction performance with near-end noise being taken into account. Following the generalized eigenvalue decomposition (GEVD) based optimal variable span filtering framework, we find that for any linear beamformer, the output signal-to-noise ratio (SNR) taking both the near-end and far-end noise into account is reference dependent. Only when the near-end noise is neglected, the output SNR of rank-1 beamformers does not depend on the reference position. However, in general for rank-<inline-formula><tex-math notation="LaTeX">$r$</tex-math></inline-formula> beamformers with <inline-formula><tex-math notation="LaTeX">$r&gt;1$</tex-math></inline-formula> (e.g., the multichannel Wiener filter) the performance does depend on the reference position. Based on these, we propose an optimal algorithm for microphone reference selection that maximizes the output SNR. In addition, we propose a lower-complexity algorithm that is still optimal for rank-1 beamformers, but sub-optimal for the general <inline-formula><tex-math notation="LaTeX">$r&gt;1$</tex-math></inline-formula> rank beamformers. Experiments using a simulated microphone array validate the effectiveness of both proposed methods and show that in terms of quality, several dB can be gained by selecting the proper reference microphone.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {671–683},
numpages = {13}
}

@article{10.1109/TASLP.2020.2966869,
author = {Dietzen, Thomas and Doclo, Simon and Moonen, Marc and van Waterschoot, Toon},
title = {Integrated Sidelobe Cancellation and Linear Prediction Kalman Filter for Joint Multi-Microphone Speech Dereverberation, Interfering Speech Cancellation, and Noise Reduction},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2966869},
doi = {10.1109/TASLP.2020.2966869},
abstract = {In multi-microphone speech enhancement, reverberation as well as additive noise and/or interfering speech are commonly suppressed by deconvolution and spatial filtering, e.g., using multi-channel linear prediction (MCLP) on the one hand and beamforming, e.g., a generalized sidelobe canceler (GSC), on the other hand. In this article, we consider several reverberant speech components, whereof some are to be dereverberated and others to be canceled, as well as a diffuse (e.g., babble) noise component to be suppressed. In order to perform both deconvolution and spatial filtering, we integrate MCLP and the GSC into a novel architecture referred to as integrated sidelobe cancellation and linear prediction (ISCLP), where the sidelobe-cancellation (SC) filter and the linear prediction (LP) filter operate in parallel, but on different microphone signal frames. Within ISCLP, we estimate both filters jointly by means of a single Kalman filter. We further propose a spectral Wiener gain post-processor, which is shown to relate to the Kalman filter's posterior state estimate. The presented ISCLP Kalman filter is benchmarked against two state-of-the-art approaches, namely first a pair of alternating Kalman filters respectively performing dereverberation and noise reduction, and second an MCLP+GSC Kalman filter cascade. While the ISCLP Kalman filter is roughly <inline-formula><tex-math notation="LaTeX">$M^2$</tex-math></inline-formula> times less expensive than both reference algorithms, where <inline-formula><tex-math notation="LaTeX">$M$</tex-math></inline-formula> denotes the number of microphones, it is shown to perform at least similarly as compared to the former, and to outperform the latter. A MATLAB implementation is available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {740–754},
numpages = {15}
}

@inproceedings{10.1145/3476098.3485050,
author = {Chhabra, Sachin and Venkateswara, Hemanth and Li, Baoxin},
title = {Iterative Image Translation for Unsupervised Domain Adaptation},
year = {2021},
isbn = {9781450386814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3476098.3485050},
doi = {10.1145/3476098.3485050},
abstract = {In this paper, we propose an image-translation-based unsupervised domain adaptation approach that iteratively trains an image translation and a classification network using each other. In Phase A, a classification network is used to guide the image translation to preserve the content and generate images. In Phase B, the generated images are used to train the classification network. With each step, the classification network and generator improve each other to learn the target domain representation. Detailed analysis and the experiments are testimony of the strength of our approach.},
booktitle = {Multimedia Understanding with Less Labeling on Multimedia Understanding with Less Labeling},
pages = {37–44},
numpages = {8},
keywords = {generative domain adaptation, unsupervised domain adaptation, image translation, source-like target, ternary feature alignment},
location = {Virtual Event, China},
series = {MULL'21}
}

@inproceedings{10.1145/3240508.3240704,
author = {Tang, Hao and Wang, Wei and Xu, Dan and Yan, Yan and Sebe, Nicu},
title = {GestureGAN for Hand Gesture-to-Gesture Translation in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240704},
doi = {10.1145/3240508.3240704},
abstract = {Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture. To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (GestureGAN). GestureGAN consists of a single generator G and a discriminator D, which takes as input a conditional hand image and a target hand skeleton image. GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycle-consistency loss. The proposed color loss handles the issue of "channel pollution" while back-propagating the gradients. In addition, we present the Frechet ResNet Distance (FRD) to evaluate the quality of generated images. Extensive experiments on two widely used benchmark datasets demonstrate that the proposed GestureGAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. Our model and code are available at https://github.com/Ha0Tang/GestureGAN.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {774–782},
numpages = {9},
keywords = {hand gesture, image translation, generative adversarial networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1145/3583067,
author = {Govers, Jarod and Feldman, Philip and Dant, Aaron and Patros, Panos},
title = {Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583067},
doi = {10.1145/3583067},
abstract = {Social media is a modern person’s digital voice to project and engage with new ideas and mobilise communities—a power shared with extremists. Given the societal risks of unvetted content-moderating algorithms for Extremism, Radicalisation, and Hate speech (ERH) detection, responsible software engineering must understand the who, what, when, where, and why such models are necessary to protect user safety and free expression. Hence, we propose and examine the unique research field of ERH context mining to unify disjoint studies. Specifically, we evaluate the start-to-finish design process from socio-technical definition-building and dataset collection strategies to technical algorithm design and performance. Our 2015–2021 51-study Systematic Literature Review (SLR) provides the first cross-examination of textual, network, and visual approaches to detecting extremist affiliation, hateful content, and radicalisation towards groups and movements. We identify consensus-driven ERH definitions and propose solutions to existing ideological and geographic biases, particularly due to the lack of research in Oceania/Australasia. Our hybridised investigation on Natural Language Processing, Community Detection, and visual-text models demonstrates the dominating performance of textual transformer-based algorithms. We conclude with vital recommendations for ERH context mining researchers and propose an uptake roadmap with guidelines for researchers, industries, and governments to enable a safer cyberspace.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {319},
numpages = {35},
keywords = {radicalisation, sociolinguistics, community detection, Natural Language Processing, hate speech, Extremism, neural networks, machine learning}
}

@article{10.1109/TASLP.2018.2882738,
author = {Gelderblom, Femke B. and Tronstad, Tron V. and Viggen, Erlend Magnus},
title = {Subjective Evaluation of a Noise-Reduced Training Target for Deep Neural Network-Based Speech Enhancement},
year = {2019},
issue_date = {March 2019},
publisher = {IEEE Press},
volume = {27},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2882738},
doi = {10.1109/TASLP.2018.2882738},
abstract = {Speech enhancement systems aim to improve the quality and intelligibility of noisy speech. In this study, we compare two speech enhancement systems based on deep neural networks. The speech intelligibility and quality of both systems were evaluated subjectively by a speech recognition test based on Hagerman sentences and a translation of the ITU-T P.835 recommendation, respectively. Results were compared with the objective measures STOI and POLQA. Neither STOI nor POLQA reliably predicted subjective results. While STOI anticipated improvement, subjective results for both models showed degradation of speech intelligibility. POLQA results were overall hardly affected, while the subjective results showed significant changes in overall quality, both positive and negative, in many of the tests. One of the systems was trained to remove all noise; a strategy that is common in speech enhancement systems found in the literature. The other system was trained to only reduce the noise such that the signal-to-noise ratio increased with 10&nbsp;dB. The latter system subjectively outperformed the system that attempted to remove noise completely. From this, we conclude that objective evaluation cannot replace subjective evaluation until a measure that reliably predicts intelligibility and quality for deep-neural-network-based systems has been identified. Results further indicate that it may be beneficial to move away from more aggressive noise removal strategies toward noise reduction strategies that cause less speech distortion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {583–594},
numpages = {12}
}

@inproceedings{10.1145/3517031.3532524,
author = {Lu, Conny and Zhang, Qian and Krishnakumar, Kapil and Chen, Jixu and Fuchs, Henry and Talathi, Sachin and Liu, Kun},
title = {Geometry-Aware Eye Image-To-Image Translation},
year = {2022},
isbn = {9781450392525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517031.3532524},
doi = {10.1145/3517031.3532524},
abstract = {Recently, image-to-image translation (I2I) has met with great success in computer vision, but few works have paid attention to the geometric changes that occur during translation. The geometric changes are necessary to reduce the geometric gap between domains at the cost of breaking correspondence between translated images and original ground truth. We propose a novel geometry-aware semi-supervised method to preserve this correspondence while still allowing geometric changes. The proposed method takes a synthetic image-mask pair as input and produces a corresponding real pair. We also utilize an objective function to ensure consistent geometric movement of the image and mask through the translation. Extensive experiments illustrate that our method yields a 11.23% higher mean Intersection-Over-Union than the current methods on the downstream eye segmentation task. The generated image has a 15.9% decrease in Frechet Inception Distance indicating higher image quality.},
booktitle = {2022 Symposium on Eye Tracking Research and Applications},
articleno = {69},
numpages = {7},
keywords = {geometry consistency, image-to-image translation, eye segmentation, syn2real, eye tracking},
location = {Seattle, WA, USA},
series = {ETRA '22}
}

@inproceedings{10.1145/3456126.3456129,
author = {Geng, Jianing and Zhu, Hao and Li, Xiang-Yang},
title = {SeeSpeech: See Emotions in The Speech},
year = {2021},
isbn = {9781450389082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456126.3456129},
doi = {10.1145/3456126.3456129},
abstract = {At present, the understanding of speech by machines mostly focuses on the understanding of semantics, but speech should also include emotions in the speech. Emotion can not only strengthen semantics, but can even change semantic information. The paper discusses how to realize the emotion classification, which is called SeeSpeech. SeeSpeech chooses MCEP as the speech emotion feature, and inputs it into CNN and Transformer respectively. In order to obtain richer features, CNN uses batch normalization, while Transformer uses layer normalization, and then combines the output of CNN and Transformer. Finally, the type of emotion is obtained through SoftMax. SeeSpeech obtained the highest classification accuracy rate of 97% on the RAVDESS data set, and also obtained the classification accuracy rate of 85% on the actual edge gateway test. It can be seen from the results that SeeSpeech has encouraging performance in speech emotion classification and has a wide range of application prospects in human-computer interaction.},
booktitle = {2021 2nd Asia Service Sciences and Software Engineering Conference},
pages = {116–122},
numpages = {7},
keywords = {Deep learning, Emothion classification, Emotions in speech},
location = {Macau, Macao},
series = {ASSE '21}
}

@article{10.1145/3605549,
author = {Do, Duc-Hao and Chau, Thanh-Duc and Tran, Thai-Son},
title = {Speech Feature Enhancement Based on Time-Frequency Analysis},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3605549},
doi = {10.1145/3605549},
abstract = {Time-frequency analysis (TFA) is a powerful method to exploit the hidden information of signals, including speech signals. Many techniques in this group were invented and developed to capture the most crucial stationary feature. However, human speech is not stable, and it contains some non-stationary elements. This work aims to design a new algorithm via the TFA technique to extract the trends and changes inside the speech signal in the time-frequency (TF) plane. We design a new algorithm to create a set of atoms for the signal transform, which can analyze the signal in many different view directions via Poly-Linear Chirplet Transform (PLCT). After processing the signal, the proposed method returns a multichannel output in which each channel results from a particular Linear Chirplet Transform (LCT). The feature then is combined with the MFCC feature to form the final representation. Although the size for speech representation rises, our extracted feature contains rich-meaning information to improve the recognition results compared to other features in gender recognition, dialect recognition, and speaker recognition.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {219},
numpages = {14},
keywords = {poly-linear chirplet transform, multichannel representation, time-frequency analysis, instantaneous frequency, Chirplet Transform, Speech feature}
}

@inproceedings{10.1145/3366423.3380252,
author = {Harting, Tom and Mesbah, Sepideh and Lofi, Christoph},
title = {LOREM: Language-Consistent Open Relation Extraction from Unstructured Text},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380252},
doi = {10.1145/3366423.3380252},
abstract = {We introduce a Language-consistent multi-lingual Open Relation Extraction Model (LOREM) for finding relation tuples of any type between entities in unstructured texts. LOREM does not rely on language-specific knowledge or external NLP tools such as translators or PoS-taggers, and exploits information and structures that are consistent over different languages. This allows our model to be easily extended with only limited training efforts to new languages, but also provides a boost to performance for a given single language. An extensive evaluation performed on 5 languages shows that LOREM outperforms state-of-the-art mono-lingual and cross-lingual open relation extractors. Moreover, experiments on languages with no or only little training data indicate that LOREM generalizes to other languages than the languages that it is trained on.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1830–1838},
numpages = {9},
keywords = {multi-lingual relation extraction, open domain relation extraction, text mining},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3337821.3337844,
author = {Lin, Chih-Min and Fu, Sheng-Yu and Hong, Ding-Yong and Liu, Yu-Ping and Wu, Jan-Jan and Hsu, Wei-Chung},
title = {Exploiting Vector Processing in Dynamic Binary Translation},
year = {2019},
isbn = {9781450362955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337821.3337844},
doi = {10.1145/3337821.3337844},
abstract = {Auto vectorization techniques have been adopted by compilers to exploit data-level parallelism in parallel processing for decades. However, since processor architectures have kept enhancing with new features to improve vector/SIMD performance, legacy application binaries failed to fully exploit new vector/SIMD capabilities in modern architectures. For example, legacy ARMv7 binaries cannot benefit from ARMv8 SIMD double precision capability, and legacy x86 binaries cannot enjoy the power of AVX-512 extensions.In this paper, we study the fundamental issues involved in cross-ISA Dynamic Binary Translation (DBT) to convert non-vectorized loops to vector/SIMD forms to achieve greater computation throughput available in newer processor architectures. The key idea is to recover critical loop information from those application binaries in order to carry out vectorization at runtime. Experiment results show that our approach achieves an average speedup of 1.42x compared to ARMv7 native run across various benchmarks in an ARMv7-to-ARMv8 dynamic binary translation system.},
booktitle = {Proceedings of the 48th International Conference on Parallel Processing},
articleno = {93},
numpages = {10},
keywords = {SIMD/Vector, Dynamic Binary Translation, Auto Vectorization, Virtual Register Promotion},
location = {Kyoto, Japan},
series = {ICPP '19}
}

@inproceedings{10.1145/3456887.3457512,
author = {Fang, Qiong},
title = {A Study of English Translation Teaching from the Perspective of Ecology},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457512},
doi = {10.1145/3456887.3457512},
abstract = {This paper conducted a research into English translation teaching at China's universities from the perspective of ecology. In the ecological environment of translation teaching, various ecological factors interact with each other to reach a dynamic equilibrium. This study finds out that the teacher of translation plays a role of restrictive factor in the ecological environment of translation teaching, whose working competence, teaching methodology, view of life, value and so on all have impact on their teaching efficiency. Through the intervention of the limiting factor, the teacher, the comprehensive performance of the translation teaching ecological system can be optimized and this will be helpful for the realization of the final objective of English translation class at universities, namely to develop students’ competence of English-Chinese translation and Chinese-English translation.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {1301–1304},
numpages = {4},
keywords = {Perspective of Ecology, Translation Teaching, English translation},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@inproceedings{10.1145/3319008.3319358,
author = {Badampudi, Deepika and Wohlin, Claes and Gorschek, Tony},
title = {Contextualizing Research Evidence through Knowledge Translation in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319358},
doi = {10.1145/3319008.3319358},
abstract = {Usage of software engineering research in industrial practice is a well-known challenge. Synthesis of knowledge from multiple research studies is needed to provide evidence-based decision-support for industry. The objective of this paper is to present a vision of how a knowledge translation framework may look like in software engineering research, in particular how to translate research evidence into practice by combining contextualized expert opinions with research evidence. We adopted the framework of knowledge translation from health care research, adapted and combined it with a Bayesian synthesis method. The framework provided in this paper includes a description of each step of knowledge translation in software engineering. Knowledge translation using Bayesian synthesis intends to provide a systematic approach towards contextualized, collaborative and consensus-driven application of research results. In conclusion, this paper contributes towards the application of knowledge translation in software engineering through the presented framework.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {306–311},
numpages = {6},
keywords = {Bayesian synthesis, Knowledge translation, Decision-making},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1109/TASLP.2020.3025638,
author = {Yu, Cheng and Zezario, Ryandhimas E. and Wang, Syu-Siang and Sherman, Jonathan and Hsieh, Yi-Yen and Lu, Xugang and Wang, Hsin-Min and Tsao, Yu},
title = {Speech Enhancement Based on Denoising Autoencoder With Multi-Branched Encoders},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3025638},
doi = {10.1109/TASLP.2020.3025638},
abstract = {Deep learning-based models have greatly advanced the performance of speech enhancement (SE) systems. However, two problems remain unsolved, which are closely related to model generalizability to noisy conditions: (1) mismatched noisy condition during testing, i.e., the performance is generally sub-optimal when models are tested with unseen noise types that are not involved in the training data; (2) local focus on specific noisy conditions, i.e., models trained using multiple types of noises cannot optimally remove a specific noise type even though the noise type has been involved in the training data. These problems are common in real applications. In this article, we propose a novel denoising autoencoder with a multi-branched encoder (termed DAEME) model to deal with these two problems. In the DAEME model, two stages are involved: training and testing. In the training stage, we build multiple component models to form a multi-branched encoder based on a decision tree (DSDT). The DSDT is built based on prior knowledge of speech and noisy conditions (the speaker, environment, and signal factors are considered in this paper), where each component of the multi-branched encoder performs a particular mapping from noisy to clean speech along the branch in the DSDT. Finally, a decoder is trained on top of the multi-branched encoder. In the testing stage, noisy speech is first processed by each component model. The multiple outputs from these models are then integrated into the decoder to determine the final enhanced speech. Experimental results show that DAEME is superior to several baseline models in terms of objective evaluation metrics, automatic speech recognition results, and quality in subjective human listening tests.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2756–2769},
numpages = {14}
}

@inproceedings{10.1145/3453483.3454030,
author = {Lopes, Nuno P. and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},
title = {Alive2: Bounded Translation Validation for LLVM},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454030},
doi = {10.1145/3453483.3454030},
abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {65–79},
numpages = {15},
keywords = {Translation Validation, IR Semantics, Automatic Software Verification, Compilers},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3448734.3450837,
author = {Cui, Jianzhou},
title = {Tourism English Translation System Based on Fuzzy Clustering Algorithm},
year = {2021},
isbn = {9781450389570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448734.3450837},
doi = {10.1145/3448734.3450837},
abstract = {With the rapid development of economic globalization and the Internet, international exchanges and cooperation have become increasingly extensive and in-depth. Language differences have become the biggest obstacle to international communication and cooperation. The main research of this paper is the tourism English translation system based on fuzzy clustering algorithm. This system uses black box testing to test and verify system functions. Performance indicators mainly include response time, throughput, and so on. Complete the performance test of the system by checking the monitoring points in the performance test cases. To confirm whether the basic performance requirements of the system are met, performance testing is very important. In this teaching system, the response time and throughput of the system are mainly tested. In the system performance test cases, the use cases are divided according to the number of online users, and the response time of the client is tested in each use case. If the indicator requirements are met, the use case is deemed qualified, otherwise it is unqualified, and the system defects are recorded according to the actual test structure. The data shows that the correct clustering data of C-FCA is further improved to 95% based on the better PCM algorithm. The results show that the fuzzy clustering algorithm can effectively improve the accuracy of the tourism English translation system.},
booktitle = {The 2nd International Conference on Computing and Data Science},
articleno = {107},
numpages = {4},
keywords = {Cluster Analysis, Tourism English Translation System, Fuzzy Clustering Algorithm, Structural Transformation},
location = {Stanford, CA, USA},
series = {CONF-CDS 2021}
}

@article{10.1109/TASLP.2021.3069193,
author = {Sato, Ryotaro and Niwa, Kenta and Kobayashi, Kazunori},
title = {Ambisonic Signal Processing DNNs Guaranteeing Rotation, Scale and Time Translation Equivariance},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3069193},
doi = {10.1109/TASLP.2021.3069193},
abstract = {We propose a novel framework to design Ambisonic signal processing deep neural networks (DNNs) that guarantee physical symmetries. In general, spatial acoustic signal processing DNNs for, e.g., sound event detection, ought to perform with the equivalent accuracy regardless of the directions of arrival of sound sources. This property is well known as rotation symmetry in natural science. However, in most conventional multichannel signal processing DNNs, rotation symmetry has not been explicitly incorporated into the model structure, and pseudo rotation symmetry has been acquired by training models with a large amount of signal datasets arriving from various directions. Therefore, the conventional methods will not perform sufficiently when the training dataset is relatively small scale or statistically biased, e.g., the distribution of the arriving directions of the sound events is inhomogeneous. Furthermore, in order to efficiently handle acoustic signals in DNNs, it is necessary to consider several additional symmetries, such as amplitude scaling and time translation of the signals. In this paper, we integratedly formulate these symmetry assumptions, which are called equivariance, in the form of constraints for our targeted DNN design. We propose a new DNN design method called Clebsch-Gordan Nets with Scale and Time translation Symmetry (CGNets-STS), which guarantees to simultaneously satisfy three types of equivariance (3D rotation, amplitude scaling, and time translation). As an instance of this method, we design a DNN model for sound event localization and detection tasks from Ambisonic signals. Experimental results show that this model is highly robust against spatial rotations for input data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1449–1462},
numpages = {14}
}

@inproceedings{10.1145/3539637.3556892,
author = {Kobellarz, Jordan K. and Silva, Thiago H.},
title = {Should We Translate? Evaluating Toxicity in Online Comments When Translating from Portuguese to English},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539637.3556892},
doi = {10.1145/3539637.3556892},
abstract = {Social media and online discussion platforms suffer from the prevalence of uncivil behavior, such as harassment and abuse, seeking to curb toxic comments. There are several approaches to classifying toxic comments automatically. Some of them have more resources and are more advanced in English, thus, stimulating the task of translating the text from a specific language to English. While researchers have shown evidence that this practice is indicated for certain tasks, such as sentiment analysis, little is known in the context of toxicity identification. In this research, we assess the performance of a freely available model for toxic language detection in online comments called Perspective API, widely adopted by some famous news media sites to identify different toxicity classes in online comments. For that, we obtained comments in Portuguese from two Brazilian news media websites during a politically polarized situation as a use case. Then, this dataset was translated to English and compared to four baseline datasets, two composed of highly toxic comments, one in Portuguese and other in English, and two composed of neutral comments, also one in Portuguese and other in English – all of them in its original language, not translated. Finally, human-annotated comments from the news comments dataset were analyzed to assess the scores provided by the Perspective API for the original and the translated versions. Results indicate that keeping the texts in their original language is preferable, even in comparing different languages. Nevertheless, if the translated version is strictly necessary, ways of dealing with the situation were suggested to preserve as much information as possible from the original version.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {89–98},
numpages = {10},
keywords = {online comments, natural language, translation, Perspective API, toxicity},
location = {Curitiba, Brazil},
series = {WebMedia '22}
}

@article{10.1109/TASLP.2020.2980372,
author = {Pan, Jia and Wan, Genshun and Du, Jun and Ye, Zhongfu},
title = {Online Speaker Adaptation Using Memory-Aware Networks for Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980372},
doi = {10.1109/TASLP.2020.2980372},
abstract = {In our previous work, we introduced our attention-based speaker adaptation method, which has been proved to be an efficient online speaker adaptation method for real-time speech recognition. In this paper, we present a more complete framework of this method named memory-aware networks, which consists of the main network, the memory module, the attention module and the connection module. A gate mechanism and a multiple-connections strategy are presented to connect the memory with the main network in order to take full advantage of the memory. An auxiliary speaker classification task is provided to improve the accuracy of the attention module. The fixed-size ordinally forgetting encoding method is used together with average pooling to gather both short-term and long-term information. Furthermore, instead of only using traditional speaker embeddings such as i-vectors or d-vectors as the memory, we design a new form of memory called residual vectors, which can represent different pronunciation habits. Experiments on both the Switchboard and AISHELL-2 tasks show that our method can perform online speaker adaptation very well with no additional adaptation data and with only a relative 3% increase in decoding computation complexity. Under the cross-entropy criterion, our method achieves a relative word error rate reduction of 9.4% and 8.3% compared to that of the speaker-independent model on the Switchboard task and the AISHELL-2 task, respectively, and approximately 7.0% compared to that of the traditional d-vector-based speaker adaptation method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1025–1037},
numpages = {13}
}

@inproceedings{10.1145/3426826.3426838,
author = {Nair, Shruti and Mohan, Madhumita and Rajesh, Jemima and Chandran, Priya},
title = {On Finding the Best Learning Model for Assessing Confidence in Speech},
year = {2020},
isbn = {9781450388344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426826.3426838},
doi = {10.1145/3426826.3426838},
abstract = {The human mind is naturally conditioned to assess the confidence of another speaker. Hence, confidence while speaking is crucial for success across most domains and situations. Confidence in speech is a highly useful trait to have when engaged in interactions and discussions. In the right amounts, it can often sound pleasant or reassuring to the listener. For a person striving to achieve a note of confidence in his/her voice, finding a human evaluator to give relevant feedback on the tone and voice is not always possible. Given the growing power of neural networks and other machine learning tools today, a machine could potentially serve as an evaluator for assessing the confidence in the user's speech, and provide scores as feedback for the user's improvement. In this paper, we present the descriptions, results and analysis of our experiments in predicting the confidence of a speaker using machine learning and audio processing tools. The project involved the building and scoring of an unbiased dataset of audio recordings based on the confidence of the speaker. The audio clips were recorded by the peers in the campus and graded based on clarity, modulation, pace, and volume. Three models were trained and tested on the built dataset: a multilayer perceptron (MLP) neural network, a support vector machine (SVM) and a convolutional neural network (CNN) to predict the confidence of a speaker. Our results show that convolutional neural networks produce scores with the highest accuracy, 86.3%, where accuracy is measured with respect to the closeness to the scores awarded by human assessment.},
booktitle = {Proceedings of the 2020 3rd International Conference on Machine Learning and Machine Intelligence},
pages = {58–64},
numpages = {7},
keywords = {confidence, convolutional neural networks, neural networks, speech, machine learning, multilayer perceptron, support vector machines},
location = {Hangzhou, China},
series = {MLMI '20}
}

@inproceedings{10.1145/3456887.3457020,
author = {Cui, Shu},
title = {Aesthetic Characteristics and Artistic Value of English Literature Translation in Multimodal Environment},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457020},
doi = {10.1145/3456887.3457020},
abstract = {In the multimodal environment, the public has a higher understanding of different cultures, and the aesthetic level and evaluation standard of English literature translation are also improved. English literature translation has a more obvious impact on the development and richness of modern culture and the renewal of social and cultural diversity. The focus of this paper is to analyze the aesthetic characteristics and artistic value of English literature translation in multimodal environment. By analyzing the relationship between the formation of multimodal environment and interactive platform of modern Internet, we can understand the adaptive characteristics of English literature translation and multimodal environment, and analyze the aesthetic characteristics and artistic value of English literature translation.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {587–590},
numpages = {4},
keywords = {English literature translation, multimodal environment, literary art, translation aesthetics},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@inproceedings{10.1145/3417313.3429385,
author = {Dey, Swarnava and Dutta, Jeet},
title = {A Low Footprint Automatic Speech Recognition System For Resource Constrained Edge Devices},
year = {2020},
isbn = {9781450381345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417313.3429385},
doi = {10.1145/3417313.3429385},
abstract = {Deep Learning (DL) has been instrumental in pushing artificial intelligence (AI)/ machine learning (ML) algorithms to edge of the network. It allows building AI/ML algorithms for computer vision, speech processing, and other timeseries analytics tasks with limited domain knowledge. As there is no mechanism to control the representations learned from a large dataset, it becomes hard to predict whether a very small DL model can learn the proper dependencies needed for a particular problem at hand.With speech recognition capability becoming important in several Internet of Things (IoT) devices, we propose an explainable AI-based methodology to build small DL models for speech recognition by controlling the representations learned by a model under a hard size constraint.We enhance the architecture of a state of the art sequence transduction model to allow the tuning of accuracy vs. model size trade-off. Using these techniques we achieve a reduction in model size and latency by a factor of 10 and 6 respectively, with only 4loss compared to the embedded implementation of a well known ASR.},
booktitle = {Proceedings of the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things},
pages = {48–54},
numpages = {7},
keywords = {model reduction, speech recognition, interpretable ai, streaming ASR, explainable ai, streaming dataset, data augmentation, RNN-T},
location = {Virtual Event, Japan},
series = {AIChallengeIoT '20}
}

@inproceedings{10.1145/3529466.3529503,
author = {Yin, Qifang and Tao, Wenqi and Liu, Xiaolong and Hong, Yusheng},
title = {Neural Sign Language Translation with SF-Transformer},
year = {2022},
isbn = {9781450395502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529466.3529503},
doi = {10.1145/3529466.3529503},
abstract = {The popular methods are based on the combination of CNNs and RNNs in the sign language translation. Recently, Transformer has also attracted the attention of researchers and achieved success in this subject. However, researchers usually only focus on the accuracy of their model, while ignoring the practical application value. In this paper, we propose the SF-Transformer, a lightweight model based on Encoder-Decoder architecture for sign language translation, which achieves new state-of-the-art performance on Chinese Sign Language (CSL) dataset. We used 2D/3D convolution blocks of SF-Net and Transformer's Decoders to build our network. Benefiting from fewer parameters and a high level of parallelization, the training and inference speed of our model is faster. We hope that our method can contribute to the practical application of sign language translation on low-computing devices such as mobile phones.},
booktitle = {Proceedings of the 2022 6th International Conference on Innovation in Artificial Intelligence},
pages = {64–68},
numpages = {5},
keywords = {Transformer Decoder, 3D Convolutional Neural Network, Sign Language Recognition and Translation},
location = {Guangzhou, China},
series = {ICIAI '22}
}

@article{10.1145/3464424,
author = {Alam, Mehreen and Hussain, Sibt Ul},
title = {Roman-Urdu-Parl: Roman-Urdu and Urdu Parallel Corpus for Urdu Language Understanding},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3464424},
doi = {10.1145/3464424},
abstract = {Availability of corpora is a basic requirement for conducting research in a particular language. Unfortunately, for a morphologically rich language like Urdu, despite being used by over a 100 million people around the globe, the dearth of corpora is a major reason for the lack of attention and advancement in research. To this end, we present the first-ever large-scale publicly available Roman-Urdu parallel corpus, Roman-Urdu-Parl, with 6.37 million sentence-pairs. It is a huge corpus collected from diverse sources, annotated using crowd-sourcing techniques, and also assured for quality. It has a total of 92.76 million Roman-Urdu words, 92.85 million Urdu words, Roman-Urdu vocabulary of 42.9 K words, and Urdu vocabulary of 43.8 K words. Roman-Urdu-Parl has been built to ensure that it not only captures the morphological and linguistic features of the language but also the heterogeneity and variations arising due to demographic conditions. We validate the authenticity and quality of our corpus by using it to address two natural language processing research problems, that is, on learning word embeddings and building a machine transliteration system. Our contribution of the corpus leads to exceptional results in both settings, for example, our machine transliteration system sets a new state-of-the-art with a Bilingual Evaluation Understudy (BLEU) score of 84.67. We believe that Roman-Urdu-Parl can serve as fuel for igniting and advancing works in many research areas related to the Urdu language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {13},
numpages = {20},
keywords = {Roman-Urdu to Urdu transliteration, neural machine translation, machine transliteration, deep learning}
}

@inproceedings{10.1145/3240508.3240708,
author = {Wei, Xingxing and Zhu, Jun and Feng, Sitong and Su, Hang},
title = {Video-to-Video Translation with Global Temporal Consistency},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240708},
doi = {10.1145/3240508.3240708},
abstract = {Although image-to-image translation has been widely studied, the video-to-video translation is rarely mentioned. In this paper, we propose an unified video-to-video translation framework to accom- plish different tasks, like video super-resolution, video colouriza- tion, and video segmentation, etc. A consequent question within video-to-video translation lies in the flickering appearance along with the varying frames. To overcome this issue, a usual method is to incorporate the temporal loss between adjacent frames in the optimization, which is a kind of local frame-wise temporal con- sistency. We instead present a residual error based mechanism to ensure the video-level consistency of the same location in different frames (called (lobal temporal consistency). The global and local consistency are simultaneously integrated into our video-to-video framework to achieve more stable videos. Our method is based on the GAN framework, where we present a two-channel discrimina- tor. One channel is to encode the video RGB space, and another is to encode the residual error of the video as a whole to meet the global consistency. Extensive experiments conducted on different video- to-video translation tasks verify the effectiveness and flexibleness of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {18–25},
numpages = {8},
keywords = {video-to-video translation, temporal consistency, generative adversarial network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3460112.3471954,
author = {Santy, Sebastin and Bali, Kalika and Choudhury, Monojit and Dandapat, Sandipan and Ganu, Tanuja and Shukla, Anurag and Shah, Jahanvi and Seshadri, Vivek},
title = {Language Translation as a Socio-Technical System:Case-Studies of Mixed-Initiative Interactions},
year = {2021},
isbn = {9781450384537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460112.3471954},
doi = {10.1145/3460112.3471954},
abstract = {Seamless access to information in a rapidly globalizing world demands for availability of information across, ideally all but at the least a large number of, languages. Machine translation has been proposed as a technological solution to this complex problem. However, despite seven decades of research, and recently seen rapid progress in the field - thanks to deep learning and availability of large data-sets, perfect machine translation across a large number of the world’s languages still remains elusive. In fact, it is a distant and perhaps even an impossible goal. Erroneous translations, on the other hand, can be detrimental in critical situations such as talking to a law enforcement officer; or, they could potentially perpetuate social biases or stereotypes, for instance, by producing mis-gendered translations. In this work, we argue that language translation is inherently a socio-technical system, which has to be viewed, studied, and optimized for, as such. The need and context of translation, the socio-demographic factors behind the human translators as well as the consumers of the translated content affect the complexity of the translation system, as much as the accuracy of the technology and its interface. Through a series of case studies on mixed-initiative interaction based approach to translation, we bring out the various socio-technical factors and their complex interactions that one has to bear in mind while designing for the ideal human-machine translation systems. Through these observations, we make multiple recommendations which, at the core, suggest that ”solving” translation in the real sense would require more coordinated efforts between the technical (NLP) and social communities (HCI + CSCW + DEV).},
booktitle = {Proceedings of the 4th ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {156–172},
numpages = {17},
keywords = {socio-technical systems, human-centered AI, human-agent interaction, mixed-initiative interaction, interactive translation},
location = {Virtual Event, Australia},
series = {COMPASS '21}
}

@inproceedings{10.1145/3410463.3414662,
author = {Kim, Bokyeong and Hwang, Soojin and Cha, Sanghoon and Park, Chang Hyun and Park, Jongse and Huh, Jaehyuk},
title = {Decoupled Address Translation for Heterogeneous Memory Systems},
year = {2020},
isbn = {9781450380751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410463.3414662},
doi = {10.1145/3410463.3414662},
abstract = {The support for the heterogeneous memory in the conventional virtual memory has an inherent problem. For the efficient translation in the critical translation lookaside buffers (TLBs), the page size has been growing. However, the heterogeneous memory management requires a nimble fine-grained migration mechanism to quickly move necessary memory portions to the precious fast memory.To address the challenges posed by the conflicting goals in the heterogeneous memory support, this paper proposes to decouple the address translation into a two-step process. The decoupling resolves the conflict as the critical core-side TLBs perform the translation to an intermediate address space, and the memory-side translation provides the actual physical location of the memory devices.},
booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
pages = {155–156},
numpages = {2},
keywords = {heterogeneous memory, virtual memory, address translation},
location = {Virtual Event, GA, USA},
series = {PACT '20}
}

@article{10.1109/TASLP.2019.2957887,
author = {Mathad, Vikram C. and Prasanna, S. R. Mahadeva},
title = {Vowel Onset Point Based Screening of Misarticulated Stops in Cleft Lip and Palate Speech},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2957887},
doi = {10.1109/TASLP.2019.2957887},
abstract = {The presence of velopharyngeal dysfunction, dental occlusion, and mislearned articulation in individuals with cleft lip and palate (CLP) results in the production of misarticulated stop consonants. The present work considers vowel onset points (VOPs) as the anchor points, around which the consonant-vowel (CV) transition regions are segmented to analyze the difference between normal and misarticulated stops. VOPs are located using an epoch-synchronously computed feature called maximum weighted inner product. Spectro-temporal dynamics of CV transitions anchored around VOP are analyzed using two-dimensional discrete cosine transform (2D-DCT) coefficients, where 2D-DCT coefficients are derived from single pole filter (SPF) based time-frequency representation. The SPF-based 2D-DCT coefficients are used to train a support vector machine for the classification of normal and misarticulated stops, where the class of misarticulated stops includes weak, nasalized, palatal, velar, pharyngeal, glottal, and devoicing errors produced by CLP speakers. The performance of the proposed VOP detection algorithm is evaluated on a database containing CV units of normal and misarticulated stops, and the results are compared with the state-of-the-art VOP detection methods. The classification results obtained for the proposed SPF-based 2D-DCT coefficients are compared with the short-time Fourier transform-based 2D-DCT coefficients and Mel-frequency cepstral coefficients. Further, the performance of the proposed system is compared with the hidden Markov model-based goodness of pronunciation approach.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {450–460},
numpages = {11}
}

@inproceedings{10.1145/3355088.3365163,
author = {Lee, Gayoung and Kim, Dohyun and Yoo, Youngjoon and Han, Dongyoon and Ha, Jung-Woo and Chang, Jaehyuk},
title = {Unpaired Sketch-to-Line Translation via Synthesis of Sketches},
year = {2019},
isbn = {9781450369459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355088.3365163},
doi = {10.1145/3355088.3365163},
abstract = {Converting hand-drawn sketches into clean line drawings is a crucial step for diverse artistic works such as comics and product designs. Recent data-driven methods using deep learning have shown their great abilities to automatically simplify sketches on raster images. Since it is difficult to collect or generate paired sketch and line images, lack of training data is a main obstacle to use these models. In this paper, we propose a training scheme that requires only unpaired sketch and line images for learning sketch-to-line translation. To do this, we first generate realistic paired sketch and line images from unpaired sketch and line images using rule-based line augmentation and unsupervised texture conversion. Next, with our synthetic paired data, we train a model for sketch-to-line translation using supervised learning. Compared to unsupervised methods that use cycle consistency losses, our model shows better performance at removing noisy strokes. We also show that our model simplifies complicated sketches better than models trained on a limited number of handcrafted paired data.},
booktitle = {SIGGRAPH Asia 2019 Technical Briefs},
pages = {45–48},
numpages = {4},
keywords = {sketch simplification, image-to-image translation, neural networks, data synthesizing},
location = {Brisbane, QLD, Australia},
series = {SA '19}
}

@inproceedings{10.1145/3507623.3507633,
author = {Njazi, Shaun and Ng, Sokchoo},
title = {Veritas: A Sign Language-To-Text Translator Using Machine Learning and Computer Vision},
year = {2022},
isbn = {9781450385930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507623.3507633},
doi = {10.1145/3507623.3507633},
booktitle = {Proceedings of the 2021 4th International Conference on Computational Intelligence and Intelligent Systems},
pages = {55–60},
numpages = {6},
keywords = {Machine learning, Sign language, Communication, Computer vision},
location = {Tokyo, Japan},
series = {CIIS '21}
}

@inproceedings{10.1145/3474085.3475445,
author = {Li, Shuang and Han, Bingfeng and Yu, Zhenjie and Liu, Chi Harold and Chen, Kai and Wang, Shuigen},
title = {I2V-GAN: Unpaired Infrared-to-Visible Video Translation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475445},
doi = {10.1145/3474085.3475445},
abstract = {Human vision is often adversely affected by complex environmental factors, especially in night vision scenarios. Thus, infrared cameras are often leveraged to help enhance the visual effects via detecting infrared radiation in the surrounding environment, but the infrared videos are undesirable due to the lack of detailed semantic information. In such a case, an effective video-to-video translation method from the infrared domain to the visible light counterpart is strongly needed by overcoming the intrinsic huge gap between infrared and visible fields. To address this challenging problem, we propose an infrared-to-visible (I2V) video translation method I2V-GAN to generate fine-grained and spatial-temporal consistent visible light videos by given unpaired infrared videos. Technically, our model capitalizes on three types of constraints: 1) adversarial constraint to generate synthetic frames that are similar to the real ones, 2) cyclic consistency with the introduced perceptual loss for effective content conversion as well as style preservation, and 3) similarity constraints across and within domains to enhance the content and motion consistency in both spatial and temporal spaces at a fine-grained level. Furthermore, the current public available infrared and visible light datasets are mainly used for object detection or tracking, and some are composed of discontinuous images which are not suitable for video tasks. Thus, we provide a new dataset for infrared-to-visible video translation, which is named IRVI. Specifically, it has 12 consecutive video clips of vehicle and monitoring scenes, and both infrared and visible light videos could be apart into 24352 frames. Comprehensive experiments on IRVI validate that I2V-GAN is superior to the compared state-of-the-art methods in the translation of infrared-to-visible videos with higher fluency and finer semantic details. Moreover, additional experimental results on the flower-to-flower dataset indicate I2V-GAN is also applicable to other video translation tasks. The code and IRVI dataset are available at https://github.com/BIT-DA/I2V-GAN.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3061–3069},
numpages = {9},
keywords = {infrared-to-visible, video-to-video translation, GANs},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3240508.3240716,
author = {Yang, Xuewen and Xie, Dongliang and Wang, Xin},
title = {Crossing-Domain Generative Adversarial Networks for Unsupervised Multi-Domain Image-to-Image Translation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240716},
doi = {10.1145/3240508.3240716},
abstract = {State-of-the-art techniques in Generative Adversarial Networks (GANs) have shown remarkable success in image-to-image translation from peer domain X to domain Y using paired image data. However, obtaining abundant paired data is a non-trivial and expensive process in the majority of applications. When there is a need to translate images across n domains, if the training is performed between every two domains, the complexity of the training will increase quadratically. Moreover, training with data from two domains only at a time cannot benefit from data of other domains, which prevents the extraction of more useful features and hinders the progress of this research area. In this work, we propose a general framework for unsupervised image-to-image translation across multiple domains, which can translate images from domain X to any a domain without requiring direct training between the two domains involved in image translation. A byproduct of the framework is the reduction of computing time and computing resources since it needs less time than training the domains in pairs as is done in state-of-the-art works. Our proposed framework consists of a pair of encoders along with a pair of GANs which learns high-level features across different domains to generate diverse and realistic samples from. Our framework shows competing results on many image-to-image tasks compared with state-of-the-art techniques.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {374–382},
numpages = {9},
keywords = {unsupervised learning, gan, image-to-image translation, neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1145/3600089,
author = {Zhou, Yufeng and Cox, Alan L. and Dwarkadas, Sandhya and Dong, Xiaowan},
title = {The Impact of Page Size and Microarchitecture on Instruction Address Translation Overhead},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3600089},
doi = {10.1145/3600089},
abstract = {As the volume of data processed by applications has increased, considerable attention has been paid to data address translation overheads, leading to the widespread use of larger page sizes (“superpages”) and multi-level translation lookaside buffers (TLBs). However, far less attention has been paid to instruction address translation and its relation to TLB and pipeline structure. In prior work, we quantified the impact of using code superpages on a variety of widely used applications, ranging from compilers to web user-interface frameworks, and the impact of sharing page table pages for executables and shared libraries. Within this article, we augment those results by first uncovering the effects that microarchitectural differences between Intel Skylake and AMD Zen+, particularly their different TLB organizations, have on instruction address translation overhead. This analysis provides some key insights into the microarchitectural design decisions that impact the cost of instruction address translation. First, a lower-level (level 2) TLB that has both instruction and data mappings competing for space within the same structure allows better overall performance and utilization when using code superpages. Code superpages not only reduce instruction address translation overhead but also indirectly reduce data address translation overhead. In fact, for a few applications, the use of just a few code superpages has a larger impact on overall performance than the use of a much larger number of data superpages. Second, a level 1 (L1) TLB with separate structures for different page sizes may require careful tuning of the superpage promotion policy for code, and a correspondingly suboptimal utilization of the level 2 TLB. In particular, increasing the number of superpages when the size of the L1 superpage structure is small may result in more L1 TLB misses for some applications. Moreover, on some microarchitectures, the cost of these misses can be highly variable, because replacement is delayed until all of the in-flight instructions mapped by the victim entry are retired. Hence, more superpage promotions can result in a performance regression. Finally, our findings also make a case for first-class OS support for superpages on ordinary files containing executables and shared libraries, as well as a more aggressive superpage policy for code.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jul},
articleno = {38},
numpages = {25},
keywords = {address translation, page table, instruction access, Translation Lookaside Buffer, microarchitecture, memory hierarchy, Superpage}
}

@inproceedings{10.1145/3373477.3373495,
author = {Ibrohim, Muhammad Okky and Setiadi, Muhammad Akbar and Budi, Indra},
title = {Identification of Hate Speech and Abusive Language on Indonesian Twitter Using the Word2vec, Part of Speech and Emoji Features},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373495},
doi = {10.1145/3373477.3373495},
abstract = {Freedom of speech for the people of Indonesia on social media makes the spread of hate speech and abusive language inevitable. If there is no proper handling, this will lead to social disharmony between individuals and communities. The identification of hate speech and abusive language on Twitter in the Indonesian language is quite challenging. Because of its ability to understand the meaning of a sentence, semantic features such as word embedding can be relied on to understand tweets that contain hateful and abusive words. In this study, word embedding (word2vec) feature and its combinations with part of speech and/or emoji were used to identify hate speech and abusive language on Twitter in the Indonesian language. Furthermore, some combinations of unigram with part of speech and/or emojis were also utilized during the experiment and the results were studied. The classification algorithms used in this study were Support Vector Machine, Random Forest Decision Tree, and Logistic Regression. The combination of unigram features, part of speech and emoji obtained the highest accuracy value of 79.85% with F-Measure of 87.51%.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {18},
numpages = {5},
keywords = {Twitter, hate speech, abusive language, machine learning},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1145/3585508,
author = {Li, Chih-Chia and Lin, I-Chen},
title = {Unpaired Translation of 3D Point Clouds with Multi-Part Shape Representation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3585508},
doi = {10.1145/3585508},
abstract = {Unpaired shape translation is an emerging task for intelligent shape modelling and editing. Recent methods for 3D shape transfer use single- or multi-scale latent codes but a single generator to generate the whole shape. The transferred shapes are prone to lose control of local details. To tackle the issue, we propose a parts-to-whole framework that employs multi-part shape representation to preserve structural details during translation. We decompose the whole shape feature into multiple part features in the latent space. These part features are then processed by individual generators respectively and transformed to point clouds. We constrain the local features of parts within the loss functions, which enable the model to generate more similar shape characteristics to the source input. Furthermore, we propose a part aggregation module that improves the performance when combining multiple point clusters to generate the final output. The experiments demonstrate that our multi-part shape representation can retain more shape characteristics compared to previous approaches.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {may},
articleno = {6},
numpages = {20},
keywords = {Unpaired domain translation, generative adversarial network, multi-part shape modeling, shape deformation}
}

@inproceedings{10.1145/3544549.3585612,
author = {Vona, Francesco and Pentimalli, Francesca and Catania, Fabio and Patti, Alberto and Garzotto, Franca},
title = {Speak in Public: An Innovative Tool for the Treatment of Stuttering through Virtual Reality, Biosensors, and Speech Emotion Recognition},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585612},
doi = {10.1145/3544549.3585612},
abstract = {Stuttering is a common speech disorder that affects over 80 million people worldwide. Exposure therapy (ET) is a well-established approach to treating stuttering, but it can be complex and risky to simulate challenging real-life situations. Virtual reality (VR) is a promising tool that can help overcome these limitations. This article presents Speak in Public, a system that combines VR, biosensors, and speech emotion recognition to provide objective measures of patients’ stress and emotional state during ET sessions. In a preliminary study with five participants, we found that the use of VR successfully replicated stressful situations as indicated by biosensors. Additionally, speech emotion recognition showed that patients primarily experienced fear during these activities. Findings are preliminary, and further research with larger samples is needed to validate them. This study highlights the potential of VR and biosensors to enhance ET for stuttering and provides insights for future research in this area.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {255},
numpages = {7},
keywords = {exposure therapy, stuttering, biosensors, speech emotion recognition, virtual reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@article{10.1109/TASLP.2019.2935891,
author = {Qi, Jun and Du, Jun and Siniscalchi, Sabato Marco and Lee, Chin-Hui},
title = {A Theory on Deep Neural Network Based Vector-to-Vector Regression With an Illustration of Its Expressive Power in Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2935891},
doi = {10.1109/TASLP.2019.2935891},
abstract = {This paper focuses on a theoretical analysis of deep neural network DNN based functional approximation. Leveraging upon two classical theorems on universal approximation, an artificial neural network ANN with a single hidden layer of neurons is used. With modified ReLU and Sigmoid activation functions, we first generalize the related concepts to vector-to-vector regression. Then, we show that the width of the hidden layer of ANN is numerically related to the approximation of the regression function. Furthermore, we increase the number of hidden layers and show that the depth of the ANN-based regression function can enhance its expressive power. We illustrate this representation with recently-emerged DNN based speech enhancement. We first compare the expressive power by varying ANN structures and then test its related regression performance under different noisy conditions in various noise types and signal-to-noise-ratio levels. Experimental results verify our theoretical prediction that an ANN of a broader hidden layer and a deeper architecture can jointly ensure a closer approximation of the vector-to-vector regression functions in terms of the Euclidean distance between the log power spectra of noisy and expected clean speech. Moreover, a DNN with a broader width at the top hidden layer can improve the regression performance relative to those with a narrower width at the top hidden layers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {1932–1943},
numpages = {12}
}

@inproceedings{10.1109/ICSE48619.2023.00072,
author = {Liu, Fang and Li, Jia and Zhang, Li},
title = {Syntax and Domain Aware Model for Unsupervised Program Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00072},
doi = {10.1109/ICSE48619.2023.00072},
abstract = {There is growing interest in software migration as the development of software and society. Manually migrating projects between languages is error-prone and expensive. In recent years, researchers have begun to explore automatic program translation using supervised deep learning techniques by learning from large-scale parallel code corpus. However, parallel resources are scarce in the programming language domain, and it is costly to collect bilingual data manually. To address this issue, several unsupervised programming translation systems are proposed. However, these systems still rely on huge monolingual source code to train, which is very expensive. Besides, these models cannot perform well for translating the languages that are not seen during the pre-training procedure. In this paper, we propose SDA-Trans, a syntax and domain-aware model for program translation, which leverages the syntax structure and domain knowledge to enhance the cross-lingual transfer ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus, including Python and Java monolingual programs. The experimental results on function translation tasks between Python, Java, and C++ show that SDA-Trans outperforms many large-scale pre-trained models, especially for unseen language translation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {755–767},
numpages = {13},
keywords = {neural networks, program translation, unsupervised learning, syntax structure},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1109/TASLP.2021.3064421,
author = {Pandey, Ashutosh and Wang, DeLiang},
title = {Dense CNN With Self-Attention for Time-Domain Speech Enhancement},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3064421},
doi = {10.1109/TASLP.2021.3064421},
abstract = {Speech enhancement in the time domain is becoming increasingly popular in recent years, due to its capability to jointly enhance both the magnitude and the phase of speech. In this work, we propose a dense convolutional network (DCN) with self-attention for speech enhancement in the time domain. DCN is an encoder and decoder based architecture with skip connections. Each layer in the encoder and the decoder comprises a dense block and an attention module. Dense blocks and attention modules help in feature extraction using a combination of feature reuse, increased network depth, and maximum context aggregation. Furthermore, we reveal previously unknown problems with a loss based on the spectral magnitude of enhanced speech. To alleviate these problems, we propose a novel loss based on magnitudes of enhanced speech and a predicted noise. Even though the proposed loss is based on magnitudes only, a constraint imposed by noise prediction ensures that the loss enhances both magnitude and phase. Experimental results demonstrate that DCN trained with the proposed loss substantially outperforms other state-of-the-art approaches to causal and non-causal speech enhancement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1270–1279},
numpages = {10}
}

@inproceedings{10.1145/3242969.3243017,
author = {Pham, Hai Xuan and Wang, Yuting and Pavlovic, Vladimir},
title = {End-to-End Learning for 3D Facial Animation from Speech},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3243017},
doi = {10.1145/3242969.3243017},
abstract = {We present a deep learning framework for real-time speech-driven 3D facial animation from speech audio. Our deep neural network directly maps an input sequence of speech spectrograms to a series of micro facial action unit intensities to drive a 3D blendshape face model. In particular, our deep model is able to learn the latent representations of time-varying contextual information and affective states within the speech. Hence, our model not only activates appropriate facial action units at inference to depict different utterance generating actions, in the form of lip movements, but also, without any assumption, automatically estimates emotional intensity of the speaker and reproduces her ever-changing affective states by adjusting strength of related facial unit activations. For example, in a happy speech, the mouth opens wider than normal, while other facial units are relaxed; or both eyebrows raise higher in a surprised state. Experiments on diverse audiovisual corpora of different actors across a wide range of facial actions and emotional states show promising results of our approach. Being speaker-independent, our generalized model is readily applicable to various tasks in human-machine interaction and animation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {361–365},
numpages = {5},
keywords = {deep learning, speech-driven, facial animation},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@inproceedings{10.1145/3343031.3351020,
author = {Liu, Yahui and De Nadai, Marco and Zen, Gloria and Sebe, Nicu and Lepri, Bruno},
title = {Gesture-to-Gesture Translation in the Wild via Category-Independent Conditional Maps},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351020},
doi = {10.1145/3343031.3351020},
abstract = {Recent works have shown Generative Adversarial Networks (GANs) to be particularly effective in image-to-image translations. However, in tasks such as body pose and hand gesture translation, existing methods usually require precise annotations, e.g. key-points or skeletons, which are time-consuming to draw. In this work, we propose a novel GAN architecture that decouples the required annotations into a category label - that specifies the gesture type - and a simple-to-draw category-independent conditional map - that expresses the location, rotation and size of the hand gesture. Our architecture synthesizes the target gesture while preserving the background context, thus effectively dealing with gesture translation in the wild. To this aim, we use an attention module and a rolling guidance approach, which loops the generated images back into the network and produces higher quality images compared to competing works. Thus, our GAN learns to generate new images from simple annotations without requiring key-points or skeleton labels. Results on two public datasets show that our method outperforms state of the art approaches both quantitatively and qualitatively. To the best of our knowledge, no work so far has addressed the gesture-to-gesture translation in the wild by requiring user-friendly annotations.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1916–1924},
numpages = {9},
keywords = {image translation, gans, hand gesture},
location = {Nice, France},
series = {MM '19}
}

@article{10.1109/TASLP.2020.2968738,
author = {Kolb\ae{}k, Morten and Tan, Zheng-Hua and Jensen, S\o{}ren Holdt and Jensen, Jesper},
title = {On Loss Functions for Supervised Monaural Time-Domain Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2968738},
doi = {10.1109/TASLP.2020.2968738},
abstract = {Many deep learning-based speech enhancement algorithms are designed to minimize the mean-square error&nbsp;(MSE) in some transform domain between a predicted and a target speech signal. However, optimizing for MSE does not necessarily guarantee high speech quality or intelligibility, which is the ultimate goal of many speech enhancement algorithms. Additionally, only little is known about the impact of the loss function on the emerging class of time-domain deep learning-based speech enhancement systems. We study how popular loss functions influence the performance of time-domain deep learning-based speech enhancement systems. First, we demonstrate that perceptually inspired loss functions might be advantageous over classical loss functions like MSE. Furthermore, we show that the learning rate is a crucial design parameter even for adaptive gradient-based optimizers, which has been generally overlooked in the literature. Also, we found that waveform matching performance metrics must be used with caution as they in certain situations can fail completely. Finally, we show that a loss function based on scale-invariant signal-to-distortion ratio (SI-SDR) achieves good general performance across a range of popular speech enhancement evaluation metrics, which suggests that SI-SDR is a good candidate as a general-purpose loss function for speech enhancement systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {825–838},
numpages = {14}
}

@article{10.1109/TASLP.2021.3076364,
author = {Hsu, Jia-Hao and Su, Ming-Hsiang and Wu, Chung-Hsien and Chen, Yi-Hsuan},
title = {Speech Emotion Recognition Considering Nonverbal Vocalization in Affective Conversations},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076364},
doi = {10.1109/TASLP.2021.3076364},
abstract = {In real-life communication, nonverbal vocalization such as laughter, cries or other emotion interjections, within an utterance play an important role for emotion expression. In previous studies, only few emotion recognition systems consider nonverbal vocalization, which naturally exists in our daily conversation. In this work, both verbal and nonverbal sounds within an utterance are considered for emotion recognition of real-life affective conversations. Firstly, a support vector machine (SVM)-based verbal and nonverbal sound detector is developed. A prosodic phrase auto-tagger is further employed to extract the verbal/nonverbal sound segments. For each segment, the emotion and sound feature embeddings are respectively extracted using the deep residual networks (ResNets). Finally, a sequence of the extracted feature embeddings for the entire dialog turn are fed to an attentive long short-term memory (LSTM)-based sequence-to-sequence model to output an emotional sequence as recognition result. The NNIME corpus (The NTHU-NTUA Chinese interactive multimodal emotion corpus), which consists of verbal and nonverbal sounds, was adopted for system training and testing. 4766 single speaker dialogue turns in the audio data of the NNIME corpus were selected for evaluation. The experimental results showed that nonverbal vocalization was helpful for speech emotion recognition. For comparison, the proposed method based on decision-level fusion achieved an accuracy of 61.92% for speech emotion recognition outperforming the traditional methods as well as the feature-level and model-level fusion approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1675–1686},
numpages = {12}
}

@inproceedings{10.1145/3582768.3582782,
author = {Julavanich, Thanakrit and Aizawa, Akiko},
title = {Measuring Text-to-SQL Semantic Parsing Model on the Question Generalizability},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582782},
doi = {10.1145/3582768.3582782},
abstract = {One of the challenges in NLP tasks, such as text-to-SQL semantic parsing, is generalization. In the text-to-SQL task, having separate training and testing data can measure one aspect of the generalization: how well the model generalizes to unseen databases. Other aspects, however, remain unaccounted for. We propose a new dataset and a more challenging and thorough evaluation process that focuses on the two challenges of generalizing the text-to-SQL model: database content references and question patterns. We create SPIDER-QG, an augmented dataset that employs three techniques, to assess generalizability. First, we replace the set of values in the existing test set with other values from the same column in the same database. Second, we use the synonym of each value as a replacement instead. Third, we generate new questions for the existing SQL query by back-translating the original question. Our evaluation setup demonstrates the generalization challenges and struggles of the current models.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {79–83},
numpages = {5},
keywords = {text-to-SQL, model generalizability, datasets},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@article{10.1145/3476085,
author = {Kim, Soomin and Oh, Changhoon and Cho, Won Ik and Shin, Donghoon and Suh, Bongwon and Lee, Joonhwan},
title = {Trkic G00gle: Why and How Users Game Translation Algorithms},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476085},
doi = {10.1145/3476085},
abstract = {Individuals interact with algorithms in various ways. Users even game and circumvent algorithms so as to achieve favorable outcomes. This study aims to come to an understanding of how various stakeholders interact with each other in tricking algorithms, with a focus towards online review communities. We employed a mixed-method approach in order to explore how and why users write machine non-translatable reviews as well as how those encrypted messages are perceived by those receiving them. We found that users are able to find tactics to trick the algorithms in order to avoid censoring, to mitigate interpersonal burden, to protect privacy, and to provide authentic information for enabling the formation of informative review communities. They apply several linguistic and social strategies in this regard. Furthermore, users perceive encrypted messages as both more trustworthy and authentic. Based on these findings, we discuss implications for online review community and content moderation algorithms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {344},
numpages = {24},
keywords = {gaming, algorithmic experience, peer-to-peer platform, human-AI interaction, recommendation algorithm, online review, translation algorithm}
}

@inproceedings{10.1145/3549865.3549892,
author = {Massaguer, Lluc and Ribera, Mireia and Alcaraz Mart\'{\i}nez, Rub\'{e}n and Satorras Fioretti, Rosa M. and Salse, Marina and Costa, Merc\`{e} and T\'{e}rmens, Miquel and Centelles, Miquel},
title = {Accessibility in Spanish Universities, Evaluation through Their Policies: Communications about Accessibility during COVID-19},
year = {2022},
isbn = {9781450397025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549865.3549892},
doi = {10.1145/3549865.3549892},
abstract = {This research analyzes the announcements issued by Spanish universities during the COVID-19 pandemic relating to students with special needs, to offer a new vision, complementary to existing audits conducted on university websites about technical accessibility compliance. The study approaches the evaluation of accessibility from the point of view of organizational support. The identified measures are mapped onto Universal Design Learning guidelines to give them context and to analyze their coverage. The research shows a landscape with an uncoordinated and uncollaborative approach, with negative consequences in the tackling of remote emergency teaching..},
booktitle = {Proceedings of the XXII International Conference on Human Computer Interaction},
articleno = {1},
numpages = {5},
keywords = {higher education, Accessibility, Spanish universities, special needs education, UDL, COVID-19},
location = {Teruel, Spain},
series = {Interacci\'{o}n '22}
}

@inproceedings{10.1145/3573942.3574120,
author = {Ke, Dengfeng and Huang, Liangjie and Yao, Wenhan and Hu, Ruixin and Zu, Xueyin and Xie, Yanlu and Zhang, Jinsong},
title = {Voicifier-LN: An Novel Approach to Elevate the Speaker Similarity for General Zero-Shot Multi-Speaker TTS},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574120},
doi = {10.1145/3573942.3574120},
abstract = {Speeches generated from neural network-based Text-to-Speech (TTS) have been becoming more natural and intelligible. However, the evident dropping performance still exists when synthesizing multi-speaker speeches in zero-shot manner, especially for those from different countries with different accents. To bridge this gap, we propose a novel method, called Voicifier. It firstly operates on high frequency mel-spectrogram bins to approximately remove the content and rhythm. Then Voicifier uses two strategies, from the shallow to the deep mixing, to further destroy the content and rhythm but retain the timbre. Furthermore, for better zero-shot performance, we propose Voice-Pin Layer Normalization (VPLN) which pins down the timbre according with the text feature. During inference, the model is allowed to synthesize high quality and similarity speeches with just around 1 sec target speech audio. Experiments and ablation studies prove that the methods are able to retain more target timbre while abandoning much more of the content and rhythm-related information. To our best knowledge, the methods are found to be universal that is to say it can be applied to most of the existing TTS systems to enhance the ability of cross-speaker synthesis.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {950–955},
numpages = {6},
keywords = {zero-shot, timbre, speech synthesis, robust training strategy},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3495018.3495348,
author = {Sui, Yiqi},
title = {Computer Intelligent Proofreading Method for English Translation Based on Foreign Language Translation Model},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495348},
doi = {10.1145/3495018.3495348},
abstract = {With the development of computer technology, intelligent proofreading of English translation by computer has gradually become one of the research directions. Due to the influence of algorithm and matching degree, there are still bottlenecks in English translation. At present, machine translation is widely used in English translation. GLR algorithm plays an important role in the field of machine translation. However, the number of recognition results calculated by general GLR algorithm is accidental, which can not effectively guarantee the accuracy of recognition results. On this basis, in order to accurately recognize foreign languages, this paper designs an optimized GLR algorithm based on intelligent recognition. First, a foreign language corpus with a rating scale of about 600000 English and Chinese words is constructed to make the foreign language have retrieval function. Then, the foreign language structure is constructed by using the foreign language center points, and the GLR algorithm is optimized. Finally, according to the syntactic function of the linear table, the English Chinese structural ambiguity in the recognition results is corrected, and the effectiveness of intelligent proofreading is evaluated. The experimental results show that the highest score of the optimized GLR algorithm is 93.9, and the lowest score of the statistical algorithm is 76.7, which effectively verifies the advantages of high precision and context consistency of the proposed system. This fully shows that the algorithm in this paper meets the design requirements in all aspects. At the same time, it is of great value to study the English translation computer intelligent proofreading method based on the foreign language translation model.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1121–1125},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1109/TASLP.2019.2959721,
author = {Li, Ruizhi and Wang, Xiaofei and Mallidi, Sri Harish and Watanabe, Shinji and Hori, Takaaki and Hermansky, Hynek},
title = {Multi-Stream End-to-End Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2959721},
doi = {10.1109/TASLP.2019.2959721},
abstract = {Attention-based methods and Connectionist Temporal Classification (CTC) network have been promising research directions for end-to-end (E2E) Automatic Speech Recognition (ASR). The joint CTC/Attention model has achieved great success by utilizing both architectures during multi-task training and joint decoding. In this article, we present a multi-stream framework based on joint CTC/Attention E2E ASR with parallel streams represented by separate encoders aiming to capture diverse information. On top of the regular attention networks, the Hierarchical Attention Network (HAN) is introduced to steer the decoder toward the most informative encoders. A separate CTC network is assigned to each stream to force monotonic alignments. Two representative framework have been proposed and discussed, which are Multi-Encoder Multi-Resolution (MEM-Res) framework and Multi-Encoder Multi-Array (MEM-Array) framework, respectively. In MEM-Res framework, two heterogeneous encoders with different architectures, temporal resolutions and separate CTC networks work in parallel to extract complementary information from same acoustics. Experiments are conducted on Wall Street Journal (WSJ) and CHiME-4, resulting in relative Word Error Rate (WER) reduction of <inline-formula><tex-math notation="LaTeX">$text{18.0}!-!text{32.1}%$</tex-math></inline-formula> and the best WER of <inline-formula><tex-math notation="LaTeX">$text{3.6}%$</tex-math></inline-formula> in the WSJ eval92 test set. The MEM-Array framework aims at improving the far-field ASR robustness using multiple microphone arrays which are activated by separate encoders. Compared with the best single-array results, the proposed framework has achieved relative WER reduction of <inline-formula><tex-math notation="LaTeX">$text{3.7}%$</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX">$text{9.7}%$</tex-math></inline-formula> in AMI and DIRHA multi-array corpora, respectively, which also outperforms conventional fusion strategies.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {646–655},
numpages = {10}
}

@article{10.1145/3428289,
author = {Gupta, Shubhani and Rose, Abhishek and Bansal, Sorav},
title = {Counterexample-Guided Correlation Algorithm for Translation Validation},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428289},
doi = {10.1145/3428289},
abstract = {Automatic translation validation across the unoptimized intermediate representation (IR) of the original source code and the optimized executable assembly code is a desirable capability, and has the potential to compete with existing approaches to verified compilation such as CompCert. A difficult subproblem is the automatic identification of the correlations across the transitions between the two programs' respective locations. We present a counterexample-guided algorithm to identify these correlations in a robust and scalable manner. Our algorithm has both theoretical and empirical advantages over prior work in this problem space.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {221},
numpages = {29},
keywords = {Certified Compilation, Automatic Verification, Translation Validation, Equivalence Checking}
}

@inproceedings{10.1145/3340555.3353745,
author = {Hussen Abdelaziz, Ahmed and Theobald, Barry-John and Binder, Justin and Fanelli, Gabriele and Dixon, Paul and Apostoloff, Nick and Weise, Thibaut and Kajareker, Sachin},
title = {Speaker-Independent Speech-Driven Visual Speech Synthesis Using Domain-Adapted Acoustic Models},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353745},
doi = {10.1145/3340555.3353745},
abstract = {Speech-driven visual speech synthesis involves mapping acoustic speech features to the corresponding lip animation controls for a face model. This mapping can take many forms, but a powerful approach is to use deep neural networks (DNNs). The lack of synchronized audio, video, and depth data is a limitation to reliably train DNNs, especially for speaker-independent models. In this paper, we investigate adapting an automatic speech recognition (ASR) acoustic model (AM) for the visual speech synthesis problem. We train the ASR-AM on ten thousand hours of audio-only transcribed speech. The ASR-AM is then adapted to the visual speech synthesis domain using ninety hours of synchronized audio-visual speech. Using a subjective assessment test, we compared the performance of the AM-initialized DNN to a randomly initialized model. The results show that viewers significantly prefer animations generated from the AM-initialized DNN than the ones generated using the randomly initialized model. We conclude that visual speech synthesis can significantly benefit from the powerful representation of speech in the ASR acoustic models.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {220–225},
numpages = {6},
keywords = {blendshape coefficients, Visual speech synthesis, DNN adaptation, automatic speech recognition, audio-visual speech},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3534678.3539174,
author = {Chennupati, Gopinath and Rao, Milind and Chadha, Gurpreet and Eakin, Aaron and Raju, Anirudh and Tiwari, Gautam and Sahu, Anit Kumar and Rastrow, Ariya and Droppo, Jasha and Oberlin, Andy and Nandanoor, Buddha and Venkataramanan, Prahalad and Wu, Zheng and Sitpure, Pankaj},
title = {ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539174},
doi = {10.1145/3534678.3539174},
abstract = {Incremental learning is one paradigm to enable model building and updating at scale with streaming data. For end-to-end automatic speech recognition (ASR) tasks, the absence of human annotated labels along with the need for privacy preserving policies for model building makes it a daunting challenge. Motivated by these challenges, in this paper we use a cloud based framework for production systems to demonstrate insights from privacy preserving incremental learning for automatic speech recognition (ILASR). By privacy preserving, we mean, usage of ephemeral data which are not human annotated. This system is a step forward for production level ASR models for incremental/continual learning that offers near real-time test-bed for experimentation in the cloud for end-to-end ASR, while adhering to privacy-preserving policies. We show that the proposed system can improve the production models significantly ($3%$) over a new time period of six months even in the absence of human annotated labels with varying levels of weak supervision and large batch sizes in incremental learning. This improvement is $20%$ over test sets with new words and phrases in the new time period. We demonstrate the effectiveness of model building in a privacy-preserving incremental fashion for ASR while further exploring the utility of having an effective teacher model and use of large batch sizes.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2780–2788},
numpages = {9},
keywords = {automatic speech recognition, incremental learning, privacy-preserving machine learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.5555/3455716.3455856,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {multi-task learning, deep learning, transfer learning, attention based models, natural language processing}
}

@inproceedings{10.1145/3510003.3510139,
author = {Jung, Chijung and Kim, Doowon and Chen, An and Wang, Weihang and Zheng, Yunhui and Lee, Kyu Hyung and Kwon, Yonghwi},
title = {Hiding Critical Program Components via Ambiguous Translation},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510139},
doi = {10.1145/3510003.3510139},
abstract = {Software systems may contain critical program components such as patented program logic or sensitive data. When those components are reverse-engineered by adversaries, it can cause significantly damage (e.g., financial loss or operational failures). While protecting critical program components (e.g., code or data) in software systems is of utmost importance, existing approaches, unfortunately, have two major weaknesses: (1) they can be reverse-engineered via various program analysis techniques and (2) when an adversary obtains a legitimate-looking critical program component, he or she can be sure that it is genuine.In this paper, we propose Ambitr, a novel technique that hides critical program components. The core of Ambitr is Ambiguous Translator that can generate the critical program components when the input is a correct secret key. The translator is ambiguous as it can accept any inputs and produces a number of legitimate-looking outputs, making it difficult to know whether an input is correct secret key or not. The executions of the translator when it processes the correct secret key and other inputs are also indistinguishable, making the analysis inconclusive. Our evaluation results show that static, dynamic and symbolic analysis techniques fail to identify the hidden information in Ambitr. We also demonstrate that manual analysis of Ambitr is extremely challenging.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1120–1132},
numpages = {13},
keywords = {program translation, reverse engineering, software protection},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3474840,
author = {Wang, Yu and Wang, Yuelin and Dang, Kai and Liu, Jie and Liu, Zhuo},
title = {A Comprehensive Survey of Grammatical Error Correction},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3474840},
doi = {10.1145/3474840},
abstract = {Grammatical error correction (GEC) is an important application aspect of natural language processing techniques, and GEC system is a kind of very important intelligent system that has long been explored both in academic and industrial communities. The past decade has witnessed significant progress achieved in GEC for the sake of increasing popularity of machine learning and deep learning. However, there is not a survey that untangles the large amount of research works and progress in this field. We present the first survey in GEC for a comprehensive retrospective of the literature in this area. We first give the definition of GEC task and introduce the public datasets and data annotation schema. After that, we discuss six kinds of basic approaches, six commonly applied performance boosting techniques for GEC systems, and three data augmentation methods. Since GEC is typically viewed as a sister task of Machine Translation (MT), we put more emphasis on the statistical machine translation (SMT)-based approaches and neural machine translation (NMT)-based approaches for the sake of their importance. Similarly, some performance-boosting techniques are adapted from MT and are successfully combined with GEC systems for enhancement on the final performance. More importantly, after the introduction of the evaluation in GEC, we make an in-depth analysis based on empirical results in aspects of GEC approaches and GEC systems for a clearer pattern of progress in GEC, where error type analysis and system recapitulation are clearly presented. Finally, we discuss five prospective directions for future GEC researches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {65},
numpages = {51},
keywords = {Grammatical error correction, natural language processing, machine translation}
}

@article{10.1145/3603500,
author = {Zhang, Zhen},
title = {Computational Technologies for Construction of Business Korean Translation Corpus Based on Association Rules Mining},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3603500},
doi = {10.1145/3603500},
abstract = {With the implementation of China's reform and opening up and China's accession to the World Trade Organization, Chinese business Korean translation plays an increasingly important role in international trade activities. Although the study of business Korean has aroused the strong research interest of scholars, its research focus is limited to word selection, grammatical transformation and various specific translation techniques, which is far from systematic. Good business translation should also involve factors such as the translator's role, cross-culture, and the client of translation. However, how to combine corpus with language teaching, make corpus enter the classroom and get practical application in daily language teaching is still in the exploratory stage. Most of the existing translation theories are tailored for literary translation, and business translation is a branch of non-literary translation, so these theories are difficult to guide business translation. Combined with the research results of translation studies and examples in business Korean, this paper discusses the characteristics and translation of business Korean as a special Korean, so as to provide reference for translation or related learners, and also want to attract more scholars to pay attention to business Korean and its translation.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
keywords = {Association rule mining, Corpus, Business Korean translation}
}

@inproceedings{10.1145/3603555.3603569,
author = {Heuer, Hendrik and Glassman, Elena Leah},
title = {Accessible Text Tools for People with Cognitive Impairments and Non-Native Readers: Challenges and Opportunities},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603555.3603569},
doi = {10.1145/3603555.3603569},
abstract = {Many people have problems with reading, which limits their ability to participate in society. This paper explores tools that make text more accessible. For this, we interviewed experts, who proposed tools for different stakeholders and scenarios. Important stakeholders of such tools are people with cognitive impairments and non-native readers. Frequently mentioned scenarios are public administration, the medical domain, and everyday life. The tools proposed by experts support stakeholders by improving how text is compressed, expanded, reviewed, and experienced. In a survey of stakeholders, we confirm that the scenarios are relevant and that the proposed tools appear helpful to them. We provide the Accessible Text Framework to help researchers understand how the different tools can be combined and discuss how individual tools can be implemented. The investigation shows that accessible text tools are an important HCI+AI challenge that a large number of people can benefit from.},
booktitle = {Proceedings of Mensch Und Computer 2023},
pages = {250–266},
numpages = {17},
keywords = {Accessibility, Non-Native Speakers, Plain Language, People With Intellectual Impairments, Text Summarization, Machine Translation, Easy Language, People With Cognitive Impairments},
location = {Rapperswil, Switzerland},
series = {MuC '23}
}

@inproceedings{10.1145/3450507.3457429,
author = {Farahi, Behnaz},
title = {"Can the Subaltern Speak?": Critical Making in Design},
year = {2021},
isbn = {9781450383608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450507.3457429},
doi = {10.1145/3450507.3457429},
abstract = {The Bandari women from the southern coast of Iran are famous for their intriguing masks, known as Niqab masks. Legend has it that the practice started during Portuguese colonial rule as a way of protecting the wearer, not only from the harsh sun of the Persian Gulf, but also from slave masters looking for pretty women. Viewed from a contemporary perspective, these masks can be seen as a means of protecting women from patriarchal and colonial oppression.},
booktitle = {ACM SIGGRAPH 2021 Art Gallery},
articleno = {2},
numpages = {3},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3406324.3410721,
author = {Gallo, Danilo and Willamowski, Jutta and Wisatekaew, Yada and Bruyat, Adrien and Grasso, Antonietta Maria},
title = {Restaurant Menu Understanding: Illustrating the Need for Culturally Augmented Translation},
year = {2021},
isbn = {9781450380522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406324.3410721},
doi = {10.1145/3406324.3410721},
abstract = {We carried out a user study on the issues foreign customers face when going to restaurants abroad. We describe our study and findings illustrating that simple translation is not enough to help customers understand restaurant menus in foreign countries. Similar to what is provided by human translation, where the translator mediates between the author and the reader, menu translation should augment the translated menu with complementary information to address cultural specificities and differences. It should for instance highlight local specialties, provide background information on the dishes, and explain different eating habits or manners. The information provided should bridge cultural differences and has to be adapted to the specific cultural background of the customer.},
booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {6},
numpages = {4},
keywords = {User study, Translation, Food, Restaurant menus, Cultural difference},
location = {Oldenburg, Germany},
series = {MobileHCI '20}
}

@article{10.1109/TASLP.2020.2986896,
author = {Taherian, Hassan and Wang, Zhong-Qiu and Chang, Jorge and Wang, DeLiang},
title = {Robust Speaker Recognition Based on Single-Channel and Multi-Channel Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2986896},
doi = {10.1109/TASLP.2020.2986896},
abstract = {Deep neural network (DNN) embeddings for speaker recognition have recently attracted much attention. Compared to i-vectors, they are more robust to noise and room reverberation as DNNs leverage large-scale training. This article addresses the question of whether speech enhancement approaches are still useful when DNN embeddings are used for speaker recognition. We investigate single- and multi-channel speech enhancement for text-independent speaker verification based on x-vectors in conditions where strong diffuse noise and reverberation are both present. Single-channel (monaural) speech enhancement is based on complex spectral mapping and is applied to individual microphones. We use masking-based minimum variance distortion-less response (MVDR) beamformer and its rank-1 approximation for multi-channel speech enhancement. We propose a novel method of deriving time-frequency masks from the estimated complex spectrogram. In addition, we investigate gammatone frequency cepstral coefficients (GFCCs) as robust speaker features. Systematic evaluations and comparisons on the NIST SRE 2010 retransmitted corpus show that both monaural and multi-channel speech enhancement significantly outperform x-vector's performance, and our covariance matrix estimate is effective for the MVDR beamformer.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1293–1302},
numpages = {10}
}

@inproceedings{10.1145/3452446.3452495,
author = {Abdurosul, Paruq and Tursun, Guzalnur and Juma, Osman},
title = {Research on Translation Strategy from the Perspective of Cognitive Linguistics},
year = {2021},
isbn = {9781450389815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452446.3452495},
doi = {10.1145/3452446.3452495},
abstract = {Translation is an important part of language activities. It is a process of transforming different language information into information expressed in another language. Cognitive linguistics is a new and important language subject. Its emergence has injected new vitality into the field of English translation, making English translation more accurate and more convenient. As international exchanges become more frequent, cognitive linguistics has also been widely used and plays a very important role. The article first analyses the current situation of cognitive linguistics in the field of English translation, and then discusses the strategies of cognitive linguistics in English translation.},
booktitle = {2021 2nd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {172–175},
numpages = {4},
keywords = {Translation, Linguistics, Perspective, Strategy, Cognition},
location = {Dalian, China},
series = {IPEC2021}
}

@inproceedings{10.1145/3290607.3312849,
author = {Lee, Chang Hee and Lockton, Dan and Stevens, John and Wang, Stephen Jia and Ahn, SungHee},
title = {Synaesthetic-Translation Tool: Synaesthesia as an Interactive Material for Ideation},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3312849},
doi = {10.1145/3290607.3312849},
abstract = {While the subject of synaesthesia has inspired various practitioners and has been utilized as a design material in different formats, research has not so far presented a way to apply this captivating phenomenon as a source of design material in HCI. The purpose of this paper is to explore the translative property of synaesthesia and introduce a tangible way to use this intangible phenomenon as an interactive design material source in HCI and design. This paper shares a card-based tool that enables practitioners to use the translative property of synaesthesia for the sake of ideation. It further introduces a potential area of where this tool may be utilized for exploring user experiences. This work has implications for the CHI community as it attempts to share a practical way of using the intangible property of synaesthesia to explore potential user experiences.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {provocation, synaesthesia, interaction design, experience translation, ideation deck, mental imagery},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{10.1145/3453187.3453394,
author = {Xu, Jincheng and Ge, Yunsheng and Wu, Zhengxia},
title = {An Improved Translation-Based Method for Knowledge Graph Representation},
year = {2021},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453394},
doi = {10.1145/3453187.3453394},
abstract = {Traditional knowledge graph representation methods based on translation models have low-quality negative samples and false negative examples. In order to solve the shortcomings of translation models, this paper proposes an improved translation model knowledge representation model which is called TransE-KC. First, introduce the K-Means algorithm for clustering, and then randomly select 20 entities in different clusters to calculate the similarity between the replaced entities, and rank them, select the highest ranked entity, and perform Replacement; secondly, for the problem of "false negatives", this article introduces a Cuckoo Filter to filter it. The experimental results on the public data set show that, compared with the TransE model, the TransE-KC model has a greater improvement in link prediction average ranking and triplet classification accuracy.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {555–560},
numpages = {6},
keywords = {Triplet Classification, Translation Model, Negative Sample},
location = {Wuhan, China},
series = {EBIMCS '20}
}

@article{10.1109/TASLP.2018.2835719,
author = {Khan, Faheem Ullah and Milner, Ben P. and Le Cornu, Thomas},
title = {Using Visual Speech Information in Masking Methods for Audio Speaker Separation},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2835719},
doi = {10.1109/TASLP.2018.2835719},
abstract = {This paper examines whether visual speech information can be effective within audio-masking-based speaker separation to improve the quality and intelligibility of the target speech. Two visual-only methods of generating an audio mask for speaker separation are first developed. These use a deep neural network to map the visual speech features to an audio feature space from which both visually derived binary masks and visually derived ratio masks are estimated, before application to the speech mixture. Second, an audio ratio masking method forms a baseline approach for speaker separation which is extended to exploit visual speech information to form audio-visual ratio masks. Speech quality and intelligibility tests are carried out on the visual-only, audio-only, and audio-visual masking methods of speaker separation at mixing levels from $-$ 10 to +10 dB. These reveal substantial improvements in the target speech when applying the visual-only and audio-only masks, but with highest performance occurring when combining audio and visual information to create the audio-visual masks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1742–1754},
numpages = {13}
}

@inproceedings{10.1145/3343031.3350864,
author = {Park, Kwanyong and Woo, Sanghyun and Kim, Dahun and Cho, Donghyeon and Kweon, In So},
title = {Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350864},
doi = {10.1145/3343031.3350864},
abstract = {In this paper, we investigate the problem of unpaired video-to-video translation. Given a video in the source domain, we aim to learn the conditional distribution of the corresponding video in the target domain, without seeing any pairs of corresponding videos. While significant progress has been made in the unpaired translation of images, directly applying these methods to an input video leads to low visual quality due to the additional time dimension. In particular, previous methods suffer from semantic inconsistency (i.e., semantic label flipping) and temporal flickering artifacts. To alleviate these issues, we propose a new framework that is composed of carefully-designed generators and discriminators, coupled with two core objective functions: 1) content preserving loss and 2) temporal consistency loss. Extensive qualitative and quantitative evaluations demonstrate the superior performance of the proposed method against previous approaches. We further apply our framework to a domain adaptation task and achieve favorable results.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1248–1257},
numpages = {10},
keywords = {semantic and temporal consistency, unpaired video-to-video translation, domain adaptation},
location = {Nice, France},
series = {MM '19}
}

@inproceedings{10.1145/3292500.3330758,
author = {Decroos, Tom and Bransen, Lotte and Van Haaren, Jan and Davis, Jesse},
title = {Actions Speak Louder than Goals: Valuing Player Actions in Soccer},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330758},
doi = {10.1145/3292500.3330758},
abstract = {Assessing the impact of the individual actions performed by soccer players during games is a crucial aspect of the player recruitment process. Unfortunately, most traditional metrics fall short in addressing this task as they either focus on rare actions like shots and goals alone or fail to account for the context in which the actions occurred. This paper introduces (1) a new language for describing individual player actions on the pitch and (2) a framework for valuing any type of player action based on its impact on the game outcome while accounting for the context in which the action happened. By aggregating soccer players' action values, their total offensive and defensive contributions to their team can be quantified. We show how our approach considers relevant contextual information that traditional player evaluation metrics ignore and present a number of use cases related to scouting and playing style characterization in the 2016/2017 and 2017/2018 seasons in Europe's top competitions.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1851–1861},
numpages = {11},
keywords = {valuing actions, event stream data, probabilistic classification, soccer match data, sports analytics},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3478905.3478950,
author = {Akashi, Shigeo and Tong, Yao},
title = {A Vulnerability of Dynamic Network Address Translation To&nbsp;Denial-of-Service&nbsp;Attacks},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478950},
doi = {10.1145/3478905.3478950},
abstract = {It is well known that Network Address Translation plays not only an important role as the soltion to the IPv4 Depletion problem but also another important one as the enhancement of network security. Actually, it sometimes happens that some newly born network skills are not always compatible with other ready-constructed network ones. In this paper, we point out that the simulutaneous use of network routing and Network Address Translation may bring about an unexpected network traffic congestion. Exactly speaking. in the former half of this paper, we point out the fact that, if we apply dynamic routing together with static one simultaneously, another type of network traffic congestion, which resembles what is brought about by the routing loop, may happen spontaneously, and in the latter half of this paper, we discuss the problem asking if this network traffic congestion can be brought about not only spontaneously but also intentionally for preventing malicious cyber attackers from using this phenomenon intentionally.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {226–230},
numpages = {5},
keywords = {Outside global IP address, Packet oscillation, Inside local IP address, Inside static network address translation, Inside dynamic network address translation, Inside global IP address, Denial of service attacks},
location = {Shanghai, China},
series = {DSIT 2021}
}

@article{10.1109/TASLP.2020.2975902,
author = {Wang, Zhong-Qiu and Wang, DeLiang},
title = {Deep Learning Based Target Cancellation for Speech Dereverberation},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2975902},
doi = {10.1109/TASLP.2020.2975902},
abstract = {This article investigates deep learning based single- and multi-channel speech dereverberation. For single-channel processing, we extend magnitude-domain masking and mapping based dereverberation to complex-domain mapping, where deep neural networks (DNNs) are trained to predict the real and imaginary (RI) components of the direct-path signal from reverberant (and noisy) ones. For multi-channel processing, we first compute a minimum variance distortionless response (MVDR) beamformer to cancel the direct-path signal, and then feed the RI components of the cancelled signal, which is expected to be a filtered version of non-target signals, as additional features to perform dereverberation. Trained on a large dataset of simulated room impulse responses, our models show excellent speech dereverberation and recognition performance on the test set of the REVERB challenge, consistently better than single- and multi-channel weighted prediction error (WPE) algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {941–950},
numpages = {10}
}

@article{10.1145/3239564,
author = {Xie, Wei and Chen, Yong and Roth, Philip C.},
title = {Exploiting Internal Parallelism for Address Translation in Solid-State Drives},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1553-3077},
url = {https://doi.org/10.1145/3239564},
doi = {10.1145/3239564},
abstract = {Solid-state Drives (SSDs) have changed the landscape of storage systems and present a promising storage solution for data-intensive applications due to their low latency, high bandwidth, and low power consumption compared to traditional hard disk drives. SSDs achieve these desirable characteristics using internal parallelism—parallel access to multiple internal flash memory chips—and a Flash Translation Layer (FTL) that determines where data are stored on those chips so that they do not wear out prematurely. However, current state-of-the-art cache-based FTLs like the Demand-based Flash Translation Layer (DFTL) do not allow IO schedulers to take full advantage of internal parallelism, because they impose a tight coupling between the logical-to-physical address translation and the data access. To address this limitation, we introduce a new FTL design called Parallel-DFTL that works with the DFTL to decouple address translation operations from data accesses. Parallel-DFTL separates address translation and data access operations into different queues, allowing the SSD to use concurrent flash accesses for both types of operations. We also present a Parallel-LRU cache replacement algorithm to improve the concurrency of address translation operations. To compare Parallel-DFTL against existing FTL approaches, we present a Parallel-DFTL performance model and compare its predictions against those for DFTL and an ideal page-mapping approach. We also implemented the Parallel-DFTL approach in an SSD simulator using real device parameters, and used trace-driven simulation to evaluate Parallel-DFTL’s efficacy. Our evaluation results show that Parallel-DFTL improved the overall performance by up to 32% for the real IO workloads we tested, and by up to two orders of magnitude with synthetic test workloads. We also found that Parallel-DFTL is able to achieve reasonable performance with a very small cache size and that it provides the best benefit for those workloads with large request size or with high write ratio.},
journal = {ACM Trans. Storage},
month = {dec},
articleno = {32},
numpages = {30},
keywords = {SSD, parallelism, Flash translation layer, address translation, DFTL}
}

@article{10.1109/TASLP.2021.3082282,
author = {Tan, Ke and Wang, DeLiang},
title = {Towards Model Compression for Deep Learning Based Speech Enhancement},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3082282},
doi = {10.1109/TASLP.2021.3082282},
abstract = {The use of deep neural networks (DNNs) has dramatically elevated the performance of speech enhancement over the last decade. However, to achieve strong enhancement performance typically requires a large DNN, which is both memory and computation consuming, making it difficult to deploy such speech enhancement systems on devices with limited hardware resources or in applications with strict latency requirements. In this study, we propose two compression pipelines to reduce the model size for DNN-based speech enhancement, which incorporates three different techniques: sparse regularization, iterative pruning and clustering-based quantization. We systematically investigate these techniques and evaluate the proposed compression pipelines. Experimental results demonstrate that our approach reduces the sizes of four different models by large margins without significantly sacrificing their enhancement performance. In addition, we find that the proposed approach performs well on speaker separation, which further demonstrates the effectiveness of the approach for compressing speech separation models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1785–1794},
numpages = {10}
}

@article{10.1109/TASLP.2019.2947741,
author = {Eskimez, Sefik Emre and Maddox, Ross K. and Xu, Chenliang and Duan, Zhiyao},
title = {Noise-Resilient Training Method for Face Landmark Generation From Speech},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2947741},
doi = {10.1109/TASLP.2019.2947741},
abstract = {Visual cues such as lip movements, when available, play an important role in speech communication. They are especially helpful for the hearing impaired population or in noisy environments. When not available, having a system to automatically generate talking faces in sync with input speech would enhance speech communication and enable many novel applications. In this article, we present a new system that can generate 3D talking face landmarks from speech in an online fashion. We employ a neural network that accepts the raw waveform as an input. The network contains convolutional layers with 1D kernels and outputs the active shape model (ASM) coefficients of face landmarks. To promote smoother transitions between video frames, we present a variant of the model that has the same architecture but also accepts the previous frame's ASM coefficients as an additional input. To cope with background noise, we propose a new training method to incorporate speech enhancement ideas at the feature level. Objective evaluations on landmark prediction show that the proposed system yields statistically significantly smaller errors than two state-of-the-art baseline methods on both a single-speaker dataset and a multi-speaker dataset. Experiments on noisy speech input with five types of non-stationary unseen noise show statistically significant improvements of the system performance thanks to the noise-resilient training method. Finally, subjective evaluations show that the generated talking faces have a significantly more convincing match with the input audio, achieving a similarly convincing level of realism as the ground-truth landmarks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {27–38},
numpages = {12}
}

@article{10.1109/TASLP.2019.2950602,
author = {Su, Rongfeng and Liu, Xunying and Wang, Lan and Yang, Jingzhou},
title = {Cross-Domain Deep Visual Feature Generation for Mandarin Audio–Visual Speech Recognition},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2950602},
doi = {10.1109/TASLP.2019.2950602},
abstract = {There has been a long term interest in using visual information to improve automatic speech recognition (ASR) system performance. Both audio and visual information are required in conventional audio visual speech recognition (AVSR) systems. This limits their wider applications when visual modality is not present. To this end, one possible solution is to use acoustic-to-visual (A2V) inversion techniques to generate visual features. Previous research in this direction used synthetic acoustic-articulatory parallel data in inversion model training. The acoustic mismatch between the audio-visual (AV) parallel data and target data was not considered. In addition, the target language to apply these technologies has been focused on English. In this article, a real 3D Audio-Visual Mandarin Continuous Speech (3DAV-MCS) corpus was used to train deep neural network based A2V inversion models. Cross-domain adaptation of the inversion models allows suitable visual features to be generated from acoustic data of mismatched domains. The proposed cross-domain deep visual feature generation techniques were evaluated on two state-of-the-art Mandarin speech recognition tasks: DAPRA GALE broadcast transcription and BOLT conversational telephone speech recognition. The AVSR systems constructed using the cross-domain generated visual features consistently outperformed the baseline convolutional neural network (CNN) ASR systems by up to 3.3% absolute (9.1% relative) character error rate (CER) reductions after both speaker adaptive training and sequence discriminative training were performed.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {185–197},
numpages = {13}
}

@article{10.1109/TASLP.2021.3095662,
author = {Liu, Andy T. and Li, Shang-Wen and Lee, Hung-yi},
title = {TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3095662},
doi = {10.1109/TASLP.2021.3095662},
abstract = {We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn by using a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous methods, we use alteration along three orthogonal axes to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from their altered counterpart, where we use a stochastic policy to alter along various dimensions: time, frequency, and magnitude. TERA can be used for speech representations extraction or fine-tuning with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, keyword spotting, speaker recognition, and speech recognition. We present a large-scale comparison of various self-supervised models. TERA achieves strong performance in the comparison by improving upon surface features and outperforming previous models. In our experiments, we study the effect of applying different alteration techniques, pre-training on more data, and pre-training on various features. We analyze different model sizes and find that smaller models are strong representation learners than larger models, while larger models are more effective for downstream fine-tuning than smaller models. Furthermore, we show the proposed method is transferable to downstream datasets not used in pre-training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2351–2366},
numpages = {16}
}

@article{10.1145/3458280,
author = {Zareapoor, Masoumeh and Yang, Jie},
title = {Equivariant Adversarial Network for Image-to-Image Translation},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3458280},
doi = {10.1145/3458280},
abstract = {Image-to-Image translation aims to learn an image from a source domain to a target domain. However, there are three main challenges, such as lack of paired datasets, multimodality, and diversity, that are associated with these problems and need to be dealt with. Convolutional neural networks (CNNs), despite of having great performance in many computer vision tasks, they fail to detect the hierarchy of spatial relationships between different parts of an object and thus do not form the ideal representative model we look for. This article presents a new variation of generative models that aims to remedy this problem. We use a trainable transformer, which explicitly allows the spatial manipulation of data within training. This differentiable module can be augmented into the convolutional layers in the generative model, and it allows to freely alter the generated distributions for image-to-image translation. To reap the benefits of proposed module into generative model, our architecture incorporates a new loss function to facilitate an effective end-to-end generative learning for image-to-image translation. The proposed model is evaluated through comprehensive experiments on image synthesizing and image-to-image translation, along with comparisons with several state-of-the-art algorithms.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {jun},
articleno = {73},
numpages = {14},
keywords = {Stylistic image generation, domain adaptation, image-to-image translation, generative model}
}

@inproceedings{10.1145/3571600.3571639,
author = {Chouhan, Avinash and Jindal, Nitesh and Sur, Arijit and Chutia, Dibyajyoti and Aggarwal, Shiv Prasad},
title = {EDCGAN: Encoder Decoder Based Conditional GAN for SAR to Optical Image Translation ✱},
year = {2023},
isbn = {9781450398220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571600.3571639},
doi = {10.1145/3571600.3571639},
abstract = {High-resolution optical images are heavily utilized in various remote sensing applications. The optical images cannot reflect the actual ground information in cloudy conditions. SAR images are used to solve this for their ability to see through clouds. But SAR images are usually available with coarser resolutions. So, there is a need to produce an optical image from a SAR image to overcome bad weather and poor resolution in a single go. In this paper, a novel deep learning architecture named EDCGAN is proposed. The proposed architecture is an encoder-decoder-based conditional GAN that uses multi-scale attentive discrimination to get accurate SAR to RGB image translation. In addition, we have used residual connections, spatial &amp; channel-wise attention for better feature representation. A set of extensive experimentations show that this architecture outperforms the existing state-of-the-art method in terms of PSNR, SSIM, and FSIM_c values for the WHU-SENCity dataset.},
booktitle = {Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {38},
numpages = {8},
keywords = {attention, image-to-image translation, Conditional GAN},
location = {Gandhinagar, India},
series = {ICVGIP '22}
}

@article{10.1109/TASLP.2021.3076372,
author = {Tesch, Kristina and Gerkmann, Timo},
title = {Nonlinear Spatial Filtering in Multichannel Speech Enhancement},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076372},
doi = {10.1109/TASLP.2021.3076372},
abstract = {The majority of multichannel speech enhancement algorithms are two-step procedures that first apply a linear spatial filter, a so-called beamformer, and combine it with a single-channel approach for postprocessing. However, the serial concatenation of a linear spatial filter and a postfilter is not generally optimal in the minimum mean square error (MMSE) sense for noise distributions other than a Gaussian distribution. Rather, the MMSE optimal filter is a <italic>joint spatial and spectral nonlinear</italic> function. While estimating the parameters of such a filter with traditional methods is challenging, modern neural networks may provide an efficient way to learn the nonlinear function directly from data. To see if further research in this direction is worthwhile, in this work we examine the potential performance benefit of replacing the common two-step procedure with a joint spatial and spectral nonlinear filter. We analyze three different forms of non-Gaussianity: First, we evaluate on super-Gaussian noise with a high kurtosis. Second, we evaluate on inhomogeneous noise fields created by five interfering sources using two microphones, and third, we evaluate on real-world recordings from the CHiME3 database. In all scenarios, considerable improvements may be obtained. Most prominently, our analyses show that a nonlinear spatial filter uses the available spatial information more effectively than a linear spatial filter as it is capable of suppressing more than <inline-formula><tex-math notation="LaTeX">$D-1$</tex-math></inline-formula> directional interfering sources with a <inline-formula><tex-math notation="LaTeX">$D$</tex-math></inline-formula>-dimensional microphone array without spatial adaptation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1795–1805},
numpages = {11}
}

@inproceedings{10.1145/3594536.3595141,
author = {Zin, May Myo and Nguyen, Ha Thanh and Satoh, Ken and Sugawara, Saku and Nishino, Fumihito},
title = {Improving Translation of Case Descriptions into Logical Fact Formulas Using LegalCaseNER},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595141},
doi = {10.1145/3594536.3595141},
abstract = {The automated translation of natural language text into structured logical representations is a critical task in various applications, including legal reasoning and decision-making. This paper presents a Name Entity Recognition (NER) based approach for translating the legal case descriptions written in natural language into PROLEG fact formulas. The approach comprises (1) extracting legal entities from the case description using a specialized NER model, namely LegalCaseNER and (2) transforming the extracted entities into PROLEG fact formulas using PROLEG rules. The experimental results demonstrate the efficacy of our proposed approach in accurately extracting relevant entities from legal case descriptions and translating them into the appropriate PROLEG fact formulas. Our approach provides a promising solution for handling complex and diverse case descriptions, enabling their representation in a structured format. This work provides a foundation for future research in the application of logical fact formulas in legal reasoning and decision-making.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {462–466},
numpages = {5},
keywords = {entity extraction, natural language, logical reasoning, translation},
location = {Braga, Portugal},
series = {ICAIL '23}
}

@article{10.1109/TASLP.2018.2872106,
author = {Kamper, Herman and Shakhnarovich, Gregory and Livescu, Karen},
title = {Semantic Speech Retrieval With a Visually Grounded Model of Untranscribed Speech},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2872106},
doi = {10.1109/TASLP.2018.2872106},
abstract = {There is a growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here, we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to semantic keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {89–98},
numpages = {10}
}

@inproceedings{10.1145/3503161.3547802,
author = {Lin, Yupei and Zhang, Sen and Chen, Tianshui and Lu, Yongyi and Li, Guangping and Shi, Yukai},
title = {Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547802},
doi = {10.1145/3503161.3547802},
abstract = {Unpaired image-to-image translation aims to find a mapping between the source domain and the target domain. To alleviate the problem of the lack of supervised labels for the source images, cycle-consistency based methods have been proposed for image structure preservation by assuming a reversible relationship between unpaired images. However, this assumption only uses limited correspondence between image pairs. Recently, contrastive learning (CL) has been used to further investigate the image correspondence in unpaired image translation by using patch-based positive/negative learning. Patch-based contrastive routines obtain the positives by self-similarity computation and recognize the rest patches as negatives. This flexible learning paradigm obtains auxiliary contextualized information at a low cost. As the negatives own an impressive sample number, with curiosity, we make an investigation based on a question: are all negatives necessary for feature contrastive learning? Unlike previous CL approaches that use negatives as much as possible, in this paper, we study the negatives from an information-theoretic perspective and introduce a new negative Pruning technology for Unpaired image-to-image Translation (PUT) by sparsifying and ranking the patches. The proposed algorithm is efficient, flexible and enables the model to learn essential information between corresponding patches stably. By putting quality over quantity, only a few negative patches are required to achieve better results. Lastly, we validate the superiority, stability, and versatility of our model through comparative experiments.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1186–1194},
numpages = {9},
keywords = {contrastive learning, generative adversarial network, image-to-image translation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3545822.3545830,
author = {Xing, Hao},
title = {Application Study of Mobile Technology Assisted Translation Teaching Mode in Applied Universities},
year = {2022},
isbn = {9781450396424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545822.3545830},
doi = {10.1145/3545822.3545830},
abstract = {Computer Assisted Language Learning (CALL) refers to the use of computer-aided means in language learning. The development and application of wireless communication technology promotes the expansion of CALL to mobile assisted language learning (MALL). With the rapid development of information technology and the wide popularization of mobile communication equipment, the advantages of MALL are gradually revealed. Assisted by modern information tools and apps, new translation teaching recourses can be constructed, and assisted by mobile learning apps and platforms, the translation recourses can be effectively applied. Therefore, combining the advantages of MALL with the learning characteristics of translation major in applied university, it is imperative to construct and practice a new mode for translation teaching. With the assistance of mobile technology, this paper tries to construct and apply the translation teaching mode based on MALL. Furthermore, it aims to innovate translation teaching in applied universities, and improve students’ English language application ability and translation practice level.},
booktitle = {Proceedings of the 2022 7th International Conference on Multimedia Systems and Signal Processing},
pages = {35–39},
numpages = {5},
keywords = {MALL, CALL, multimodality, mobile learning apps, translation teaching mode, online learning platforms},
location = {Shenzhen, China},
series = {ICMSSP '22}
}

@article{10.1109/TASLP.2020.2976193,
author = {Liu, Chang-Le and Fu, Sze-Wei and Li, You-Jin and Huang, Jen-Wei and Wang, Hsin-Min and Tsao, Yu},
title = {Multichannel Speech Enhancement by Raw Waveform-Mapping Using Fully Convolutional Networks},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2976193},
doi = {10.1109/TASLP.2020.2976193},
abstract = {In recent years, waveform-mapping-based speech enhancement (SE) methods have garnered significant attention. These methods generally use a deep learning model to directly process and reconstruct speech waveforms. Because both the input and output are in waveform format, the waveform-mapping-based SE methods can overcome the distortion caused by imperfect phase estimation, which may be encountered in spectral-mapping-based SE systems. So far, most waveform-mapping-based SE methods have focused on single-channel tasks. In this article, we propose a novel fully convolutional network (FCN) with Sinc and dilated convolutional layers (termed SDFCN) for multichannel SE that operates in the time domain. We also propose an extended version of SDFCN, called the residual SDFCN (termed rSDFCN). The proposed methods are evaluated on three multichannel SE tasks, namely the dual-channel inner-ear microphones SE task, the distributed microphones SE task, and the CHiME-3 dataset. The experimental results confirm the outstanding denoising capability of the proposed SE systems on the three tasks and the benefits of using the residual architecture on the overall SE performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1888–1900},
numpages = {13}
}

@article{10.1145/3310331,
author = {Gr\"{o}ndahl, Tommi and Asokan, N.},
title = {Text Analysis in Adversarial Settings: Does Deception Leave a Stylistic Trace?},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3310331},
doi = {10.1145/3310331},
abstract = {Textual deception constitutes a major problem for online security. Many studies have argued that deceptiveness leaves traces in writing style, which could be detected using text classification techniques. By conducting an extensive literature review of existing empirical work, we demonstrate that while certain linguistic features have been indicative of deception in certain corpora, they fail to generalize across divergent semantic domains. We suggest that deceptiveness as such leaves no content-invariant stylistic trace, and textual similarity measures provide a superior means of classifying texts as potentially deceptive. Additionally, we discuss forms of deception beyond semantic content, focusing on hiding author identity by writing style obfuscation. Surveying the literature on both author identification and obfuscation techniques, we conclude that current style transformation methods fail to achieve reliable obfuscation while simultaneously ensuring semantic faithfulness to the original text. We propose that future work in style transformation should pay particular attention to disallowing semantically drastic changes.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {45},
numpages = {36},
keywords = {Stylometry, deception, deanonymization, author identification, text obfuscation}
}

@article{10.1109/TASLP.2020.3023632,
author = {Parthasarathy, Srinivas and Busso, Carlos},
title = {Semi-Supervised Speech Emotion Recognition With Ladder Networks},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3023632},
doi = {10.1109/TASLP.2020.3023632},
abstract = {Speech emotion recognition (SER) systems find applications in various fields such as healthcare, education, and security and defense. A major drawback of these systems is their lack of generalization across different conditions. For example, systems that show superior performance on certain databases show poor performance when tested on other corpora. This problem can be solved by training models on large amounts of labeled data from the target domain, which is expensive and time-consuming. Another approach is to increase the generalization of the models. An effective way to achieve this goal is by regularizing the models through multitask learning (MTL), where auxiliary tasks are learned along with the primary task. These methods often require the use of labeled data which is computationally expensive to collect for emotion recognition (gender, speaker identity, age or other emotional descriptors). This study proposes the use of ladder networks for emotion recognition, which utilizes an unsupervised auxiliary task. The primary task is a regression problem to predict emotional attributes. The auxiliary task is the reconstruction of intermediate feature representations using a denoising autoencoder. This auxiliary task does not require labels so it is possible to train the framework in a semi-supervised fashion with abundant unlabeled data from the target domain. This study shows that the proposed approach creates a powerful framework for SER, achieving superior performance than fully supervised single-task learning (STL) and MTL baselines. We implement the approach with sentence-level or frame-level features, demonstrating the flexibility of our approach. Additionally, the generalization of the ladder networks is evaluated in cross-corpus settings using sentence-level features, obtaining important improvements. Compared to the STL baselines, the proposed approach achieves relative gains in concordance correlation coefficient (CCC) between 3.0% and 3.5% for within corpus evaluations, and between 16.1% and 74.1% for cross corpus evaluations, highlighting the power of the architecture.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2697–2709},
numpages = {13}
}

@inproceedings{10.1145/3575693.3575744,
author = {Sun, Jinghan and Li, Shaobo and Sun, Yunxin and Sun, Chao and Vucinic, Dejan and Huang, Jian},
title = {LeaFTL: A Learning-Based Flash Translation Layer for Solid-State Drives},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575744},
doi = {10.1145/3575693.3575744},
abstract = {In modern solid-state drives (SSDs), the indexing of flash pages is a critical component in their storage controllers. It not only affects the data access performance, but also determines the efficiency of the precious in-device DRAM resource. A variety of address mapping schemes and optimizations have been proposed. However, most of them were developed with human-driven heuristics. In this paper, we present a learning-based flash translation layer (FTL), named LeaFTL, which learns the address mapping to tolerate dynamic data access patterns via linear regression at runtime. By grouping a large set of mapping entries into a learned segment, it significantly reduces the memory footprint of the address mapping table, which further benefits the data caching in SSD controllers. LeaFTL also employs various optimization techniques, including out-of-band metadata verification to tolerate mispredictions, optimized flash allocation, and dynamic compaction of learned index segments. We implement LeaFTL with both a validated SSD simulator and a real open-channel SSD board. Our evaluation with various storage workloads demonstrates that LeaFTL saves the memory consumption of the mapping table by 2.9\texttimes{} and improves the storage performance by 1.4\texttimes{} on average, in comparison with state-of-the-art FTL schemes.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {442–456},
numpages = {15},
keywords = {Solid-State Drive, Flash Translation Layer, Learning-Based Storage},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3473714.3473731,
author = {Fu, Xiang and Liu, Hongpeng and Wang, Baojun and Yuan, Guiyong and Zhang, Hao},
title = {Application Research of Speech Instruction Recognition in Human-Computer Interaction},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473731},
doi = {10.1145/3473714.3473731},
abstract = {Speech instruction recognition is an important research direction in the field of human and intelligent machine interaction. With the development of deep learning, more and more researchers pay attention to the automatic speech recognition technology based on convolutional neural network. By establishing an end-to-end speech recognition model based on deep neural network, taking logarithmic amplitude spectrum features as the network input, using connectionist temporal classifiers to carry out time sequence classification, and using convolutional neural network to deal with the correlation between frames, an end-to-end speech recognition system is implemented. On this basis, taking the speech interaction scene of intelligent home appliances in the living room as an example, the speech control instruction database of smart home appliances is constructed, and the application research of speech instruction recognition and data instruction generation is carried out. The method of transfer learning is adopted. We train a pre-training model with open source data set firstly, and then re-train the model with self-recorded speech control instruction database. The experimental results show that the proposed algorithm achieves 12.31% WER on public AISHELL-1 dataset and 2.1% WER on our self-recorded speech control instruction database. The speech instruction recognition system realized in this paper can quickly recognize speech instructions and convert them into formatted data instructions that can drive intelligent home appliances, thus improving the communication efficiency between human and machine.},
booktitle = {Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics},
pages = {97–101},
numpages = {5},
keywords = {Transfer learning, Human computer interaction, Speech instruction recognition, Convolution neural network},
location = {Guangzhou, China},
series = {ICCIR '21}
}

@article{10.1109/TASLP.2018.2887337,
author = {Zhao, Ziyue and Liu, Huijun and Fingscheidt, Tim},
title = {Convolutional Neural Networks to Enhance Coded Speech},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2887337},
doi = {10.1109/TASLP.2018.2887337},
abstract = {Enhancing coded speech suffering from far-end acoustic background noise, quantization noise, and potentially transmission errors is a challenging task. In this paper, we propose two postprocessing approaches applying convolutional neural networks either in the time domain or the cepstral domain to enhance the coded speech without any modification of the codecs. The time-domain approach follows an end-to-end fashion, whereas the cepstral domain approach uses analysis–synthesis with cepstral domain features. The proposed postprocessors in both domains are evaluated for various narrowband and wideband speech codecs in a wide range of conditions. The proposed postprocessor improves perceptual evaluation of speech quality by up to 0.25 mean opinion score listening quality objective points for G.711, 0.30 points for G.726, 0.82 points for G.722, and 0.26 points for adaptive multirate wideband codec. In a subjective comparison category rating listening test, the proposed postprocessor on G.711-coded speech exceeds the speech quality of an ITU-T-standardized postfilter by 0.36 CMOS points, and obtains a clear preference of 1.77 CMOS points compared to legacy G.711, even better than uncoded speech with statistical significance. The source code for the cepstral domain approach to enhance G.711-coded speech is made available.11https://github.com/ifnspaml/Enhancement-Coded-Speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {663–678},
numpages = {16}
}

@article{10.1109/TASLP.2020.3016487,
author = {Pandey, Ashutosh and Wang, DeLiang},
title = {On Cross-Corpus Generalization of Deep Learning Based Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3016487},
doi = {10.1109/TASLP.2020.3016487},
abstract = {In recent years, supervised approaches using deep neural networks (DNNs) have become the mainstream for speech enhancement. It has been established that DNNs generalize well to untrained noises and speakers if trained using a large number of noises and speakers. However, we find that DNNs fail to generalize to new speech corpora in low signal-to-noise ratio (SNR) conditions. In this work, we establish that the lack of generalization is mainly due to the channel mismatch, i.e. different recording conditions between the trained and untrained corpus. Additionally, we observe that traditional channel normalization techniques are not effective in improving cross-corpus generalization. Further, we evaluate publicly available datasets that are promising for generalization. We find one particular corpus to be significantly better than others. Finally, we find that using a smaller frame shift in short-time processing of speech can significantly improve cross-corpus generalization. The proposed techniques to address cross-corpus generalization include channel normalization, better training corpus, and smaller frame shift in short-time Fourier transform (STFT). These techniques together improve the objective intelligibility and quality scores on untrained corpora significantly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2489–2499},
numpages = {11}
}

@article{10.1109/TASLP.2022.3180676,
author = {Kim, Minseung and Shin, Jong Won},
title = {Improved Speech Enhancement Considering Speech PSD Uncertainty},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3180676},
doi = {10.1109/TASLP.2022.3180676},
abstract = {Speech enhancement based on statistical models has been studied for several decades. Recently, the speech enhancement adopting a speech power spectral density (PSD) uncertainty model has been proposed. This approach distinguishes the true speech PSD from its estimate and considers both as random variables. It incorporates a prior distribution of speech spectra and speech PSD estimators to derive the PSD uncertainty-aware counterpart to conventional clean speech estimators, which results in performance improvement. However, the speech PSD uncertainty model has not yet been adopted for parameter estimations such as <inline-formula><tex-math notation="LaTeX">$mathit{a ; posteriori}$</tex-math></inline-formula> speech presence probability (SPP), noise PSD, and speech power spectra estimations in the speech enhancement framework. In this paper, we incorporate the speech PSD uncertainty model to all the components of the statistical model-based speech enhancement framework by deriving PSD uncertainty-aware counterparts to conventional parameter estimators. Specifically, we derive the <inline-formula><tex-math notation="LaTeX">$mathit{a; posteriori}$</tex-math></inline-formula> SPP where the likelihood function for each hypothesis is based on the speech PSD uncertainty. With this <inline-formula><tex-math notation="LaTeX">$mathit{a; posteriori}$</tex-math></inline-formula> SPP, a novel SPP-based noise PSD estimator is derived. Also, we derive the minimum mean-square error (MMSE) estimator for the power spectrum of the clean speech in the current frame under speech PSD uncertainty which is exploited to refine the speech PSD estimator. Finally, the refined speech PSD estimator is incorporated into the spectral gain function based on the speech PSD uncertainty model. The proposed approach showed improved noise PSD estimation performance in terms of the averaged logarithmic error distance, and improved speech enhancement performance in terms of the noise reduction, segmental signal-to-noise ratio, perceptual evaluation of speech quality (PESQ) scores and short-time objective intelligibility in our experiments. It also exhibited comparable performance with a real-time deep learning-based speech enhancement system in terms of the PESQ scores and composite measures for the VoiceBank-DEMAND dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1939–1951},
numpages = {13}
}

@inproceedings{10.1145/3419635.3419729,
author = {Tianmei, Bao},
title = {Research on English Vocabulary Translation Technology Based on Dynamic Bilingual Corpus},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419729},
doi = {10.1145/3419635.3419729},
abstract = {The goal of this study is to use modern Internet technology to collect dynamic English-language contrastive corpora in a large-scale information database, build a bilingual dynamic corpus in Chinese and English, and use bilingual translation memory to complete bilingual assistance based on computer-assisted translation Construction of translation search engine system. The main content of this article mainly includes the design of constructing the corpus and the implementation of the translation search engine system. In terms of building a dynamic corpus, according to the general process of Web content mining, the paper first uses web spiders to search and collect raw data in the Internet's massive information database, and then preprocesses and rearranges them. After purification and identification, the Two-page bilingual texts and two-page bilingual texts are used for word segmentation matching and web page structure matching, respectively. Chinese-English bilingual translation corpora are extracted and stored in the database. The deployment of the auxiliary translation search engine system is based on the construction of a dynamic corpus. The indexing tool is used to index the corpus, and manual review and user feedback modules are added to provide users with Chinese-English bilingual auxiliary translation services.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {288–291},
numpages = {4},
keywords = {Dynamic Bilingual Corpus, English Vocabulary Translation, Web Content Mining},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/3578356.3592591,
author = {Shi, Hongrui and Radu, Valentin and Yang, Po},
title = {Distributed Training for Speech Recognition Using Local Knowledge Aggregation and Knowledge Distillation in Heterogeneous Systems},
year = {2023},
isbn = {9798400700842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578356.3592591},
doi = {10.1145/3578356.3592591},
abstract = {Data privacy and data protection are crucial issues for automatic speech recognition (ASR) system when relying on client generated data for training. The best protection is achieved when training is distributed fashion, close to the client local data, rather than centralising the training. However, distributed training suffers from system heterogeneity, due to clients having unequal computation resources, and data heterogeneity, due to training data being non-independent and identically distributed (non-IID). To tackle these challenges, we introduce FedKAD, a Federated Learning (FL) framework that uses local Knowledge Aggregation over top level feature maps and Knowledge Distillation. We show that our FedKAD achieves better communication efficiency than standard FL methods that use uniform models, due to transferring parameters of smaller size client models, and overall better accuracy than FedMD, an alternative KD-based approach designed for heterogeneous data. Our work enables faster, cheaper and more inclusive participation of clients in heterogeneous distributed training.},
booktitle = {Proceedings of the 3rd Workshop on Machine Learning and Systems},
pages = {64–70},
numpages = {7},
keywords = {heterogeneous systems, speech recognition, distributed training, federated learning},
location = {Rome, Italy},
series = {EuroMLSys '23}
}

@inproceedings{10.1145/3579371.3589051,
author = {Jin, Wenjing and Jang, Wonsuk and Park, Haneul and Lee, Jongsung and Kim, Soosung and Lee, Jae W.},
title = {DRAM Translation Layer: Software-Transparent DRAM Power Savings for Disaggregated Memory},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589051},
doi = {10.1145/3579371.3589051},
abstract = {Memory disaggregation is a promising solution to scale memory capacity and bandwidth shared by multiple server nodes in a flexible and cost-effective manner. DRAM power consumption, which is reported to be around 40% of the total system power in the datacenter server, will become an even more serious concern in this high-capacity environment. Exploiting the low average utilization of DRAM capacity in today's datacenters, it is appealing to put unallocated/cold DRAM ranks into a power-saving mode. However, the conventional DRAM address mapping with fine-grained interleaving to maximize rank-level parallelism is incompatible with such rank-level DRAM power management techniques. Furthermore, existing DRAM power-saving techniques often require intrusive changes to the system stack, including OS, memory controller (MC), or even DRAM devices, to pose additional challenges for deployment. Thus, we propose DRAM Translation Layer (DTL) for host software/MC-transparent DRAM power management with commodity DRAM devices. Inspired by Flash Translation Layer (FTL) in modern SSDs, DTL is placed in the CXL memory controller to provide (i) flexible address mappings between host physical address and DRAM device physical address and (ii) host-transparent memory page migration. Leveraging DTL, we propose two DRAM power-saving techniques with different temporal granularities to maximize the number of DRAM ranks that can enter low-power states while provisioning sufficient DRAM bandwidth: rank-level power-down and hotness-aware self-refresh. The first technique consolidates unallocated memory pages into a subset of ranks at deallocation of a virtual machine (VM) and turns them off transparently to both OS and host MC. Our evaluation with CloudSuite benchmarks demonstrates that this technique saves DRAM power by 31.6% on average at a 1.6% performance cost. The hotness-aware self-refresh scheme further reduces DRAM energy consumption by up to 14.9% with negligible performance loss via opportunistically migrating cold pages into a rank and making it enter self-refresh mode.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {16},
numpages = {13},
keywords = {pooled memory, power management, datacenters, disaggregated memory, CXL, DRAM, address translation},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@article{10.1109/TASLP.2021.3061939,
author = {Birnie, Lachlan and Abhayapala, Thushara and Tourbabin, Vladimir and Samarasinghe, Prasanga},
title = {Mixed Source Sound Field Translation for Virtual Binaural Application With Perceptual Validation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3061939},
doi = {10.1109/TASLP.2021.3061939},
abstract = {Non-interactive and linear experienceslike cinema film offer high quality surround sound audio to enhance immersion, however, the perspective is usually fixed to the recording microphone position. With the rise of virtual reality, there is a demand for recording and recreating real-world experiences that allow users to move throughout the reproduction. Sound field translation achieves this by building an equivalent environment of virtual sources to recreate the recording spatially. However, the technique remains to restrict the maximum distance a user can translate away from the recording microphone's perspective due to the discrete sampling by commercial higher order microphones only being capable of recording an acoustic sweet-spot. In this paper, we propose a method for binaurally reproducing a microphone recording in a virtual application that allows the user to freely translate their body further beyond the recording position. The method incorporates a mixture of near-field and far-field sources in a sparsely expanded virtual environment to maintain a perceptually accurate reproduction. We perceptually validate the method through a Multiple Stimulus with Hidden Reference and Anchor (MUSHRA) experiment. Compared to the planewave benchmark, the proposed method offers both improved source localizability and robustness to spectral distortions at translated listening positions. A cross-examination with numerical simulations demonstrated that the sparse expansion relaxes the inherent sweet-spot constraint, leading to the improved localizability for sparse environments. Additionally, the proposed method is seen to better reproduce the intensity and binaural room impulse response spectra of near-field environments, further supporting the perceptual results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1188–1203},
numpages = {16}
}

@inproceedings{10.1145/3335595.3335632,
author = {Tessore, Juan Pablo and Esnaola, Leonardo Mart\'{\i}n and Russo, Claudia Cecilia and Baldassarri, Sandra},
title = {Comparative Analysis of Preprocessing Tasks over Social Media Texts in Spanish},
year = {2019},
isbn = {9781450371766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335595.3335632},
doi = {10.1145/3335595.3335632},
abstract = {One of the key aspects of the texts coming from social media is that they tend to be very noisy. This is mainly because of the usage of informal language and non-standard grammatical structures. Therefore, in order to use these contents as input for a text analysis process, it is highly recommended to previously clean and reduce the noise of the data. This work focuses on measuring the effectiveness that diverse cleaning and repairing tasks have on the data. The results obtained, indicate that the tasks of "tokens with no letters removal", and "stressed words processing" are the most effective. In addition, some tasks like hashtags or usernames processing, which behave very well in other datasets, are not that relevant in this one. This research is part of a more general one that pursues to build an automatic emotion classifier that makes use of the preprocessed comments as input.},
booktitle = {Proceedings of the XX International Conference on Human Computer Interaction},
articleno = {27},
numpages = {8},
keywords = {Sentiment Analysis, Text mining, Text classification, Text preprocessing},
location = {Donostia, Gipuzkoa, Spain},
series = {Interacci\'{o}n '19}
}

@inproceedings{10.1145/3474085.3475456,
author = {Jin, Tao and Zhao, Zhou},
title = {Contrastive Disentangled Meta-Learning for Signer-Independent Sign Language Translation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475456},
doi = {10.1145/3474085.3475456},
abstract = {Sign language translation aims at directly translating a sign language video into a natural sentence. The majority of existing methods take the video-sentence pairs labeled by multiple specific signers as training and testing samples. However, such setting does not fit in with the real-world applications. A practicable sign language translation system is supposed to provide accurate translation results for unseen signers. In this paper, we mainly attack the signer-independent setting and focus on augmenting the generalization ability of translation model. To adapt to the challenging setting, we propose a novel framework called contrastive disentangled meta-learning (CDM), which develops several improvements in both deep architecture and training mode. Specifically, based on the minimax entropy objective, a disentangled module with adaptive gated units is developed to decouple the signer-specific and task-specific representation in the encoder. Besides, we facilitate the frame-word alignments by leveraging contrastive constraints between the obtained task-specific representation and the decoding output. The disentangled and contrastive modules could provide complementary information for each other. As for the training mode, we encourage the model to perform well in the simulated signer-independent scenarios by finding the generalized learning directions in the meta-learning process. Considering that vanilla meta-learning methods utilize the multiple specific signers insufficiently, we adopt a fine-grained learning strategy that simultaneously conducts meta-learning in a variety of domain shift scenarios in each iteration. Extensive experiments on the benchmark dataset RWTH-PHOENIX-Weather-2014T(PHOENIX14T) show that CDM could achieve competitive results compared with the state-of-the-art methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5065–5073},
numpages = {9},
keywords = {contrastive learning, meta learning, sign language translation},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3472749.3474771,
author = {Li, Daniel and Chen, Thomas and Tung, Albert and Chilton, Lydia B},
title = {Hierarchical Summarization for Longform Spoken Dialog},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474771},
doi = {10.1145/3472749.3474771},
abstract = {Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {582–597},
numpages = {16},
keywords = {machine learning applications, summarization, information retrieval, natural language interaction, automatic speech recognition},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1109/TASLP.2020.2971417,
author = {Perrotin, Olivier and McLoughlin, Ian V.},
title = {Glottal Flow Synthesis for Whisper-to-Speech Conversion},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2971417},
doi = {10.1109/TASLP.2020.2971417},
abstract = {Whisper-to-speech conversion is motivated by laryngeal disorders, in which malfunction of the vocal folds leads to loss of voicing. Many patients with laryngeal disorders can still produce functional whispers, since these are characterised by the absence of vocal fold vibration. Whispers therefore constitute a common ground for speech rehabilitation across many kinds of laryngeal disorder. Whisper-to-speech conversion involves recreating natural-sounding speech from recorded whispers, and is a non-invasive and non-surgical rehabilitation that can maintain a natural method of speaking, unlike the existing methods of rehabilitation. This article proposes a new rule-based method for whisper-to-speech conversion that replaces the noisy whisper sound source with a synthesised speech-like harmonic source, while maintaining the vocal tract component unaltered. In particular, a novel glottal source generator is developed in which whisper information is used to parameterise the excitation through a high-quality glottis model. Evaluation of the system against the standard pulse train excitation method reveals significantly improved performance. Since our method is glottis-based, it is potentially compatible with the many existing vocal tract component adaptation systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {889–900},
numpages = {12}
}

@article{10.1145/3301488,
author = {Liu, Yu-Ping and Hong, Ding-Yong and Wu, Jan-Jan and Fu, Sheng-Yu and Hsu, Wei-Chung},
title = {Exploiting SIMD Asymmetry in ARM-to-X86 Dynamic Binary Translation},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3301488},
doi = {10.1145/3301488},
abstract = {Single instruction multiple data (SIMD) has been adopted for decades because of its superior performance and power efficiency. The SIMD capability (i.e., width, number of registers, and advanced instructions) has diverged rapidly on different SIMD instruction-set architectures (ISAs). Therefore, migrating existing applications to another host ISA that has fewer but longer SIMD registers and more advanced instructions raises the issues of asymmetric SIMD capability. To date, this issue has been overlooked and the host SIMD capability is underutilized, resulting in suboptimal performance. In this article, we present a novel binary translation technique called spill-aware superword level parallelism (saSLP), which combines short ARMv8 instructions and registers in the guest binaries to exploit the x86 AVX2 host’s parallelism, register capacity, and gather instructions. Our experiment results show that saSLP improves the performance by 1.6\texttimes{} (2.3\texttimes{}) across a number of benchmarks and reduces spilling by 97% (99%) for ARMv8 to x86 AVX2 (AVX-512) translation. Furthermore, with AVX2 (AVX-512) gather instructions, saSLP speeds up several data-irregular applications that cannot be vectorized on ARMv8 NEON by up to 3.9\texttimes{} (4.2\texttimes{}).},
journal = {ACM Trans. Archit. Code Optim.},
month = {feb},
articleno = {2},
numpages = {24},
keywords = {SLP vectorization, Dynamic binary translation, SIMD}
}

@inproceedings{10.1145/3508230.3508239,
author = {Ambrocio Sagum, Ria and A. Lopez, Kier Bryan and D. Bonzol, Steven John and L. Pidlaoan, Wendell Vryan},
title = {TAGSYNTA: Tagalog Speech Synthesizer Using Part of Speech in Prosody Analyzation},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508239},
doi = {10.1145/3508230.3508239},
abstract = {In this paper, we developed a system called, TagSynta: Tagalog Text to Speech Synthesizer using Part of Speech in Prosody Analyzation, which is a Tagalog Text-to-Speech Synthesizer with proper intonation of the word given a Tagalog sentence. It used Part of Speech as its identifier to reduce the reuse of audio processor for faster output which aims to solve the problem of accuracy in an automatic Text-to-Speech conversion of a Tagalog language. The system obtained 92.50% in converting a text into speech and 96.63% in giving a correct prosody of the words. The respondents of the research agreed that the system's output was above average in terms of the naturalness of the voice produced by the system.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {61–66},
numpages = {6},
keywords = {Speech Synthesizer using Hidden Markov Model, Prosody Analysis using Part of Speech, Text to Speech using Prosody, Tagalog Text to Speech Synthesizer},
location = {Sanya, China},
series = {NLPIR '21}
}

@inproceedings{10.1145/3510858.3510992,
author = {Lu, Nan and Liu, Lei},
title = {Application of Intelligent Translation Software for English Translation Based on Data Management Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510992},
doi = {10.1145/3510858.3510992},
abstract = {With the development of new technologies related to computers, data in all walks of life has grown rapidly, and people's requirements for the use and storage time of data have become higher and higher, making efficient management of data and accurate and fast querying of data have become a social hot spot. Especially the application of data management technology in English-to-English intelligent translation software has attracted the attention of many educators. This article mainly summarizes the characteristics of data management technology and English-to-English intelligent translation software, as well as the advantages and main applications of data management technology in English-to-English intelligent translation software. Based on this, this article issued 300 questionnaires based on the current situation of college students using English-to-English intelligent translation software. The analysis of the survey results showed that: 81.15% of the people who have used Youdao's English-to-English intelligent translation software, it is the most used English-to-English intelligent translation software, which shows that the choices of different individuals are affected by personal preferences; the number of language majors using Sogou's English translation intelligent translation software is 34.57%, and the number of non-language majors using the software is only 10.26%, indicating that language majors have a higher demand for translation software; Although computer translation is stable, only 38 people use the English-to-English intelligent translation software through the computer, and 143 people use the English-to-English intelligent translation software through the mobile phone, indicating that students tend to choose more convenient equipment for English translation learning.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {486–490},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3267935.3267947,
author = {Xue, Liumeng and Zhu, Xiaolian and An, Xiaochun and Xie, Lei},
title = {A Comparison of Expressive Speech Synthesis Approaches Based on Neural Network},
year = {2018},
isbn = {9781450359856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267935.3267947},
doi = {10.1145/3267935.3267947},
abstract = {Adaptability and controllability in changing speaking styles and speaker characteristics are the advantages of deep neural networks (DNNs) based statistical parametric speech synthesis (SPSS). This paper presents a comprehensive study on the use of DNNs for expressive speech synthesis with a small set of emotional speech data. Specifically, we study three typical model adaptation approaches: (1) retraining a neural model by emotion-specific data (retrain), (2) augmenting the network input using emotion-specific codes (code) and (3) using emotion-dependent output layers with shared hidden layers (multi-head). Long-short term memory (LSTM) networks are used as the acoustic models. Objective and subjective evaluations have demonstrated that the multi-head approach consistently outperforms the other two approaches with more natural emotion delivered in the synthesized speech.},
booktitle = {Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data},
pages = {15–20},
numpages = {6},
keywords = {statistical parametric speech synthesis, code, multi-head network, text-to-speech, retrain, neural networks, expressive speech synthesis},
location = {Seoul, Republic of Korea},
series = {ASMMC-MMAC'18}
}

@inproceedings{10.1145/3342827.3342847,
author = {Chao, Guan-Lin and Shen, John Paul and Lane, Ian},
title = {Deep Speaker Embedding for Speaker-Targeted Automatic Speech Recognition},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342847},
doi = {10.1145/3342827.3342847},
abstract = {In this work, we investigate three types of deep speaker embedding as text-independent features for speaker-targeted speech recognition in cocktail party environments. The text-independent speaker embedding is extracted from the target speaker's existing speech segment (i-vector and x-vector) or face image (f-vector), which is concatenated with acoustic features of any new speech utterances as input features. Since the proposed model extracts the speaker embedding of the target speaker once and for all, it is computationally more efficient than many prior approaches which estimate the target speaker's characteristics on the fly. Empirical evaluation shows that using speaker embedding along with acoustic features improves Word Error Rate over the audio-only model, from 65.7% to 29.5%. Among the three types of speaker embedding, x-vector and f-vector show robustness against environment variations while i-vector tends to overfit to the specific speaker and environment condition.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {39–43},
numpages = {5},
keywords = {acoustic modeling, speaker-targeted speech recognition, robust speaker embeddings},
location = {Tokushima, Japan},
series = {NLPIR '19}
}

@article{10.1145/3529394,
author = {Jaiswal, Rahul Kumar and Dubey, Rajesh Kumar},
title = {CAQoE: A Novel No-Reference Context-Aware Speech Quality Prediction Metric},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3529394},
doi = {10.1145/3529394},
abstract = {The quality of speech degrades while communicating over Voice over Internet Protocol applications, for example, Google Meet, Microsoft Skype, and Apple FaceTime, due to different types of background noise present in the surroundings. It reduces human perceived Quality of Experience (QoE). Along this line, this article proposes a novel speech quality prediction metric that can meet human’s desired QoE level. Our motivation is driven by the lack of evidence showing speech quality metrics that can distinguish different noise degradations before predicting the quality of speech. The quality of speech in noisy environments is improved by speech enhancement algorithms, and for measuring and monitoring the quality of speech, objective speech quality metrics are used. With the integration of these components, a novel no-reference context-aware QoE prediction metric (CAQoE) is proposed in this article, which initially identifies the context or noise type or degradation type of the input noisy speech signal and then predicts context-specific speech quality for that input speech signal. It will have of great importance in deciding the speech enhancement algorithms if the types of degradations causing poor speech quality are known along with the quality metric. Results demonstrate that the proposed CAQoE metric outperforms in different contexts as compared to the metric where contexts are not identified before predicting the quality of speech, even in the presence of limited size speech corpus having different contexts available from the NOIZEUS speech database.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {35},
numpages = {23},
keywords = {quality of experience (QoE), VoIP, speech quality, Classifier, deep neural network, voice activity detector, no-reference, speech enhancement}
}

@inproceedings{10.5555/3523760.3523999,
author = {Laban, Guy and Le Maguer, Sebastien and Lee, Minha and Kontogiorgos, Dimosthenis and Reig, Samantha and Torre, Ilaria and Tejwani, Ravi and Dennis, Matthew J. and Pereira, Andre},
title = {Robo-Identity: Exploring Artificial Identity and Emotion via Speech Interactions},
year = {2022},
publisher = {IEEE Press},
abstract = {Following the success of the first edition of Robo-Identity, the second edition will provide an opportunity to expand the discussion about artificial identity. This year, we are focusing on emotions that are expressed through speech and voice. Synthetic voices of robots can resemble and are becoming indistinguishable from expressive human voices. This can be an opportunity and a constraint in expressing emotional speech that can (falsely) convey a human-like identity that can mislead people, leading to ethical issues. How should we envision an agent's artificial identity? In what ways should we have robots that maintain a machine-like stance, e.g., through robotic speech, and should emotional expressions that are increasingly human-like be seen as design opportunities? These are not mutually exclusive concerns. As this discussion needs to be conducted in a multidisciplinary manner, we welcome perspectives on challenges and opportunities from variety of fields. For this year's edition, the special theme will be "speech, emotion and artificial identity''.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1265–1268},
numpages = {4},
keywords = {voice, emotion, artificial identity, affective science, affective computing, human-robot interaction, speech},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3341981.3344236,
author = {Bonab, Hamed and Allan, James and Sitaraman, Ramesh},
title = {Simulating CLIR Translation Resource Scarcity Using High-Resource Languages},
year = {2019},
isbn = {9781450368810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341981.3344236},
doi = {10.1145/3341981.3344236},
abstract = {We study the impact of translation resource scarcity on the performance of cross-language information retrieval (CLIR) systems. To do that, we develop a contrastive analysis framework that uses high-resource languages to simulate low-resource languages. In the framework, we focus on parallel translation corpora and aim to better understand the factors that impact CLIR performance. We argue that both low- and high-resource corpora are needed to develop that understanding. Hence, we take the approach of starting with a true low-resource language and systematically down-sampling a high-resource language to become an artificial low-resource language-the reverse perspective of existing research. We formalize the problem as the Resource Scarcity Simulation (RSS) problem. We model the problem with a family of set covering problems, formulate with integer linear programming, and prove that the problem is actually NP-hard. To this end, we provide two greedy algorithms with polynomial complexities. We compare and analyze our approach with alternate techniques using four high-resource languages (French, Italian, German, and Finnish) down-sampled to simulate two low-resource languages (Somali and Swahili). Our experimental results suggest that language families are important for the RSS problem. We simulate Somali with German, and Swahili with Finnish, achieving 98% and 97% on the similarity percentage in terms of CLIR performance, respectively.},
booktitle = {Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {129–136},
numpages = {8},
keywords = {language simulation, low-resource languages, cross-lingual information retrieval, translation resources},
location = {Santa Clara, CA, USA},
series = {ICTIR '19}
}

@article{10.1109/TASLP.2021.3078883,
author = {Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong},
title = {Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078883},
doi = {10.1109/TASLP.2021.3078883},
abstract = {Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on <italic>TF masking</italic>, <italic>Filter&amp;Sum</italic> and <italic>mask-based MVDR</italic> neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {2067–2082},
numpages = {16}
}

@inproceedings{10.1145/3459637.3482395,
author = {Singla, Yaman Kumar and Gupta, Avyakt and Bagga, Shaurya and Chen, Changyou and Krishnamurthy, Balaji and Shah, Rajiv Ratn},
title = {Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482395},
doi = {10.1145/3459637.3482395},
abstract = {Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a candidate's speaking proficiency in a language. ASS systems face many challenges like open grammar, variable pronunciations, and unstructured or semi-structured content. Recent deep learning approaches have shown some promise in this domain. However, most of these approaches focus on extracting features from single audio, making them suffer from the lack of speaker-specific context required to model such a complex task. We propose a novel deep learning technique for non-native ASS, called speaker-conditioned hierarchical modelling. In our technique, we take advantage of the fact that oral proficiency tests rate multiple responses for a candidate. We extract context vectors from these responses and feed them as additional speaker-specific context to our network to score a particular response. We compare our technique with strong baselines and find that such modelling improves the model's average performance by 6.92% (maximum = 12.86%, minimum = 4.51%). We further show both quantitative and qualitative insights into the importance of this additional context in solving the problem of ASS.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1681–1691},
numpages = {11},
keywords = {spontaneous speech, hierarchical modeling, ai in education, multi-modal deep learning, end-to-end neural networks, automated speech scoring, interpretability in ai},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3616373,
author = {Han, Lei},
title = {Deep Neural Network-Based Mixed Speech Recognition Technology for Chinese and English},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616373},
doi = {10.1145/3616373},
abstract = {In the field of human-computer interaction, the current more advanced speech recognition systems are all single speech recognition, and it is urgent to adopt new in-depth learning technology to improve the existing speech recognition system. In this context, this research is based on DNN and investigates mixed speech recognition techniques for both Chinese and English. A single speech recognition algorithm based on DNN is first investigated, and then a new hybrid Chinese and English speech recognition model is constructed by fusing the attention mechanism and CTC loss function. In the construction of the hybrid speech recognition model, the end-to-end model and Transformer framework are used to combine the monotonic alignment property of the CTC loss function, which allows complex sound units to be transformed into characters for easy extraction and recognition. The performance of the constructed models was tested on Chinese speech dataset, English speech dataset and mixed Chinese and English speech dataset to determine the recognition accuracy and speed of the models. The results show that the proposed recognition model achieves 81.2% recognition accuracy and 100 recognition speed/minute on the Chinese-English mixed speech dataset, which is much better than the other three models. This study successfully addresses the need for improved speech recognition systems by introducing a novel hybrid model for mixed Chinese-English speech recognition. The experimental results confirm the superiority of the proposed model, achieving high accuracy and rapid recognition speed. The developed model holds promising potential for enhancing human-computer interaction and enabling efficient communication between Chinese and English speakers.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
keywords = {Speech recognition, End-to-end architecture, DNN, Mixed language}
}

@inproceedings{10.1145/3395035.3425186,
author = {Lyakso, Elena E.},
title = {Speech Acquisition in Children with Typical and Atypical Development},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425186},
doi = {10.1145/3395035.3425186},
abstract = {The keynote will present comparative experimental data on the formation of speech and communication skills of typically developing children and children with atypical development - with Autism Spectrum Disorders, Down syndrome, and intellectual disabilities. Specificity of the analysis of children's speech will be noted, databases of children's speech and their use will be presented. The main emphasis will be placed on the reflection in the characteristics of the voice of the pathological states of infants and children, on the revealing biomarkers of diseases according to the features of the speech and voice of children.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {524–525},
numpages = {2},
keywords = {speech ontogenesis and dysontogenesis, linguistics, phonetics, child speech acoustics},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1145/3281664,
author = {Hong, Ding-Yong and Wu, Jan-Jan and Liu, Yu-Ping and Fu, Sheng-Yu and Hsu, Wei-Chung},
title = {Processor-Tracing Guided Region Formation in Dynamic Binary Translation},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3281664},
doi = {10.1145/3281664},
abstract = {Region formation is an important step in dynamic binary translation to select hot code regions for translation and optimization. The quality of the formed regions determines the extent of optimizations and thus determines the final execution performance. Moreover, the overall performance is very sensitive to the formation overhead, because region formation can have a non-trivial cost. For addressing the dual issues of region quality and region formation overhead, this article presents a lightweight region formation method guided by processor tracing, e.g., Intel PT. We leverage the branch history information stored in the processor to reconstruct the program execution profile and effectively form high-quality regions with low cost. Furthermore, we present the designs of lightweight hardware performance monitoring sampling and the branch instruction decode cache to minimize region formation overhead. Using ARM64 to x86-64 translations, the experiment results show that our method achieves a performance speedup of up to 1.53\texttimes{} (1.16\texttimes{} on average) for SPEC CPU2006 benchmarks with reference inputs, compared to the well-known software-based trace formation method, Next Executing Tail (NET). The performance results of x86-64 to ARM64 translations also show a speedup of up to 1.25\texttimes{} over NET for CINT2006 benchmarks with reference inputs. The comparison with a relaxed NETPlus region formation method further demonstrates that our method achieves the best performance and lowest compilation overhead.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {52},
numpages = {25},
keywords = {Dynamic binary translation, region formation, processor tracing, hardware performance monitoring, next executing tail}
}

@inproceedings{10.1145/3511176.3511206,
author = {Zhou, Lijuan and Jiao, Xuri and Liu, Tao and Niu, Changyong},
title = {Sign Language Video Translation Based on BERT-SLSTM and Reinforcement Learning},
year = {2022},
isbn = {9781450385893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511176.3511206},
doi = {10.1145/3511176.3511206},
abstract = {This paper proposes a novel method based on BERT-Stacked LSTM and Reinforcement Learning (BSRL) model for joint sign language recognition and sign language translation. The proposed model applies the BERT model as the encoder to process the features of multi-channel communication of sign language videos. The stacked LSTM is applied as the decoder to effectively extract the text representation. A reinforcement learning algorithm based on self-critical sequence training is applied to model parameters based on evaluation indicators so that the generated texts have a high similarity to the ground truth texts. Experiment results on Phoenix2014T dataset demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 2021 5th International Conference on Video and Image Processing},
pages = {198–204},
numpages = {7},
keywords = {Reinforcement learning, Video description generation, Sign language translation},
location = {Hayward, CA, USA},
series = {ICVIP '21}
}

@article{10.1109/TASLP.2022.3205757,
author = {Zezario, Ryandhimas E. and Fu, Szu-Wei and Chen, Fei and Fuh, Chiou-Shann and Wang, Hsin-Min and Tsao, Yu},
title = {Deep Learning-Based Non-Intrusive Multi-Objective Speech Assessment Model With Cross-Domain Features},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3205757},
doi = {10.1109/TASLP.2022.3205757},
abstract = {This study proposes a cross-domain multi-objective speech assessment model, called MOSA-Net, which can simultaneously estimate the speech quality, intelligibility, and distortion assessment scores of an input speech signal. MOSA-Net comprises a convolutional neural network and bidirectional long short-term memory architecture for representation extraction, and a multiplicative attention layer and a fully connected layer for each assessment metric prediction. Additionally, cross-domain features (spectral and time-domain features) and latent representations from self-supervised learned (SSL) models are used as inputs to combine rich acoustic information to obtain more accurate assessments. Experimental results show that in both seen and unseen noise environments, MOSA-Net can improve the linear correlation coefficient (LCC) scores in perceptual evaluation of speech quality (PESQ) prediction, compared to Quality-Net, an existing single-task model for PESQ prediction, and improve LCC scores in short-time objective intelligibility (STOI) prediction, compared to STOI-Net, an existing single-task model for STOI prediction. Moreover, MOSA-Net can be used as a pre-trained model to be effectively adapted to an assessment model for predicting subjective quality and intelligibility scores with a limited amount of training data. Experimental results show that MOSA-Net can improve LCC scores in mean opinion score (MOS) predictions, compared to MOS-SSL, a strong single-task model for MOS prediction. We further adopt the latent representations of MOSA-Net to guide the speech enhancement (SE) process and derive a quality-intelligibility (QI)-aware SE (QIA-SE) approach. Experimental results show that QIA-SE outperforms the baseline SE system with improved PESQ scores in both seen and unseen noise environments over a baseline SE model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {54–70},
numpages = {17}
}

@article{10.1145/3608950,
author = {Gao, Ziqi},
title = {Chinese Speech Enhancement and Adaptive Recognition Technology for Complex Language Environments},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3608950},
doi = {10.1145/3608950},
abstract = {The development of intelligent technology has also made rapid progress in relevant speech fields. In order to increase the application scenarios of speech recognition systems, the research has improved the traditional Speech enhancement algorithm, namely the Ideal Binary Mask (IBM) algorithm, and combined it with the unimproved IBM algorithm to propose an adaptive IBM algorithm. Based on this algorithm, the research has built a new speech recognition system, The system uses an FIR filter to realize pre-emphasis processing and uses Berouti spectral subtraction to preprocess speech. The Speech enhancement model is built using a deep learning network model. The results showed that the IBM algorithm had the highest score in the Perceptual Evaluation of Speech Quality (PESQ) at 3.5596, followed by the Ideal Ratio Mask (IRM) algorithm at 3.3429. The improvement of the IBM algorithm was feasible when the noise intensity coefficient was greater than 0.008. When the noise intensity coefficient was greater than 0.08, the average score of the improved IBM algorithm was 2.1079, and the average score of the unimproved IBM algorithm was 1.9418. The proposed adaptive IBM algorithm has higher performance in complex speech environments compared to the original system.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
keywords = {IRM, IBM, Berouti spectral subtraction, Speech recognition system, PESQ, Speech Enhancement, Adaptive recognition}
}

@article{10.1109/TASLP.2019.2895969,
author = {Abel, Johannes and Fingscheidt, Tim},
title = {Sinusoidal-Based Lowband Synthesis for Artificial Speech Bandwidth Extension},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895969},
doi = {10.1109/TASLP.2019.2895969},
abstract = {Conventional narrowband NB telephony suffers from limited acoustic bandwidth at the receiver side, leading to degraded speech quality and intelligibility. In this paper, artificial speech bandwidth extension ABE of NB speech toward missing frequencies below about 300 Hz low-frequency band, LB is proposed to enhance the speech quality. The LB-ABE in this paper is employed together with a preexisting ABE toward high-frequency components to obtain spectrally balanced speech signals. In an instrumental quality assessment, the spectral distance in the LB was improved by more than 5 dB compared to NB speech. In a subjective listening test, the gap of speech quality between wideband and NB speech was significantly reduced when employing the proposed ABE toward low frequencies. The LB extension was found to further improve the preexisting ABE toward higher frequencies by a significant 0.26 CMOS points.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {765–776},
numpages = {12}
}

@inproceedings{10.1145/3544548.3581385,
author = {Reitmaier, Thomas and Wallington, Electra and Klejch, Ond\v{r}ej and Markl, Nina and Lam-Yee-Mui, L\'{e}a-Marie and Pearson, Jennifer and Jones, Matt and Bell, Peter and Robinson, Simon},
title = {Situating Automatic Speech Recognition Development within Communities of Under-Heard Language Speakers},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581385},
doi = {10.1145/3544548.3581385},
abstract = {In this paper we develop approaches to automatic speech recognition (ASR) development that suit the needs and functions of under-heard language speakers. Our novel contribution to HCI is to show how community-engagement can surface key technical and social issues and opportunities for more effective speech-based systems. We introduce a bespoke toolkit of technologies and showcase how we utilised the toolkit to engage communities of under-heard language speakers; and, through that engagement process, situate key aspects of ASR development in community contexts. The toolkit consists of (1) an information appliance to facilitate spoken-data collection on topics of community interest, (2) a mobile app to create crowdsourced transcripts of collected data, and (3) demonstrator systems to showcase ASR capabilities and to feed back research results to community members. Drawing on the sensibilities we cultivated through this research, we present a series of challenges to the orthodoxy of state-of-the-art approaches to ASR development.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {406},
numpages = {17},
keywords = {automatic speech recognition, Text/speech/language, mobile devices: phones/tablets},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1109/TASLP.2020.2996503,
author = {Tu, Yan-Hui and Du, Jun and Gao, Tian and Lee, Chin-Hui},
title = {A Multi-Target SNR-Progressive Learning Approach to Regression Based Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2996503},
doi = {10.1109/TASLP.2020.2996503},
abstract = {We propose a multi-target, signal-to-noise-ratio (SNR)-progressive learning (SNR-PL) framework for regression based speech enhancement (SE). At low SNR levels, it is often not easy to directly learn the complicated regression required in SE. We therefore decompose the original SE problem of mapping noisy to clean speech features, with a large SNR gap, into a series of sub-problems, each with a small SNR increment and presumably easier to learn. In our configurations, each hidden layer of the proposed regression neural network is guided to explicitly learn an intermediate target with a specified but small SNR gain. Tested on both deep neural network (DNN) and long short-term memory (LSTM) architectures, SNR-PL consistently outperforms the conventional “black box” DNN framework in terms of both objective measure superiority and network model compactness. Furthermore, with the best configured LSTM-based SNR-PL model, we often observe that the performance is easily saturated or even degraded when increasing the number of intermediate targets, due to the fact that useful information is lost in dimension reduction when involving more target layers. Accordingly, to address this information loss issue, we explore densely connected networks on top of the LSTM structure where the input and the preceding intermediate targets are concatenated together to learn the next target. Finally, to fully utilize the rich and complementary information of intermediate targets, a simple post-processing strategy is adopted to further improve the performance. Evaluated on the simulation speech data, experimental results in unseen noises cases demonstrate that the proposed approach consistently performs better than the conventional LSTM approach in terms of objective speech enhancement measures for speech intelligibility and quality. Furthermore, when evaluated on real data provided by the CHiME-4 Challenge for automatic speech recognition (ASR) of noisy microphone array speech, we show that the proposed approach with intermediate outputs can directly improve the ASR performance, while the conventional LSTM approach increases the word error rate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1608–1619},
numpages = {12}
}

@article{10.1109/TASLP.2019.2956145,
author = {Wang, Xin and Takaki, Shinji and Yamagishi, Junichi},
title = {Neural Source-Filter Waveform Models for Statistical Parametric Speech Synthesis},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2956145},
doi = {10.1109/TASLP.2019.2956145},
abstract = {Neural waveform models have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. One of the best models, called WaveNet, uses an autoregressive (AR) approach to model the distribution of waveform sampling points, but it has to generate a waveform in a time-consuming sequential manner. Some new models that use inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner but require either a larger amount of training time or a complicated model architecture plus a blend of training criteria. As an alternative to AR and IAF-based frameworks, we propose a neural source-filter (NSF) waveform modeling framework that is straightforward to train and fast to generate waveforms. This framework requires three components to generate waveforms: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the input acoustic features for the source and filter modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented using short-time Fourier transform routines. As an initial NSF study, we designed three NSF models under the proposed framework and compared them with WaveNet using our deep learning toolkit. It was demonstrated that the NSF models generated waveforms at least 100 times faster than our WaveNet-vocoder, and the quality of the synthetic speech from the best NSF model was comparable to that from WaveNet on a large single-speaker Japanese speech corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {402–415},
numpages = {14}
}

@inproceedings{10.1145/3302425.3302489,
author = {Jin, Xin and Zhang, Keliang and Huang, Xian and Miao, Min},
title = {On Continuous Speech Recognition of Indian English},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302489},
doi = {10.1145/3302425.3302489},
abstract = {Indian English (IE) derives from British English, but they differ in many aspects. It varies from speech to word, and is a typical variety of English. At present, it is hard to see researches on continuous speech recognition (CSR) of lesser-known English varieties such as Indian English.Indian English has developed some distinctive features of its own with phonological features being the most remarkable. Compared with British English (or American English) which has a large amount of annotated speech data, IE is a relatively low-resourced language. What's more, the performance of existing English CSR systems perform unsatisfactorily when dealing with IE spontaneous conversations.To date, CSR of low-resourced languages (minority languages, dialects and varieties of a certain language with relatively few annotated speech data) performs unsatisfactorily. This paper takes Indian English for example, focuses on CSR of IE under low-resourced conditions, extracts acoustic features and trains acoustic models with different methods, explores the effective method in recognizing low-resourced languages.Firstly, we employ MFCC and PLP to extract features respectively and choose the GMM-HMM acoustic model to build the baseline system. Secondly, it comes up with another 6 acoustic models while using BLSTM-RNN and TDNN neural network algorithms. Then the models are tested on the test set and results are analyzed in detail. Thirdly, we use an acoustic model of American English trained by a large number of annotated data which is then transferred to the optimization of the acoustic model of Indian English. Finally, the decoding test is carried out on the test set and generates the experimental analysis.According to the experimental results, we find that the performances of the two neural network recognition systems are both improved with transfer learning technology while the BLSTM-RNN is more remarkable.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {74},
numpages = {6},
keywords = {Indian English (IE), GMM-HMM, Neural network, Acoustic model, Feature extraction, TDNN, BLSTM-RNN, Transfer learning, Continuous speech recognition (CSR)},
location = {Sanya, China},
series = {ACAI '18}
}

@article{10.1109/TASLP.2018.2851145,
author = {Sahraeian, Reza and Van Compernolle, Dirk},
title = {Cross-Entropy Training of DNN Ensemble Acoustic Models for Low-Resource ASR},
year = {2018},
issue_date = {November 2018},
publisher = {IEEE Press},
volume = {26},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2851145},
doi = {10.1109/TASLP.2018.2851145},
abstract = {Deep neural networks DNNs have shown a great promise in exploiting out-of-language data, particularly for under-resourced languages. The common trend is to merge data from various source languages to train a multilingual DNN and then reuse the hidden layers as language-independent feature extractors for a low-resource target language. While there is a consensus that using as much data from various languages results in a better and more general multilingual DNN, employing only source languages similar to the target language has proven effective. In this study, we propose a novel framework for multilingual DNN training, which employs all the available training data and exploits complementary information from individual source languages at the same time. Toward this goal, we borrow the idea of an ensemble with one generalist and many specialists. The generalist is derived from a multilingual DNN acoustic model trained on all available multilingual data; the specialists are the DNNs derived from the source languages individually. Then, the constituents in the ensemble are combined using weighted averaging schemes, where the combination weights are trained to minimize the cross-entropy objective function. In this framework, we seek for complementary information among the constituents while it is possible to get at least the performance equal to the baseline. Moreover, unlike previous well-known system combination schemes, only one model is required during decoding. We successfully examined two combination methodologies and demonstrated their usefulness in different scenarios using the multilingual GlobalPhone dataset. It is observed that, specifically, speech recognition systems developed in low-resource settings profit from the proposed strategy.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {1991–2001},
numpages = {11}
}

@article{10.1109/TASLP.2021.3111566,
author = {Li, Haoyu and Yamagishi, Junichi},
title = {Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3111566},
doi = {10.1109/TASLP.2021.3111566},
abstract = {The intelligibility of speech severely degrades in the presence of environmental noise and reverberation. In this paper, we propose a novel deep learning based system for modifying the speech signal to increase its intelligibility under the equal-power constraint, i.e., signal power before and after modification must be the same. To achieve this, we use generative adversarial networks (GANs) to obtain time-frequency dependent amplification factors, which are then applied to the input raw speech to reallocate the speech energy. Instead of optimizing only a single, simple metric, we train a deep neural network (DNN) model to simultaneously optimize multiple advanced speech metrics, including both intelligibility- and quality-related ones, which results in notable improvements in performance and robustness. Our system can not only work in non-real-time mode for offline audio playback but also support practical real-time speech applications. Experimental results using both objective measurements and subjective listening tests indicate that the proposed system significantly outperforms state-of-the-art baseline systems under various noisy and reverberant listening conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {3000–3011},
numpages = {12}
}

@inproceedings{10.1145/3503161.3548069,
author = {Jin, Tao and Zhao, Zhou and Zhang, Meng and Zeng, Xingshan},
title = {MC-SLT: Towards Low-Resource Signer-Adaptive Sign Language Translation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548069},
doi = {10.1145/3503161.3548069},
abstract = {One of the challenging factors in real application of sign language translation (SLT) is inter-signer variation. With the assumption that the pre-trained translation model cannot cover all the signers, the adaptation capability for unseen signers is of great concern. In this paper, we take a completely different perspective for SLT, called signer-adaptive SLT, which mainly considers the transferable ability of SLT systems. To attack this challenging problem, we propose MC-SLT, a novel meta-learning framework that could exploit additional new-signer data via a support set, and output a signer-adaptive model via a few-gradient-step update. Considering the various degrees of style discrepancies of different words performed by multiple signers, we further devise diversity-aware meta-adaptive weights for the token-wise cross-entropy losses. Besides, to improve the training robustness, we adopt the self-guided curriculum learning scheme that first captures the global curricula from each signer to avoid falling into a bad local optimum early, and then learns the curricula of individualities to improve the model adaptability for learning signer-specific knowledge. We re-construct the existing standard datasets of SLT for the signer-adaptive setting and establish a new benchmark for subsequent research.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4939–4947},
numpages = {9},
keywords = {signer adaptation, sign language translation, low resource},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3373477.3373695,
author = {Yan, Bi-Cheng and Liu, Shih-Hung and Chen, Berlin},
title = {Modulation Spectrum Augmentation for Robust Speech Recognition},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373695},
doi = {10.1145/3373477.3373695},
abstract = {Data augmentation is a crucial mechanism being employed to increase the diversity of training data in order to avoid overfitting and improve robustness of statistical models in various applications. In the context of automatic speech recognition (ASR), a recent trend has been to develop effective methods to augment training speech data by warping or masking utterances based on their waveforms or spectrograms. Extending this line of research, we make attempts to explore novel ways to generate augmented training speech data, in comparison to the existing state-of-the-art approaches. The main contribution of this paper is at least two-fold. First, we propose to warp the intermediate representation of the cepstral feature vector sequence of an utterance in a holistic manner. This intermediate representation can be embodied in different modulation domains by performing discrete Fourier transform (DFT) along the either the time- or the component-axis of a cepstral feature vector sequence. Second, we also develop a two-stage augmentation approach, which successively conduct perturbation in the waveform domain and warping in different modulation domains of cepstral speech feature vector sequences, to further enhance robustness. A series of experiments are carried out on the Aurora-4 database and task, in conjunction with a typical DNN-HMM based ASR system. The proposed augmentation method that conducts warping in the component-axis modulation domain of cepstral feature vector sequences can yield a word error rate reduction (WERR) of 17.6% and 0.69%, respectively, for the clean-and multi-condition training settings. In addition, the proposed two-stage augmentation method can at best achieve a WERR of 1.13% when using the multi-condition training setup.},
booktitle = {Proceedings of the 1st International Conference on Advanced Information Science and System},
articleno = {29},
numpages = {6},
keywords = {speech recognition, modulation spectra, robustness, data augmentation},
location = {Singapore, Singapore},
series = {AISS '19}
}

@article{10.1109/TASLP.2019.2906484,
author = {Juvela, Lauri and Bollepalli, Bajibabu and Tsiaras, Vassilis and Alku, Paavo},
title = {GlotNet—A Raw Waveform Model for the Glottal Excitation in Statistical Parametric Speech Synthesis},
year = {2019},
issue_date = {June 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2906484},
doi = {10.1109/TASLP.2019.2906484},
abstract = {Recently, generative neural network models which operate directly on raw audio, such as WaveNet, have improved the state of the art in text-to-speech synthesis TTS. Moreover, there is increasing interest in using these models as statistical vocoders for generating speech waveforms from various acoustic features. However, there is also a need to reduce the model complexity, without compromising the synthesis quality. Previously, glottal pulseforms i.e., time-domain waveforms corresponding to the source of human voice production mechanism have been successfully synthesized in TTS by glottal vocoders using straightforward deep feedforward neural networks. Therefore, it is natural to extend the glottal waveform modeling domain to use the more powerful WaveNet-like architecture. Furthermore, due to their inherent simplicity, glottal excitation waveforms permit scaling down the waveform generator architecture. In this study, we present a raw waveform glottal excitation model, called GlotNet, and compare its performance with the corresponding direct speech waveform model, WaveNet, using equivalent architectures. The models are evaluated as part of a statistical parametric TTS system. Listening test results show that both approaches are rated highly in voice similarity to the target speaker, and obtain similar quality ratings with large models. Furthermore, when the model size is reduced, the quality degradation is less severe for GlotNet.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1019–1030},
numpages = {12}
}

@inproceedings{10.1145/3482632.3483146,
author = {Chen, Chen},
title = {Establishment and Design of English Translation Memory and Terminology Database Based on Artificial Intelligence},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483146},
doi = {10.1145/3482632.3483146},
abstract = {In the application field of professional translation and localization market, since the quality of computer translation basically meets actual needs, most of the translation products that can be sold in the market use computer-assisted translation technology. However, due to the complexity and variability of linguistic knowledge, some people have proposed translation memory, which makes full use of the powerful functions of files and databases to improve the reliability and reuse of existing translation materials. Therefore, in some professional translation technology applications with high data and repetitiveness, the use of this type of technology can effectively eliminate the double operation of translators, avoid multiple translation results of the same word or two words, and significantly improve its work efficiency. This article is based on artificial intelligence-based English translation memory and terminology establishment and design. First of all, it uses literature research to explain the functions of English translation memory technology and English translation screening methods. After analyzing the memory and terminology, according to analyzing the results of the design of the memory bank and the design library, this article locates the granularity of the memory bank at the simple sentence level, and establishes a correspondence between the various components of a sentence based on the establishment of a simple word-sentence alignment. The English sentence similarity algorithm based on syntax and semantics can well measure the structural and semantic similarity between two English sentences, and can correctly return the most similar example sentences in a memory bank. After we obtained the most similar example sentences, we translated them according to the corresponding relationship between the sentence to be translated and the various components of the most similar example sentences.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1337–1341},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1109/TASLP.2022.3198555,
author = {Tian, Jinchuan and Yu, Jianwei and Weng, Chao and Zou, Yuexian and Yu, Dong},
title = {Integrating Lattice-Free MMI Into End-to-End Speech Recognition},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3198555},
doi = {10.1109/TASLP.2022.3198555},
abstract = {In automatic speech recognition (ASR) research, discriminative criteria have achieved superior performance in DNN-HMM systems. Given this success, the adoption of discriminative criteria is promising to boost the performance of end-to-end (E2E) ASR systems. With this motivation, previous works have introduced the minimum Bayesian risk (MBR, one of the discriminative criteria) into E2E ASR systems. However, the effectiveness and efficiency of the MBR-based methods are compromised: the MBR criterion is only used in system training, which creates a mismatch between training and decoding; the on-the-fly decoding process in MBR-based methods results in the need for pre-trained models and slow training speeds. To this end, novel algorithms are proposed in this work to integrate another widely used discriminative criterion, lattice-free maximum mutual information (LF-MMI), into E2E ASR systems not only in the training stage but also in the decoding process. The proposed LF-MMI training and decoding methods show their effectiveness on two widely used E2E frameworks: Attention-Based Encoder-Decoders (AEDs) and Neural Transducers (NTs). Compared with MBR-based methods, the proposed LF-MMI method: maintains the consistency between training and decoding; eschews the on-the-fly decoding process; trains from randomly initialized models with superior training efficiency. Experiments suggest that the LF-MMI method outperforms its MBR counterparts and consistently leads to statistically significant performance improvements on various frameworks and datasets from 30 hours to 14.3 k hours. The proposed method achieves state-of-the-art (SOTA) results on Aishell-1 (CER 4.10%) and Aishell-2 (CER 5.02%) datasets. Code is released<sup>1</sup>.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {25–38},
numpages = {14}
}

@article{10.1145/3606019,
author = {Gambhir, Pooja and Dev, Amita and Bansal, Poonam and Sharma, Deepak Kumar},
title = {End-to-End Multi-Modal Low-Resourced Speech Keywords Recognition Using Sequential Conv2D Nets},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3606019},
doi = {10.1145/3606019},
abstract = {Advanced Neural Networks are widely used to recognize multi-modal conversational speech with significant improvements in accuracy automatically. Significantly, Convolutional Neural (CN) sheets have retreated cutting-edge performance in Automatic Voice Recognition (AVR) recently more appropriately in English; however, the Hindi language has not been explored and examined well on AVR systems. The work in this paper has exposed a 3-layered two-dimensional Sequential Convolutional neural architecture. The Sequential Conv2D is an end-to-end system that can instantaneously exploit speech signal spectral and temporal structures. The network has been trained and tested on different cepstral features such as Frequency and Time variant- Mel-Filters, Gamma-tone Filter Cepstral Quantities (GFCQ), Bark-Filter band Coefficients, and Spectrogram features of speech structures. The experiment was performed on two low-resourced speech command datasets; Hindi with 27,145 Speech Keywords developed by TIFR and 23,664 (1-second utterances) of English speech commands by Google TensorFlow and AIY English Speech Commands. The experimental outcome showed that the model achieves significant performance of Convolutional layers trained on spectrograms with 91.60% accuracy, compared to that achieved in other cepstral feature labels for English speech. However, the model achieved an accuracy of 69.65% for Hindi audio words in which BFCC features outperformed spectrogram features.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
keywords = {Speech Recognition, Sequential, Spectrogram, Neural Networks, Convolution layers}
}

@inproceedings{10.1109/CGO51591.2021.9370312,
author = {Zhao, Ziyi and Jiang, Zhang and Chen, Ying and Gong, Xiaoli and Wang, Wenwen and Yew, Pen-Chung},
title = {Enhancing Atomic Instruction Emulation for Cross-ISA Dynamic Binary Translation},
year = {2021},
isbn = {9781728186139},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO51591.2021.9370312},
doi = {10.1109/CGO51591.2021.9370312},
abstract = {Dynamic Binary Translation (DBT) is a key enabler for cross-ISA emulation, system virtualization, runtime instrumentation, and many other important applications. Among several critical requirements for DBT, it is important to provide equivalent semantics for atomic synchronization instructions such as Load - Link / Store - Conditional (LL/SC), which are mostly included in the reduced-instruction set architectures (RISC) and Compare-and-Swap(CAS), which is mostly in the complex instruction set architectures (CISC). However, the state-of-the-art DBT tools often do not provide a fully correct translation of these atomic instructions, in particular, from RISC atomic instructions (i.e. LL/SC) to CISC atomic instructions (i.e. CAS), due to performance concerns. As a result, some may cause the well-known ABA problem, which could lead to wrong results or program crashes. In our experimental studies on QEMU, a state-of-the-art DBT, that runs multi-threaded lock-free stack operations implemented with ARM instruction set (i.e. using LL/SC) on Intel x86 platforms (i.e. using CAS), it often crashes within 2 seconds.Although attempts have been made to provide correct emulation for such atomic instructions, they either result in heavy execution overheads or require additional hardware support. In this paper, we propose several schemes to address those issues and implement them on QEMU to evaluate their performance overheads. The results show that all of the proposed schemes can provide correct emulation and, for the best solution, can achieve a min, max, geomean speedup of 1.25x, 3.21x, 2.03x respectively, over the best existing software-based scheme.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {351–362},
numpages = {12},
keywords = {dynamic binary translation, scalability},
location = {Virtual Event, Republic of Korea},
series = {CGO '21}
}

@inproceedings{10.1145/3441296.3441394,
author = {Tanaka, Akira},
title = {Coq to C Translation with Partial Evaluation},
year = {2021},
isbn = {9781450383059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441296.3441394},
doi = {10.1145/3441296.3441394},
abstract = {Coq proof assistant can be used to prove various properties of programs written in the Gallina language. It is also possible to translate Gallina programs to OCaml programs. However, OCaml is not suitable for low-level programs. Therefore, we are developing a Coq plugin for Gallina to C translation. This plugin transforms functions written in Gallina into a form as close to C as possible within Gallina. This transformation includes partial evaluation, which improves execution efficiency and eliminates polymorphism and dependent types. We can easily verify in Coq that this transformation does not change the execution result, and thus it is highly reliable. And Gallina functions after this transformation can be easily translated to C.},
booktitle = {Proceedings of the 2021 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
pages = {14–31},
numpages = {18},
keywords = {Coq, partial evaluation, translator, tail recursion, verification, C, Gallina, compiler},
location = {Virtual, Denmark},
series = {PEPM 2021}
}

@inproceedings{10.1145/3452446.3452683,
author = {Zhang, Liying},
title = {The Effect Evaluation of Flipped Classroom in College English Translation Teaching under the Blended Teaching Mode},
year = {2021},
isbn = {9781450389815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452446.3452683},
doi = {10.1145/3452446.3452683},
abstract = {In recent decades, the reform of College English translation teaching (CETT) in China is mainly in view of the theoretical knowledge of predecessors. Nowadays, there are many problems in College English classroom translation teaching, such as single teaching method (classroom teaching as the main method, practice as the auxiliary). In various fields of English learning, including listening, speaking, reading and writing, the theoretical and empirical research of blended learning mode has been gradually improved. Therefore, how to create a clear and efficient classroom and change the teaching mode of College English translation scientifically and reasonably has become an important teaching topic for teachers. This paper holds that the teaching of English translation in the digital age should also be reformed. The application of blended teaching mode in CETT can fully stimulate students' enthusiasm, prolong learning time and improve classroom efficiency. The results show that 75.2% of students and teachers are satisfied with the FC, and the application prospect of flipped classroom (FC) is very broad. The aim of this article is to explore how to apply this new teaching method to CETT, so that the learning of English translation is no longer boring, and improve students' interest in English translation.},
booktitle = {2021 2nd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {988–991},
numpages = {4},
keywords = {FC, Effect Evaluation, Mixed Teaching, College English Translation},
location = {Dalian, China},
series = {IPEC2021}
}

@article{10.1109/TASLP.2020.2987752,
author = {Miao, Haoran and Cheng, Gaofeng and Zhang, Pengyuan and Yan, Yonghong},
title = {Online Hybrid CTC/Attention End-to-End Automatic Speech Recognition Architecture},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2987752},
doi = {10.1109/TASLP.2020.2987752},
abstract = {Recently, there has been increasing progress in end-to-end automatic speech recognition (ASR) architecture, which transcribes speech to text without any pre-trained alignments. One popular end-to-end approach is the hybrid Connectionist Temporal Classification (CTC) and attention (CTC/attention) based ASR architecture, which utilizes the advantages of both CTC and attention. The hybrid CTC/attention ASR systems exhibit performance comparable to that of the conventional deep neural network (DNN)/ hidden Markov model (HMM) ASR systems. However, how to deploy hybrid CTC/attention systems for online speech recognition is still a non-trivial problem. This article describes our proposed online hybrid CTC/attention end-to-end ASR architecture, which replaces all the offline components of conventional CTC/attention ASR architecture with their corresponding streaming components. Firstly, we propose stable monotonic chunk-wise attention (sMoChA) to stream the conventional global attention, and further propose monotonic truncated attention (MTA) to simplify sMoChA and solve the training-and-decoding mismatch problem of sMoChA. Secondly, we propose truncated CTC (T-CTC) prefix score to stream CTC prefix score calculation. Thirdly, we design dynamic waiting joint decoding (DWJD) algorithm to dynamically collect the predictions of CTC and attention in an online manner. Finally, we use latency-controlled bidirectional long short-term memory (LC-BLSTM) to stream the widely-used offline bidirectional encoder network. Experiments with LibriSpeech English and HKUST Mandarin tasks demonstrate that, compared with the offline CTC/attention model, our proposed online CTC/attention model improves the real time factor in human-computer interaction services and maintains its performance with moderate degradation. To the best of our knowledge, this is the first work to provide the full-stack online solution for CTC/attention end-to-end ASR architecture.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1452–1465},
numpages = {14}
}

@inproceedings{10.1145/3369199.3369228,
author = {Hoi, Huynh Tan},
title = {Efficiency of Japanese-Vietnamese Translation Job Thanks to the Use of Technology in the Fourth Industrial Revolution},
year = {2020},
isbn = {9781450372206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369199.3369228},
doi = {10.1145/3369199.3369228},
abstract = {Vietnamese translation background is still unfamiliar with the translation support technologies that international friends have used for a long time. So, what to do to get a perfect translation is considered as an extremely important factor for anyone who has been working in the translation field. In addition to accuracy, the translation must be clear and understandable. Traditional translation methods still have their values but may not meet the ever-changing requirements of constantly moving and developing science. We need to use different technologies to suit our projects. This will help customers satisfy, trust and choose us. The article is done with the support of filling out online survey forms of FPT University students and office staffs currently working at Gifu Kogyo Software Company, Quang Trung park, District 12, Ho Chi Minh City.},
booktitle = {Proceedings of the 3rd International Conference on Digital Technology in Education},
pages = {181–184},
numpages = {4},
keywords = {Technology, Software, Japanese-Vietnamese, Benefits, Translation},
location = {Yamanashi, Japan},
series = {ICDTE '19}
}

@article{10.1109/TASLP.2020.2991537,
author = {Du, Zhihao and Zhang, Xueliang and Han, Jiqing},
title = {A Joint Framework of Denoising Autoencoder and Generative Vocoder for Monaural Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2991537},
doi = {10.1109/TASLP.2020.2991537},
abstract = {Conventional monaural speech enhancement methods usually enhance the magnitude spectrum of noisy speech and leave the phase unchanged. Recent studies suggest that phase is also important for both speech intelligibility and perceptual quality. Although deep learning exhibits great potential on enhancing the magnitude and phase spectra in complex spectrogram domain and waveform domain, complex spectrogram and waveform are always more difficult to predict than the magnitude spectrum due to lack of clear structure in them. In this study, a Mel-domain denoising autoencoder and a deep generative vocoder are stacked to form a joint framework for monaural speech enhancement, in which the clean speech waveform is reconstructed without using the phase. Specifically, a convolutional recurrent network (CRN) is employed as the denoising autoencoder to enhance the Mel power spectrum of noisy speech. Then, the enhanced Mel power spectrum is fed to a deep generative vocoder to synthesize the speech waveform. Furthermore, the denoising autoencoder and generative vocoder are jointly fine-tuned. Experimental results show that the proposed method significantly improves speech intelligibility and perceptual quality. More importantly, our method achieves much better generalization ability for untrained noises than previous methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1493–1505},
numpages = {13}
}

@article{10.1109/TASLP.2021.3066047,
author = {Zhang, Mingyang and Zhou, Yi and Zhao, Li and Li, Haizhou},
title = {Transfer Learning From Speech Synthesis to Voice Conversion With Non-Parallel Training Data},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3066047},
doi = {10.1109/TASLP.2021.3066047},
abstract = {We present a novel voice conversion (VC) framework by learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC transfer learning or TTL-VC for short. We first develop a multi-speaker speech synthesis system with sequence-to-sequence encoder-decoder architecture, where the encoder extracts the linguistic representations of input text, while the decoder, conditioned on target speaker embedding, takes the context vectors and the attention recurrent network cell output to generate target acoustic features. We take advantage of the fact that TTS system maps input text to speaker independent context vectors, thus re-purpose such a mapping to supervise the training of the latent representations of an encoder-decoder voice conversion system. In the voice conversion system, the encoder takes speech instead of text as the input, while the decoder is functionally similar to the TTS decoder. As we condition the decoder on a speaker embedding, the system can be trained on non-parallel data for any-to-any voice conversion. During voice conversion training, we present both text and speech to speech synthesis and voice conversion networks respectively. At run-time, the voice conversion network uses its own encoder-decoder architecture without the need of text input. Experiments show that the proposed TTL-VC system outperforms two competitive voice conversion baselines consistently, namely phonetic posteriorgram and AutoVC methods, in terms of speech quality, naturalness, and speaker similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1290–1302},
numpages = {13}
}

@inproceedings{10.1145/3411764.3445565,
author = {Pandey, Laxmi and Arif, Ahmed Sabbir},
title = {LipType: A Silent Speech Recognizer Augmented with an Independent Repair Model},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445565},
doi = {10.1145/3411764.3445565},
abstract = {Speech recognition is unreliable in noisy places, compromises privacy and security when around strangers, and inaccessible to people with speech disorders. Lip reading can mitigate many of these challenges but the existing silent speech recognizers for lip reading are error prone. Developing new recognizers and acquiring new datasets is impractical for many since it requires enormous amount of time, effort, and other resources. To address these, first, we develop LipType, an optimized version of LipNet for improved speed and accuracy. We then develop an independent repair model that processes video input for poor lighting conditions, when applicable, and corrects potential errors in output for increased accuracy. We then test this model with LipType and other speech and silent speech recognizers to demonstrate its effectiveness.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {1},
numpages = {19},
keywords = {text input., deep learning, language modeling, Silent speech recognition},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3358186,
author = {Kim, Minsu and Park, Jeong-Keun and Kim, Sungyeol and Yang, Insu and Jung, Hyunsoo and Moon, Soo-Mook},
title = {Output-Based Intermediate Representation for Translation of Test-Pattern Program},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358186},
doi = {10.1145/3358186},
abstract = {An Intermediate Representation (IR) used by compilers is normally generated statically, as a result of parsing or analyzing the source program. This paper proposes a completely different type of IR, generated as a result of running the source program, the output-based IR. There is a practical translation problem where such an IR is useful, in the domain of test-pattern programs.Test-pattern programs run on ATE (automatic test equipment), a special embedded system to test semiconductors such as DRAMs. They generate a pattern for each clock, a bit vector input to the pins of the chip. One issue is that different ATEs require different programming since each ATE manufacturer has its own programming language. Nonetheless, we should be able to test a memory chip on different ATEs as long as they generate the same patterns with the same speed. Therefore, a memory chipmaker wants to make a pattern program portable across ATEs, to fully utilize their ATE resources.One solution is translating between pattern programs, for which we need an IR since there are multiple source ATEs and target ATEs. Instead of a conventional, static IR, we propose using the output pattern itself as an IR. Since the pattern is independent of ATEs and easily obtainable, the output-based IR obviates designing a static IR considering all ATE programming languages and hardware differences. Moreover, we might synthesize a better target program from the IR, more optimized to the target ATE. However, the full pattern generated by a product-level pattern program is huge, so we propose using an IR of abbreviated patterns, annotated with the repetition information obtained while executing the source program. Our experimental results with product-level pattern programs show that our approach is feasible.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {55},
numpages = {22},
keywords = {domain-specific language, test-pattern program, translation, Intermediate representation, automatic test equipment}
}

@article{10.1109/TASLP.2021.3066274,
author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Wen, Zhengqi and Tian, Zhengkun and Zhang, Shuai},
title = {Integrating Knowledge Into End-to-End Speech Recognition From External Text-Only Data},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3066274},
doi = {10.1109/TASLP.2021.3066274},
abstract = {Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because of the end-to-end training, an AED model is usually trained with speech-text paired data. It is challenging to incorporate external text-only data into AED models. Another issue of the AED model is that it does not use the right context of a text token while predicting the token. To alleviate the above two issues, we propose a unified method called LST (Learn Spelling from Teachers) to integrate knowledge into an AED model from the external text-only data and leverage the whole context in a sentence. The method is divided into two stages. First, in the representation stage, a language model is trained on the text. It can be seen as that the knowledge in the text is compressed into the LM. Then, at the transferring stage, the knowledge is transferred to the AED model via teacher-student learning. To further use the whole context of the text sentence, we propose an LM called causal cloze completer (COR), which estimates the probability of a token, given both the left context and the right context of it. Therefore, with LST training, the AED model can leverage the whole context in the sentence. Different from fusion based methods, which use LM during decoding, the proposed method does not increase any extra complexity at the inference stage. We conduct experiments on two scales of public Chinese datasets AISHELL-1 and AISHELL-2. The experimental results demonstrate the effectiveness of leveraging external text-only data and the whole context in a sentence with our proposed method, compared with baseline hybrid systems and AED model based systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1340–1351},
numpages = {12}
}

@inproceedings{10.1145/3613917.3613920,
author = {Li, Tian},
title = {Research on Intelligent Construction of Bilingual Translation Corpus Based on Differential Evolution Algorithm},
year = {2023},
isbn = {9798400700606},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613917.3613920},
doi = {10.1145/3613917.3613920},
abstract = {In the construction stage of bilingual translation corpus, due to the lack of analysis of the relationship between the basic corpora, the translation quality is low. Therefore, a research on intelligent construction of bilingual translation corpus based on differential evolution algorithm is proposed. The differential evolution algorithm is used to optimize the reverse learning of bilingual translation corpus, improve the quality of natural language corpus, and expand the search area of corpus to complement the global exploration ability of bilingual translation corpus. The method of random neighborhood variation is used to extract bilingual sentence pairs. From the perspective of diversity to ensure the quality of natural language corpus, the method of adaptive updating is used to establish the relationship between the basic corpus through differentiated neighborhood size and scaling factors, and to extract bilingual sentence pairs.In this test, the translation quality of the designed bilingual translation corpus pair and the sentence level and text level test data is at a high level.},
booktitle = {Proceedings of the 2023 8th International Conference on Multimedia Systems and Signal Processing},
pages = {1–5},
numpages = {5},
keywords = {Optimizing reverse learning, Adaptive update, Bilingual translation corpus, Differential evolution algorithm, Random neighborhood variation},
location = {Shenzhen, China},
series = {ICMSSP '23}
}

@inproceedings{10.1145/3404835.3463257,
author = {Srinivasan, Krishna and Raman, Karthik and Chen, Jiecao and Bendersky, Michael and Najork, Marc},
title = {WIT: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463257},
doi = {10.1145/3404835.3463257},
abstract = {The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information across image and text modalities. In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.5 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. WIT Dataset is available for download and use via a Creative Commons license here: https://github.com/google-research-datasets/wit.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2443–2449},
numpages = {7},
keywords = {image-text retrieval, multilingual, machine learning, neural networks, dataset, wikipedia, multimodal},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3503161.3547789,
author = {Wang, Yijun and Liang, Tao and Lin, Jianxin},
title = {CACOLIT: Cross-Domain Adaptive Co-Learning for Imbalanced Image-to-Image Translation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547789},
doi = {10.1145/3503161.3547789},
abstract = {State-of-the-art unsupervised image-to-image translation (I2I) methods have made great progress on transferring images from a source domain X to a target domain Y. However, training these unsupervised I2I models on imbalanced target domain (e.g., Y with limited samples) usually causes mode collapse, which has not been well solved in current literature. In this work, we propose a new Cross-domain Adaptive Co-learning paradigm, CACOLIT, to alleviate the imbalanced unsupervised I2I training problem. Concretely, CACOLIT first constructs a teacher translation model by introducing an auxiliary domain along with source domain as well as two complementary student translation models formulating an I2I closed loop. Then, the two student models are simultaneously learned by transferring correspondence knowledge from teacher model in an interactive way. With extensive experiments on both human face style transfer and animal face translation tasks, we demonstrate that our adaptive co-learning model effectively transfers correspondence knowledge from teacher model to student models and generates more diverse and realistic images than existing I2I methods both qualitatively and quantitatively.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1068–1076},
numpages = {9},
keywords = {generative model, image-to-image translation, imbalanced learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2020.2979603,
author = {Nugraha, Aditya Arie and Sekiguchi, Kouhei and Yoshii, Kazuyoshi},
title = {A Flow-Based Deep Latent Variable Model for Speech Spectrogram Modeling and Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2979603},
doi = {10.1109/TASLP.2020.2979603},
abstract = {This article describes a deep latent variable model of speech power spectrograms and its application to semi-supervised speech enhancement with a deep speech prior. By integrating two major deep generative models, a variational autoencoder (VAE) and a normalizing flow (NF), in a mutually-beneficial manner, we formulate a flexible latent variable model called the NF-VAE that can extract low-dimensional latent representations from high-dimensional observations, akin to the VAE, and does not need to explicitly represent the distribution of the observations, akin to the NF. In this article, we consider a variant of NF called the generative flow (GF a.k.a. Glow) and formulate a latent variable model called the GF-VAE. We experimentally show that the proposed GF-VAE is better than the standard VAE at capturing fine-structured harmonics of speech spectrograms, especially in the high-frequency range. A similar finding is also obtained when the GF-VAE and the VAE are used to generate speech spectrograms from latent variables randomly sampled from the standard Gaussian distribution. Lastly, when these models are used as speech priors for statistical multichannel speech enhancement, the GF-VAE outperforms the VAE and the GF.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1104–1117},
numpages = {14}
}

@article{10.1145/3498324,
author = {B\'{a}ez, Pablo and Bravo-Marquez, Felipe and Dunstan, Jocelyn and Rojas, Mat\'{\i}as and Villena, Fabi\'{a}n},
title = {Automatic Extraction of Nested Entities in Clinical Referrals in Spanish},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3498324},
doi = {10.1145/3498324},
abstract = {Here we describe a new clinical corpus rich in nested entities and a series of neural models to identify them. The corpus comprises de-identified referrals from the waiting list in Chilean public hospitals. A subset of 5,000 referrals (58.6% medical and 41.4% dental) was manually annotated with 10 types of entities, six attributes, and pairs of relations with clinical relevance. In total, there are 110,771 annotated tokens. A trained medical doctor or dentist annotated these referrals, and then, together with three other researchers, consolidated each of the annotations. The annotated corpus has 48.17% of entities embedded in other entities or containing another one. We use this corpus to build models for Named Entity Recognition (NER). The best results were achieved using a Multiple Single-entity architecture with clinical word embeddings stacked with character and Flair contextual embeddings. The entity with the best performance is abbreviation, and the hardest to recognize is finding. NER models applied to this corpus can leverage statistics of diseases and pending procedures. This work constitutes the first annotated corpus using clinical narratives from Chile and one of the few in Spanish. The annotated corpus, clinical word embeddings, annotation guidelines, and neural models are freely released to the community.},
journal = {ACM Trans. Comput. Healthcare},
month = {apr},
articleno = {28},
numpages = {22},
keywords = {data mining, Natural language processing, named entity recognition, data curation, supervised machine learning, clinical text mining}
}

@article{10.1109/TASLP.2020.2982029,
author = {Fan, Cunhang and Tao, Jianhua and Liu, Bin and Yi, Jiangyan and Wen, Zhengqi and Liu, Xuefei},
title = {End-to-End Post-Filter for Speech Separation With Deep Attention Fusion Features},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2982029},
doi = {10.1109/TASLP.2020.2982029},
abstract = {In this article, we propose an end-to-end post-filter method with deep attention fusion features for monaural speaker-independent speech separation. At first, a time-frequency domain speech separation method is applied as the pre-separation stage. The aim of pre-separation stage is to separate the mixture preliminarily. Although this stage can separate the mixture, it still contains the residual interference. In order to enhance the pre-separated speech and improve the separation performance further, the end-to-end post-filter (E2EPF) with deep attention fusion features is proposed. The E2EPF can make full use of the prior knowledge of the pre-separated speech, which contributes to speech separation. It is a fully convolutional speech separation network and uses the waveform as the input features. Firstly, the 1-D convolutional layer is utilized to extract the deep representation features for the mixture and pre-separated signals in the time domain. Secondly, to pay more attention to the outputs of the pre-separation stage, an attention module is applied to acquire deep attention fusion features, which are extracted by computing the similarity between the mixture and the pre-separated speech. These deep attention fusion features are conducive to reduce the interference and enhance the pre-separated speech. Finally, these features are sent to the post-filter to estimate each target signals. Experimental results on the WSJ0-2mix dataset show that the proposed method outperforms the state-of-the-art speech separation method. Compared with the pre-separation method, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative improvements in scale-invariant source-to-noise ratio (SI-SNR), the signal-to-distortion ratio (SDR), the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility (STOI) measures, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1303–1314},
numpages = {12}
}

@article{10.1109/TASLP.2020.2980436,
author = {Sterpu, George and Saam, Christian and Harte, Naomi},
title = {How to Teach DNNs to Pay Attention to the Visual Modality in Speech Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980436},
doi = {10.1109/TASLP.2020.2980436},
abstract = {Audio-Visual Speech Recognition (AVSR) seeks to model, and thereby exploit, the dynamic relationship between a human voice and the corresponding mouth movements. A recently proposed multimodal fusion strategy, <italic>AV Align</italic>, based on state-of-the-art sequence to sequence neural networks, attempts to model this relationship by explicitly aligning the acoustic and visual representations of speech. This study investigates the inner workings of <italic>AV Align</italic> and visualises the audio-visual alignment patterns. Our experiments are performed on two of the largest publicly available AVSR datasets, TCD-TIMIT and LRS2. We find that <italic>AV Align</italic> learns to align acoustic and visual representations of speech at the frame level on TCD-TIMIT in a generally monotonic pattern. We also determine the cause of initially seeing no improvement over audio-only speech recognition on the more challenging LRS2. We propose a regularisation method which involves predicting lip-related Action Units from visual representations. Our regularisation method leads to better exploitation of the visual modality, with performance improvements between 7% and 30% depending on the noise level. Furthermore, we show that the alternative <italic>Watch, Listen, Attend, and Spell</italic> network is affected by the same problem as <italic>AV Align</italic>, and that our proposed approach can effectively help it learn visual representations. Our findings validate the suitability of the regularisation method to AVSR and encourage researchers to rethink the multimodal convergence problem when having one dominant modality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1052–1064},
numpages = {13}
}

@article{10.1109/TASLP.2022.3228629,
author = {Neumann, Thilo von and Kinoshita, Keisuke and Boeddeker, Christoph and Delcroix, Marc and Haeb-Umbach, Reinhold},
title = {Segment-Less Continuous Speech Separation of Meetings: Training and Evaluation Criteria},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3228629},
doi = {10.1109/TASLP.2022.3228629},
abstract = {Continuous Speech Separation (CSS) has been proposed to address speech overlaps during the analysis of realistic meeting-like conversations by eliminating any overlaps before further processing. CSS separates a recording of arbitrarily many speakers into a small number of overlap-free output channels, where each output channel may contain speech of multiple speakers. Often, a separation model is trained with Utterance-level Permutation Invariant Training (uPIT), which exclusively maps a speaker to an output channel, and applied in a sliding window approach called stitching. Recently, we introduced an alternative training scheme called Graph-PIT that teaches the separator to produce a speaker-shared output channel format without stitching. It can handle an arbitrary number of speakers as long as the number of overlapping speakers is never larger than the number of output channels. Models trained in this way are able to perform segment-less CSS, i.e., without stitching, and achieve comparable and often better separation quality than the conventional CSS with uPIT and stitching. In this contribution, we further investigate the Graph-PIT training scheme. We show in extended experiments that Graph-PIT also works in challenging reverberant conditions. We simplify the training schedule for Graph-PIT with the recently proposed Source Aggregated Signal-to-Distortion Ratio (SA-SDR) loss, which eliminates unfavorable properties of the previously used A-SDR loss to enable training with Graph-PIT from scratch. Furthermore, we introduce novel signal-level evaluation metrics for meeting scenarios, namely the source-aggregated scale- and convolution-invariant Signal-to-Distortion Ratio (SA-SI-SDR and SA-CI-SDR), which are generalizations of the commonly used SDR-based metrics for the CSS case.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {576–589},
numpages = {14}
}

@inproceedings{10.1145/3313831.3376322,
author = {Zhang, Xinlei and Miyaki, Takashi and Rekimoto, Jun},
title = {WithYou: Automated Adaptive Speech Tutoring With Context-Dependent Speech Recognition},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376322},
doi = {10.1145/3313831.3376322},
abstract = {Learning to speak in foreign languages is hard. Speech shadowing has been rising as a proven way to practice speaking, which asks a learner to listen and repeat a native speech template as simultaneously as possible. However, shadowing can be hard to do because learners can frequently fail to follow the speech and unintentionally interrupt a practice session. Worse, as a technical way to evaluate shadowing performance in real-time has not been established, no automated solutions are available to help. In this paper, we propose a technical framework with context-dependent speech recognition to evaluate shadowing in real-time. We propose a shadowing tutor system called WithYou, which can automatically adjust the playback and the difficulty of a speech template when learners fail, so shadowing becomes smooth and tailored. Results from a user study show that WithYou provides greater speech improvements (14%) than the conventional method (2.7%) with a lower cognitive load.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {computer assisted language learning (call), shadowing, intelligent tutoring system, language learning, speaking, speech recognition},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3491102.3517639,
author = {Reitmaier, Thomas and Wallington, Electra and Kalarikalayil Raju, Dani and Klejch, Ondrej and Pearson, Jennifer and Jones, Matt and Bell, Peter and Robinson, Simon},
title = {Opportunities and Challenges of Automatic Speech Recognition Systems for Low-Resource Language Speakers},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517639},
doi = {10.1145/3491102.3517639},
abstract = {Automatic Speech Recognition (ASR) researchers are turning their attention towards supporting low-resource languages, such as isiXhosa or Marathi, with only limited training resources. We report and reflect on collaborative research across ASR &amp; HCI to situate ASR-enabled technologies to suit the needs and functions of two communities of low-resource language speakers, on the outskirts of Cape Town, South Africa and in Mumbai, India. We build on longstanding community partnerships and draw on linguistics, media studies and HCI scholarship to guide our research. We demonstrate diverse design methods to: remotely engage participants; collect speech data to test ASR models; and ultimately field-test models with users. Reflecting on the research, we identify opportunities, challenges, and use-cases of ASR, in particular to support pervasive use of WhatsApp voice messaging. Finally, we uncover implications for collaborations across ASR &amp; HCI that advance important discussions at CHI surrounding data, ethics, and AI.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {299},
numpages = {17},
keywords = {mobile devices, Speech/language, automatic speech recognition},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1109/TASLP.2021.3093392,
author = {Medina, C. and Coelho, R. and Z\~{a}o, L.},
title = {Impulsive Noise Detection for Speech Enhancement in HHT Domain},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3093392},
doi = {10.1109/TASLP.2021.3093392},
abstract = {This paper introduces a novel single channel speech enhancement method in the time domain to mitigate the effects of acoustic impulsive noises. The ensemble empirical mode decomposition is applied to analyze the noisy speech signal. The estimation and selection of noise components is based on the impulsiveness index of decomposition modes. An adaptive threshold is proposed to define the criterion to select the noise components. The proposed method is evaluated in speech enhancement experiments considering four acoustic noises with different impulsiveness indices and non-stationarity degrees under various signal-to-noise ratios. Four speech enhancement algorithms are adopted as baseline in the evaluation analysis considering spectral and time domains. Seven objective measures are adopted to compare the proposed and baseline approaches in terms of speech quality and intelligibility. Results show that the proposed solution outperforms the competing algorithms for most of the noisy scenarios. The novel method shows particularly interesting performance when speech signals are corrupted by highly impulsive acoustic noises.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2244–2253},
numpages = {10}
}

@inproceedings{10.1145/3347317.3357243,
author = {Ratajczak, R\'{e}mi and Crispim-Junior, Carlos and Fervers, B\'{e}atrice and Faure, Elodie and Tougne, Laure},
title = {Pseudo-Cyclic Network for Unsupervised Colorization with Handcrafted Translation and Output Spatial Pyramids},
year = {2019},
isbn = {9781450369107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347317.3357243},
doi = {10.1145/3347317.3357243},
abstract = {We present a novel pseudo-cyclic adversarial learning approach for unsupervised colorization of grayscale images. We investigate the use of a non-trainable, lightweight and well-defined Handcrafted Translation to enforce the generation of realistic images and replace one of the two deep convolutional generative adversarial neural networks classically used in cyclic models. Additionally, we propose to use Output Spatial Pyramids to jointly constrain the deep latent spaces of an encoder-decoder generator to preserve spatial structures and improve the quality of the generated images. We demonstrate the interest of our approach compared with the state of the art on standard datasets (paintings, landscapes, aerial, thumbnails) that we modified for the purpose of colorization. We evaluate colorization quality of the generated images along the training with deterministic and reproducible criteria. In complement, we demonstrate the ability of our method to generate representations that are prone to make a classification network generalize well to slightly different color spaces. We believe our approach has potential applications in arts and cultural heritage to produce alternative representations without requiring paired data.},
booktitle = {Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia HeritAge Contents},
pages = {5–13},
numpages = {9},
keywords = {spatial pyramids, unsupervised colorization, handcrafted translation, gan, pseudo-cyclic network},
location = {Nice, France},
series = {SUMAC '19}
}

@article{10.1109/TASLP.2021.3092585,
author = {Zhang, Lu and Wang, Mingjiang and Zhang, Qiquan and Wang, Xinsheng and Liu, Ming},
title = {PhaseDCN: A Phase-Enhanced Dual-Path Dilated Convolutional Network for Single-Channel Speech Enhancement},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3092585},
doi = {10.1109/TASLP.2021.3092585},
abstract = {Recent deep neural network (DNN) based single-channel speech enhancement methods have achieved remarkable results in the time-frequency (TF) magnitude domain. To further improve the quality and intelligibility of enhanced speech, the attention to phase enhancement is also increasing. In this paper, we propose a novel dilated convolutional network (DCN) model to simultaneously enhance the magnitude and phase of noisy speech. Unlike the direct complex spectral mapping methods, we take the complex spectrum of the signal as the main target and the ideal ratio mask (IRM) as the auxiliary target in a multi-target learning framework to achieve their complementary advantages. Firstly, a feature extraction module is introduced to achieve the fusion of local and long-term features. Two different targets are learned separately, but share the common feature extraction module, which is helpful to extract more general and suitable features. During the joint learning, the intermediate estimation of the IRM target in the auxiliary path, contributing as the attention gating factors, helps to distinguish the speech or non-speech components of the complex-valued signals in the main path. To leverage more fine-grained long-term contextual information, we introduce a multi-scale dilated convolution approach for feature encoding. Moreover, the proposed model is a causal system, which can fully meet the low latency requirements of real-time speech products. Experimental results show that, compared with other advanced systems, the proposed model not only has better speech denoising performance and phase estimation accuracy, but also generalizes better in the speaker, noise, and channel mismatch cases.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2561–2574},
numpages = {14}
}

@inproceedings{10.1145/3404983.3405523,
author = {Rzayev, Rufat and Hartl, Sabrina and Wittmann, Vera and Schwind, Valentin and Henze, Niels},
title = {Effects of Position of Real-Time Translation on AR Glasses},
year = {2020},
isbn = {9781450375405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404983.3405523},
doi = {10.1145/3404983.3405523},
abstract = {Augmented reality (AR) provides users with contextually relevant multimedia content by overlaying it on real-world objects. However, overlaying virtual content on real-world objects can cause occlusion. Especially for learning use cases, the occlusion might result in missing real-world information important for learning gain. Therefore, it is important to understand how virtual content should be positioned relative to the related real-world information without negatively affecting the learning experience. Thus, we conducted a study with 12 participants using AR glasses to investigate the position of virtual content using a vocabulary learning task. Participants learned foreign words shown in the surrounding while viewing translations using AR glasses as an overlay, on the right or below the foreign word. We found that showing virtual translations on top of foreign words significantly decreases comprehension and increase users' task load. Insights from our study inform the design of applications for AR glasses supporting vocabulary learning.},
booktitle = {Proceedings of Mensch Und Computer 2020},
pages = {251–257},
numpages = {7},
keywords = {overlay, vocabulary learning, real-time translation, AR, augmented reality},
location = {Magdeburg, Germany},
series = {MuC '20}
}

@article{10.1109/TASLP.2021.3071662,
author = {Yoon, Ji Won and Lee, Hyeonseung and Kim, Hyung Yong and Cho, Won Ik and Kim, Nam Soo},
title = {TutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3071662},
doi = {10.1109/TASLP.2021.3071662},
abstract = {In recent years, there has been a great deal of research in developing end-to-end speech recognition models, which enable simplifying the traditional pipeline and achieving promising results. Despite their remarkable performance improvements, end-to-end models typically require expensive computational cost to show successful performance. To reduce this computational burden, knowledge distillation (KD), which is a popular model compression method, has been used to transfer knowledge from a deep and complex model (teacher) to a shallower and simpler model (student). Previous KD approaches have commonly designed the architecture of the student by reducing the width per layer or the number of layers of the teacher. This structural reduction scheme might limit the flexibility of model selection since the student model structure should be similar to that of the given teacher. To cope with this limitation, we propose a KD method for end-to-end speech recognition, namely TutorNet, that applies KD techniques across different types of neural networks at the hidden representation-level as well as the output-level. For concrete realizations, we firstly apply representation-level knowledge distillation (RKD) during the initialization step, and then apply the softmax-level knowledge distillation (SKD) combined with the original task learning. When the student is trained with RKD, we make use of frame weighting that points out the frames to which the teacher pays more attention. Through a number of experiments, it is verified that TutorNet not only distills the knowledge between networks with different topologies but also significantly contributes to improving the performance of the distilled student.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1626–1638},
numpages = {13}
}

@article{10.1109/TASLP.2021.3049344,
author = {Lee, Hyeonseung and Kang, Woo Hyun and Cheon, Sung Jun and Kim, Hyeongju and Kim, Nam Soo},
title = {Gated Recurrent Context: Softmax-Free Attention for Online Encoder-Decoder Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3049344},
doi = {10.1109/TASLP.2021.3049344},
abstract = {Recently, attention-based encoder-decoder (AED) models have shown state-of-the-art performance in automatic speech recognition (ASR). As the original AED models with global attentions are not capable of online inference, various online attention schemes have been developed to reduce ASR latency for better user experience. However, a common limitation of the conventional softmax-based online attention approaches is that they introduce an additional hyperparameter related to the length of the attention window, requiring multiple trials of model training for tuning the hyperparameter. In order to deal with this problem, we propose a novel softmax-free attention method and its modified formulation for online attention, which does not need any additional hyperparameter at the training phase. Through a number of ASR experiments, we demonstrate the tradeoff between the latency and performance of the proposed online attention technique can be controlled by merely adjusting a threshold at the test phase. Furthermore, the proposed methods showed competitive performance to the conventional global and online attentions in terms of word-error-rates (WERs).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {710–719},
numpages = {10}
}

@article{10.1109/TASLP.2021.3067154,
author = {Jeon, Kwang Myung and Lee, Geon Woo and Kim, Nam Kyun and Kim, Hong Kook},
title = {TAU-Net: Temporal Activation U-Net Shared With Nonnegative Matrix Factorization for Speech Enhancement in Unseen Noise Environments},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3067154},
doi = {10.1109/TASLP.2021.3067154},
abstract = {In this paper, a novel speech enhancement method based on a hybrid machine-learning architecture consisting of U-Net and nonnegative matrix factorization (NMF) is proposed. The proposed method attempts to take advantage of both the accurate separation for known noise environments by U-Net and the adaptation to unseen noises by an NMF with an online dictionary learning technique. To merge the two different architectures, a modified U-Net with a temporal activation layer (TAU-Net) is jointly optimized with NMF models that represent universal speech and noise. The proposed method first estimates the temporal activations from the encoder of the proposed TAU-Net. Then, an NMF with online dictionary learning adjusts the initially given temporal activations to suppress their cross-activations due to unseen noises that are unknown in the training phase of TAU-Net. Finally, clean speech is obtained by adjusting temporal activations to the TAU-Net decoder. The effectiveness of the proposed TAU-Net-based speech enhancement method is evaluated in various unseen noise environments. Consequently, the proposed method achieves a substantial improvement with average signal-to-distortion ratios of 2.32 dB and 5.68 dB, which are higher than those of the baseline methods such asspeech enhancement generative adversarial network (SEGAN) and U-Net, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {3400–3414},
numpages = {15}
}

@inproceedings{10.1145/3461615.3485439,
author = {Lyakso, Elena E. and Frolova, Olga V.},
title = {Recording the Speech of Children with Atypical Development: Peculiarities and Perspectives},
year = {2021},
isbn = {9781450384711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461615.3485439},
doi = {10.1145/3461615.3485439},
abstract = {The paper considers the possibility of using the unified speech recording protocol for research of speech in typically and atypically developing children. The research protocol includes using the model situations: dialogue, repetition, picture description, and playing. The peculiarities of speech recording in model situations for children with atypical development are described. The data on the speech of children with autism spectrum disorders, Down syndrome, and intellectual disabilities obtained in model situations are presented. The perspectives of future work taking into account creating an interactive computer program - a virtual assistant (“friend”) to avoid the contribution of individual characteristics of the experimenter and parents in the model situations are discussed.},
booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction},
pages = {408–413},
numpages = {6},
keywords = {Typical development, Atypical development, Speech recording, Child speech acoustics},
location = {Montreal, QC, Canada},
series = {ICMI '21 Companion}
}

@article{10.1109/TASLP.2019.2955276,
author = {Tan, Ke and Wang, DeLiang},
title = {Learning Complex Spectral Mapping With Gated Convolutional Recurrent Networks for Monaural Speech Enhancement},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955276},
doi = {10.1109/TASLP.2019.2955276},
abstract = {Phase is important for perceptual quality of speech. However, it seems intractable to directly estimate phase spectra through supervised learning due to their lack of spectrotemporal structure in it. Complex spectral mapping aims to estimate the real and imaginary spectrograms of clean speech from those of noisy speech, which simultaneously enhances magnitude and phase responses of speech. Inspired by multi-task learning, we propose a gated convolutional recurrent network (GCRN) for complex spectral mapping, which amounts to a causal system for monaural speech enhancement. Our experimental results suggest that the proposed GCRN substantially outperforms an existing convolutional neural network (CNN) for complex spectral mapping in terms of both objective speech intelligibility and quality. Moreover, the proposed approach yields significantly higher STOI and PESQ than magnitude spectral mapping and complex ratio masking. We also find that complex spectral mapping with the proposed GCRN provides an effective phase estimate.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {380–390},
numpages = {11}
}

@article{10.1145/3287058,
author = {Maruri, H\'{e}ctor A. Cordourier and Lopez-Meyer, Paulo and Huang, Jonathan and Beltman, Willem Marco and Nachman, Lama and Lu, Hong},
title = {V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287058},
doi = {10.1145/3287058},
abstract = {Smart glasses are often used in public environments or industrial scenarios that are relatively noisy. Background noise and sound from competing speakers deteriorate voice communication or performance of automatic speech recognition (ASR). Typically, signal processing techniques are used to reduce noise and enhance voice quality, but they have limitations in performance, hardware and/or computing resources. Voice capturing techniques using bone conducting on the head have been proposed in some experimental and commercial devices, with good robustness against environmental noise, but limited by signal distortions inherent to the capturing method. We present V-Speech, a novel sensing and signal processing solution that enables speech recognition and human-to-human communication in very noisy environments. It captures the voice signal with a vibration sensor located in the nasal pads of smart glasses and performs a transformation to the sensor signal in order to mimic that of a regular microphone in low noise conditions. The signal transformation is key, as it eliminates the "nasal distortion" that is introduced for nasal phonemes in the speech induced vibrations of the nasal bone. The output of V-Speech has low noise, sounds natural, and can be used in voice communication or as input to an off-the-shelf ASR service. We evaluated V-Speech in noise-free and noisy conditions with 30 volunteer speakers uttering 145 phrases and validated its performance on ASR engines and with assessments of voice quality using the Perceptual Evaluation of Speech Quality (PESQ) metric. The results show in extreme noise conditions a mean improvement of 50% for Word Error Rate (WER), and 1.0 on a scale of 5.0 for PESQ. In addition, real life recordings were made under various representative noise conditions, some with sound pressure levels of 93 dBA, which require hearing protection. Subjective listening tests were conducted according to a modified ITU P.835 approach to determine intelligibility, naturalness and overall quality. Under these extreme conditions, where V-Speech achieved 30 dB SNR, subjective results show the speech is intelligible, and the naturalness of the speech is rated as fair to good. This enables clear voice communication in challenging work environments, for example in places with industrial, factory, mining and construction noise. With our proposed smart switching technique between a regular microphone signal and V-Speech, the optimal quality can be maintained from low to high noise conditions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {180},
numpages = {23},
keywords = {Head worn devices, Accelerometer, Vibration sensing, Voice capturing, Smart glasses}
}

@article{10.1109/TASLP.2021.3082299,
author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Wen, Zhengqi and Zhang, Shuai},
title = {Fast End-to-End Speech Recognition Via Non-Autoregressive Models and Cross-Modal Knowledge Transferring From BERT},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3082299},
doi = {10.1109/TASLP.2021.3082299},
abstract = {Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because the decoder predicts text tokens (such as characters or words) in an autoregressive manner, it is difficult for an AED model to predict all tokens in parallel. This makes the inference speed relatively slow. In contrast, we propose an end-to-end non-autoregressive speech recognition model called LASO (Listen Attentively, and Spell Once). The model aggregates encoded speech features into the hidden representations corresponding to each token with attention mechanisms. Thus, the model can capture the token relations by self-attention on the aggregated hidden representations from the whole speech signal rather than autoregressive modeling on tokens. Without explicitly autoregressive language modeling, this model predicts all tokens in the sequence in parallel so that the inference is efficient. Moreover, we propose a cross-modal transfer learning method to use a text-modal language model to improve the performance of speech-modal LASO by aligning token semantics. We conduct experiments on two scales of public Chinese speech datasets AISHELL-1 and AISHELL-2. Experimental results show that our proposed model achieves a speedup of about <inline-formula><tex-math notation="LaTeX">$50times$</tex-math></inline-formula> and competitive performance, compared with the autoregressive transformer models. And the cross-modal knowledge transferring from the text-modal model can improve the performance of the speech-modal model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1897–1911},
numpages = {15}
}

@article{10.1145/3314945,
author = {Maimaiti, Mieradilijiang and Liu, Yang and Luan, Huanbo and Sun, Maosong},
title = {Multi-Round Transfer Learning for Low-Resource NMT Using Multiple High-Resource Languages},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314945},
doi = {10.1145/3314945},
abstract = {Neural machine translation (NMT) has made remarkable progress in recent years, but the performance of NMT suffers from a data sparsity problem since large-scale parallel corpora are only readily available for high-resource languages (HRLs). In recent days, transfer learning (TL) has been used widely in low-resource languages (LRLs) machine translation, while TL is becoming one of the vital directions for addressing the data sparsity problem in low-resource NMT. As a solution, a transfer learning method in NMT is generally obtained via initializing the low-resource model (child) with the high-resource model (parent). However, leveraging the original TL to low-resource models is neither able to make full use of highly related multiple HRLs nor to receive different parameters from the same parents. In order to exploit multiple HRLs effectively, we present a language-independent and straightforward multi-round transfer learning (MRTL) approach to low-resource NMT. Besides, with the intention of reducing the differences between high-resource and low-resource languages at the character level, we introduce a unified transliteration method for various language families, which are both semantically and syntactically highly analogous with each other. Experiments on low-resource datasets show that our approaches are effective, significantly outperform the state-of-the-art methods, and yield improvements of up to 5.63 BLEU points.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {38},
numpages = {26},
keywords = {transliteration, low-resource language, Neural machine translation, high-resource language, multi-round, transfer learning}
}

@article{10.1109/TASLP.2021.3097935,
author = {Li, Yuling and Yu, Kui and Zhang, Yuhong},
title = {Learning Cross-Lingual Mappings in Imperfectly Isomorphic Embedding Spaces},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3097935},
doi = {10.1109/TASLP.2021.3097935},
abstract = {One mainstream method in cross-lingual word embeddings is to learn a linear mapping between two monolingual embedding spaces using a training dictionary. Successful linear mappings require isomorphic embedding spaces. However, monolingual embedding spaces are not perfectly isomorphic, and therefore, a linear mapping cannot align them accurately. In this study, we assume that two embedding spaces are composed of near-isomorphic translation pairs (NearITP) and non-isomorphic translation pairs. Owing to the nature of similar substructures, NearITP can make linear mapping work well. Motivated by this, we design a screening strategy to identify NearITP effectively. Based on this strategy, we find that the proportion of NearITP in the commonly used training dictionary is relatively low, leading to sub-optimal results. To address this problem, we propose a general framework that can be combined with any of the mapping methods, which further boosts subsequent mapping. Experimental results demonstrate that our framework is an improvement over existing mapping-based methods, and outperforms state-of-the-art models on two public data sets. Moreover, we show that our framework can be successfully generalized to contextual word embeddings such as multilingual BERT (mBERT), and further enhances the cross-lingual properties of mBERT.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2630–2642},
numpages = {13}
}

@article{10.1109/TASLP.2018.2870725,
author = {Zhao, Yan and Wang, Zhong-Qiu and Wang, DeLiang},
title = {Two-Stage Deep Learning for Noisy-Reverberant Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2870725},
doi = {10.1109/TASLP.2018.2870725},
abstract = {In real-world situations, speech reaching our ears is commonly corrupted by both room reverberation and background noise. These distortions are detrimental to speech intelligibility and quality, and also pose a serious problem to many speech-related applications, including automatic speech and speaker recognition. In order to deal with the combined effects of noise and reverberation, we propose a two-stage strategy to enhance corrupted speech, where denoising and dereverberation are conducted sequentially using deep neural networks. In addition, we design a new objective function that incorporates clean phase during model training to better estimate spectral magnitudes, which would in turn yield better phase estimates when combined with iterative phase reconstruction. The two-stage model is then jointly trained to optimize the proposed objective function. Systematic evaluations and comparisons show that the proposed algorithm improves objective metrics of speech intelligibility and quality substantially, and significantly outperforms previous one-stage enhancement systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {53–62},
numpages = {10}
}

@article{10.1109/TASLP.2020.3039600,
author = {Fan, Cunhang and Yi, Jiangyan and Tao, Jianhua and Tian, Zhengkun and Liu, Bin and Wen, Zhengqi},
title = {Gated Recurrent Fusion With Joint Training Framework for Robust End-to-End Speech Recognition},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3039600},
doi = {10.1109/TASLP.2020.3039600},
abstract = {The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition (ASR). However, these methods only utilize the enhanced feature as the input of the speech recognition component, which are affected by the speech distortion problem. In order to address this problem, this paper proposes a gated recurrent fusion (GRF) method with joint training framework for robust end-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, the GRF is applied to address the speech distortion problem. Thirdly, to improve the performance of ASR, the state-of-the-art speech transformer algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate (CER) reduction of 10.04% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio (0 dB), our proposed method can achieves better performances with 12.67% CER reduction, which suggests the potential of our proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {198–209},
numpages = {12}
}

@inproceedings{10.1145/3404649.3404661,
author = {Huang, Hui-Wen and Lin, Qingxin and Darragh, Janine Julianna},
title = {Understanding EFL Learners' Self-Efficacy of Collaborative Translation in a Blended English Course},
year = {2020},
isbn = {9781450387781},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404649.3404661},
doi = {10.1145/3404649.3404661},
abstract = {This case study examined the effectiveness and students' perceptions of using collaborative translation in an EFL educational setting in China. Sixty-two sophomores majoring in a translation program participated in this study. Students' collaborative translation performance and their perceptions of applying collaborative translation, through pre-test, post-test, and two questionnaires, were investigated to obtain the findings. The results indicated that students' translation skills in the post-test significantly increased and they favored working on collaborative translation. Also, their self-efficacy level was relatively high towards collaborative translation after intervention. This study indicates that collaborative translation can serve as a useful pedagogy to help EFL students improve their translation skills.},
booktitle = {Proceedings of the 2020 4th International Conference on E-Education, E-Business and E-Technology},
pages = {78–83},
numpages = {6},
keywords = {Self-Efficacy, Collaborative Translation, EFL Learning, Group Collaboration},
location = {Shanghai, China},
series = {ICEBT '20}
}

@article{10.1109/TASLP.2018.2870742,
author = {Zheng, Naijun and Zhang, Xiao-Lei},
title = {Phase-Aware Speech Enhancement Based on Deep Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2870742},
doi = {10.1109/TASLP.2018.2870742},
abstract = {Short-time frequency transform STFT is fundamental in speech processing. Because of the difficulty of processing highly unstructured STFT phase, most speech-processing algorithms only operate with STFT magnitude, leaving the STFT phase far from explored. However, with the recent development of deep neural network DNN based speech processing, e.g., speech enhancement and recognition, phase processing is becoming more important than ever before as a new growing point of DNN-based methods. In this paper, we propose a phase-aware speech enhancement algorithm based on DNN. Specifically, in the training stage, when incorporating phase as a target, our core idea is to transform an unstructured phase spectrogram to its derivative along the time axis, i.e., instantaneous frequency deviation IFD, which has a similar structure with its corresponding magnitude spectrogram. We further propose to optimize both IFD and magnitude jointly in a multiobjective learning framework. In the test stage, we propose a postprocessing method to recover the phase spectrogram from the estimated IFD. Experimental results demonstrate the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {63–76},
numpages = {14}
}

@inproceedings{10.1145/3395035.3425657,
author = {van Hessen, Arjan and Calamai, Silvia and van den Heuvel, Henk and Scagliola, Stefania and Karrouche, Norah and Beeken, Jeannine and Corti, Louise and Draxler, Christoph},
title = {Speech, Voice, Text, And Meaning: A Multidisciplinary Approach to Interview Data through the Use of Digital Tools},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425657},
doi = {10.1145/3395035.3425657},
abstract = {Interview data is multimodal data: it consists of speech sound, facial expression and gestures, captured in a particular situation, and containing textual information and emotion. This workshop shows how a multidisciplinary approach may exploit the full potential of interview data. The workshop first gives a systematic overview of the research fields working with interview data. It then presents the speech technology currently available to support transcribing and annotating interview data, such as automatic speech recognition, speaker diarization, and emotion detection. Finally, scholars who work with interview data and tools may present their work and discover how to make use of existing technology.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {454–455},
numpages = {2},
keywords = {automatic speech recognition, multi-media information retrieval, oral history},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1109/ASE51524.2021.9678912,
author = {Jung, Chijung and Kim, Doowon and Wang, Weihang and Zheng, Yunhui and Lee, Kyu Hyung and Kwon, Yonghwi},
title = {Defeating Program Analysis Techniques via Ambiguous Translation},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678912},
doi = {10.1109/ASE51524.2021.9678912},
abstract = {This research explores the possibility of a new anti-analysis technique, carefully designed to attack weaknesses of the existing program analysis approaches. It encodes a program code snippet to hide, and its decoding process is implemented by a sophisticated state machine that produces multiple outputs depending on inputs. The key idea of the proposed technique is to ambiguously decode the program code, resulting in multiple decoded code snippets that are challenging to distinguish from each other. Our approach is stealthier than previous similar approaches as its execution does not exhibit different behaviors between when it decodes correctly or incorrectly. This paper also presents analyses of weaknesses of existing techniques and discusses potential improvements. We implement and evaluate the proof of concept approach, and our preliminary results show that the proposed technique imposes various new unique challenges to the program analysis technique.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1382–1387},
numpages = {6},
keywords = {anti-analysis, translation, program analysis},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3313950.3313968,
author = {Samonte, Mary Jane C. and Guce, Frances Casey D. and Peraja, Janina Mikaela P. and Sambile, Gwynel Daniel V.},
title = {Assistive Gamification and Speech Recognition E-Tutor System for Speech Impaired Students},
year = {2019},
isbn = {9781450360920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313950.3313968},
doi = {10.1145/3313950.3313968},
abstract = {In a single school year, 5,857 young Filipino students have been found to suffer from evident speech or language impairments. The Philippines has this number of children, all of whom experience a great disadvantage in learning and communication, both of which are crucial and very much needed. Speech impairment does not necessarily mean that the person cannot speak but rather finding it difficult to do so. It is a condition that can affect the academic performance of a child because of the difficulty in communicating by stuttering and such. The deaf or the hard-of-hearing (HoH) also sometimes develop this kind of impairment due to development issues. This paper concentrates on the design of a Gamified E-Tutor System that utilizes speech recognition in teaching Statistics to senior high school students with speech impairment with the help of Filipino Sign Language (FSL). The said components (gamification/speech recognition) were integrated into the system to improve the learning engagement of students with speech impairment and supplement the speech therapies they go through. By integrating gamification elements with learning strategies for students with disabilities, a speech therapist can use this system to supplement their sessions and monitor their status, as well as their progress. Through this E-Learning system, information regarding gamification elements may be extracted to help determine the most effective learning components for students with speech impairment.},
booktitle = {Proceedings of the 2nd International Conference on Image and Graphics Processing},
pages = {37–41},
numpages = {5},
keywords = {Filipino sign language, gamification, speech recognition, assistive technology, e-tutor},
location = {Singapore, Singapore},
series = {ICIGP '19}
}

@inproceedings{10.1109/ASONAM49781.2020.9381410,
author = {Ma, Hao-Shang and Huang, Jen-Wei},
title = {User Preference Translation Model for Recommendation System with Item Influence Diffusion Embedding},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381410},
doi = {10.1109/ASONAM49781.2020.9381410},
abstract = {Recommendation systems which are designed to understand and predict user interest based on user preferences play an important role in the era of information explosion. We propose the item influence embedding which adopts the social influence diffusion concept to model the item relations. We can learn the activation paths in items-item relation graph. In addition, for generating top-k items, most of recommendation systems calculate the similarity between user embedding and embedding of all items. The calculation costs too much time when number of users and items are huge. Therefore, we propose the User Preference Translation Model (UPTM) to recommend the Top-k items based on the language translation technology. UPTM directly generates the recommendation items based on translating the user preference. We can avoid to calculate the similarity of user embedding and item embedding. From the experimental results, UPTM not only outperforms the compared methods but also save the time in real large datasets.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {50–54},
numpages = {5},
keywords = {recommendation systems, translation-based recommendation model, item influence embedding},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3503161.3548194,
author = {Wang, Yongqi and Zhao, Zhou},
title = {FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548194},
doi = {10.1145/3503161.3548194},
abstract = {Unconstrained lip-to-speech synthesis aims to generate corresponding speeches from silent videos of talking faces with no restriction on head poses or vocabulary. Current works mainly use sequence-to-sequence models to solve this problem, either in an autoregressive architecture or a flow-based non-autoregressive architecture. However, these models suffer from several drawbacks: 1) Instead of directly generating audios, they use a two-stage pipeline that first generates mel-spectrograms and then reconstructs audios from the spectrograms. This causes cumbersome deployment and degradation of speech quality due to error propagation; 2) The audio reconstruction algorithm used by these models limits the inference speed and audio quality, while neural vocoders are not available for these models since their output spectrograms are not accurate enough; 3) The autoregressive model suffers from high inference latency, while the flow-based model has high memory occupancy: neither of them is efficient enough in both time and memory usage. To tackle these problems, we propose FastLTS, a non-autoregressive end-to-end model which can directly synthesize high-quality speech audios from unconstrained talking videos with low latency, and has a relatively small model size. Besides, different from the widely used 3D-CNN visual frontend for lip movement encoding, we for the first time propose a transformer-based visual frontend for this task. Experiments show that our model achieves 19.76x speedup for audio waveform generation compared with the current autoregressive model on input sequences of 3 seconds, and obtains superior audio quality.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5678–5687},
numpages = {10},
keywords = {multimodal translation, lip-to-speech synthesis, deep learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2021.3069080,
author = {Hu, Shoukang and Xie, Xurong and Liu, Shansong and Yu, Jianwei and Ye, Zi and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
title = {Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3069080},
doi = {10.1109/TASLP.2021.3069080},
abstract = {Discriminative training techniques define state-of-the-art performance for automatic speech recognition systems. However, they are inherently prone to overfitting, leading to poor generalization performance when using limited training data. In order to address this issue, this paper presents a full Bayesian framework to account for model uncertainty in sequence discriminative training of factored TDNN acoustic models. Several Bayesian learning based TDNN variant systems are proposed to model the uncertainty over weight parameters and choices of hidden activation functions, or the hidden layer outputs. Efficient variational inference approaches using as few as one single parameter sample ensure their computational cost in both training and evaluation time comparable to that of the baseline TDNN systems. Statistically significant word error rate (WER) reductions of 0.4%–1.8% absolute (5%-11% relative) were obtained over a state-of-the-art 900&nbsp;h speed perturbed Switchboard corpus trained baseline LF-MMI factored TDNN system using multiple regularization methods including F-smoothing, L2 norm penalty, natural gradient, model averaging and dropout, in addition to i-Vector plus learning hidden unit contribution (LHUC) based speaker adaptation and RNNLM rescoring. The efficacy of the proposed Bayesian techniques is further demonstrated in a comparison against the state-of-the-art performance obtained on the same task using the most recent hybrid and end-to-end systems reported in the literature. Consistent performance improvements were also obtained on a 450-h HKUST conversational Mandarin telephone speech recognition task. On a third cross domain adaptation task requiring rapidly porting a 1000-h LibriSpeech data trained system to a small DementiaBank elderly speech corpus, the proposed Bayesian TDNN LF-MMI systems outperformed the baseline system using direct weight fine-tuning by up to 2.5% absolute WER reduction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1514–1529},
numpages = {16}
}

@article{10.1109/TASLP.2020.2988788,
author = {Ram, Dhananjay and Miculicich, Lesly and Bourlard, Herv\'{e}},
title = {Neural Network Based End-to-End Query by Example Spoken Term Detection},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2988788},
doi = {10.1109/TASLP.2020.2988788},
abstract = {This article focuses on the problem of query by example spoken term detection (QbE-STD) in zero-resource scenario. State-of-the-art approaches primarily rely on dynamic time warping (DTW) based template matching techniques using phone posterior or bottleneck features extracted from a deep neural network (DNN). We use both monolingual and multilingual bottleneck features, and show that multilingual features perform increasingly better with more training languages. Previously, it has been shown that the DTW based matching can be replaced with a CNN based matching while using posterior features. Here, we show that the CNN based matching outperforms DTW based matching using bottleneck features as well. In this case, the feature extraction and pattern matching stages of our QbE-STD system are optimized independently of each other. We propose to integrate these two stages in a fully neural network based end-to-end learning framework to enable joint optimization of those two stages simultaneously. The proposed approaches are evaluated on two challenging multilingual datasets: Spoken Web Search 2013 and Query by Example Search on Speech Task 2014, demonstrating in each case significant improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1416–1427},
numpages = {12}
}

@inproceedings{10.1145/3242969.3243014,
author = {Sterpu, George and Saam, Christian and Harte, Naomi},
title = {Attention-Based Audio-Visual Fusion for Robust Automatic Speech Recognition},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3243014},
doi = {10.1145/3242969.3243014},
abstract = {Automatic speech recognition can potentially benefit from the lip motion patterns, complementing acoustic speech to improve the overall recognition performance, particularly in noise. In this paper we propose an audio-visual fusion strategy that goes beyond simple feature concatenation and learns to automatically align the two modalities, leading to enhanced representations which increase the recognition accuracy in both clean and noisy conditions. We test our strategy on the TCD-TIMIT and LRS2 datasets, designed for large vocabulary continuous speech recognition, applying three types of noise at different power ratios. We also exploit state of the art Sequence-to-Sequence architectures, showing that our method can be easily integrated. Results show relative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality alone, depending on the acoustic noise level. We anticipate that the fusion strategy can easily generalise to many other multimodal tasks which involve correlated modalities.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {111–115},
numpages = {5},
keywords = {multimodal interfaces, multimodal fusion, lipreading, audio-visual speech recognition},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@article{10.1109/TASLP.2019.2946789,
author = {Wang, Peidong and Tan, Ke and Wang, De Liang},
title = {Bridging the Gap Between Monaural Speech Enhancement and Recognition With Distortion-Independent Acoustic Modeling},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946789},
doi = {10.1109/TASLP.2019.2946789},
abstract = {Monaural speech enhancement has made dramatic advances since the introduction of deep learning a few years ago. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this article, we analyze the distortion problem, compare different acoustic models, and investigate a distortion-independent training scheme for monaural speech recognition. Experimental results suggest that distortion-independent acoustic modeling is able to overcome the distortion problem. Such an acoustic model can also work with speech enhancement models different from the one used during training. Moreover, the models investigated in this paper outperform the previous best system on the CHiME-2 corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {39–48},
numpages = {10}
}

@inproceedings{10.1145/3461615.3491114,
author = {Chen, Yi and Yang, Shan and Hu, Na and Xie, Lei and Su, Dan},
title = {TeNC: Low Bit-Rate Speech Coding with VQ-VAE and GAN},
year = {2021},
isbn = {9781450384711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461615.3491114},
doi = {10.1145/3461615.3491114},
abstract = {Speech coding aims at compressing digital speech signals with fewer bits and reconstructing it back to raw signals, maintaining the speech quality as much as possible. But conventional codecs usually need a high bit-rate to achieve reconstructed speech with reasonable high quality. In this paper, we propose an end-to-end neural generative codec with a VQ-VAE based auto-encoder and the generative adversarial network (GAN), which achieves reconstructed speech with high-fidelity at a low bit-rate about 2 kb/s. The compression process of speech coding is carried out by a down-sampling module of the encoder and a learnable discrete codebook. GAN is used to further improve the reconstructed quality. Our experiments confirm the effectiveness of the proposed model in both objective and subjective tests, which significantly outperforms the conventional codecs at low bit-rate in terms of speech quality and speaker similarity.},
booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction},
pages = {126–130},
numpages = {5},
keywords = {neural speech coding, VQ-VAE, low bit-rate, GAN, Codec},
location = {Montreal, QC, Canada},
series = {ICMI '21 Companion}
}

@article{10.1109/TASLP.2019.2945485,
author = {Tiwari, Nitya and Pandey, Prem C.},
title = {Speech Enhancement Using Noise Estimation With Dynamic Quantile Tracking},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2945485},
doi = {10.1109/TASLP.2019.2945485},
abstract = {A technique for quantile-based noise estimation is presented for single-input speech enhancement in hearing aids and speech communication devices. The noise spectrum is updated by dynamic tracking of quantiles of the samples of the magnitude spectrum of the noisy speech without sorting of the past samples. Another technique is presented for improved tracking of nonstationary noise using adaptive quantiles, which are calculated by estimation of the quantile functions. The two noise estimation techniques are compared with some of the earlier techniques in terms of computational requirement, error in noise tracking, and speech enhancement using spectral subtraction based on the geometric approach. The technique with fixed quantiles has the lowest computational requirement and its performance in terms of noise tracking and speech enhancement for different SNRs and noise types is found to be better than or comparable to the earlier techniques. The technique with adaptive quantiles, having a higher computational requirement, provides better performance at low SNRs and for nonstationary noises.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2301–2312},
numpages = {12}
}

@inproceedings{10.1145/3475720.3484443,
author = {Alrasheed, Nouf and Prasanna, Shivika and Rowland, Ryan and Rao, Praveen and Grieco, Viviana and Wasserman, Martin},
title = {Evaluation of Deep Learning Techniques for Content Extraction in Spanish Colonial Notary Records},
year = {2021},
isbn = {9781450386685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475720.3484443},
doi = {10.1145/3475720.3484443},
abstract = {Processing and analyzing historical manuscripts is considered one of the most challenging problems in the document analysis and recognition domain. Manuscripts written in cursive are even more difficult due to overlapping words with random spacing, irregular and varying characters' shapes, poor scan quality, and insufficient labeled data. Despite the significant achievements of deep learning approaches in computer vision, handwritten word recognition is far from solved. Most of the existing methods focus on well-segmented word datasets. In this paper, we present an empirical study investigating how well state-of-the-art deep learning models perform on detection and recognition of handwritten words in Spanish American notary records. Professional historians were involved in preparing a labeled dataset of 26,482 Spanish words employed in the experiments. We investigate the performance of some state-of-the-art models on optical character recognition (OCR) on handwritten text documents: Keras-OCR, the object detection algorithm "You Only Look Once" (YOLO), Tesseract OCR, Kraken, and Calamari-OCR. Since YOLO does not include a text recognizer, we propose YOLO-OCR, an innovative model to detect and recognize words in historical manuscripts written in Spanish. Our results show the performance of pre-trained models on our dataset and that Keras-OCR and YOLO-OCR models are highly valuable for content extraction.},
booktitle = {Proceedings of the 3rd Workshop on Structuring and Understanding of Multimedia HeritAge Contents},
pages = {23–30},
numpages = {8},
keywords = {content extraction, deep learning, word recognition, spanish manuscripts, word detection},
location = {Virtual Event, China},
series = {SUMAC'21}
}

@article{10.1109/TASLP.2021.3100682,
author = {Xu, Chenglin and Rao, Wei and Wu, Jibin and Li, Haizhou},
title = {Target Speaker Verification With Selective Auditory Attention for Single and Multi-Talker Speech},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3100682},
doi = {10.1109/TASLP.2021.3100682},
abstract = {Speaker verification has been studied mostly under the single-talker condition. It is adversely affected in the presence of interference speakers. Inspired by the study on target speaker extraction, e.g., SpEx, we propose a unified speaker verification framework for both single- and multi-talker speech, that is able to pay selective auditory attention to the target speaker. This target speaker verification (tSV) framework jointly optimizes a speaker attention module and a speaker representation module via multi-task learning. We study four different target speaker embedding schemes under the tSV framework. The experimental results show that all four target speaker embedding schemes significantly outperform other competitive solutions for multi-talker speech. Notably, the best tSV speaker embedding scheme achieves 76.0% and 55.3% relative improvements over the baseline system on the WSJ0-2mix-extr and Libri2Mix corpora in terms of equal-error-rate for 2-talker speech, while the performance of tSV for single-talker speech is on par with that of traditional speaker verification system, that is trained and evaluated under the same single-talker condition.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2696–2709},
numpages = {14}
}

@inproceedings{10.1145/3457784.3457811,
author = {Lin, Peizhen and Liu, Baoyu and Wang, Lei and Lei, Zetong and Cheng, Jun},
title = {Face Translation Based on Semantic Style Transfer and Rendering from One Single Image},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457811},
doi = {10.1145/3457784.3457811},
abstract = {Many avatar characters have been animated in films or games, which always need a lot of time for post-processing with the computer graphics technologies. In recent years, lots of deep learning based methods have been proposed for face translation and image generation, which always require a large amount of data for training. However, there are few samples for special characters' prototype. In this paper, we present one face translation framework for translating human faces to that with visual effects from one single prototype image. The proposed framework consists of three modules. We first design one module to generate semantic face mask–the semantic mask generating (SMG) module. According to the semantic mask, the face color tone can be changed to that of the prototype. So we design the semantic color transfer (SCT) module. For the local textures, we design the deformation and rendering (DR) module. Experiments show that the proposed framework can generate images with prototype's visual effects while preserving the original person's identification and expression information.},
booktitle = {Proceedings of the 2021 10th International Conference on Software and Computer Applications},
pages = {166–172},
numpages = {7},
keywords = {Deep learning, Face translation, Mask Generative Adversarial Networks,},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA '21}
}

@inproceedings{10.1145/3343031.3350981,
author = {Shen, Guangyao and Huang, Wenbing and Gan, Chuang and Tan, Mingkui and Huang, Junzhou and Zhu, Wenwu and Gong, Boqing},
title = {Facial Image-to-Video Translation by a Hidden Affine Transformation},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350981},
doi = {10.1145/3343031.3350981},
abstract = {There has been a prominent emergence of work on video prediction, aiming to extrapolate the future video frames from the past. Existing temporal-based methods are limited to certain numbers of frames. In this paper, we study video prediction from a single still image in the facial expression domain, a.k.a, facial image-to-video translation. Our main approach, dubbed AffineGAN, associates each facial image with an expression intensity and leverages an affine transformation in the latent space. AffineGAN allows users to control the number of frames to predict as well as the expression intensity for each of them. Unlike previous intensity-based methods, We derive an inverse formulation to the affine transformation, enabling automatic inference of the facial expression intensities from videos --- manual annotation is not only tedious but also ambiguous as people express in various ways and have different opinions about the intensity of a facial image. Both quantitative and qualitative results verify the superiority of AffineGAN over the state of the arts. Notably, in a Turing test with web faces, more than 50% of the facial expression videos generated by AffineGAN are considered real by the Amazon Mechanical Turk workers. This work could improve users' communication experience by enabling them to conveniently and creatively produce expression GIFs, which are popular art forms in online messaging and social networks.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2505–2513},
numpages = {9},
keywords = {image-to-video translation, face manipulation, gan},
location = {Nice, France},
series = {MM '19}
}

@article{10.1109/TASLP.2021.3129357,
author = {Xu, Junhao and Yu, Jianwei and Hu, Shoukang and Liu, Xunying and Meng, Helen},
title = {Mixed Precision Low-Bit Quantization of Neural Network Language Models for Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3129357},
doi = {10.1109/TASLP.2021.3129357},
abstract = {State-of-the-art language models (LMs) represented by long-short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming increasingly complex and expensive for practical applications. Low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. Current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of LMs to quantization errors. To this end, novel mixed precision neural network LM quantization methods are proposed in this paper. The optimal local precision choices for LSTM-RNN and Transformer based neural LMs are automatically learned using three techniques. The first two approaches are based on quantization sensitivity metrics in the form of either the KL-divergence measured between full precision and quantized LMs, or Hessian trace weighted quantization perturbation that can be approximated efficiently using matrix free techniques. The third approach is based on mixed precision neural architecture search. In order to overcome the difficulty in using gradient descent methods to directly estimate discrete quantized weights, alternating direction methods of multipliers (ADMM) are used to efficiently train quantized LMs. Experiments were conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation on two tasks: Switchboard telephone speech and AMI meeting transcription. The proposed mixed precision quantization techniques achieved “lossless” quantization on both tasks, by producing model size compression ratios of up to approximately 16 times over the full precision LSTM and Transformer baseline LMs, while incurring no statistically significant word error rate increase.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3679–3693},
numpages = {15}
}

@inproceedings{10.1145/3532719.3543223,
author = {Royo, Diego and Garcia, Jorge and Luesia-Lahoz, Pablo and Marco, Julio and Guti\'{e}rrez, Diego and Mu\~{n}oz, Adolfo and Jarabo, Adri\'{a}n},
title = {Non-Line-of-Sight Transient Rendering},
year = {2022},
isbn = {9781450393614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532719.3543223},
doi = {10.1145/3532719.3543223},
abstract = {Transient imaging methods often analyze time-resolved light transport for applications such as range imaging, reflectance estimation and, especially, non-line-of-sight (NLOS) imaging, which targets the reconstruction of hidden geometry using measurements of indirect diffuse reflections emitted by a laser. Transient rendering is a key tool for developing such new applications. In this work, we introduce a set of simple, yet effective subpath sampling techniques targeting transient light transport simulation in occluded scenes. We analyze the usual capture setups of NLOS scenes, where the light and camera indirectly aim at hidden geometry through a secondary surface. We leverage that configuration to reduce the integration path space. We implement our techniques in our modified version of Mitsuba 2, adapted for transient light transport, allowing us to support parallelization, polarization, and differentiable rendering.},
booktitle = {ACM SIGGRAPH 2022 Posters},
articleno = {39},
numpages = {2},
keywords = {transient rendering, non-line-of-sight imaging, light transport},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@article{10.1145/3563944,
author = {Chakraborty, Chinmay and Dash*, Tusar Kanti and Panda, Ganapati and Solanki, Sandeep Singh},
title = {Phase-Based Cepstral Features for Automatic Speech Emotion Recognition of Low Resource Indian Languages},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3563944},
doi = {10.1145/3563944},
abstract = {Automatic speech emotion recognition (SER) is a crucial task in communication-based systems, where feature extraction plays an important role. Recently, a lot of SER models have been developed and implemented successfully in English and other western languages. However, the performance of the traditional Indian languages in SER is not up to the mark. This problem of SER in low-resource Indian languages mainly the Bengali language is dealt with in this paper. In the first step, the relevant phase-based information from the speech signal is extracted in the form of phase-based cepstral features (PBCC) using cepstral, and statistical analysis. Several pre-processing techniques are combined with features extraction and gradient boosting machine-based classifier in the proposed SER model. Finally, the evaluation and comparison of simulation results on speaker-dependent, speaker-independent tests are performed using multiple language datasets, and independent test sets. It is observed that the proposed PBCC features-based model is performing well with an average of 96% emotion recognition efficiency as compared to standard methods.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
keywords = {Low Resource Indian languages, Phase-based features, Cepstral features, LGBM, Speech Emotion Recognition}
}

@inproceedings{10.1145/3549206.3549269,
author = {Mehrotra, Utkarsh and Garg, Sparsh and Gurugubelli, Krishna and Vuppala, Anil Kumar},
title = {Towards Improving Disfluency Detection from Speech Using Shifted Delta Cepstral Coefficients},
year = {2022},
isbn = {9781450396752},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549206.3549269},
doi = {10.1145/3549206.3549269},
abstract = {In this paper, we investigate the use of shifted delta cepstral (SDC) coefficients for detecting disfluencies in two types of speech - stuttered speech and spontaneous lecture-mode speech. SDC features capture temporal variations in the speech signal effectively across several frames. The UCLASS stuttered speech dataset and IIITH-IED dataset are used here to develop frame-level automatic disfluency detection systems for four types of disfluencies and the effect of SDC features on the detection of each disfluency type is examined using MFCC and SFFCC cepstral representations. Overall, it is found that using MFCC+SDC features gives an absolute improvement of 2.98% and 6.02% for stuttered and spontaneous speech disfluency detection respectively over the static MFCC features, while SFFCC+SDC features give an absolute improvement of 4.62% for stutter disfluencies and 7.28% for spontaneous speech disfluencies over the static SFFCC features, showing the importance of considering temporal variations for disfluency detection.},
booktitle = {Proceedings of the 2022 Fourteenth International Conference on Contemporary Computing},
pages = {350–355},
numpages = {6},
keywords = {Speech Disfluencies, Shifted Delta Cepstral Coefficients., Single Frequency Filtering},
location = {Noida, India},
series = {IC3-2022}
}

@inproceedings{10.1145/3503161.3548221,
author = {Yu, Zhenjie and Chen, Kai and Li, Shuang and Han, Bingfeng and Liu, Chi Harold and Wang, Shuigen},
title = {ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime Infrared to Daytime Visible Video Translation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548221},
doi = {10.1145/3503161.3548221},
abstract = {Infrared cameras are often utilized to enhance the night vision since the visible light cameras exhibit inferior efficacy without sufficient illumination. However, infrared data possesses inadequate color contrast and representation ability attributed to its intrinsic heat-related imaging principle, which hinders its application. Although, the domain gaps between unpaired nighttime infrared and daytime visible videos are even huger than paired ones that captured at the same time, establishing an effective translation mapping will greatly contribute to various fields. In this case, the structural knowledge within nighttime infrared videos and semantic information contained in the translated daytime visible pairs could be utilized simultaneously. To this end, we propose a tailored framework ROMA that couples with our introduced cRoss-domain regiOn siMilarity mAtching technique for bridging the huge gaps. To be specific, ROMA could efficiently translate the unpaired nighttime infrared videos into fine-grained daytime visible ones, meanwhile maintain the spatiotemporal consistency via matching the cross-domain region similarity. Furthermore, we design a multiscale region-wise discriminator to distinguish the details from synthesized visible results and real references. Moreover, we provide a new and challenging dataset encouraging further research for unpaired nighttime infrared and daytime visible video translation, named InfraredCity, which is $20$ times larger than the recently released infrared-related dataset IRVI. Codes and datasets are available https://github.com/BIT-DA/ROMA here.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5294–5302},
numpages = {9},
keywords = {nighttime infrared, video-to-video translation, daytime visible},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3507657.3528558,
author = {Walker, Payton and McClaran, Nathan and Zheng, Zihao and Saxena, Nitesh and Gu, Guofei},
title = {BiasHacker: Voice Command Disruption by Exploiting Speaker Biases in Automatic Speech Recognition},
year = {2022},
isbn = {9781450392167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507657.3528558},
doi = {10.1145/3507657.3528558},
abstract = {Modern speech recognition systems that are widely deployed today still suffer from known gender and racial biases. In this work, we demonstrate the potential to exploit the existing biases in these systems to achieve a new attack goal. We consider the potential for command disruption by an attacker that can be conducted in a manner that allows for access and control of a victim's voice assistant device. We present a novel attack, BiasHacker, which crafts specialized chatter noise to exploit racial and gender biases in speech recognition systems for the purposes of command disruption. Our experimental results confirm both racial and gender bias that is still present in the speech recognition systems of two modern smart speaker devices. We also evaluated the effectiveness of three types of chatter noise (American English (AE)-Male, Nigerian-Female, Korean-Female) for disruption and demonstrate that the AE-Male chatter is consistently more successful. Comparing the average success rate of each chatter type, in scenarios where disruption was achieved, we find that when targeting the Google Home mini smart speaker, the AE-Male chatter noise increases average disruption success compared to the Nigerian-Female and Korean-Female chatter noises by 112% and 121%, respectively. Also, when targeting the Amazon Echo Dot 2 the AE-Male chatter noise increases average disruption success compared to the Nigerian-Female and Korean-Female chatter noises by 42% and 69%, respectively.},
booktitle = {Proceedings of the 15th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {119–124},
numpages = {6},
keywords = {voice assistant, speech recognition, command disruption, bias},
location = {San Antonio, TX, USA},
series = {WiSec '22}
}

@article{10.1145/3600228,
author = {Mirishkar, Ganesh S. and Raju V, Vishnu Vidyadhara and Naroju, Meher Dinesh and Maity, Sudhamay and Yalla, Prakash and Vuppala, Anil Kumar},
title = {IIITH-CSTD Corpus: Crowdsourced Strategies for the Collection of a Large-Scale Telugu Speech Corpus},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3600228},
doi = {10.1145/3600228},
abstract = {Due to the lack of a large annotated speech corpus, many low-resource Indian languages struggle to utilize recent advancements in deep neural network architectures for Automatic Speech Recognition (ASR) tasks. Collecting large-scale databases is an expensive and time-consuming task. Current approaches lack extensive traditional expert-based data acquisition guidelines, as they are tedious and complex. In this work, we present the International Institute of Information Technology Hyderabad-Crowd Sourced Telugu Database (IIITH-CSTD), a Telugu corpus collected through crowdsourcing. In particular, our main objective is to mitigate the low-resource problem for Telugu. We also present the sources, crowdsourcing pipeline, and the protocols used to collect the corpus for a low-resource language, namely, Telugu. Data of approximately 2,000 hours of transcribed audio is presented and released in this article, covering three major regional dialects of the Telugu language in three different (i.e., read, conversational and spontaneous) speaking styles on topics such as politics, sports, and arts, science, and so on.1 We also present the experimental results of the collected corpus on ASR tasks. We hope this work will motivate researchers to curate large-scale annotated speech data for other low-resource Indic languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {195},
numpages = {26},
keywords = {End-to-End, Speech recognition, TDNN, low-resource languages, resource creation, dialects}
}

@article{10.1109/TASLP.2021.3122291,
author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3122291},
doi = {10.1109/TASLP.2021.3122291},
abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.<xref ref-type="fn" rid="fn1"><sup>1</sup></xref><xref ref-type="fn" rid="fn2"><sup>2</sup></xref>},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {3451–3460},
numpages = {10}
}

@article{10.1145/3524019,
author = {Mone, Gregory},
title = {Raising Robovoices},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524019},
doi = {10.1145/3524019},
abstract = {New systems can model and synthesize voices, and even translate them into other languages.},
journal = {Commun. ACM},
month = {apr},
pages = {30–31},
numpages = {2}
}

@inproceedings{10.1145/3448139.3448147,
author = {Barbosa, Arthur and Ferreira, M\'{a}verick and Ferreira Mello, Rafael and Dueire Lins, Rafael and Gasevic, Dragan},
title = {The Impact of Automatic Text Translation on Classification of Online Discussions for Social and Cognitive Presences},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448147},
doi = {10.1145/3448139.3448147},
abstract = {This paper reports the findings of a study that measured the effectiveness of employing automatic text translation methods in automated classification of online discussion messages according to the categories of social and cognitive presences. Specifically, we examined the classification of 1,500 Portuguese and 1,747 English discussion messages using classifiers trained on the datasets before and after the application of text translation. While the English model generated, with the original and translated texts, achieved results (accuracy and Cohen’s κ) similar to those of the previously reported studies, the translation to Portuguese led to a decrease in the performance. The indicates the general viability of the proposed approach when converting the text to English. Moreover, this study highlighted the importance of different features and resources, and the limitations of the resources for Portuguese as reasons of the results obtained.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {77–87},
numpages = {11},
keywords = {Online Discussion, Text Translation, Community of Inquiry Model, Content Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@article{10.1145/3389010,
author = {List, Johann-Mattis and Sims, Nathaniel A. and Forkel, Robert},
title = {Toward a Sustainable Handling of Interlinear-Glossed Text in Language Documentation},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3389010},
doi = {10.1145/3389010},
abstract = {While the amount of digitally available data on the worlds’ languages is steadily increasing, with more and more languages being documented, only a small proportion of the language resources produced are sustainable. Data reuse is often difficult due to idiosyncratic formats and a negligence of standards that could help to increase the comparability of linguistic data. The sustainability problem is nicely reflected in the current practice of handling interlinear-glossed text, one of the crucial resources produced in language documentation. Although large collections of glossed texts have been produced so far, the current practice of data handling makes data reuse difficult. In order to address this problem, we propose a first framework for the computer-assisted, sustainable handling of interlinear-glossed text resources. Building on recent standardization proposals for word lists and structural datasets, combined with state-of-the-art methods for automated sequence comparison in historical linguistics, we show how our workflow can be used to lift a collection of interlinear-glossed Qiang texts (an endangered language spoken in Sichuan, China), and how the lifted data can assist linguists in their research.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {20},
numpages = {15},
keywords = {interlinear-glossed text, Sino-tibetan, standardization, qiang, computer-assisted language comparison}
}

@article{10.1109/TASLP.2018.2868407,
author = {Bao, Feng and Abdulla, Waleed H. and Bao, Feng and Abdulla, Waleed H.},
title = {A New Ratio Mask Representation for CASA-Based Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2868407},
doi = {10.1109/TASLP.2018.2868407},
abstract = {In the computational auditory scene analysis method, the ideal ratio mask or alternatively the ideal binary mask is the key point to reconstruct the enhanced signal. The ratio mask in its Wiener filtering or its square root form is currently considered. However, this kind of ratio mask overlooked one important issue. It does not exploit the inter-channel correlation ICC in the noisy speech, noise, and clean speech spectra. Thus, in this paper, we first propose a novel ratio mask representation by utilizing the ICC. In this way, we adaptively reallocate the power ratio of the speech and noise during the construction of ratio mask, thus, more speech and noise components are retained and masked at the same time, respectively. Second, the channel-weight contour based on the equal loudness hearing attribute is adopted to revise this new ratio mask in each Gammatone filterbank channel. Finally, the revised ratio mask is effectively used to train a five-layer structured deep neural network. Experiments show that the proposed ratio mask performs better than the conventional ratio mask representation and other series of enhancement algorithms in terms of speech quality, intelligibility, and spectral distortion under different signal to noise ratio conditions using six types of noises.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {7–19},
numpages = {13}
}

@article{10.1109/TASLP.2021.3067202,
author = {Cho, Byung Joon and Park, Hyung-Min},
title = {Convolutional Maximum-Likelihood Distortionless Response Beamforming With Steering Vector Estimation for Robust Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3067202},
doi = {10.1109/TASLP.2021.3067202},
abstract = {Beamforming has been one of the most successful approaches using multi-microphones for robust speech recognition. Although a beamforming method, called the “maximum-likelihood distortionless response (MLDR)” beamformer, was recently presented to achieve promising performance, it requires an accurate steering vector for a target speaker in advance like many kinds of beamformers. In this paper, we present a method for steering vector estimation (SVE) by replacing the noise spatial covariance matrix estimate with a normalized version of the variance-weighted spatial covariance matrix estimate for the observed noisy speech signal obtained by the iterative update rule in the MLDR beamforming framework. In addition, an MLDR beamforming method without a steering vector for a target speaker given in advance is presented where the SVE and the beamforming are alternately repeated. Furthermore, an online algorithm based on recursive least squares (RLS) is derived to cope with various practical applications including time-varying situations, and the power method is introduced for further efficient online processing. We also present batch and online convolutional MLDR beamforming with SVE for simultaneous beamforming and dereverberation where the weighted prediction error (WPE) dereverberation and the MLDR beamforming with the SVE were jointly optimized based on the maximum-likelihood estimation (MLE) for a zero-mean complex Gaussian signal with time-varying variances. Moreover, input signals masked by a neural network (NN) for estimating target speech or noise components can be used to further improve the presented beamformers. Experimental results on the CHiME-4 and REVERB challenge datasets demonstrate the effectiveness of the presented methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1352–1367},
numpages = {16}
}

@inproceedings{10.1145/3474995.3475040,
author = {Duc Hoang, Doan},
title = {Learners’ Perspectives on the Benefits of Authentic Materials in Learning Vietnamese-English and English-Vietnamese Translation},
year = {2021},
isbn = {9781450390033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474995.3475040},
doi = {10.1145/3474995.3475040},
abstract = {In the global context and international integration, Vietnam has set up relationships with multiple countries around the world. Bridging the nations together, translated documents play a significant role for maintaining correspondence between the related parties. The use of authentic materials in the teaching of English translation has been considered a way to narrow the gaps between theory and practice in training English translators. By applying qualitative and quantitative method through surveying 31 translation course learners, the paper focuses on investigating the perspectives from the learners of Translation courses on the benefits of authentic materials in learning translation. The findings show that the use of authentic material in translation teaching has great impacts on students’ learning motivation, employability, and students’ competence of solving translation related real-work problems.},
booktitle = {Proceedings of the 2021 6th International Conference on Distance Education and Learning},
pages = {266–270},
numpages = {5},
keywords = {authentic materials, translation teaching, employability, competence},
location = {Shanghai, China},
series = {ICDEL '21}
}

@inproceedings{10.1145/3352460.3358294,
author = {Margaritov, Artemiy and Ustiugov, Dmitrii and Bugnion, Edouard and Grot, Boris},
title = {Prefetched Address Translation},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358294},
doi = {10.1145/3352460.3358294},
abstract = {With explosive growth in dataset sizes and increasing machine memory capacities, per-application memory footprints are commonly reaching into hundreds of GBs. Such huge datasets pressure the TLB, resulting in frequent misses that must be resolved through a page walk -- a long-latency pointer chase through multiple levels of the in-memory radix tree-based page table.Anticipating further growth in dataset sizes and their adverse affect on TLB hit rates, this work seeks to accelerate page walks while fully preserving existing virtual memory abstractions and mechanisms -- a must for software compatibility and generality. Our idea is to enable direct indexing into a given level of the page table, thus eliding the need to first fetch pointers from the preceding levels. A key contribution of our work is in showing that this can be done by simply ordering the pages containing the page table in physical memory to match the order of the virtual memory pages they map to. Doing so enables direct indexing into the page table using a base-plus-offset arithmetic.We introduce Address Translation with Prefetching (ASAP), a new approach for reducing the latency of address translation to a single access to the memory hierarchy. Upon a TLB miss, ASAP launches prefetches to the deeper levels of the page table, bypassing the preceding levels. These prefetches happen concurrently with a conventional page walk, which observes a latency reduction due to prefetching while guaranteeing that only correctly-predicted entries are consumed. ASAP requires minimal extensions to the OS and trivial microarchitectural support. Moreover, ASAP is fully legacy-preserving, requiring no modifications to the existing radix tree-based page table, TLBs and other software and hardware mechanisms for address translation. Our evaluation on a range of memory-intensive workloads shows that under SMT colocation, ASAP is able to reduce page walk latency by an average of 25% (42% max) in native execution, and 45% (55% max) under virtualization.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1023–1036},
numpages = {14},
keywords = {virtual memory, virtualization, microarchitecture},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@article{10.1109/TASLP.2020.2970241,
author = {Ai, Yang and Ling, Zhen-Hua},
title = {A Neural Vocoder With Hierarchical Generation of Amplitude and Phase Spectra for Statistical Parametric Speech Synthesis},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2970241},
doi = {10.1109/TASLP.2020.2970241},
abstract = {This article presents a neural vocoder named HiNet which reconstructs speech waveforms from acoustic features by predicting amplitude and phase spectra hierarchically. Different from existing neural vocoders such as WaveNet, SampleRNN and WaveRNN which directly generate waveform samples using single neural networks, the HiNet vocoder is composed of an amplitude spectrum predictor (ASP) and a phase spectrum predictor (PSP). The ASP is a simple DNN model which predicts log amplitude spectra (LAS) from acoustic features. The predicted LAS are sent into the PSP for phase recovery. Considering the issue of phase warping and the difficulty of phase modeling, the PSP is constructed by concatenating a neural source-filter (NSF) waveform generator with a phase extractor. We also introduce generative adversarial networks (GANs) into both ASP and PSP. Finally, the outputs of ASP and PSP are combined to reconstruct speech waveforms by short-time Fourier synthesis. Since there are no autoregressive structures in both predictors, the HiNet vocoder can generate speech waveforms with high efficiency. Objective and subjective experimental results show that our proposed HiNet vocoder achieves better naturalness of reconstructed speech than the conventional STRAIGHT vocoder, a 16-bit WaveNet vocoder using open source implementation and an NSF vocoder with similar complexity to the PSP and obtains similar performance with a 16-bit WaveRNN vocoder. We also find that the performance of HiNet is insensitive to the complexity of the neural waveform generator in PSP to some extend. After simplifying its model structure, the time consumed for generating 1&nbsp;s waveforms of 16&nbsp;kHz speech using a GPU can be further reduced from 0.34&nbsp;s to 0.19&nbsp;s without significant quality degradation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {839–851},
numpages = {13}
}

@article{10.5555/3322706.3362000,
author = {May, Avner and Garakani, Alireza Bagheri and Lu, Zhiyun and Guo, Dong and Liu, Kuan and Bellet, Aur\'{e}lien and Fan, Linxi and Collins, Michael and Hsu, Daniel and Kingsbury, Brian and Picheny, Michael and Sha, Fei},
title = {Kernel Approximation Methods for Speech Recognition},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We study the performance of kernel methods on the acoustic modeling task for automatic speech recognition, and compare their performance to deep neural networks (DNNs). To scale the kernel methods to large data sets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, we propose a simple but effective feature selection method which reduces the number of random features required to attain a fixed level of performance. Second, we present a number of metrics which correlate strongly with speech recognition performance when computed on the heldout set; we attain improved performance by using these metrics to decide when to stop training. Additionally, we show that the linear bottleneck method of Sainath et al. (2013a) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Leveraging these three methods, the kernel methods attain token error rates between 0:5% better and 0:1% worse than fully-connected DNNs across four speech recognition data sets, including the TIMIT and Broadcast News benchmark tasks.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2121–2156},
numpages = {36},
keywords = {feature selection, deep neural networks, automatic speech recognition, logistic regression, acoustic modeling, kernel methods}
}

@article{10.1109/TASLP.2020.3040850,
author = {Xue, Wei and Moore, Alastair H. and Brookes, Mike and Naylor, Patrick A.},
title = {Speech Enhancement Based on Modulation-Domain Parametric Multichannel Kalman Filtering},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3040850},
doi = {10.1109/TASLP.2020.3040850},
abstract = {Recently we presented a modulation-domain multichannel Kalman filtering (MKF) algorithm for speech enhancement, which jointly exploits the inter-frame modulation-domain temporal evolution of speech and the inter-channel spatial correlation to estimate the clean speech signal. The goal of speech enhancement is to suppress noise while keeping the speech undistorted, and a key problem is to achieve the best trade-off between speech distortion and noise reduction. In this paper, we extend the MKF by presenting a modulation-domain parametric MKF (PMKF) which includes a parameter that enables flexible control of the speech enhancement behaviour in each time-frequency (TF) bin. Based on the decomposition of the MKF cost function, a new cost function for PMKF is proposed, which uses the controlling parameter to weight the noise reduction and speech distortion terms. An optimal PMKF gain is derived using a minimum mean squared error (MMSE) criterion. We analyse the performance of the proposed MKF, and show its relationship to the speech distortion weighted multichannel Wiener filter (SDW-MWF). To evaluate the impact of the controlling parameter on speech enhancement performance, we further propose PMKF speech enhancement systems in which the controlling parameter is adaptively chosen in each TF bin. Experiments on a publicly available head-related impulse response (HRIR) database in different noisy and reverberant conditions demonstrate the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {393–405},
numpages = {13}
}

@inproceedings{10.1145/3459212.3459222,
author = {Cai, Yueqing and Rao, Wenbi},
title = {A Transformer-Based Chinese Non-Autoregressive Speech Synthesis Scheme},
year = {2021},
isbn = {9781450388917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459212.3459222},
doi = {10.1145/3459212.3459222},
abstract = {At present, the main research hotspot in the field of speech synthesis is still English speech synthesis, and there are few non-autoregressive Chinese speech synthesis models. During the Chinese migration process of FastSpeech2, we found that the naturalness of the synthesized audio was not good enough and there were some abnormal interruptions and incorrect pronunciation. Inspired by the training method of generative adversarial network, we use FastSpeech2 as the generator, and add a discriminator to force FastSpeech2 to generate audio more similar to the real audio. In order to realize a complete text to Mel spectrogram speech synthesis scheme, we design a text-to-phoneme converter based on corpus and rule constraints. And we conduct experiments on Baker dataset. The results show that our model achieves a better Mel Cepstral Distance than that of FastSpeech2. And our model can achieve a mean opinion score of 3.94, which is slightly better than the original model.},
booktitle = {Proceedings of the 2021 3rd International Conference on Image, Video and Signal Processing},
pages = {59–64},
numpages = {6},
keywords = {Non-autoregressive model, generative adversarial network, Chinese text-to-speech model, Speech synthesis},
location = {Singapore, Singapore},
series = {IVSP '21}
}

@inproceedings{10.1145/3495018.3495104,
author = {Li, Bo},
title = {Research on English Translation Based on Recursive Deep Neural Network},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495104},
doi = {10.1145/3495018.3495104},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {483–487},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1109/TASLP.2020.3006331,
author = {Luo, Hui and Han, Jiqing},
title = {Nonnegative Matrix Factorization Based Transfer Subspace Learning for Cross-Corpus Speech Emotion Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3006331},
doi = {10.1109/TASLP.2020.3006331},
abstract = {This article focuses on the cross-corpus speech emotion recognition (SER) task. To overcome the problem that the distribution of training (source) samples is inconsistent with that of testing (target) samples, we propose a non-negative matrix factorization based transfer subspace learning method (NMFTSL). Our method tries to find a shared feature subspace for the source and target corpora, in which the discrepancy between the two distributions is eliminated as much as possible and their individual components are excluded, thus the knowledge of the source corpus can be transferred to the target corpus. Specifically, in this induced subspace, we minimize the distances not only between the marginal distributions but also between the conditional distributions, where both distances are measured by the maximum mean discrepancy criterion. To estimate the conditional distribution of the target corpus, we propose to integrate the prediction of target label and the learning of feature representation into a joint learning model. Meanwhile, we introduce a difference loss to exclude the individual components from the shared subspace, which can further reduce the mutual interference between the source and target individual components. Moreover, we propose a discrimination loss to introduce the labels into the shared subspace, which can improve the discrimination ability of the feature representation. We also provide the solution for the corresponding optimization problem. To evaluate the performance of our method, we construct 30 cross-corpus SER schemes using 6 popular speech emotion corpora. Experimental results show that our approach achieves better overall performance than state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2047–2060},
numpages = {14}
}

@article{10.1145/3463498,
author = {Park, HyeonJung and Lee, Youngki and Ko, JeongGil},
title = {Enabling Real-Time Sign Language Translation on Mobile Platforms with On-Board Depth Cameras},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3463498},
doi = {10.1145/3463498},
abstract = {In this work we present SUGO, a depth video-based system for translating sign language to text using a smartphone's front camera. While exploiting depth-only videos offer benefits such as being less privacy-invasive compared to using RGB videos, it introduces new challenges which include dealing with low video resolutions and the sensors' sensitiveness towards user motion. We overcome these challenges by diversifying our sign language video dataset to be robust to various usage scenarios via data augmentation and design a set of schemes to emphasize human gestures from the input images for effective sign detection. The inference engine of SUGO is based on a 3-dimensional convolutional neural network (3DCNN) to classify a sequence of video frames as a pre-trained word. Furthermore, the overall operations are designed to be light-weight so that sign language translation takes place in real-time using only the resources available on a smartphone, with no help from cloud servers nor external sensing components. Specifically, to train and test SUGO, we collect sign language data from 20 individuals for 50 Korean Sign Language words, summing up to a dataset of ~5,000 sign gestures and collect additional in-the-wild data to evaluate the performance of SUGO in real-world usage scenarios with different lighting conditions and daily activities. Comprehensively, our extensive evaluations show that SUGO can properly classify sign words with an accuracy of up to 91% and also suggest that the system is suitable (in terms of resource usage, latency, and environmental robustness) to enable a fully mobile solution for sign language translation.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jun},
articleno = {77},
numpages = {30},
keywords = {Depth image processing, Mobile applications and services, Sign language translation}
}

@article{10.1109/TASLP.2020.3036611,
author = {Cheng, Jiaming and Liang, Ruiyu and Liang, Zhenlin and Zhao, Li and Huang, Chengwei and Schuller, Bj\"{o}rn},
title = {A Deep Adaptation Network for Speech Enhancement: Combining a Relativistic Discriminator With Multi-Kernel Maximum Mean Discrepancy},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3036611},
doi = {10.1109/TASLP.2020.3036611},
abstract = {In deep-learning-based speech enhancement (SE) systems, trained models are often used to handle unseen noise types and language environments in real-life scenarios. However, since production environments differ from training conditions, mismatch problems arise that may cause a serious decrease in the performance of an SE system. In this study, a domain adaptive method combining two adaptation strategies is proposed to improve the generalization of unlabeled noisy speech. In the proposed encoder-decoder-based SE framework, a domain discriminator and a domain confusion adaptation layer are introduced to conduct adversarial training. The model has two main innovations. First, the algorithm optimizes adversarial training by introducing a relativistic discriminator that relies on relative values by applying the difference, thus avoiding possible bias and better reflecting domain differences. Second, the multi-kernel maximum mean discrepancy (MK-MMD) between domains is taken as the regularization term of the domain adversarial loss, thereby further decreasing the edge distribution distance between domains. The proposed model improves the adaptability to unseen noises by encouraging the feature encoder to generate domain-invariant features. The model was evaluated using cross-noise and cross-language-and-noise experiments, and the results show that the proposed method provides considerable improvements over the baseline without an adaptation in the perceptual evaluation of speech quality (PESQ), the short time objective intelligibility (STOI) and the frequency-weighted signal-to-noise ratio (FWSNR).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {41–53},
numpages = {13}
}

@inproceedings{10.1145/3411764.3445430,
author = {Pandey, Laxmi and Hasan, Khalad and Arif, Ahmed Sabbir},
title = {Acceptability of Speech and Silent Speech Input Methods in Private and Public},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445430},
doi = {10.1145/3411764.3445430},
abstract = {Silent speech input converts non-acoustic features like tongue and lip movements into text. It has been demonstrated as a promising input method on mobile devices and has been explored for a variety of audiences and contexts where the acoustic signal is unavailable (e.g., people with speech disorders) or unreliable (e.g., noisy environment). Though the method shows promise, very little is known about peoples’ perceptions regarding using it. In this work, first, we conduct two user studies to explore users’ attitudes towards the method with a particular focus on social acceptance and error tolerance. Results show that people perceive silent speech as more socially acceptable than speech input and are willing to tolerate more errors with it to uphold privacy and security. We then conduct a third study to identify a suitable method for providing real-time feedback on silent speech input. Results show users find an abstract feedback method effective and significantly more private and secure than a commonly used video feedback method.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {251},
numpages = {13},
keywords = {speech, voice assistant, social acceptance, contactless interaction, silent speech, input and interaction},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3447687,
author = {Jiang, Di and Tan, Conghui and Peng, Jinhua and Chen, Chaotao and Wu, Xueyang and Zhao, Weiwei and Song, Yuanfeng and Tong, Yongxin and Liu, Chang and Xu, Qian and Yang, Qiang and Deng, Li},
title = {A GDPR-Compliant Ecosystem for Speech Recognition with Transfer, Federated, and Evolutionary Learning},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3447687},
doi = {10.1145/3447687},
abstract = {Automatic Speech Recognition (ASR) is playing a vital role in a wide range of real-world applications. However, Commercial ASR solutions are typically “one-size-fits-all” products and clients are inevitably faced with the risk of severe performance degradation in field test. Meanwhile, with new data regulations such as the European Union’s General Data Protection Regulation (GDPR) coming into force, ASR vendors, which traditionally utilize the speech training data in a centralized approach, are becoming increasingly helpless to solve this problem, since accessing clients’ speech data is prohibited. Here, we show that by seamlessly integrating three machine learning paradigms (i.e., Transfer learning, Federated learning, and Evolutionary learning (TFE)), we can successfully build a win-win ecosystem for ASR clients and vendors and solve all the aforementioned problems plaguing them. Through large-scale quantitative experiments, we show that with TFE, the clients can enjoy far better ASR solutions than the “one-size-fits-all” counterpart, and the vendors can exploit the abundance of clients’ data to effectively refine their own ASR products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {30},
numpages = {19},
keywords = {evolutionary learning, transfer learning, Speech recognition, federated learning}
}

@inproceedings{10.1145/3447548.3467156,
author = {Sodhi, Sukhdeep S. and Chio, Ellie Ka-In and Jash, Ambarish and Onta\~{n}\'{o}n, Santiago and Apte, Ajit and Kumar, Ankit and Jeje, Ayooluwakunmi and Kuzmin, Dima and Fung, Harry and Cheng, Heng-Tze and Effrat, Jon and Bali, Tarush and Jindal, Nitin and Cao, Pei and Singh, Sarvjeet and Zhou, Senqiang and Khan, Tameen and Wankhede, Amol and Alzantot, Moustafa and Wu, Allen and Chandra, Tushar},
title = {Mondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467156},
doi = {10.1145/3447548.3467156},
abstract = {As more and more online search queries come from voice, automatic speech recognition becomes a key component to deliver relevant search results. Errors introduced by automatic speech recognition (ASR) lead to irrelevant search results returned to the user, thus causing user dissatisfaction. In this paper, we introduce an approach, "Mondegreen", to correct voice queries in text space without depending on audio signals, which may not always be available due to system constraints or privacy or bandwidth (for example, some ASR systems run on-device) considerations. We focus on voice queries transcribed via several proprietary commercial ASR systems. These queries come from users making internet, or online service search queries. We first present an analysis showing how different the language distribution coming from user voice queries is from that in traditional text corpora used to train off-the-shelf ASR systems. We then demonstrate that Mondegreen can achieve significant improvements in increased user interaction by correcting user voice queries in one of the largest search systems in Google. Finally, we see Mondegreen as complementing existing highly-optimized production ASR systems, which may not be frequently retrained and thus lag behind due to vocabulary drifts.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3569–3575},
numpages = {7},
keywords = {ASR error correction, speech recognition, voice query correction},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3501409.3501423,
author = {Zhou, Xiaolu and Lei, Xiaofeng and Li, Danping},
title = {Design and Implementation of Process Form Translation Component Based on Business Service Catalog},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501423},
doi = {10.1145/3501409.3501423},
abstract = {Many enterprises manage their IT operation and maintenance services through the operation and maintenance management platform, i. e. ITSM. With the expansion and improvement of the scope and degree of business informatization, IT operation and maintenance services are facing more and more businesses, and the content in the service catalog is richer and finer. In order to realize the decoupling of operation and maintenance service management from different business areas and improve the scalability of ITSM, it is proposed to build ITSM as the "Middle Platform" solution. The process form translation component based on the business service catalog provides simple configuration to complete the conversion between the business application form of different business platforms and the service form of the operation middle platform, the implementation form of the operation middle platform and the standing books of the business platform through simple configuration, so as to realize the unification and aggregation of data from multiple business sources. In addition, it supports the rapid integration of the operation center with new businesses or new business platforms of the business platform, greatly improving the scalability of the operation middle platform.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {71–75},
numpages = {5},
keywords = {Service application process, Form translation component, The service catalog, Operation middle platform},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1145/3483446,
author = {Raval, Deepang and Pathak, Vyom and Patel, Muktan and Bhatt, Brijesh},
title = {Improving Deep Learning Based Automatic Speech Recognition for Gujarati},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3483446},
doi = {10.1145/3483446},
abstract = {We present a novel approach for improving the performance of an End-to-End speech recognition system for the Gujarati language. We follow a deep learning-based approach that includes Convolutional Neural Network, Bi-directional Long Short Term Memory layers, Dense layers, and Connectionist Temporal Classification as a loss function. To improve the performance of the system with the limited size of the dataset, we present a combined language model (Word-level language Model and Character-level language model)-based prefix decoding technique and Bidirectional Encoder Representations from Transformers-based post-processing technique. To gain key insights from our Automatic Speech Recognition (ASR) system, we used the inferences from the system and proposed different analysis methods. These insights help us in understanding and improving the ASR system as well as provide intuition into the language used for the ASR system. We have trained the model on the Microsoft Speech Corpus, and we observe a 5.87% decrease in Word Error Rate (WER) with respect to base-model WER.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {dec},
articleno = {47},
numpages = {18},
keywords = {automatic speech recognition, Deep learning, BERT, prefix decoding, recurrent neural network}
}

@inproceedings{10.1145/3503961.3503969,
author = {Deng, Shiping and Uchida, Kaoru and Yin, Zhengwei},
title = {Cross-Modal and Semantics-Augmented Asymmetric CycleGAN for Data-Imbalanced Anime Style Face Translation},
year = {2022},
isbn = {9781450385886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503961.3503969},
doi = {10.1145/3503961.3503969},
abstract = {Human face to anime face translation has attracted the attention of many researchers in recent years, and various works have achieved high-quality style transfer on conventional tasks. However, existing works often have fatal shortcomings when the target domain training data is heavily insufficient, which is named as imbalanced setting. Here the imbalanced (low-resource) task, generally means there is no sufficient data on the training dataset compared with the conventional task, e.g. the training data size is less than 100. To solve this problem, we propose a multi-modal translation model for a specific style. Based on the cyclic adversarial network and class activation map, we import semantic modality to enhance data information and attention modules, which help the model focus more on the discriminative areas between source and target domain. The experimental results show that our method has superiority in low-resource settings compared with similar existing work.},
booktitle = {Proceedings of the 2021 3rd International Conference on Video, Signal and Image Processing},
pages = {43–51},
numpages = {9},
keywords = {Deep neural networks, low-resource, image translation},
location = {Wuhan, China},
series = {VSIP '21}
}

@inproceedings{10.1145/3410352.3410838,
author = {Ortega, Maricela Pinargote and Mendoza, Lorena Bowen and Hormaza, Jaime Meza and Soto, Sebasti\'{a}n Ventura},
title = {Accuracy' Measures of Sentiment Analysis Algorithms for Spanish Corpus Generated in Peer Assessment},
year = {2020},
isbn = {9781450377362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410352.3410838},
doi = {10.1145/3410352.3410838},
abstract = {The purpose of this study is to test a model that classifies some sentiment as positive or negative from some feedback in Spanish that are generated through peer assessment in Higher Education. The Supervised Machine Learning method is implemented. Several experiments are performed with a manually tagged data set to test different combinations of N-grams with Term Frequency-Inverse Document Frequency (TF-IDF), and classification algorithms: Multinomial Naive Bayes, Support Vector Machine, Logistic Regression, and also Random Forest, in order to obtain the right combination that gives the best performance. The simulation results displayed that the Support Vector Machine classifier with the combination of 1-grams + 2-grams + TF-IDF is the best model in Precision, Recall and F-Measure.},
booktitle = {Proceedings of the 6th International Conference on Engineering &amp; MIS 2020},
articleno = {102},
numpages = {7},
keywords = {Sentiment analysis, peer assessment, machine learning, classification algorithms, higher education, Spanish corpus},
location = {Almaty, Kazakhstan},
series = {ICEMIS'20}
}

@inproceedings{10.1145/3501409.3501474,
author = {Chen, Hongyan and Hu, Yan},
title = {Research on Speech Enhancement Based on Full-Scale Connection},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501474},
doi = {10.1145/3501409.3501474},
abstract = {In order to solve the problem that the popular monaural speech enhancement models that based on encoder-decoder do not make full use of full-scale features, a full-scale feature connected speech enhancement model FSC-SENet is proposed. Firstly, this paper constructs a speech enhancement model based on CRN architecture. Convolutional encoder and decoder are used to extract features and recover speech signals, and LSTM modules are used to extract temporal features at the bottleneck of the model. Then a full-scale connection method and multi feature dynamic fusion mechanism are proposed, so that the decoder can make full use of the full-scale features to recover clean speech in the decoding process. Experimental results on TIMIT corpus show that compared with CRN, our FSC-SENet improves PESQ score by 0.39 and STOI score by 2.8% under seen noise cases, and PESQ score by 0.43 and STOI score by 3.1% under unseen noise cases, which proves that the proposed full-scale connection and dynamic feature fusion mechanism can make CRN have better speech enhancement performance.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {350–354},
numpages = {5},
keywords = {Feature fusion, Speech enhancement, Full-scale skip connection},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3308561.3354606,
author = {Hair, Adam and Ballard, Kirrie J. and Ahmed, Beena and Gutierrez-Osuna, Ricardo},
title = {Evaluating Automatic Speech Recognition for Child Speech Therapy Applications},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3354606},
doi = {10.1145/3308561.3354606},
abstract = {Automatic speech recognition (ASR) technology can be a useful tool in mobile apps for child speech therapy, empowering children to complete their practice with limited caregiver supervision. However, little is known about the feasibility of performing ASR on mobile devices, particularly when training data is limited. In this study, we investigated the performance of two low-resource ASR systems on disordered speech from children. We compared the open-source PocketSphinx (PS) recognizer using adapted acoustic models and a custom template-matching (TM) recognizer. TM and the adapted models significantly out-perform the default PS model. On average, maximum likelihood linear regression and maximum a posteriori adaptation increased PS accuracy from 59.4% to 63.8% and 80.0%, respectively, suggesting that the models successfully captured speaker-specific word production variations. TM reached a mean accuracy of 75.8%},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {578–580},
numpages = {3},
keywords = {computer-assisted pronunciation training (capt), assistive technology},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{10.1145/3544794.3558458,
author = {Igarashi, Yuya and Futami, Kyosuke and Murao, Kazuya},
title = {Silent Speech Eyewear Interface: Silent Speech Recognition Method Using Eyewear with Infrared Distance Sensors},
year = {2022},
isbn = {9781450394246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544794.3558458},
doi = {10.1145/3544794.3558458},
abstract = {As eyewear devices such as smart glasses become more common, it is important to provide input methods that can be used at all times for such situations and people. Silent speech interaction (SSI) has the potential to be useful as a hands-free input method for various situations and people, including those who have difficulty with voiced speech. However, previous methods have involved sensor devices that are difficult to use anytime and anywhere. We propose a method for SSI that involves using an eyewear device equipped with infrared distance sensors. The proposed method measures facial skin movements associated with a speech from the infrared distance sensor mounted on an eyewear device and recognizes silent speech commands by applying machine learning to the time-series sensor data. The proposed method was applied to a prototype system including a sensor device consisting of eyewear and ear-mounted microphones to measure the movements of the cheek, jaw joint, and jaw. From the evaluation results, F-measure was 0.90 with five speech commands and 0.83 with ten longer speech commands, which indicates the feasibility of the proposed method as a simple hands-free input interface. Our study provides the first wearable sensing method that can easily apply SSI functions to eyewear devices.},
booktitle = {Proceedings of the 2022 ACM International Symposium on Wearable Computers},
pages = {33–38},
numpages = {6},
keywords = {hands-free input, infrared distance sensor, skin motion sensing, eyewear, silent speech interaction},
location = {Cambridge, United Kingdom},
series = {ISWC '22}
}

@inproceedings{10.1145/3597926.3598126,
author = {Lau, Julia Kaiwen and Kong, Kelvin Kai Wen and Yong, Julian Hao and Tan, Per Hoong and Yang, Zhou and Yong, Zi Qian and Low, Joshua Chern Wey and Chong, Chun Yong and Lim, Mei Kuan and Lo, David},
title = {Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598126},
doi = {10.1145/3597926.3598126},
abstract = {Recent studies have proposed the use of Text-To-Speech (TTS) systems to automatically synthesise speech test cases on a scale and uncover a large number of failures in ASR systems. However, the failures uncovered by synthetic test cases may not reflect the actual performance of an ASR system when it transcribes human audio, which we refer to as false alarms. Given a failed test case synthesised from TTS systems, which consists of TTS-generated audio and the corresponding ground truth text, we feed the human audio stating the same text to an ASR system. If human audio can be correctly transcribed, an instance of a false alarm is detected. In this study, we investigate false alarm occurrences in five popular ASR systems using synthetic audio generated from four TTS systems and human audio obtained from two commonly used datasets. Our results show that the least number of false alarms is identified when testing Deepspeech, and the number of false alarms is the highest when testing Wav2vec2. On average, false alarm rates range from 21% to 34% in all five ASR systems. Among the TTS systems used, Google TTS produces the least number of false alarms (17%), and Espeak TTS produces the highest number of false alarms (32%) among the four TTS systems. Additionally, we build a false alarm estimator that flags potential false alarms, which achieves promising results: a precision of 98.3%, a recall of 96.4%, an accuracy of 98.5%, and an F1 score of 97.3%. Our study provides insight into the appropriate selection of TTS systems to generate high-quality speech to test ASR systems. Additionally, a false alarm estimator can be a way to minimise the impact of false alarms and help developers choose suitable test inputs when evaluating ASR systems. The source code used in this paper is publicly available on GitHub at https://github.com/julianyonghao/FAinASRtest.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1169–1181},
numpages = {13},
keywords = {Software Testing, Automated Speech Recognition, False Alarms},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3606370,
author = {Chan, Chia-Pang and Yang, Jun-He},
title = {Instagram Text Sentiment Analysis Combining Machine Learning and NLP},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3606370},
doi = {10.1145/3606370},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
keywords = {machine learning, deep learning, Instagram, natural language processing, word embedding technology}
}

@article{10.1145/3597494,
author = {Kivaisi, Alexander R. and Zhao, Qingjie and Mbelwa, Jimmy T.},
title = {Swahili Speech Dataset Development and Improved Pre-Training Method for Spoken Digit Recognition},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3597494},
doi = {10.1145/3597494},
abstract = {Speech dataset is an essential component in building commercial speech applications. However, low-resource languages such as Swahili lack such a resource that is vital for spoken digit recognition. For languages where such resources exist, they are usually insufficient. Thus, pre-training methods have been used with external resources to improve continuous speech recognition. However, to the best of our knowledge, no study has investigated the effect of pre-training methods specifically for spoken digit recognition. This study aimed at addressing these problems. First, we developed a Swahili spoken digit dataset for Swahili spoken digit recognition. Then, we investigated the effect of cross-lingual and multi-lingual pre-training methods on spoken digit recognition. Finally, we proposed an effective language-independent pre-training method for spoken digit recognition. The proposed method has the advantage of incorporating target language data during the pre-training stage that leads to an optimal solution when using less training data. Experiments on Swahili (being developed), English, and Gujarati datasets show that our method achieves better performance compared with all the baselines listed in this study.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jul},
articleno = {190},
numpages = {24},
keywords = {spoken digit recognition, multi-lingual, Swahili language, low-resource language, convolutional neural network, pre-training, cross-lingual}
}

@article{10.1109/TASLP.2022.3145294,
author = {Hoang, Poul and de Haan, Jan Mark and Tan, Zheng-Hua and Jensen, Jesper},
title = {Multichannel Speech Enhancement With Own Voice-Based Interfering Speech Suppression for Hearing Assistive Devices},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145294},
doi = {10.1109/TASLP.2022.3145294},
abstract = {Enhancementof a desired speech signal in the presence of competing or interfering speech remains an unsolved problem, as it can be hard to determine which of the speech signals is the one of interest. In this paper, we propose a multichannel noise reduction algorithm which uses the presence of the user’s own voice signal, e.g. during conversations with the target speaker, as an asset to efficiently identify interfering speech and noise. Specifically, following the typical speech pattern in natural conversations, the presence of an own voice may indicate the absence of the target speech, hence undesired speech and noise can be identified and estimated during own voice presence. In contrast to conventional noise reduction systems, the proposed noise reduction systems use the user’s own voice to identify interfering speech that otherwise could be confused with the target speech. We demonstrate the performance of the proposed noise reduction systems in a comparison against state-of-the-art noise reduction systems in terms of beamforming performance for hearing assistive devices. The results show that the proposed beamforming scheme in particular outperforms state-of-the-art methods in terms of ESTOI and PESQ in situations with a target speaker and a strong interfering speaker.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {706–720},
numpages = {15}
}

@inproceedings{10.1145/3558100.3563854,
author = {Redondo-Gutierrez, Luis \'{A}ngel and J\'{a}\~{n}ez-Martino, Francisco and Fidalgo, Eduardo and Alegre, Enrique and Gonz\'{a}lez-Castro, V\'{\i}ctor and Alaiz-Rodr\'{\i}guez, Roc\'{\i}o},
title = {Detecting Malware Using Text Documents Extracted from Spam Email through Machine Learning},
year = {2022},
isbn = {9781450395441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558100.3563854},
doi = {10.1145/3558100.3563854},
abstract = {Spam has become an effective way for cybercriminals to spread malware. Although cybersecurity agencies and companies develop products and organise courses for people to detect malicious spam email patterns, spam attacks are not totally avoided yet. In this work, we present and make publicly available "Spam Email Malware Detection - 600" (SEMD-600), a new dataset, based on Bruce Guenter's, for malware detection in spam using only the text of the email. We also introduce a pipeline for malware detection based on traditional Natural Language Processing (NLP) techniques. Using SEMD-600, we compare the text representation techniques Bag of Words and Term Frequency-Inverse Document Frequency (TF-IDF), in combination with three different supervised classifiers: Support Vector Machine, Naive Bayes and Logistic Regression, to detect malware in plain text documents. We found that combining TF-IDF with Logistic Regression achieved the best performance, with a macro F1 score of 0.763.},
booktitle = {Proceedings of the 22nd ACM Symposium on Document Engineering},
articleno = {17},
numpages = {4},
keywords = {spam email, natural language processing, malware detection, cybersecurity, machine learning},
location = {San Jose, California},
series = {DocEng '22}
}

@article{10.1109/TASLP.2018.2843537,
author = {Xiong, Feifei and Goetze, Stefan and Kollmeier, Birger and Meyer, Bernd T.},
title = {Exploring Auditory-Inspired Acoustic Features for Room Acoustic Parameter Estimation From Monaural Speech},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2843537},
doi = {10.1109/TASLP.2018.2843537},
abstract = {Room acoustic parameters that characterize acoustic environments can help to improve signal enhancement algorithms such as for dereverberation, or automatic speech recognition by adapting models to the current parameter set. The reverberation time&nbsp;RT and the early-to-late reverberation ratio&nbsp;ELR are two key parameters. In this paper, we propose a blind ROom Parameter Estimator&nbsp;ROPE based on an artificial neural network that learns the mapping to discrete ranges of the RT and the ELR from single-microphone speech signals. Auditory-inspired acoustic features are used as neural network input, which are generated by a temporal modulation filter bank applied to the speech time-frequency representation. ROPE performance is analyzed in various reverberant environments in both clean and noisy conditions for both fullband and subband RT and ELR estimations. The importance of specific temporal modulation frequencies is analyzed by evaluating the contribution of individual filters to the ROPE performance. Experimental results show that ROPE is robust against different variations caused by room impulse responses measured versus&nbsp;simulated, mismatched noise levels, and speech variability reflected through different corpora. Compared to state-of-the-art algorithms that were tested in the acoustic characterisation of environments ACE challenge, the ROPE model is the only one that is among the best for all individual tasks RT and ELR estimation from fullband and subband signals. Improved fullband estimations are even obtained by ROPE when integrating speech-related frequency subbands. Furthermore, the model requires the least computational resources with a real time factor that is at least two times faster than competing algorithms. Results are achieved with an average observation window of 3&nbsp;s, which is important for real-time applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1809–1820},
numpages = {12}
}

@inproceedings{10.1145/3406522.3446057,
author = {Ghosh, Souvick and Ghosh, Satanu},
title = {Classifying Speech Acts Using Multi-Channel Deep Attention Network for Task-Oriented Conversational Search Agents},
year = {2021},
isbn = {9781450380553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406522.3446057},
doi = {10.1145/3406522.3446057},
abstract = {Understanding human spoken dialogues in an information-seeking scenario is a significant challenge for IR researchers. Prior literature in intelligent systems suggests that by identifying speech acts in spoken dialogues, we can identify the search intent and the information needs of the user. Therefore, in this paper, we have used speech acts to address the problem of natural language understanding in conversational search systems. First, we collected human-system interaction data through a Wizard-of-Oz study. Next, we developed a gold-standard dataset where the human-system conversations are labeled with corresponding speech acts. Finally, we built the Multi-channel Deep Attention Network (MDAN) to identify the speech acts in information-seeking dialogues. The results highlight that the best performing model predicts speech acts with 90.2% accuracy. The MDAN architecture outperforms not only all traditional machine learning models but also the state-of-the-art single-channel BERT by 3.3 absolute points. We performed ablation analysis to show the impact of the three channels of MDAN individually and in combination. The results indicate that the best performance is achieved using all three channels for speech act prediction.},
booktitle = {Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
pages = {267–272},
numpages = {6},
keywords = {deep neural network, intelligent personal assistants, conversational information retrieval, speech acts, conversational search systems, spoken search},
location = {Canberra ACT, Australia},
series = {CHIIR '21}
}

@article{10.1145/3580494,
author = {Garg, Tanmay and Masud, Sarah and Suresh, Tharun and Chakraborty, Tanmoy},
title = {Handling Bias in Toxic Speech Detection: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3580494},
doi = {10.1145/3580494},
abstract = {Detecting online toxicity has always been a challenge due to its inherent subjectivity. Factors such as the context, geography, socio-political climate, and background of the producers and consumers of the posts play a crucial role in determining if the content can be flagged as toxic. Adoption of automated toxicity detection models in production can thus lead to a sidelining of the various groups they aim to help in the first place. It has piqued researchers’ interest in examining unintended biases and their mitigation. Due to the nascent and multi-faceted nature of the work, complete literature is chaotic in its terminologies, techniques, and findings. In this article, we put together a systematic study of the limitations and challenges of existing methods for mitigating bias in toxicity detection.We look closely at proposed methods for evaluating and mitigating bias in toxic speech detection. To examine the limitations of existing methods, we also conduct a case study to introduce the concept of bias shift due to knowledge-based bias mitigation. The survey concludes with an overview of the critical challenges, research gaps, and future directions. While reducing toxicity on online platforms continues to be an active area of research, a systematic study of various biases and their mitigation strategies will help the research community produce robust and fair models.1},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {264},
numpages = {32},
keywords = {Toxic speech, unintended bias, social networks, bias shift, hate speech, bias mitigation}
}

@inproceedings{10.1145/3578741.3578761,
author = {WANG, Ziyun and GUO, Xiao},
title = {Research on Mandarin Chinese in Speech Emotion Recognition},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578761},
doi = {10.1145/3578741.3578761},
abstract = {Tone refers to the form of speech attached to a whole sentence and has both cognitive and expressive functions. People can recognize emotion by tone. Chinese is a typical intonation language. The variations of tone in Chinese are manifested in pitch, intonation, stress movement, and pauses. These tone changes are all related to semantics, they are important features for determining speech emotion. In this paper, we explore the characteristic expressions and feature functions of Mandarin Chinese in speech emotion recognition. Comparing Chinese speech emotion research with English, the mainstream model study language, we find that intonation performance is inconsistent, and emotional expressions differ across languages. But even in different languages, the recognition features used are largely the same. So the ability of the models to recognize emotion from features is particularly important. We compare the different processing of two models for Chinese speech emotion recognition, the fusion features using VGGish and LLDs are more accurate than single features, with an unweighted accuracy of 56.2 on CASIA. Additionally, the use of audio pre-processing can also increase the accuracy from 66.2% to 88.9%. It concludes that the utilization of emotional features is improved through feature fusion and audio pre-processing.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {99–103},
numpages = {5},
keywords = {Voice Tone, Speech Emotion Recognition, Intonation},
location = {Sanya, China},
series = {MLNLP '22}
}

@article{10.1109/TASLP.2019.2921890,
author = {Jati, Arindam and Georgiou, Panayiotis},
title = {Neural Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning of Speaker Characteristics},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2921890},
doi = {10.1109/TASLP.2019.2921890},
abstract = {Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding NPC, to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce “speaker embeddings” by learning to separate “same” versus “different” speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1577–1589},
numpages = {13}
}

@inproceedings{10.1145/3488933.3488984,
author = {Tong, Qiujuan and Han, Huan and Huang, Lu and Wang, Jun},
title = {Single-Channel Speech Enhancement Using Amplitude Estimation and Phase Reconstruction},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3488984},
doi = {10.1145/3488933.3488984},
abstract = {In traditional speech enhancement algorithms, the amplitude of noise spectrum is only modified, while the phase and power spectral density estimation of noise power spectrum is inaccurate, resulting in serious distortion of synthesized speech signal. Aiming at addressing this problem, a speech enhancement algorithm combining amplitude estimation and phase reconstruction is proposed in this paper. First, we use the minimum controlled recursive averaging (MCRA) function to estimate the power spectrum of the noise, instead of relying only on the initial silent frame to achieve the amplitude estimation of the enhanced signal. Secondly, the phase spectrum of noisy speech signal is reconstructed. Finally, the reconstructed phase spectrum is combined with the estimated speech amplitude spectrum to generate clean speech signal. In order to evaluate the performance of the proposed algorithm, the NOIZEUS data set is used for simulation. The objective experiments of perceptual estimation of speech quality (PESQ), normalized covariance (NCM), and spectrogram analysis show that the proposed method can suppress noise signals greatly, reduce signal distortion at low signal-noise ratio (SNR), and improve speech quality and intelligibility.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {429–434},
numpages = {6},
keywords = {Speech enhancement, Noise estimation, Phase reconstruction},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.1145/3327962.3331456,
author = {Zhang, Jiajie and Zhang, Bingsheng and Zhang, Bincheng},
title = {Defending Adversarial Attacks on Cloud-Aided Automatic Speech Recognition Systems},
year = {2019},
isbn = {9781450367882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3327962.3331456},
doi = {10.1145/3327962.3331456},
abstract = {With the advancement of deep learning based speech recognition technology, an increasing number of cloud-aided automatic voice assistant applications, such as Google Home, Amazon Echo, and cloud AI services, such as IBM Watson, are emerging in our daily life. In a typical usage scenario, after keyword activation, the user's voice will be recorded and submitted to the cloud for automatic speech recognition (ASR) and then further action(s) might be triggered depending on the user's command(s). However, recent researches show that the deep learning based systems could be easily attacked by adversarial examples. Subsequently, the ASR systems are found being vulnerable to audio adversarial examples. Unfortunately, very few works about defending audio adversarial attack are known in the literature. Constructing a generic and robust defense mechanism to resolve this issue remains an open problem. In this work, we propose several proactive defense mechanisms against targeted audio adversarial examples in the ASR systems via code modulation and audio compression. We then show the effectiveness of the proposed strategies through extensive evaluation on natural dataset.},
booktitle = {Proceedings of the Seventh International Workshop on Security in Cloud Computing},
pages = {23–31},
numpages = {9},
keywords = {deep learning, adversarial examples, cloud-aided speech recognition},
location = {Auckland, New Zealand},
series = {SCC '19}
}

@inproceedings{10.1145/3297280.3297375,
author = {Franciscatto, Maria Helena and Lima, Jo\~{a}o Carlos Damasceno and Trois, Celio and Maran, Vin\'{\i}cius and Soares, M\'{a}rcia Keske and Rocha, Cristiano Cortez da},
title = {A Case-Based Approach Using Phonological Knowledge for Identifying Error Patterns in Children's Speech},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297375},
doi = {10.1145/3297280.3297375},
abstract = {Case-Based Reasoning (CBR) covers a range of different methods for organizing, retrieving, and indexing knowledge from previous cases. Thus, this methodology has been successfully applied in medical domain, due to its human and intelligent properties to diagnose the case of a patient. In the speech therapy domain, an early identification of speech sound disorders allows the diagnosis and treatment of various pathologies and may aid clinical decision-making. However, there are few proposals that use knowledge modeling for supporting speech therapists. Moreover, there is no indicative in related literature of CBR being used for detecting the phonological processes (PPs) that may occur in pronunciations. So, in this paper, we present a case-based approach that uses machine learning for predicting PPs, aiming to provide clinical support in the identification of error patterns in children's speech. The method was evaluated through a speech corpus containing near one hundred thousand audio files, collected from pronunciation assessments performed by speech-language pathologists with more than 1,000 children. Using our knowledge base along with incremental learning, we obtained an accuracy of over 93% for predicting the PPs, showing the efficiency of our method for clinical decision support.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {968–975},
numpages = {8},
keywords = {phonological knowledge base, phonological processes, speech therapy, case-based reasoning, machine learning},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1109/TASLP.2020.3043655,
author = {Borgstr\"{o}m, Bengt J. and Brandstein, Michael S.},
title = {Speech Enhancement via Attention Masking Network (SEAMNET): An End-to-End System for Joint Suppression of Noise and Reverberation},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3043655},
doi = {10.1109/TASLP.2020.3043655},
abstract = {This paper proposes the Speech Enhancement via Attention Masking Network (SEAMNET), a neural network-based end-to-end single-channel speech enhancement system designed for joint suppression of noise and reverberation. It formalizes an end-to-end network architecture, referred to as <italic>b-Net</italic>, which accomplishes noise suppression through attention masking in a learned embedding space. A key contribution of SEAMNET is that the b-Net architecture contains both an enhancement and an autoencoder path. This paper proposes a novel loss function which simultaneously trains both the enhancement and the autoencoder paths, so that disabling the masking mechanism during inference causes SEAMNET to reconstruct the input speech signal. This allows dynamic control of the level of suppression applied by SEAMNET via a minimum gain level, which is not possible in other state-of-the-art approaches to end-to-end speech enhancement. This paper also proposes a perceptually-motivated waveform distance measure. In addition to the b-Net architecture, this paper proposes a novel method for designing target waveforms for network training, so that joint suppression of additive noise and reverberation can be performed by an end-to-end enhancement system, which has not been previously possible. Experimental results show the SEAMNET system to outperform a variety of state-of-the-art baselines systems, both in terms of objective speech quality measures and subjective listening tests. Finally, this paper draws parallels between SEAMNET and conventional statistical model-based enhancement approaches, offering interpretability of many network components.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {515–526},
numpages = {12}
}

@inproceedings{10.1145/3582099.3582128,
author = {Yu, Chongchong and Yu, Jiaqi and Qian, Zhaopeng and Tan, Yuchen},
title = {Endangered Tujia Language Speech Recognition Research Based on Audio-Visual Fusion},
year = {2023},
isbn = {9781450398749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582099.3582128},
doi = {10.1145/3582099.3582128},
abstract = {As an endangered language, Tujia language is a non-renewable intangible cultural resource. Automatic speech recognition (ASR) uses artificial intelligence technology to facilitate manually label Tujia language, which is an effective means to protect this language. However, due to the fact that Tujia language has few native speakers, few labeled corpus, and much noise in the corpus. The acoustic models thus suffer from over fitting and lowe noise immunity, which seriously harms the accuracy of recognition. To tackle the deficiencies, an approach of audio-visual speech recognition (AVSR) based on Transformer-CTC is proposed, which reduces the dependence of acoustic models on noise and the quantity of data by introducing visual modality in-formation including lip movements. Specifically, the new approach enhances the expression of speakers’ feature space through the fusion of audio and visual information, thus solving the problem of less available information for single modality. Experiment results show that the optimal CER of AVSR is 8.2% lower than that of traditional models, and 11.8% lower than that for lip reading. The proposed AVSR tackles the issue of low accuracy in recognizing endangered languages. Therefore, AVSR is of great significance in studying the protection and preservation of endangered languages through AI.},
booktitle = {Proceedings of the 2022 5th Artificial Intelligence and Cloud Computing Conference},
pages = {190–195},
numpages = {6},
keywords = {audiovisual speech recognition, automatic speech recognition, Endangered language},
location = {Osaka, Japan},
series = {AICCC '22}
}

@inproceedings{10.1145/3287921.3287938,
author = {Nguyen, Quoc Bao and Mai, Van Tuan and Le, Quang Trung and Dam, Ba Quyen and Do, Van Hai},
title = {Development of a Vietnamese Large Vocabulary Continuous Speech Recognition System under Noisy Conditions},
year = {2018},
isbn = {9781450365390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287921.3287938},
doi = {10.1145/3287921.3287938},
abstract = {In this paper, we first present our effort to collect a 500-hour corpus for Vietnamese read speech. After that, various techniques such as data augmentation, recurrent neural network language model rescoring, language model adaptation, bottleneck feature, system combination are applied to build the speech recognition system. Our final system achieves a low word error rate at 6.9% on the noisy test set.},
booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
pages = {222–226},
numpages = {5},
keywords = {model adaptation, noisy condition, Vietnamese speech recognition, speech corpus, system combination},
location = {Danang City, Viet Nam},
series = {SoICT '18}
}

@article{10.1109/TASLP.2020.2964953,
author = {Kukanov, Ivan and Trong, Trung Ngo and Hautam\"{a}ki, Ville and Siniscalchi, Sabato Marco and Salerno, Valerio Mario and Lee, Kong Aik},
title = {Maximal Figure-of-Merit Framework to Detect Multi-Label Phonetic Features for Spoken Language Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2964953},
doi = {10.1109/TASLP.2020.2964953},
abstract = {Bottleneck features (BNFs) generated with a deep neural network (DNN) have proven to boost spoken language recognition accuracy over basic spectral features significantly. However, BNFs are commonly extracted using language-dependent tied-context phone states as learning targets. Moreover, BNFs are less phonetically expressive than the output layer in a DNN, which is usually not used as a speech feature because of its very high dimensionality hindering further post-processing. In this article, we put forth a novel deep learning framework to overcome all of the above issues and evaluate it on the 2017 NIST Language Recognition Evaluation (LRE) challenge. We use manner and place of articulation as speech attributes, which lead to low-dimensional “universal” phonetic features that can be defined across all spoken languages. To model the asynchronous nature of the speech attributes while capturing their intrinsic relationships in a given speech segment, we introduce a new training scheme for deep architectures based on a Maximal Figure of Merit (MFoM) objective. MFoM introduces non-differentiable metrics into the backpropagation-based approach, which is elegantly solved in the proposed framework. The experimental evidence collected on the recent NIST LRE 2017 challenge demonstrates the effectiveness of our solution. In fact, the performance of speech language recognition (SLR) systems based on spectral features is improved for more than 5% absolute Cavg. Finally, the F1 metric can be brought from 77.6% up to 78.1% by combining the conventional baseline phonetic BNFs with the proposed articulatory attribute features.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {682–695},
numpages = {14}
}

@inproceedings{10.1145/3503162.3503173,
author = {Joshi, Raviraj and Kannan, Venkateshan},
title = {Attention Based End to End Speech Recognition for Voice Search in Hindi and English},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503173},
doi = {10.1145/3503162.3503173},
abstract = {We describe here our work with automatic speech recognition (ASR) in the context of voice search functionality on the Flipkart e-Commerce platform. Starting with the deep learning architecture of Listen-Attend-Spell (LAS), we build upon and expand the model design and attention mechanisms to incorporate innovative approaches including multi-objective training, multi-pass training, and external rescoring using language models and phoneme based losses. We report a relative WER improvement of 15.7% on top of state-of-the-art LAS models using these modifications. Overall, we report an improvement of 36.9% over the phoneme-CTC system on the Flipkart Voice Search dataset. The paper also provides an overview of different components that can be tuned in a LAS based system.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {107–113},
numpages = {7},
keywords = {encoder-decoder models, automatic speech recognition, attention, listen attend spell},
location = {Virtual Event, India},
series = {FIRE '21}
}

@inproceedings{10.1145/3408127.3408131,
author = {Tu, Jingxian and Yao, Yunzhou and Qin, Guijiang},
title = {An Improved Multichannel Subspace Speech Enhancement Algorithm for Balance between Noise Reduction and Speech Distortion},
year = {2020},
isbn = {9781450376877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408127.3408131},
doi = {10.1145/3408127.3408131},
abstract = {This paper proposes an improved multichannel subspace speech enhancement algorithm for balance between noise reduction and speech distortion. In the proposed algorithm, the weighting between noise reduction and speech distortion is adjusted adaptively. Moreover, the exact relational expression between this weighting and the input short time signal to noise (SNR) is given. When the weighting is bigger, more noise reduction and more speech distortion is achieved. Generally, the weighing value ranges from 0 to positive infinity, and the higher the input short time SNR is, the smaller the weighting is. Simulation results show that the proposed algorithm achieve the better performance in terms of the perceptual evaluation of speech quality than the conventional multichannel subspace speech enhancement algorithm.},
booktitle = {Proceedings of the 2020 4th International Conference on Digital Signal Processing},
pages = {150–154},
numpages = {5},
keywords = {Subspace, Speech distortion, Multichannel speech enhancement, Noise reduction},
location = {Chengdu, China},
series = {ICDSP '20}
}

@article{10.1145/3485666,
author = {Nanni, Federico and Glava\v{s}, Goran and Rehbein, Ines and Ponzetto, Simone Paolo and Stuckenschmidt, Heiner},
title = {Political Text Scaling Meets Computational Semantics},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3485666},
doi = {10.1145/3485666},
abstract = {During the past 15 years, automatic text scaling has become one of the key tools of the Text as Data community in political science. Prominent text-scaling algorithms, however, rely on the assumption that latent positions can be captured just by leveraging the information about word frequencies in documents under study. We challenge this traditional view and present a new, semantically aware text-scaling algorithm, SemScale, which combines recent developments in the area of computational linguistics with unsupervised graph-based clustering. We conduct an extensive quantitative analysis over a collection of speeches from the European Parliament in five different languages and from two different legislative terms, and we show that a scaling approach relying on semantic document representations is often better at capturing known underlying political dimensions than the established frequency-based (i.e., symbolic) scaling method. We further validate our findings through a series of experiments focused on text preprocessing and feature selection, document representation, scaling of party manifestos, and a supervised extension of our algorithm. To catalyze further research on this new branch of text-scaling methods, we release a Python implementation of SemScale with all included datasets and evaluation procedures.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {29},
numpages = {27},
keywords = {political text scaling, text-as-data, Automated political text analysis, multilinguality}
}

@inproceedings{10.1145/3379336.3381496,
author = {Schmitz, Hans-Christian and Kurth, Frank and Wilkinghoff, Kevin and M\"{u}llerschkowski, Uwe and Karrasch, Christian and Schmid, Volker},
title = {Towards Robust Speech Interfaces for the ISS},
year = {2020},
isbn = {9781450375139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379336.3381496},
doi = {10.1145/3379336.3381496},
abstract = {The International Space Station ISS is a scientific laboratory in which astronauts conduct a great variety of experiments on a tight schedule. In order to fulfill their tasks efficiently and correctly, astronauts need assistance, which (at least partially) can be provided by IT systems on board, among them robotic assistants like the Crew Interactive Mobile Companion CIMON. However, the creation of user interfaces for such systems is a challenge, because astronauts often have to interact hands-free or cannot direct their attention to a visual user interface. These challenges can be met by providing multimodal user interfaces that enable speech interaction, among other modalities. We describe the use context for speech interfaces on the ISS, specific requirements and possible solutions. Our concepts rely on previous work carried out in acoustically demanding environments.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces Companion},
pages = {110–111},
numpages = {2},
keywords = {Dialogue Systems, Keyword Spotting, International Space Station ISS, Crew Interactive Mobile Companion CIMON, Automatic Speech Recognition},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1145/3397271.3401447,
author = {Arango, Aym\'{e}},
title = {Language Agnostic Hate Speech Detection},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401447},
doi = {10.1145/3397271.3401447},
abstract = {The growth in social Web platforms in the past years has brought an increase in displays of online hate speech. This subject is considered as a critical matter in the Web community, since it can be related to potentially dangerous actions that affect individuals and groups in the physical world. The automatic detection of this type of expressions has been the center of several investigations over the past few years. However, most research on this subject has been done for the English language and on rather limited datasets. In addition, although some works approach the problem from a multilingual perspective, analyzing different language separately, across-lingual perspective of this problem has not been used so far.The main research proposal of this thesis is to characterize hate speech and other forms of online harassment from different perspectives and use this characterizations to create novel models for online hate speech detection across different languages and domains.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2475},
numpages = {1},
keywords = {machine learning, social media, experimental evaluation, hate speech classification},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3266302.3266306,
author = {Lu, Chih-Chuan and Li, Jeng-Lin and Lee, Chi-Chun},
title = {Learning an Arousal-Valence Speech Front-End Network Using Media Data In-the-Wild for Emotion Recognition},
year = {2018},
isbn = {9781450359832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266302.3266306},
doi = {10.1145/3266302.3266306},
abstract = {Recent progress in speech emotion recognition (SER) technology has benefited from the use of deep learning techniques. However, expensive human annotation and difficulty in emotion database collection make it challenging for rapid deployment of SER across diverse application domains. An initialization - fine-tuning strategy help mitigate these technical challenges. In this work, we propose an initialization network that gears toward SER applications by learning the speech front-end network on a large media data collected in-the-wild jointly with proxy arousal-valence labels that are multimodally derived from audio and text information, termed as the Arousal-Valence Speech Front-End Network (AV-SpNET). The AV-SpNET can then be easily stacked simply with the supervised layers for the target emotion corpus of interest. We evaluate our proposed AV-SpNET on tasks of SER for two separate emotion corpora, the USC IEMOCAP and the NNIME database. The AV-SpNET outperforms other initialization techniques and reach the best overall performances requiring only 75% of the in-domain annotated data. We also observe that generally, by using the AV-SpNET as front-end network, it requires as little as 50% of the fine-tuned data to surpass method based on randomly-initialized network with fine-tuning on the complete training set.},
booktitle = {Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop},
pages = {99–105},
numpages = {7},
keywords = {speech emotion recognition, convolutional neural network, speech front-end network, media data in-the-wild},
location = {Seoul, Republic of Korea},
series = {AVEC'18}
}

@inproceedings{10.1145/3410530.3414372,
author = {Chlebek, Piotr and Shriberg, Elizabeth and Lu, Yang and Rutowski, Tomasz and Harati, Amir and Oliveira, Ricardo},
title = {Comparing Speech Recognition Services for HCI Applications in Behavioral Health},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414372},
doi = {10.1145/3410530.3414372},
abstract = {Behavioral health conditions such as depression and anxiety are a global concern, and there is growing interest in employing speech technology to screen and monitor patients remotely. Language modeling approaches require automatic speech recognition (ASR) and multiple privacy-compliant ASR services are commercially available. We use a corpus of over 60 hours of speech from a behavioral health task, and compare ASR performance for four commercial vendors. We expected similar performance, but found large differences between the top and next-best performer, for both mobile (48% relative WER increase) and laptop (67% relative WER increase) data. Results suggest the importance of benchmarking ASR systems in this domain. Additionally we find that WER is not systematically related to depression itself. Performance is however affected by diverse audio quality from users' personal devices, and possibly from the overall style of speech in this domain.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {483–487},
numpages = {5},
keywords = {speech recognition, natural language processing, digital health, depression, behavioral health, anxiety, word error rate, telehealth},
location = {Virtual Event, Mexico},
series = {UbiComp-ISWC '20}
}

@inproceedings{10.1145/3598469.3598473,
author = {Alsamman, Ahmad and Schmitz, Andreas and Wimmer, Maria A.},
title = {Towards an Organically Growing Hate Speech Dataset in Hate Speech Detection Systems in a Smart Mobility Application},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598469.3598473},
doi = {10.1145/3598469.3598473},
abstract = {The automatic detection of hate speech online poses several challenges. A top challenge is that hate speech changes its targets and its format periodically. While the lack of available training data is a general issue in many natural language processing applications, the forementioned challenge amplifies the problem especially when taking into consideration the challenge of producing well labelled datasets. Based on the concepts of quarantining hate speech and integrating a linguistics expert in a smart mobility service provided in an administrative district in Germany, this paper proposes an approach that targets improving the training dataset quantitively and qualitatively in a running smart mobility app, the SWIA app. This proactive approach provides a long-term solution for hate speech detection models that rely on labelled datasets for training. The paper also discusses technical and practical challenges unanswered by this approach.},
booktitle = {Proceedings of the 24th Annual International Conference on Digital Government Research},
pages = {36–43},
numpages = {8},
keywords = {Quarantining, Datasets, Hate Speech Classification, Co-Creation, German Languages, Smart Mobility},
location = {Gda?sk, Poland},
series = {DGO '23}
}

@article{10.1109/TASLP.2020.3009477,
author = {Liu, Qi and Chen, Zhehuai and Li, Hao and Huang, Mingkun and Lu, Yizhou and Yu, Kai},
title = {Modular End-to-End Automatic Speech Recognition Framework for Acoustic-to-Word Model},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3009477},
doi = {10.1109/TASLP.2020.3009477},
abstract = {End-to-end (E2E) systems have played a more and more important role in automatic speech recognition (ASR) and achieved great performance. However, E2E systems recognize output word sequences directly with the input acoustic feature, which can only be trained on limited acoustic data. The extra text data is widely used to improve the results of traditional artificial neural network-hidden Markov model (ANN-HMM) hybrid systems. The involving of extra text data to standard E2E ASR systems may break the E2E property during decoding. In this paper, a novel modular E2E ASR system is proposed. The modular E2E ASR system consists of two parts: an acoustic-to-phoneme (A2P) model and a phoneme-to-word (P2W) model. The A2P model is trained on acoustic data, while extra data including large scale text data can be used to train the P2W model. This additional data enables the modular E2E ASR system to model not only the acoustic part but also the language part. During the decoding phase, the two models will be integrated and act as a standard acoustic-to-word (A2W) model. In other words, the proposed modular E2E ASR system can be easily trained with extra text data and decoded in the same way as a standard E2E ASR system. Experimental results on the Switchboard corpus show that the modular E2E model achieves better word error rate (WER) than standard A2W models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2174–2183},
numpages = {10}
}

@inproceedings{10.1145/3394885.3431416,
author = {Badaroux, Marie and P\'{e}trot, Fr\'{e}d\'{e}ric},
title = {Arbitrary and Variable Precision Floating-Point Arithmetic Support in Dynamic Binary Translation},
year = {2021},
isbn = {9781450379991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394885.3431416},
doi = {10.1145/3394885.3431416},
abstract = {Floating-point hardware support has more or less been settled 35 years ago by the adoption of the IEEE 754 standard. However, many scientific applications require higher accuracy than what can be represented on 64 bits, and to that end make use of dedicated arbitrary precision software libraries. To reach a good performance/accuracy trade-off, developers use variable precision, requiring e.g. more accuracy as the computation progresses. Hardware accelerators for this kind of computations do not exist yet, and independently of the actual quality of the underlying arithmetic computations, defining the right instruction set architecture, memory representations, etc, for them is a challenging task. We investigate in this paper the support for arbitrary and variable precision arithmetic in a dynamic binary translator, to help gain an insight of what such an accelerator could provide as an interface to compilers, and thus programmers. We detail our design and present an implementation in QEMU using the MPRF library for the RISC-V processor1.},
booktitle = {Proceedings of the 26th Asia and South Pacific Design Automation Conference},
pages = {325–330},
numpages = {6},
keywords = {System-Level Simulation, Arbitrary Precision Floating-Point, Dynamic Binary Translation},
location = {Tokyo, Japan},
series = {ASPDAC '21}
}

@article{10.14778/3407790.3407806,
author = {Bashardoost, Bahar Ghadiri and Miller, Ren\'{e}e J. and Lyons, Kelly and Nargesian, Fatemeh},
title = {Knowledge Translation},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407806},
doi = {10.14778/3407790.3407806},
abstract = {We introduce Kensho, a tool for generating mapping rules between two Knowledge Bases (KBs). To create the mapping rules, Kensho starts with a set of correspondences and enriches them with additional semantic information automatically identified from the structure and constraints of the KBs. Our approach works in two phases. In the first phase, semantic associations between resources of each KB are captured. In the second phase, mapping rules are generated by interpreting the correspondences in a way that respects the discovered semantic associations among elements of each KB. Kensho's mapping rules are expressed using SPARQL queries and can be used directly to exchange knowledge from source to target. Kensho is able to automatically rank the generated mapping rules using a set of heuristics. We present an experimental evaluation of Kensho and assess our mapping generation and ranking strategies using more than 50 synthesized and real world settings, chosen to showcase some of the most important applications of knowledge translation. In addition, we use three existing benchmarks to demonstrate Kensho's ability to deal with different mapping scenarios.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2018–2032},
numpages = {15}
}

@inproceedings{10.1145/3503161.3548261,
author = {Liu, Qiang and Zhou, Tongqing and Cai, Zhiping and Tang, Yonghao},
title = {Opportunistic Backdoor Attacks: Exploring Human-Imperceptible Vulnerabilities on Speech Recognition Systems},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548261},
doi = {10.1145/3503161.3548261},
abstract = {Speech recognition systems, trained and updated based on large-scale audio data, are vulnerable to backdoor attacks that inject dedicated triggers in system training. The used triggers are generally human-inaudible audio, such as ultrasonic waves. However, we note that such a design is not feasible, as it can be easily filtered out via pre-processing. In this work, we propose the first audible backdoor attack paradigm for speech recognition, characterized by passively triggering and opportunistically invoking. Traditional device-synthetic triggers are replaced with ambient noise in daily scenarios. For adapting triggers to the application dynamics of speech interaction, we exploit the observed knowledge inherited from the context to a trained model and accommodate the injection and poisoning with certainty-based trigger selection, performance-oblivious sample binding, and trigger late-augmentation. Experiments on two datasets under various environments evaluate the proposal's effectiveness in maintaining a high benign rate and facilitating outstanding attack success rate (99.27%, ~4% higher than BadNets), robustness (bounded infectious triggers), feasibility in real-world scenarios. It requires less than 1% data to be poisoned and is demonstrated to be able to resist typical speech enhancement techniques and general countermeasures (e.g., dedicated fine-tuning). The code and data will be made available at https://github.com/lqsunshine/DABA.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {2390–2398},
numpages = {9},
keywords = {backdoor attacks, speech recognition, AI security},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2018.2864535,
author = {Surendran, Sudeep and Kumar, T. Kishore},
title = {Oblique Projection and Cepstral Subtraction in Signal Subspace Speech Enhancement for Colored Noise Reduction},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2864535},
doi = {10.1109/TASLP.2018.2864535},
abstract = {In this paper, a subspace speech enhancement method handling the case of colored noise using oblique projection in the cepstral domain is proposed. Perceptual features and variance normalization are used to reduce the residual noise and improve the intelligibility of the output speech. Initially, the additive noise present in the noisy speech is removed by removing the orthogonal noise subspace from the noisy speech subspace to obtain the speech subspace. Then, the oblique projection of the noise subspace on the speech subspace along the additive noise subspace is used to determine the colored noise that remains. Colored noise removal is performed by power spectral subtraction in the cepstral domain. The spectral domain constrained estimator that incorporates the combined masking property of the human auditory system is employed to estimate the clean speech signal using the variance of the colored noise. To avoid the occurrence of any abrupt spikes in the output, variance normalization is performed by adaptively changing the control parameter of the estimator's gain matrix. The spectrograms, the objective measures and the subjective intelligibility test show the superior performance of the proposed method over the other existing speech enhancement methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2328–2340},
numpages = {13}
}

@inproceedings{10.1145/3593013.3594123,
author = {Saxon, Michael and Wang, William Yang},
title = {Disparities in Text-to-Image Model Concept Possession Across Languages},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594123},
doi = {10.1145/3593013.3594123},
abstract = {We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive “creative” generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1870},
numpages = {1},
keywords = {text-to-image models, dall-e, multilingual accessibility, benchmark, stable diffusion},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3536220.3558038,
author = {Deschamps-Berger, Theo and Lamel, Lori and Devillers, Laurence},
title = {Investigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations.},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536220.3558038},
doi = {10.1145/3536220.3558038},
abstract = {There has been growing interest in using deep learning techniques to recognize emotions from speech. However, real-life emotion datasets collected in call centers are relatively rare and small, making the use of deep learning techniques quite challenging. This research focuses on the study of Transformer-based models to improve the speech emotion recognition of patients’ speech in French emergency call center dialogues. The experiments were conducted on a corpus called CEMO, which was collected in a French emergency call center. It includes telephone conversations with more than 800 callers and 6 agents. Four emotion classes were selected for these experiments: Anger, Fear, Positive and Neutral state. We compare different Transformer encoders based on the wav2vec2 and BERT models, and explore their fine-tuning as well as fusion of the encoders for emotion recognition from speech. Our objective is to explore how to use these pre-trained models to improve model robustness in the context of a real-life application. We show that the use of specific pre-trained Transformer encoders improves the model performance for emotion recognition in the CEMO corpus. The Unweighted Accuracy (UA) of the french pre-trained wav2vec2 adapted to our task is 73.1%, whereas the UA of our baseline model (Temporal CNN-LSTM without pre-training) is 55.8%. We also tested BERT encoders models: in particular FlauBERT obtained good performance for both manual 67.1% and automatic 67.9% transcripts. The late and model-level fusion of the speech and text models also improve performance (77.1% (late) - 76.9% (model-level)) compared to our best speech pre-trained model, 73.1% UA. In order to place our work in the scientific community, we also report results on the widely used IEMOCAP corpus with our best fusion strategy, 70.8% UA. Our results are promising for constructing more robust speech emotion recognition system for real-world applications.},
booktitle = {Companion Publication of the 2022 International Conference on Multimodal Interaction},
pages = {144–153},
numpages = {10},
keywords = {real-life emotional corpus, emergency call center, Transformer-based models, model-level fusion, late fusion, speech emotion recognition},
location = {Bengaluru, India},
series = {ICMI '22 Companion}
}

@inproceedings{10.1145/3603781.3603785,
author = {Yao, Qingxue and Wang, Jinpeng and Kang, Yufang and Zhang, Bo and Xie, Xufen},
title = {Research on Fabry-Perot Fiber Optic Speech Sensor and Speech Enhancement},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603785},
doi = {10.1145/3603781.3603785},
abstract = {With the wide application of optical speech sensors, the quality of speech signal detected by an optical speech sensor has attracted the eyesight of technical personnel. In this paper, the sensing principle of the Fabry-Perot (F-P) optical speech sensor array is studied theoretically. To solve the problem of noise overlap on the output signal of the F-P optical voice sensor array, a multi-window spectral estimation spectral subtraction method based on endpoint detection is proposed. To achieve a more accurate noise reduction effect, this article uses endpoint detection to judge the voice and noise segments, identify the starting and estimate ending points of the voice and the average noise amplitude. The improved spectral subtraction method can enhance the key information in speech signals and reduce noise interference.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {19–23},
numpages = {5},
keywords = {multi-window spectrum estimation, endpoint detection, Fabry-Perot fiber optic speech sensor array},
location = {Xiamen, China},
series = {CNIOT '23}
}

@inproceedings{10.1145/3458380.3458405,
author = {Zhang, Ya-Jie and Ling, Zhen-Hua},
title = {Learning Deep and Wide Contextual Representations Using BERT for Statistical Parametric Speech Synthesis},
year = {2021},
isbn = {9781450389365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458380.3458405},
doi = {10.1145/3458380.3458405},
abstract = {In this paper, we propose a method of learning deep and wide contextual representations for statistical parametric speech synthesis (SPSS) using BERT, a pre-trained language representation model. Traditional acoustic models in SPSS utilize phoneme sequences and prosody labels as input, and can not make full use of the deep linguistic representations of current and surrounding sentences. Therefore, this paper designs two context encoders, i.e., a sentence-window context encoder and a paragraph-level context encoder, to integrate the contextual representations extracted from multiple sentences by BERT into Tacotron2 via an extra attention module. The parameters of BERT are pre-trained and then fine-tuned together with other components in the model. Experimental results on the Blizzard Challenge 2019 dataset show that both context encoders can reduce the errors of acoustic feature prediction and improve the subjective performance of synthetic speech comparing with the baseline Tacotron2 model.},
booktitle = {Proceedings of the 2021 5th International Conference on Digital Signal Processing},
pages = {146–150},
numpages = {5},
keywords = {BERT, speech synthesis, Tacotron2},
location = {Chengdu, China},
series = {ICDSP '21}
}

@article{10.1109/TASLP.2020.3000037,
author = {Gowda, Dhananjaya and Kadiri, Sudarsana Reddy and Story, Brad and Alku, Paavo},
title = {Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3000037},
doi = {10.1109/TASLP.2020.3000037},
abstract = {In this paper, we propose a new method for the accurate estimation and tracking of formants in speech signals using time-varying quasi-closed-phase (TVQCP) analysis. Conventional formant tracking methods typically adopt a two-stage estimateand-track strategy wherein an initial set of formant candidates are estimated using short-time analysis (e.g., 10-50 ms), followed by a tracking stage based on dynamic programming or a linear state-space model. One of the main disadvantages of these approaches is that the tracking stage, however good it may be, cannot improve upon the formant estimation accuracy of the first stage. The proposed TVQCP method provides a single-stage formant tracking that combines the estimation and tracking stages into one. TVQCP analysis combines three approaches to improve formant estimation and tracking: (1) it uses temporally weighted quasi-closed-phase analysis to derive closed-phase estimates of the vocal tract with reduced interference from the excitation source, (2) it increases the residual sparsity by using the L1 optimization and (3) it uses time-varying linear prediction analysis overlong time windows (e.g., 100-200 ms) to impose a continuity constraint on the vocal tract model and hence on the formant trajectories. Formant tracking experiments with a wide variety of synthetic and natural speech signals show that the proposed TVQCP method performs better than conventional and popular formant tracking tools, such as Wavesurfer and Praat (based on dynamic programming), the KARMA algorithm (based on Kalman filtering), and DeepFormants (based on deep neural networks trained in a supervised manner). Matlab scripts for the proposed method can be found at:},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1901–1914},
numpages = {14}
}

@inproceedings{10.1145/3577163.3595104,
author = {Xiang, Ziyue and Yadav, Amit Kumar Singh and Tubaro, Stefano and Bestagini, Paolo and Delp, Edward J.},
title = {Extracting Efficient Spectrograms From MP3 Compressed Speech Signals for Synthetic Speech Detection},
year = {2023},
isbn = {9798400700545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577163.3595104},
doi = {10.1145/3577163.3595104},
abstract = {Many speech signals are compressed with MP3 to reduce the data rate. In many synthetic speech detection methods the spectrogram of the speech signal is used. This usually requires the speech signal to be fully decompressed. We show that the design of MP3 compression allows one to approximate the spectrogram of the MP3 compressed speech efficiently without fully decoding the compressed speech. We denote the spectograms obtained using our proposed approach by Efficient Spectrograms (E-Specs). E-Spec can reduce the complexity of spectrogram computation by ~77.60 percentage points (p.p.) and save ~37.87 p.p. of MP3 decoding time. E-Spec bypasses the reconstruction artifacts introduced by the MP3 synthesis filterbank, which makes it useful in speech forensics tasks. We tested E-Spec in the synthetic speech detection, where a detector is asked to determine whether a speech signal is synthesized or recorded from a human. We examined 4 different neural network architectures to evaluate the performance of E-Spec compared to speech features extracted from the fully decoded speech signal. E-Spec achieved the best synthetic speech detection performance for 3 architectures; it also achieved the best overall detection performance across architectures. The computation of E-Spec is an approximation to Short Time Fourier Transform (STFT). E-Spec can be extended to other audio compression methods.},
booktitle = {Proceedings of the 2023 ACM Workshop on Information Hiding and Multimedia Security},
pages = {163–168},
numpages = {6},
keywords = {audio compression, signal processing, asvspoof19, synthetic speech detection, deep learning, mp3 compression},
location = {Chicago, IL, USA},
series = {IH&amp;MMSec '23}
}

@article{10.1162/coli_a_00349,
author = {Zhang, Hao and Sproat, Richard and Ng, Axel H. and Stahlberg, Felix and Peng, Xiaochang and Gorman, Kyle and Roark, Brian},
title = {Neural Models of Text Normalization for Speech Applications},
year = {2019},
issue_date = {June 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00349},
doi = {10.1162/coli_a_00349},
abstract = {Machine learning, including neural network techniques, have been applied to virtually every domain in natural language processing. One problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text-to-speech synthesis TTS. In this application, one must decide, for example, that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 King Ave. For this task, state-of-the-art industrial systems depend heavily on hand-written language-specific grammars.We propose neural network models that treat text normalization for TTS as a sequence-to-sequence problem, in which the input is a text token in context, and the output is the verbalization of that token. We find that the most effective model, in accuracy and efficiency, is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization. This model allows for a great deal of flexibility in terms of representing the context, and also allows us to integrate tagging and segmentation into the process.These models perform very well overall, but occasionally they will predict wildly inappropriate verbalizations, such as reading 3 cm as three kilometers. Although rare, such verbalizations are a major issue for TTS applications. We thus use finite-state covering grammars to guide the neural models, either during training and decoding, or just during decoding, away from such "unrecoverable" errors. Such grammars can largely be learned from data.},
journal = {Comput. Linguist.},
month = {jun},
pages = {293–337},
numpages = {45}
}

@inproceedings{10.1145/3374664.3379535,
author = {Hu, Ruijia (Roger) and Dorris, Wyatt and Vishwamitra, Nishant and Luo, Feng and Costello, Matthew},
title = {On the Impact of Word Representation in Hate Speech and Offensive Language Detection and Explanation},
year = {2020},
isbn = {9781450371070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374664.3379535},
doi = {10.1145/3374664.3379535},
abstract = {Online hate speech and offensive language have been widely recognized as critical social problems. To defend against this problem, several recent works have emerged that focus on the detection and explanation of hate speech and offensive language using machine learning approaches. Although these approaches are quite effective in the detection and explanation of hate speech and offensive language samples, they do not explore the impact of the representation of such samples. In this work, we introduce a novel, pronunciation-based representation of hate speech and offensive language samples to enable its detection with high accuracy. To demonstrate the effectiveness of our pronunciation-based representation, we extend an existing hate-speech and offensive language defense model based on deep Long Short-term Memory (LSTM) neural networks by using our pronunciation-based representation of hate speech and offensive language samples to train this model. Our work finds that the pronunciation-based presentation significantly reduces noise in the datasets and enhances the overall performance of the existing model.},
booktitle = {Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},
pages = {171–173},
numpages = {3},
keywords = {natural language processing, offensive language detection, explanation, hate speech detection},
location = {New Orleans, LA, USA},
series = {CODASPY '20}
}

@article{10.1109/TASLP.2019.2907015,
author = {Shimada, Kazuki and Bando, Yoshiaki and Mimura, Masato and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Unsupervised Speech Enhancement Based on Multichannel NMF-Informed Beamforming for Noise-Robust Automatic Speech Recognition},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2907015},
doi = {10.1109/TASLP.2019.2907015},
abstract = {This paper describes multichannel speech enhancement for improving automatic speech recognition ASR in noisy environments. Recently, the minimum variance distortionless response MVDR beamforming has widely been used because it works well if the steering vector of speech and the spatial covariance matrix SCM of noise are given. To estimating such spatial information, conventional studies take a supervised approach that classifies each time-frequency TF bin into noise or speech by training a deep neural network DNN. The performance of ASR, however, is degraded in an unknown noisy environment. To solve this problem, we take an unsupervised approach that decomposes each TF bin into the sum of speech and noise by using multichannel nonnegative matrix factorization MNMF. This enables us to accurately estimate the SCMs of speech and noise not from observed noisy mixtures but from separated speech and noise components. In this paper, we propose online MVDR beamforming by effectively initializing and incrementally updating the parameters of MNMF. Another main contribution is to comprehensively investigate the performances of ASR obtained by various types of spatial filters, i.e., time-invariant and variant versions of MVDR beamformers and those of rank-1 and full-rank multichannel Wiener filters, in combination with MNMF. The experimental results showed that the proposed method outperformed the state-of-the-art DNN-based beamforming method in unknown environments that did not match training data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {960–971},
numpages = {12}
}

@article{10.1145/3616012,
author = {Zhang, Weizhao and Yang, Hongwu},
title = {Improving Sequence-to-Sequence Tibetan Speech Synthesis with Prosodic Information},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616012},
doi = {10.1145/3616012},
abstract = {There are about 6,000 languages worldwide, most of which are low-resource languages. Although the current speech synthesis (or text-to-speech, TTS) for major languages (e.g., Mandarin, English, French) has achieved good results, the voice quality of TTS for low-resource languages (e.g., Tibetan) still needs to be further improved. Because prosody plays a significant role in natural speech, the article proposes two sequence-to-sequence (seq2seq) Tibetan TTS models with prosodic information fusion to improve the voice quality of synthesized Tibetan speech. We first constructed a large-scale Tibetan corpus for seq2seq TTS. Then we designed a prosody generator to extract prosodic information from the Tibetan sentences. Finally, we trained two seq2seq Tibetan TTS models by fusing prosodic information, including feature-level and model-level prosodic information fusion. The experimental results showed that the proposed two seq2seq Tibetan TTS models, which fuse prosodic information, could effectively improve the voice quality of synthesized speech. Furthermore, the model-level prosodic information fusion only needs 60% ~ 70% of the training data to synthesize a voice similar to the baseline seq2seq Tibetan TTS. Therefore, the proposed prosodic information fusion methods can improve the voice quality of synthesized speech for low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {sep},
articleno = {225},
numpages = {13},
keywords = {low-resource language, Tibetan speech synthesis, prosodic information fusion, Sequence-to-sequence speech synthesis}
}

@article{10.1145/3332146,
author = {Gallardo, Laura Fern\'{a}ndez and Sanchez-Iborra, Ramon},
title = {On the Impact of Voice Encoding and Transmission on the Predictions of Speaker Warmth and Attractiveness},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3332146},
doi = {10.1145/3332146},
abstract = {Modern human-computer interaction systems may not only be based on interpreting natural language but also on detecting speaker interpersonal characteristics in order to determine dialog strategies. This may be of high interest in different fields such as telephone marketing or automatic voice-based interactive services. However, when such systems encounter signals transmitted over a communication network instead of clean speech, e.g., in call centers, the speaker characterization accuracy might be impaired by the degradations caused in the speech signal by the encoding and communication processes. This article addresses a binary classification of high versus low warm--attractive speakers over different channel and encoding conditions. The ground truth is derived from ratings given to clean speech extracted from an extensive subjective test. Our results show that, under the considered conditions, the AMR-WB+ codec permits good levels of classification accuracy, comparable to the classification with clean, non-degraded speech. This is especially notable for the case of a Random Forest-based classifier, which presents the best performance among the set of evaluated algorithms. The impact of different packet loss rates has been examined, whereas jitter effects have been found to be negligible.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {40},
numpages = {17},
keywords = {predictive modeling, speech processing, Speaker characteristics, transmission channels}
}

@inproceedings{10.1145/3397271.3401270,
author = {Nair, Suraj and Galuscakova, Petra and Oard, Douglas W.},
title = {Combining Contextualized and Non-Contextualized Query Translations to Improve CLIR},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401270},
doi = {10.1145/3397271.3401270},
abstract = {In cross-language information retrieval using probabilistic structured queries (PSQ), translation probabilities from statistical machine translation act as a bridge between the query and document vocabulary. These translation probabilities are typically estimated from a sentence-aligned corpus on a word to word basis without taking into account the context. Neural methods, by contrast, can learn to translate using the context around the words, and this can be used as a basis for estimating context-dependent translation probabilities. However, sparsity limits the accuracy of context-specific translation probabilities for rare words, which can be important in retrieval applications. This paper presents evidence that combining such context-dependent translation probabilities with context-independent translation probabilities learned from the same parallel corpus can yield improvements in the effectiveness of cross-language ranked retrieval.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1581–1584},
numpages = {4},
keywords = {CLIR, machine translation},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3487553.3524714,
author = {Georgantas, Costa and Richiardi, Jonas},
title = {Multi-View Omics Translation with Multiplex Graph Neural Networks},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524714},
doi = {10.1145/3487553.3524714},
abstract = {The rapid development of high-throughput experimental technologies for biological sampling has made the collection of omics data (e.g., genomics, epigenomics, transcriptomics and metabolomics) possible at a small cost. While multi-view approaches to omics data have a long history, omics-to-omics translation is a relatively new strand of research with useful applications such as recovering missing or censored data and finding new correlations between samples. As the relations between omics can be non-linear and exhibit long-range dependencies between parts of the genome, deep neural networks can be an effective tool. Graph neural networks have been applied successfully in many different areas of research, especially in problems where annotated data is sparse, and have recently been extended to the heterogeneous graph case, allowing for the modelling of multiple kinds of similarities and entities. Here, we propose a meso-scale approach to construct multiplex graphs from multi-omics data, which can construct several graphs per omics and cross-omics graphs. We also propose a neural network architecture for omics-to-omics translation from these multiplex graphs, featuring a graph neural network encoder, coupled with an attention layer. We evaluate the approach on the open The Cancer Genome Atlas dataset (N=3023), showing that for MicroRNA expression prediction our approach has lower prediction error than regularized linear regression or modern generative adversarial networks.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1030–1036},
numpages = {7},
keywords = {machine learning, heterogeneous graph, gene expression, methylation, graph representation, microRNA, autoencoder},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3325061.3325065,
author = {Hong, Ding-Yong and Lin, Shih-Kai and Fu, Sheng-Yu and Wu, Jan-Jan and Hsu, Wei-Chung},
title = {Enhancing Transactional Memory Execution via Dynamic Binary Translation},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1559-6915},
url = {https://doi.org/10.1145/3325061.3325065},
doi = {10.1145/3325061.3325065},
abstract = {Transactional Synchronization Extensions (TSX) have been introduced for hardware transactional memory since the 4th generation Intel Core processors. TSX provides two software programming interfaces: Hardware Lock Elision (HLE) and Restricted Transactional Memory (RTM). HLE is easy to use and maintains backward compatibility with processors without TSX support, while RTM is more flexible and scalable. Previous researches have shown that critical sections protected by RTM with a well-designed retry mechanism as its fallback code path can often achieve better performance than HLE. More parallel programs may be programmed in HLE, however, using RTM may obtain greater performance. To embrace both productivity and high performance of parallel programs with TSX, we present a framework built on QEMU that can dynamically transform HLE instructions in an application binary to fragments of RTM codes with adaptive tuning on the fly. Compared to HLE execution, our prototype achieves 1.56 x speedup with 8 threads on average. Due to the scalability of RTM, the speedup will be more significant as the number of threads increases.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {apr},
pages = {48–58},
numpages = {11},
keywords = {dynamic binary translation, transactional memory, adaptive tuning, transactional synchronization extensions}
}

@inproceedings{10.1145/3461615.3485440,
author = {Kothalkar, Prasanna V. and Datla, Sathvik and Dutta, Satwik and Hansen, John H. L. and Seven, Yagmur and Irvin, Dwight and Buzhardt, Jay},
title = {Measuring Frequency of Child-Directed WH-Question Words for Alternate Preschool Locations Using Speech Recognition and Location Tracking Technologies},
year = {2021},
isbn = {9781450384711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461615.3485440},
doi = {10.1145/3461615.3485440},
abstract = {Speech and language development in children are crucial for ensuring effective skills in their long-term learning ability. A child’s vocabulary size at the time of entry into kindergarten is an early indicator of their learning ability to read and potential long-term success in school. The preschool classroom is thus a promising venue for assessing growth in young children by measuring their interactions with teachers as well as classmates. However, to date limited studies have explored such naturalistic audio communications. Automatic Speech Recognition (ASR) technologies provide an opportunity for ’Early Childhood’ researchers to obtain knowledge through automatic analysis of naturalistic classroom recordings in measuring such interactions. For this purpose, 208 hours of audio recordings across 48 daylong sessions are collected in a childcare learning center in the United States using Language Environment Analysis (LENA) devices worn by the preschool children. Approximately 29 hours of adult speech and 26 hours of child speech is segmented using manual transcriptions provided by CRSS transcription team. Traditional as well as End-to-End ASR models are trained on adult/child speech data subset. Factorized Time Delay Neural Network provides a best Word-Error-Rate (WER) of 35.05% on the adult subset of the test set. End-to-End transformer models achieve 63.5% WER on the child subset of the test data. Next, bar plots demonstrating the frequency of WH-question words in Science vs. Reading activity areas of the preschool are presented for sessions in the test set. It is suggested that learning spaces could be configured to encourage greater adult-child conversational engagement given such speech/audio assessment strategies.},
booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction},
pages = {414–418},
numpages = {5},
keywords = {speech recognition, neural networks, education, early childhood},
location = {Montreal, QC, Canada},
series = {ICMI '21 Companion}
}

@inproceedings{10.1145/3544109.3544356,
author = {Li, Shuying},
title = {Construction of English-Assisted Translation Learning System Based on Improved Attention Mechanism Model},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544356},
doi = {10.1145/3544109.3544356},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {795–799},
numpages = {5},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3584202.3584247,
author = {Alfaidi, Aseel and Alshahrani, Abdullah and Aljohani, Maha},
title = {Artificial Intelligence-Based Speech Signal for COVID-19 Diagnostics},
year = {2023},
isbn = {9781450399050},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584202.3584247},
doi = {10.1145/3584202.3584247},
abstract = {The speech signal has numerous features that represent the characteristics of a specific language and recognize emotions. It also contains information that can be used to identify the mental, psychological, and physical states of the speaker. Recently, the acoustic analysis of speech signals offers a practical, automated, and scalable method for medical diagnosis and monitoring symptoms of many diseases. In this paper, we explore the deep acoustic features from confirmed positive and negative cases of COVID-19 and compare the performance of the acoustic features and COVID-19 symptoms in terms of their ability to diagnose COVID-19. The proposed methodology consists of the pre-trained Visual Geometry Group (VGG-16) model based on Mel spectrogram images to extract deep audio features. In addition to the K-means algorithm that determines effective features, followed by a Genetic Algorithm-Support Vector Machine (GA-SVM) classifier to classify cases. The experimental findings indicate the proposed methodology’s capability to classify COVID-19 and NOT COVID-19 from acoustic features compared to COVID-19 symptoms, achieving an accuracy of 97%. The experimental results show that the proposed method remarkably improves the accuracy of COVID-19 detection over the handcrafted features used in previous studies.},
booktitle = {Proceedings of the 6th International Conference on Future Networks &amp; Distributed Systems},
pages = {311–317},
numpages = {7},
keywords = {Machine learning, COVID-19 diagnosis, Deep learning, Mel spectrogram, Speech signal},
location = {Tashkent, TAS, Uzbekistan},
series = {ICFNDS '22}
}

@inproceedings{10.1145/3527188.3563910,
author = {Obremski, David and Lugrin, Birgit},
title = {Mixed-Cultural Speech for Mixed-Cultural Users - Natural vs. Synthetic Speech for Virtual Agents},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3563910},
doi = {10.1145/3527188.3563910},
abstract = {This study investigates how different levels of a non-native Turkish accent in German speech are perceived by Turkish-German listeners, using either natural or synthetic speech. The participants listened to six audio recordings and rated the respective speaker regarding her mother tongue, warmth, competence, and intelligibility. The results show that the naturalness of speech had no impact on the non-native speakers’ ability to assign the correct mother tongue to the respective speaker. It did, however have an impact on the speakers’ perceived warmth, competence and intelligibility.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {290–292},
numpages = {3},
keywords = {synthetic speech, intelligent virtual agents, mixed-cultural},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@inproceedings{10.1145/3349569.3351534,
author = {Lee, Young-Min and Yang, Joon-Sung},
title = {Computation Offloading of Acoustic Model for Client-Edge-Based Speech-Recognition: Work-in-Progress},
year = {2019},
isbn = {9781450369251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349569.3351534},
doi = {10.1145/3349569.3351534},
abstract = {Speech recognition technology combined with artificial intelligence represents a quantum leap more accurate than past pattern recognition methods. And server-based system support for scalability, virtualization and huge amounts of unlimited storage resources that greatly contributed to the improvement of the accuracy of its prediction. However, the implementation of server-oriented reforms led to enormous latency and connectivity problems. Therefore, we propose a novel client-edge speech recognition system to enhance latency by using what we call semi-offloading technology This proposal is promising big performance gains by offloading computing power-dependent tasks to edge nodes and processing throughput-dependent tasks by a client. The merit of semi-offloading as well as a division of workload allows for parallelism and re-ordering among the process. The experimental results show that, 23%~62% improvement in response time.},
booktitle = {Proceedings of the International Conference on Compliers, Architectures and Synthesis for Embedded Systems Companion},
articleno = {1},
numpages = {2},
keywords = {semi-offloading, edge computing, speech-recognition},
location = {New York, New York},
series = {CASES '19}
}

@inproceedings{10.1145/3503161.3547731,
author = {Song, Yuanfeng and Lian, Rongzhong and Chen, Yixin and Jiang, Di and Zhao, Xuefang and Tan, Conghui and Xu, Qian and Wong, Raymond Chi-Wing},
title = {A Platform for Deploying the TFE Ecosystem of Automatic Speech Recognition},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547731},
doi = {10.1145/3503161.3547731},
abstract = {Since data regulations such as the European Union's General Data Protection Regulation (GDPR) have taken effect, the traditional two-step Automatic Speech Recognition (ASR) optimization strategy (i.e., training a one-size-fits-all model with vendor's centralized data and fine-tuning the model with clients' private data) has become infeasible. To meet these privacy requirements, TFE, a novel GDPR-compliant ASR ecosystem, has been proposed by us to incorporate transfer learning, federated learning, and evolutionary learning towards effective ASR model optimization. In this demonstration, we further design and implement a novel platform to promote the deployment and applicability of TFE. Our proposed platform allows enterprises to easily conduct the ASR optimization task using TFE across organizations.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6952–6954},
numpages = {3},
keywords = {speech recognition, evolutionary learning, federated learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3377713.3377794,
author = {Fantaye, Tessfu Geteye and Yu, Junqing and Hailu, Tulu Tilahun},
title = {Syllable-Based Speech Recognition for a Very Low-Resource Language, Chaha},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377794},
doi = {10.1145/3377713.3377794},
abstract = {Chaha is a very low-resource language, which is suffered from lack of language resources to develop human language technologies, namely, speech recognition. Moreover, Chaha writing system is syllabic with a consonant-vowel (CV) syllable structure. The Chaha orthography is a one-to-one correspondence with syllable sound units. By considering the above facts of Chaha, this study is the first endeavor that explores the use of CV syllables as acoustic modeling units for developing speech recognizers, using the Gaussian mixture model (GMM) and unilingual and transfer learning deep neural network (DNN) models. Our experimental results demonstrate that the syllablebased unilingual DNN and transfer learning DNN models outperform the corresponding GMM and unilingual DNN models with absolute performance improvements of 2.8 to 3.09% and 1.07 to 4.94%, respectively. The best performing syllable-based recognizer is achieved using a shared hidden layer (SHL) time delay deep neural network (TDNN) model with a word error rate (WER) of 23.11%. Hence, the CV syllables are suitable acoustic units to develop Chaha speech recognition systems under sufficient training corpus.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {415–420},
numpages = {6},
keywords = {Chaha, Transfer learning, Syllables, Speech recognition, Lowresource language},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3478512.3488603,
author = {Chen, Li-Yu and Shen, I-Chao and Chen, Bing-Yu},
title = {Guided Image Weathering Using Image-to-Image Translation},
year = {2021},
isbn = {9781450390736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478512.3488603},
doi = {10.1145/3478512.3488603},
abstract = {In this paper, we present a guided image weathering method that allows the user to generate the weathering process. The core of our method is a three-step method to generate textures at different time steps of the weathering process. The input texture is analyzed first to obtain the weathering degree (age map) for each pixel, then we train a conditional adversarial network to generate texture patches with diverse weathering effects. Once the training is finished, new weathering results can be generated by manipulating the age map, such as automatic interpolation and manually modified by the user.},
booktitle = {SIGGRAPH Asia 2021 Technical Communications},
articleno = {10},
numpages = {4},
keywords = {image weathering, image to image translation},
location = {Tokyo, Japan},
series = {SA '21}
}

@inproceedings{10.1145/3310411.3310417,
author = {Bouraoui, Hasna and Castrillon, Jeronimo and Jerad, Chadlia},
title = {Comparing Dataflow and OpenMP Programming for Speaker Recognition Applications},
year = {2019},
isbn = {9781450363211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310411.3310417},
doi = {10.1145/3310411.3310417},
abstract = {The still increasing number of transistors per chip offered by Moore's law, together with the Post-Dennard scaling era shifted the performance gain from frequency increase to multi-core processing. Consequently, the support of parallel execution of applications is becoming mandatory. Furthermore, the need for efficient parallel models and languages is more critical for the embedded domain, due to power consumption and memory constraints, among others. This work focuses on parallelizing an embedded speaker recognition application, which is a biometric technique for identification. While a lot of work has been done for speech recognition, fewer efforts have focused on recognizing who the speaker is. In this paper, we analyze two implementations for speaker recognition applications (SRA), namely dataflow and shared memory programming models. More precisely, we use Process Networks (PNs) as a dataflow representation, which is an intuitive way to design streaming applications. We use the language "C for Process Networks" for the dataflow implementation and OpenMP for the shared memory one. For two different target platforms, we compared two implementations using OpenMP (exploring data-level parallelism only and with pipelining) against a dataflow-based compiled implementation that allows for functional optimization. Despite faster communication over shared memory, we show that the dataflow model is superior in terms of performance (up to twice as fast).},
booktitle = {Proceedings of the 10th and 8th Workshop on Parallel Programming and Run-Time Management Techniques for Many-Core Architectures and Design Tools and Architectures for Multicore Embedded Computing Platforms},
articleno = {4},
numpages = {6},
keywords = {Speaker recognition, Datalow models, Multicore programming, Shared memory programming},
location = {Valencia, Spain},
series = {PARMA-DITAM 2019}
}

@article{10.1145/3564769,
author = {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
title = {State of the Art of Automation in Sign Language: A Systematic Review},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564769},
doi = {10.1145/3564769},
abstract = {Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {apr},
articleno = {94},
numpages = {80},
keywords = {virtual avatar, HamNoSys, Interlingua, SiGML, Machine translation}
}

@article{10.1109/TASLP.2019.2946086,
author = {Ali, Randall and Van Waterschoot, Toon and Moonen, Marc},
title = {Integration of &lt;italic&gt;a Priori&lt;/Italic&gt; and Estimated Constraints Into an MVDR Beamformer for Speech Enhancement},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2946086},
doi = {10.1109/TASLP.2019.2946086},
abstract = {Conventionally, the single constraint of the minimum variance distortionless response (MVDR) beamformer for speech enhancement has been defined using one of two approaches. Either it is based on a priori assumptions such as microphone characteristics, position, speech source location, and room acoustics, or on a relative transfer function (RTF) vector estimate using a data dependent method. Each approach has its respective merits and drawbacks and a decision usually has to be made between one of the approaches. In this paper, an alternative approach of using an integrated MVDR beamformer is investigated, where both the hard constraints from the two conventional approaches are softened to yield two tuning parameters. It will be shown that this integrated MVDR beamformer can be expressed as a convex combination of the conventional MVDR beamformers, a linearly constrained minimum variance (LCMV) beamformer, and an all-zero vector, with real, positive-valued coefficients. By analysing how the tuning parameters affect these coefficients, two tuning rules for a practical implementation of the integrated MVDR are subsequently proposed. An evaluation with simulated and recorded data demonstrates that the integrated MVDR beamformer can be beneficial as opposed to relying on either of the conventional MVDR beamformers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {2288–2300},
numpages = {13}
}

@article{10.1109/TASLP.2021.3057492,
author = {Vukovic, Maria and Stolar, Melissa and Lech, Margaret},
title = {Cognitive Load Estimation From Speech Commands to Simulated Aircraft},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3057492},
doi = {10.1109/TASLP.2021.3057492},
abstract = {This paper investigates and compares methods for cognitive load (CL) estimation from speech. The majority of previous studies of CL estimation used speech collected in laboratory conditions and conventional speech classification methods. Traditionally laboratory speech contains balanced classes that are labeled by a third party after the speech has been collected. In contrast, the speech used in this research was recorded during an experiment focused on human-machine interaction - where spoken commands were used to control simulated aircraft. The speech was labeled using subjective assessments of CL during an experiment that manipulated workload. Current state-of-the-art Convolutional Neural Network (CNN) classification was used for cognitive load estimation and was compared with conventional Support Vector Machine (SVM) and k-Nearest Neighbor (k-NN) classification. Different speaker-dependence models were compared across 2 and 3 classes. In addition, class boundary selection was optimized to reflect the subjective human workload response sigmoidal curve and compared with linear class boundaries. Results for 3-class CL estimation showed that CNN classifiers trained using speech spectrograms for Partially Speaker Dependent (PSD) models using sigmoidal curve class boundaries provided up to 83.7% accuracy. CNN classifiers outperformed baseline SVM and k-NN classifiers (that used acoustic features) on the same dataset by 13.2% and 10.5% respectively. These outcomes indicate that spectrogram-trained CNN classifiers are a worthy consideration in paralinguistic classification problems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1011–1022},
numpages = {12}
}

@article{10.1109/TASLP.2021.3054302,
author = {Wang, Heming and Wang, DeLiang},
title = {Towards Robust Speech Super-Resolution},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3054302},
doi = {10.1109/TASLP.2021.3054302},
abstract = {Speech super-resolution (SR) aims to increase the sampling rate of a given speech signal by generating high-frequency components. This paper proposes a convolutional neural network (CNN) based SR model that takes advantage of information from both time and frequency domains. Specifically, the proposed CNN is a time-domain model that takes the raw waveform of low-resolution speech as the input, and outputs an estimate of the corresponding high-resolution waveform. During the training stage, we employ a cross-domain loss to optimize the network. We compare our model with several deep neural network (DNN) based SR models, and experiments show that our model outperforms existing models. Furthermore, the robustness of DNN-based models is investigated, in particular regarding microphone channels and downsampling schemes, which have a major impact on the performance of DNN-based SR models. By training with proper datasets and preprocessing, we improve the generalization capability for untrained microphone channels and unknown downsampling schemes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {2058–2066},
numpages = {9}
}

@article{10.1145/3342352,
author = {Prakash, Jeena J. and Rajan, Golda Brunet and Murthy, Hema A.},
title = {Importance of Signal Processing Cues in Transcription Correction for Low-Resource Indian Languages},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342352},
doi = {10.1145/3342352},
abstract = {Accurate phonetic transcriptions are crucial for building robust acoustic models for speech recognition as well as speech synthesis applications. Phonetic transcriptions are not usually provided with speech corpora. A lexicon is used to generate phone-level transcriptions of speech corpora with sentence-level transcriptions. When lexical entries are not available, letter-to-sound (LTS) rules are used. Whether it is a lexicon or LTS, the rules for pronunciation are generic and may not match the spoken utterance. This can lead to transcription errors. The objective of this study is to address the issue of mismatch between the transcription and its acoustic realisation. In particular, the issue of vowel deletions is studied. Group-delay-based segmentation is used to determine insertion/deletion of vowels in the speech utterance. The transcriptions are corrected in the training data based on this. The corrected data are used in automatic speech recognition (ASR) and text to speech synthesis (TTS) systems. ASR and TTS systems built with the corrected transcriptions show improvements in the performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {aug},
articleno = {14},
numpages = {26},
keywords = {automatic speech recognition, signal processing cue based on group delay, Transcription mismatch errors, hidden Markov model-forced Viterbi alignment, text-to-speech synthesis}
}

@article{10.1145/3314942,
author = {Onyenwe, Ikechukwu E. and Hepple, Mark and Chinedu, Uchechukwu and Ezeani, Ignatius},
title = {Toward an Effective Igbo Part-of-Speech Tagger},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3314942},
doi = {10.1145/3314942},
abstract = {Part-of-speech (POS) tagging is a well-established technology for most Western European languages and a few other world languages, but it has not been evaluated on Igbo, an agglutinative African language. This article presents POS tagging experiments conducted using an Igbo corpus as a test bed for identifying the POS taggers and the Machine Learning (ML) methods that can achieve a good performance with the small dataset available for the language. Experiments have been conducted using different well-known POS taggers developed for English or European languages, and different training data styles and sizes. Igbo has a number of language-specific characteristics that present a challenge for effective POS tagging. One interesting case is the wide use of verbs (and nominalizations thereof) that have an inherent noun complement, which form “linked pairs” in the POS tagging scheme, but which may appear discontinuously. Another issue is Igbo’s highly productive agglutinative morphology, which can produce many variant word forms from a given root. This productivity is a key cause of the out-of-vocabulary (OOV) words observed during Igbo tagging. We report results of experiments on a promising direction for improving tagging performance on such morphologically-inflected OOV words.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {42},
numpages = {26},
keywords = {language technology, text processing, Igbo, morphological analysis, machine learning, Natural language processing (NLP), part-of-speech (POS) tagging, corpora, POS tagger, tagset, African language, corpus annotation}
}

@article{10.1145/3479159,
author = {Suissa, Omri and Zhitomirsky-Geffet, Maayan and Elmalech, Avshalom},
title = {Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3479159},
doi = {10.1145/3479159},
abstract = {Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters’ optimization for building an effective neural network for OCR post-correction in Hebrew. To evaluate the proposed approach, a series of experiments using several literary Hebrew corpora from various periods and genres were conducted. The obtained results demonstrate that (1) training a network on texts from a similar period dramatically improves the network's ability to fix OCR errors, (2) using the proposed error injection algorithm, based on character-level period-specific errors, minimizes the need for manually corrected data and improves the network accuracy by 9%, (3) the optimized network design improves the accuracy by 3% compared to the state-of-the-art network, and (4) the constructed optimized network outperforms neural machine translation models and industry-leading spellcheckers. The proposed methodology may have practical implications for digital humanities projects that aim to search and analyze OCRed documents in Hebrew and potentially other morphologically rich languages.},
journal = {J. Comput. Cult. Herit.},
month = {apr},
articleno = {24},
numpages = {20},
keywords = {dataset generation, Hebrew, historical newspapers, digital humanities, neural machine translation, OCR post-correction, DNN}
}

@article{10.1145/3427384.3427392,
author = {Cordourier Maruri, H\'{e}ctor A. and Lopez-Meyer, Paulo and Huang, Jonathan and Beltman, Willem and Nachman, Lama and Lu, Hong},
title = {V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {2375-0529},
url = {https://doi.org/10.1145/3427384.3427392},
doi = {10.1145/3427384.3427392},
abstract = {Smart glasses are often used in noisy public spaces or industrial settings. Voice commands and automatic speech recognition (ASR) are good user interfaces for such a form factor, but the background noise and interfering speakers pose important challenges. Typical signal processing techniques have limitations in performance and/or hardware resources. V-Speech is a novel solution that captures the voice signal with a vibration sensor located in the nasal pads of smart glasses. Although signal-to-noise ratio (SNR) is much higher with vibration sensor capture, it introduces a "nasal distortion," which must be dealt with. The second part of our proposed solution involves a voice transformation of the vibration signal using a neural network to produce an output that mimics the characteristics of a conventional microphone. We evaluated V-Speech in noise-free and very noisy conditions with 30 volunteer speakers uttering 145 phrases each, and validated its performance on ASR engines, with assessments of voice quality using the Perceptual Evaluation of Speech Quality (PESQ) metric, and with subjective listeners to determine intelligibility, naturalness and overall quality. The results show, in extreme noise conditions, a mean improvement of 50% for Word Error Rate (WER), 1.0 on a scale of 5.0 for PESQ, and speech regarded intelligible, with naturalness rated as fair to good. The output of V-Speech has low noise, sounds natural, and enables clear voice communication in challenging environments.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {sep},
pages = {18–24},
numpages = {7}
}

@inproceedings{10.1109/ICSE43902.2021.00107,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {automatic program repair, software reliability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3369555.3369564,
author = {Emiru, Eshete Derb and Li, Yaxing and Xiong, Shengwu and Fesseha, Awet},
title = {Speech Recognition System Based on Deep Neural Network Acoustic Modeling for Low Resourced Language-Amharic},
year = {2020},
isbn = {9781450371803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369555.3369564},
doi = {10.1145/3369555.3369564},
abstract = {In this paper automatic speech recognition is investigated using deep neural network (DNN) acoustic modeling method for Amharic language at syllabic acoustic units. In grapheme based database; graphemes/characters are basic units of lexicon and language model. A large portion of them represents syllables which are a combination of consonants and vowels (CV). Grapheme to phoneme (G2P) conversion was required to represent all text corpuses into CV phoneme representations via G2P conversion algorithm developed for this purpose. This algorithm used to develop syllable based pronunciation dictionary and language modeling which are vital for speech recognizer. DNN based acoustic model (AM) such as tanh-DNNs, tanh-fast-DNNs, p-norm-DNNs and p-norm-fast-DNNs are also explored with different hidden layers, hidden units and parameter settings. These DNN AMs are trained with morpheme based Amharic read speech in order to develop models. The recognition performance of our methods is evaluated in testing data and the reduced WER is achieved in p-norm-fast(p=2) DNN AMs.},
booktitle = {Proceedings of the 3rd International Conference on Telecommunications and Communication Engineering},
pages = {141–145},
numpages = {5},
keywords = {DNN, Amharic, speech recognition, grapheme-to-phoneme (G2P), acoustic modeling},
location = {Tokyo, Japan},
series = {ICTCE '19}
}

@article{10.1109/TASLP.2020.3037457,
author = {Lee, Hung-Shin and Tsao, Yu and Jeng, Shyh-Kang and Wang, Hsin-Min},
title = {Subspace-Based Representation and Learning for Phonotactic Spoken Language Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3037457},
doi = {10.1109/TASLP.2020.3037457},
abstract = {Phonotactic constraints can be employed to distinguish languages by representing a speech utterance as a multinomial distribution or phone events. In the present study, we propose a new learning mechanism based on subspace-based representation, which can extract concealed phonotactic structures from utterances, for language verification and dialect/accent identification. The framework mainly involves two successive parts. The first part involves subspace construction. Specifically, it decodes each utterance into a sequence of vectors filled with phone-posteriors and transforms the vector sequence into a linear orthogonal subspace based on low-rank matrix factorization or dynamic linear modeling. The second part involves subspace learning based on kernel machines, such as support vector machines and the newly developed subspace-based neural networks (SNNs). The input layer of SNNs is specifically designed for the sample represented by subspaces. The topology ensures that the same output can be derived from identical subspaces by modifying the conventional feed-forward pass to fit the mathematical definition of subspace similarity. Evaluated on the “General LR” test of NIST LRE 2007, the proposed method achieved up to 52%, 46%, 56%, and 27% relative reductions in equal error rates over the sequence-based PPR-LM, PPR-VSM, and PPR-IVEC methods and the lattice-based PPR-LM method, respectively. Furthermore, on the dialect/accent identification task of NIST LRE 2009, the SNN-based system performed better than the aforementioned four baseline methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {3065–3079},
numpages = {15}
}

@inproceedings{10.1145/3544548.3580801,
author = {Zhang, Ruidong and Li, Ke and Hao, Yihong and Wang, Yufan and Lai, Zhengnan and Guimbreti\`{e}re, Fran\c{c}ois and Zhang, Cheng},
title = {EchoSpeech: Continuous Silent Speech Recognition on Minimally-Obtrusive Eyewear Powered by Acoustic Sensing},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580801},
doi = {10.1145/3544548.3580801},
abstract = {We present EchoSpeech, a minimally-obtrusive silent speech interface (SSI) powered by low-power active acoustic sensing. EchoSpeech uses speakers and microphones mounted on a glass-frame and emits inaudible sound waves towards the skin. By analyzing echos from multiple paths, EchoSpeech captures subtle skin deformations caused by silent utterances and uses them to infer silent speech. With a user study of 12 participants, we demonstrate that EchoSpeech can recognize 31 isolated commands and 3-6 figure connected digits with 4.5% (std 3.5%) and 6.1% (std 4.2%) Word Error Rate (WER), respectively. We further evaluated EchoSpeech under scenarios including walking and noise injection to test its robustness. We then demonstrated using EchoSpeech in demo applications in real-time operating at 73.3mW, where the real-time pipeline was implemented on a smartphone with only 1-6 minutes of training data. We believe that EchoSpeech takes a solid step towards minimally-obtrusive wearable SSI for real-life deployment.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {852},
numpages = {18},
keywords = {Silent Speech Recognition, Smart Glasses, Acoustic Sensing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3579365,
author = {V\'{a}squez, Francisco and Calder\'{o}n, Juan Felipe and Meza, Federico and V\'{a}squez, Andrea},
title = {Validation of a Spanish-Language Version of a Computer Programming Aptitude Test for First-Year University Students},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
url = {https://doi.org/10.1145/3579365},
doi = {10.1145/3579365},
abstract = {There is increasing interest in computer science and computing bachelor programs due to the growing importance of technology in the globalized world. Thus, as higher education institutions strive to serve a diverse student demographic, it is salient to gauge their programming abilities to improve guidance on learning processes regarding their initial knowledge state. Despite the availability of certain instruments to measure student programming skills, these are traditionally aimed at younger populations and do not accurately discriminate the different levels of ability among university students. This article introduces a translation into Spanish and validation of an existing English-language aptitude test for computing jobs that can be used to measure the programming abilities of students with no prior experience in the field. Following a cyclic research methodology, two iterations were carried out in this article. First, the aforementioned test was translated and validated via expert judgment and focus groups, in which certain items were removed subsequent to a quantitative analysis. The resultant instrument underwent a second validation using a larger population of students. Analysis conducted after the second iteration showed this instrument to deliver good internal consistency, good difficulty and discrimination indices, and a moderate correlation with the grades of the midterm exam of a programming course undertaken by first year engineering students. This work contributes to both increasing the number of tests available in the Spanish language with which to assess programming abilities, as well as to the broader literature regarding test adaptation, translation and validation.},
journal = {ACM Trans. Comput. Educ.},
month = {mar},
articleno = {21},
numpages = {20},
keywords = {programming aptitude, validation, quantitative research, assessment, cs1, Programming assessment}
}

@article{10.1109/TASLP.2021.3126949,
author = {Sar\i{}, Leda and Hasegawa-Johnson, Mark and Yoo, Chang D.},
title = {Counterfactually Fair Automatic Speech Recognition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126949},
doi = {10.1109/TASLP.2021.3126949},
abstract = {Widelyused automatic speech recognition (ASR) systems have been empirically demonstrated in various studies to be unfair, having higher error rates for some groups of users than others. One way to define fairness in ASR is to require that changing the demographic group affiliation of any individual (e.g., changing their gender, age, education or race) should not change the probability distribution across possible speech-to-text transcriptions. In the paradigm of counterfactual fairness, all variables independent of group affiliation (e.g., the text being read by the speaker) remain unchanged, while variables dependent on group affiliation (e.g., the speaker’s voice) are counterfactually modified. Hence, we approach the fairness of ASR by training the ASR to minimize change in its outcome probabilities despite a counterfactual change in the individual’s demographic attributes. Starting from the individualized counterfactual equal odds criterion, we provide relaxations to it and compare their performances for connectionist temporal classification (CTC) based end-to-end ASR systems. We perform our experiments on the Corpus of Regional African American Language (CORAAL) and the LibriSpeech dataset to accommodate for differences due to gender, age, education, and race. We show that with counterfactual training, we can reduce average character error rates while achieving lower performance gap between demographic groups, and lower error standard deviation among individuals.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3515–3525},
numpages = {11}
}

@article{10.1145/3571258,
author = {Lemerre, Matthieu},
title = {SSA Translation Is an Abstract Interpretation},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571258},
doi = {10.1145/3571258},
abstract = {Static single assignment (SSA) form is a popular intermediate representation that helps implement useful static analyses, including global value numbering (GVN), sparse dataflow analyses, or SMT-based abstract interpretation or model checking. However, the precision of the SSA translation itself depends on static analyses, and a priori static analysis is even indispensable in the case of low-level input languages like machine code. To solve this chicken-and-egg problem, we propose to turn the SSA translation into a standard static analysis based on abstract interpretation. This allows the SSA translation to be combined with other static analyses in a single pass, taking advantage of the fact that it is more precise to combine analyses than applying passes in sequence. We illustrate the practicality of these results by writing a simple dataflow analysis that performs SSA translation, optimistic global value numbering, sparse conditional constant propagation, and loop-invariant code motion in a single small pass; and by presenting a multi-language static analyzer for both C and machine code that uses the SSA abstract domain as its main intermediate representation.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {65},
numpages = {30},
keywords = {Cyclic term graph, Abstract interpretation, Static Single Assignment (SSA)}
}

@inproceedings{10.1145/3394171.3414392,
author = {Song, Yuanfeng and Jiang, Di and Huang, Xiaoling and Li, Yawen and Xu, Qian and Wong, Raymond Chi-Wing and Yang, Qiang},
title = {GoldenRetriever: A Speech Recognition System Powered by Modern Information Retrieval},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414392},
doi = {10.1145/3394171.3414392},
abstract = {Existing Automatic Speech Recognition (ASR) systems usually generate the N-best hypotheses list first, and then rescore them with the language model score and the acoustic model score to find the best one. This procedure is essentially analogous to the working mechanism of modern Information Retrieval (IR) systems, which retrieve a relatively large amount of relevant candidates first, re-rank them, and output the top-N list. Exploiting their commonality, this demonstration proposes a novel system named GoldenRetriever that marries IR with ASR. GoldenRetriever transforms the problem of N-best hypotheses rescoring as a Learning-to-Rescore (L2RS) problem and utilizes a wide range of features beyond the language model score and the acoustic model score. In this demonstration, the audience can experience the great potential of marrying IR with ASR for the first time. GoldenRetriever should inspire more research on transferring the state-of-the-art IR techniques to ASR.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4500–4502},
numpages = {3},
keywords = {N-best rescoring, learning-to-rescore, speech recognition},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3479645.3479675,
author = {Prasetio, Barlian Henryranu and Widasari, Edita Rosana and Tamura, Hiroki},
title = {Automatic Multiscale-Based Peak Detection on Short Time Energy and Spectral Centroid Feature Extraction for Conversational Speech Segmentation},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479675},
doi = {10.1145/3479645.3479675},
abstract = {In this paper, we present a conversational speech segmentation system. We assume that the speech/non-speech has different energy in time and frequency domain. Therefore, the short time energy and spectral centroid are proposed as the feature extraction technique and the automatic multiscale algorithm as the signal peak detection. In the pre-processing phase, the Savitzky-Golay filter is performed to reduce the noise before feature extraction process. The short time feature serves to capture a short-stroke character and the spectral centroid feature is used to response the spectral characteristic. For observation, experimental and validation process, we use the six conversations of the SUSAS Database. The experimental result shows that the average accuracy of the proposed system is 98.26%.},
booktitle = {Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology},
pages = {44–49},
numpages = {6},
keywords = {Short time energy, AMPD algorithm, SUSAS database, Speech segmentation, Spectral centroid},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/3600160.3600165,
author = {Teng, Wil Liam and Rasmussen, Kasper},
title = {Actions Speak Louder Than Passwords: Dynamic Identity for Machine-to-Machine Communication},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3600165},
doi = {10.1145/3600160.3600165},
abstract = {Machine-to-Machine (M2M) communication is communication between computers without a human user involved. This is a very common paradigm whenever automated tasks are executed routinely, e.g., backup data to a cloud storage, update a local database cache, fetch the latest updates for software, etc. One challenge in this setting is that the credentials to establish secure connections between machines during execution must be available to the machines without any human interaction. Typically that means the credentials must reside on the machine itself, in the form of a secret such as a password, API key, single sign-on token, etc. In practice the secret is often embedded directly into an automatically executed script, but regardless it needs to be stored either in the clear or encrypted with another secret that is available to the machine during execution. This exposes the credentials to anyone who can gain access to the machine. In this paper we present ActionID, a scheme that mitigates the problem of credential exposure by making a desired sequence of actions for execution as part of the machine’s identity. This way, even if the credentials are exposed, they are only temporarily valid for one particular action sequence that cannot be changed for future executions. We introduce a trusted third party who issues new identities, validates new action requests, and acts as a centralised location for managing access control policies for an arbitrary number of clients and servers. In addition to yielding strong security guarantees, it also simplifies the management of complex access control for an organisation. We present detailed protocols for ActionID, along with a thorough security analysis. We implement ActionID as a Python library to show the ease of integration into existing applications, and to demonstrate the performance of the scheme, which is on par with SSH.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {7},
numpages = {11},
keywords = {Machine-to-Machine (M2M) Communication, Machine Identity},
location = {Benevento, Italy},
series = {ARES '23}
}

@article{10.1109/TASLP.2019.2948794,
author = {Laufer, Yaron and Gannot, Sharon},
title = {Scoring-Based ML Estimation and CRBs for Reverberation, Speech, and Noise PSDs in a Spatially Homogeneous Noise Field},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2948794},
doi = {10.1109/TASLP.2019.2948794},
abstract = {Hands-free speech systems are subject to performance degradation due to reverberation and noise. Common methods for enhancing reverberant and noisy speech require the knowledge of the speech, reverberation and noise power spectral densities (PSDs). Most literature on this topic assumes that the noise power spectral density (PSD) matrix is known. However, in many practical acoustic scenarios, the noise PSD is unknown and should be estimated along with the speech and the reverberation PSDs. In this article, the noise is modeled as a spatially homogeneous sound field, with an unknown time-varying PSD multiplied by a known time-invariant spatial coherence matrix. We derive two maximum likelihood estimators (MLEs) for the various PSDs, including the noise: The first is a non-blocking-based estimator, that jointly estimates the PSDs of the speech, reverberation and noise components. The second MLE is a blocking-based estimator, that blocks the speech signal and estimates the reverberation and noise PSDs. Since a closed-form solution does not exist, both estimators iteratively maximize the likelihood using the Fisher scoring method. In order to compare both methods, the corresponding Cram\'{e}r-Rao Bounds (CRBs) are derived. For both the reverberation and the noise PSDs, it is shown that the non-blocking-based CRB is lower than the blocking-based CRB. Performance evaluation using both simulated and real reverberant and noisy signals, shows that the proposed estimators outperform competing estimators, and greatly reduce the effect of reverberation and noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {61–76},
numpages = {16}
}

@inproceedings{10.1145/3288155.3288182,
author = {Samonte, Mary Jane C. and Bahia, Renz Jirhel D. and Forlaje, Samantha Bernadine A. and Del Monte, John Gabriel J. and Gonzales, Jecelle Anne J. and Sultan, Mariebeth V.},
title = {Assistive Mobile App for Children with Hearing &amp; Speech Impairment Using Character and Speech Recognition},
year = {2018},
isbn = {9781450365574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3288155.3288182},
doi = {10.1145/3288155.3288182},
abstract = {Ninety-seven (97%) of children with disabilities (CWDs) are still unreached by the public school system in the Philippines. This means that only a small portion has access to special education despite having a national law that mandates inclusivity of CWDs and the right to basic education. Some of the disabilities common in this population group is mental, motor-related, speech, and hearing impairments. The modern time sees disability as a dimension of human culture, both "abled" and "disabled" are part of a social function in the community. This human rights perspective gives emphasis that PWD's treatment does not lie medically but in recognition that they are entitled to the same rights as everyone else and are capable of making their own decisions. For CWDs to be motivated to take part in the society, their early childhood development must be given importance. It is believed that these children must be given supplemental educational materials that can help them level with non-disabled children. In this paper, the researchers provided a supplementary tool for preschool students, with speech and hearing impairment under Special Education (SPED) class, in learning basic counting, English alphabet and recognizing basic shapes by using the technological advancement of speech and character recognition in the general education. The developed system were used and tested by SPED teachers, parents and target learners and has been an acceptable technology in developing students' writing and speaking skills.},
booktitle = {Proceedings of the 4th International Conference on Industrial and Business Engineering},
pages = {265–270},
numpages = {6},
keywords = {Speech Recognition, Children with Disabilities, Assistive Technology, Mobile Application, Character Recognition},
location = {Macau, Macao},
series = {ICIBE' 18}
}

@inproceedings{10.1145/3526114.3558734,
author = {Sun, Wenxin and Huang, Mengjie and Wu, Chenxin and Yang, Rui},
title = {Exploring Virtual Object Translation in Head-Mounted Augmented Reality for Upper Limb Motor Rehabilitation with Motor Performance and Eye Movement Characteristics},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558734},
doi = {10.1145/3526114.3558734},
abstract = {Head-mounted augmented reality (AR) technology is currently employed in upper limb motor rehabilitation, and the degrees of freedom (DoF) of virtual object translation modes become critical for rehabilitation tasks in AR settings. Since motor performance is the primary focus of motor rehabilitation, this study assessed it across different translation modes (1DoF and 3DoF) via task efficiency and accuracy analysis. In addition, eye movement characteristics were used to further illustrate motor performance. This research revealed 1DoF and 3DoF modes showing their own benefits in upper limb motor rehabilitation tasks. Finally, this study recommended selecting or integrating these two translation modes for future rehabilitation task design.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {45},
numpages = {3},
keywords = {Translation modes, Motor performance, Rehabilitation, Head-mounted augmented reality, Eye movement},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3292500.3330761,
author = {Zhao, Shiwen and Westing, Brandt and Scully, Shawn and Nieto, Heri and Holenstein, Roman and Jeong, Minwoo and Sridhar, Krishna and Newendorp, Brandon and Bastian, Mike and Raman, Sethu and Paek, Tim and Lynch, Kevin and Guestrin, Carlos},
title = {Raise to Speak: An Accurate, Low-Power Detector for Activating Voice Assistants on Smartwatches},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330761},
doi = {10.1145/3292500.3330761},
abstract = {The two most common ways to activate intelligent voice assistants (IVAs) are button presses and trigger phrases. This paper describes a new way to invoke IVAs on smartwatches: simply raise your hand and speak naturally. To achieve this experience, we designed an accurate, low-power detector that works on a wide range of environments and activity scenarios with minimal impact to battery life, memory footprint, and processor utilization. The raise to speak (RTS) detector consists of four main compo- nents: an on-device gesture convolutional neural network (CNN) that uses accelerometer data to detect specific poses; an on-device speech CNN to detect proximal human speech; a policy model to combine signals from the motion and speech detector; and an off-device false trigger mitigation (FTM) system to reduce unin- tentional invocations trigged by the on-device detector. Majority of the components of the detector run on-device to preserve user privacy. The RTS detector was released in watchOS 5.0 and is running on millions of devices worldwide.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2736–2744},
numpages = {9},
keywords = {speech detection, multimodal, gesture recognition, neural network},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1109/TASLP.2021.3099291,
author = {Zeghidour, Neil and Grangier, David},
title = {Wavesplit: End-to-End Speech Separation by Speaker Clustering},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3099291},
doi = {10.1109/TASLP.2021.3099291},
abstract = {We introduce Wavesplit, an end-to-end source separation system. From a single mixture, the model infers a representation for each source and then estimates each source signal given the inferred representations. The model is trained to jointly perform both tasks from the raw waveform. Wavesplit infers a set of source representations via clustering, which addresses the fundamental permutation problem of separation. For speech separation, our sequence-wide speaker representations provide a more robust separation of long, challenging recordings compared to prior work. Wavesplit redefines the state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and reverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent LibriMix dataset. Finally, we show that Wavesplit is also applicable to other domains, by separating fetal and maternal heart rates from a single abdominal electrocardiogram.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2840–2849},
numpages = {10}
}

@inproceedings{10.1145/3340037.3340044,
author = {Wenceslao, Stephen John Matthew C. and Estuar, Maria Regina Justina E.},
title = {Using CTAKES to Build a Simple Speech Transcriber Plugin for an EMR},
year = {2019},
isbn = {9781450371995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340037.3340044},
doi = {10.1145/3340037.3340044},
abstract = {Electronic medical records (EMR) in general provide significant benefits to healthcare organizations and clinicians. However, a major challenge of clinicians who use EMRs is the lowered perceived quality of patient-doctor communication and interaction as a result of doctors being distracted with EMR use during consultations. A unique approach to this problem is through applications that automatically document clinical encounters in real-time. This study aims to develop a speech transcriber plugin for a web-based EMR for real-time clinical encounter documentation. We make use of available speech-to-text services on the web as well as cTAKES for clinical annotation. A draft summary of the clinical encounter is presented to the user in editable SOAP format. Blockchain technology for the speech recording is also explored to secure access to the recording. Internal testings showed that the prototype is able to capture audio conversations into text and parse the transcription for medical concepts. However, after a single formal usability evaluation we found that there is much to be done in terms of the usability of the summarization component.},
booktitle = {Proceedings of the 3rd International Conference on Medical and Health Informatics},
pages = {78–86},
numpages = {9},
keywords = {usability, automated medical scribes, speech recognition for clinical conversations, web technologies, Natural language processing},
location = {Xiamen, China},
series = {ICMHI '19}
}

@article{10.1109/TASLP.2020.3003165,
author = {Kubo, Yuki and Takamune, Norihiro and Kitamura, Daichi and Saruwatari, Hiroshi},
title = {Blind Speech Extraction Based on Rank-Constrained Spatial Covariance Matrix Estimation With Multivariate Generalized Gaussian Distribution},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3003165},
doi = {10.1109/TASLP.2020.3003165},
abstract = {In this article, we propose a new blind speech extraction (BSE) method that robustly extracts a directional speech from background diffuse noise by combining independent low-rank matrix analysis (ILRMA) and efficient rank-constrained spatial covariance matrix (SCM) estimation. To achieve more accurate BSE than ILRMA, which assumes each source to be a point source (rank-1 spatial model), the proposed method restores the lost spatial basis for the full-rank SCM of diffuse noise. We adopt the multivariate complex generalized Gaussian distribution (GGD) as the statistical generative model to express various types of observed signal. To estimate the model parameters for an arbitrary shape parameter of the multivariate GGD, we derive a new inequality for rank-constrained SCMs. Also, we propose new acceleration methods to accomplish much faster extraction than conventional blind source separation methods. In BSE experiments using simulated and real recorded data, we confirm that the proposed method achieves more accurate and faster speech extraction than conventional methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1948–1963},
numpages = {16}
}

@article{10.1145/3306346.3323030,
author = {Wei, Shih-En and Saragih, Jason and Simon, Tomas and Harley, Adam W. and Lombardi, Stephen and Perdoch, Michal and Hypes, Alexander and Wang, Dawei and Badino, Hernan and Sheikh, Yaser},
title = {VR Facial Animation via Multiview Image Translation},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323030},
doi = {10.1145/3306346.3323030},
abstract = {A key promise of Virtual Reality (VR) is the possibility of remote social interaction that is more immersive than any prior telecommunication media. However, existing social VR experiences are mediated by inauthentic digital representations of the user (i.e., stylized avatars). These stylized representations have limited the adoption of social VR applications in precisely those cases where immersion is most necessary (e.g., professional interactions and intimate conversations). In this work, we present a bidirectional system that can animate avatar heads of both users' full likeness using consumer-friendly headset mounted cameras (HMC). There are two main challenges in doing this: unaccommodating camera views and the image-to-avatar domain gap. We address both challenges by leveraging constraints imposed by multiview geometry to establish precise image-to-avatar correspondence, which are then used to learn an end-to-end model for real-time tracking. We present designs for a training HMC, aimed at data-collection and model building, and a tracking HMC for use during interactions in VR. Correspondence between the avatar and the HMC-acquired images are automatically found through self-supervised multiview image translation, which does not require manual annotation or one-to-one correspondence between domains. We evaluate the system on a variety of users and demonstrate significant improvements over prior work.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {67},
numpages = {16},
keywords = {differentiable rendering, face tracking, unsupervised image style transfer}
}

@inproceedings{10.1145/3510454.3528639,
author = {Lano, K.},
title = {Program Translation Using Model-Driven Engineering},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3528639},
doi = {10.1145/3510454.3528639},
abstract = {The porting or translation of software applications from one programming language to another is a common requirement of organisations that utilise software, and the increasing number and diversity of programming languages makes this capability as relevant today as in previous decades.Several approaches have been used to address this challenge, including machine learning and the manual definition of explicit translation rules. We define a novel approach using model-driven engineering (MDE) techniques: reverse-engineering source programs into specifications in the UML and OCL formalisms, and then forward-engineering the specifications to the required target language. This approach has the additional advantage of extracting specifications of software from code. We provide an evaluation based on a comprehensive dataset of examples, including industrial cases, and compare our results to those of other approaches and tools.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {362–363},
numpages = {2},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3290607.3308461,
author = {Glasser, Abraham},
title = {Automatic Speech Recognition Services: Deaf and Hard-of-Hearing Usability},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3308461},
doi = {10.1145/3290607.3308461},
abstract = {Nowadays, speech is becoming a more common, if not standard, interface to technology. This can be seen in the trend of technology changes over the years. Increasingly, voice is used to control programs, appliances and personal devices within homes, cars, workplaces, and public spaces through smartphones and home assistant devices using Amazon's Alexa, Google's Assistant and Apple's Siri, and other proliferating technologies. However, most speech interfaces are not accessible for Deaf and Hard-of-Hearing (DHH) people. In this paper, performances of current Automatic Speech Recognition (ASR) with voices of DHH speakers are evaluated. ASR has improved over the years, and is able to reach Word Error Rates (WER) as low as 5-6% [1][2][3], with the help of cloud-computing and machine learning algorithms that take in custom vocabulary models. In this paper, a custom vocabulary model is used, and the significance of the improvement is evaluated when using DHH speech.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {speech usability, automatic speech recognition, deaf and hard-of-hearing},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{10.1145/3503161.3548338,
author = {Huang, Hongxiang and Yang, Daihui and Dai, Gang and Han, Zhen and Wang, Yuyi and Lam, Kin-Man and Yang, Fan and Huang, Shuangping and Liu, Yongge and He, Mengchao},
title = {AGTGAN: Unpaired Image Translation for Photographic Ancient Character Generation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548338},
doi = {10.1145/3503161.3548338},
abstract = {The study of ancient writings has great value for archaeology and philology. Essential forms of material are photographic characters, but manual photographic character recognition is extremely time-consuming and expertise-dependent. Automatic classification is therefore greatly desired. However, the current performance is limited due to the lack of annotated data. Data generation is an inexpensive but useful solution to data scarcity. Nevertheless, the diverse glyph shapes and complex background textures of photographic ancient characters make the generation task difficult, leading to unsatisfactory results of existing methods. To this end, we propose an unsupervised generative adversarial network called AGTGAN in this paper. By explicitly modeling global and local glyph shape styles, followed by a stroke-aware texture transfer and an associate adversarial learning mechanism, our method can generate characters with diverse glyphs and realistic textures. We evaluate our method on photographic ancient character datasets, e.g., OBC306 and CSDD. Our method outperforms other state-of-the-art methods in terms of various metrics and performs much better in terms of the diversity and authenticity of generated samples. With our generated images, experiments on the largest photographic oracle bone character dataset show that our method can achieve a significant increase in classification accuracy, up to 16.34%. The source code is available at https://github.com/Hellomystery/AGTGAN.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5456–5467},
numpages = {12},
keywords = {ancient character generation, gan, image-to-image translation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3482632.3482752,
author = {Chai, Jinlian},
title = {Design of English Translation Computer Intelligent Proofreading System Based on Fuzzy Decision Algorithm},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482752},
doi = {10.1145/3482632.3482752},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {568–571},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3355088.3365148,
author = {Wang, Yuan and Zhang, Weibo and Chen, Peng},
title = {ChinaStyle: A Mask-Aware Generative Adversarial Network for Chinese Traditional Image Translation},
year = {2019},
isbn = {9781450369459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355088.3365148},
doi = {10.1145/3355088.3365148},
abstract = {GANs make it effective to generate artworks using appropriate collections. However, most training dataset either contain paintings that were only from one artist or contain only one category. There is few training datasets for Chinese traditional figure paintings. This paper presents a new high-quality dataset named ChinaStyle Dataset including six categories, containing 1913 images totally. We further proposes Mask-Aware Generative Adversarial Networks (MA-GAN) to transfer realistic portraiture to different styles of Chinese paintings. Different from existing mothed, MA-GAN uses a single model only once with our unpaired dataset. Besides, Mask-aware strategy is used to generate free-hand style of Chinese paintings. In addition, a color preserved loss is proposed to alleviate the color free problem. Experimental results and user study demonstrate that MA-GAN achieves a natural and competitive performance compared with existing methods.},
booktitle = {SIGGRAPH Asia 2019 Technical Briefs},
pages = {5–8},
numpages = {4},
keywords = {Generative Adversarial Networks, Image-to-Image Translation, Color preserved.},
location = {Brisbane, QLD, Australia},
series = {SA '19}
}

@article{10.1145/3588900,
author = {Dalai, Tusarkanta and Mishra, Tapas Kumar and Sa, Pankaj K.},
title = {Part-of-Speech Tagging of Odia Language Using Statistical and Deep Learning Based Approaches},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588900},
doi = {10.1145/3588900},
abstract = {Automatic part-of-speech (POS) tagging is a preprocessing step of many natural language processing tasks, such as named entity recognition, speech processing, information extraction, word sense disambiguation, and machine translation. It has already gained promising results in English and European languages. However, in Indian languages, particularly in the Odia language, it is not yet well explored because of the lack of supporting tools, resources, and morphological richness of the language. Unfortunately, we were unable to locate an open source POS tagger for the Odia language, and only a handful of attempts have been made to develop POS taggers for the Odia language. The main contribution of this research work is to present statistical approaches such as the maximum entropy Markov model and conditional random field (CRF), as well as deep learning based approaches, including the convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM) to develop the Odia POS tagger. A publicly accessible corpus annotated with the Bureau of Indian Standards (BIS) tagset is used in our work. However, most of the languages around the globe have used the dataset annotated with the Universal Dependencies (UD) tagset. Hence, to maintain uniformity, the Odia dataset should use the same tagset. Thus, following the BIS and UD guidelines, we constructed a mapping from the BIS tagset to the UD tagset. The maximum entropy Markov model, CRF, Bi-LSTM, and CNN models are trained using the Indian Languages Corpora Initiative corpus with the BIS and UD tagsets. We have experimented with various feature sets as input to the statistical models to prepare a baseline system and observed the impact of constructed feature sets. The deep learning based model includes the Bi-LSTM network, the CNN network, the CRF layer, character sequence information, and a pre-trained word vector. Seven different combinations of neural sequence labeling models are implemented, and their performance measures are investigated. It has been observed that the Bi-LSTM model with the character sequence feature and pre-trained word vector achieved a result with 94.58% accuracy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jun},
articleno = {167},
numpages = {24},
keywords = {Part of speech (POS), conditional random field (CRF), deep learning, word embedding}
}

@inproceedings{10.1145/3419604.3419752,
author = {Boumehdi, Ahmed and Yousfi, Abdellah},
title = {Construction of a Database for Speech Recognition of Isolated Arabic Words},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419752},
doi = {10.1145/3419604.3419752},
abstract = {Automatic speech recognition for the Arabic language is a field that is in a current remarkable development and still attracts many researchers who try to improve year after year the recognition rate. Many works, nowadays, have been focused on automatic speech recognition (ASR) for the Arabic language. The paper presents the significance of the ASR systems built in the past few years. This work also aims to introduce a new Arabic database for isolated word by defining a new concept of phonetic units: semi-syllable units. Thus, the corpus contains a collection of semi-syllable audio files as well as their corresponding transcription files. This database will help us in future works.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {25},
numpages = {4},
keywords = {Kaldi, Mel-frequency cepstrum coefficients (MFCC), Modern Standard Arabic (MSA), Sphinx, Hidden Markov Model (HMM), Automatic Speech Recognition (ASR), Hidden Markov Model Toolkit (HTK), Word error rate (WER)},
location = {Rabat, Morocco},
series = {SITA'20}
}

@article{10.1109/TASLP.2021.3061885,
author = {Pal, Monisankha and Kumar, Manoj and Peri, Raghuveer and Park, Tae Jin and Kim, So Hyun and Lord, Catherine and Bishop, Somer and Narayanan, Shrikanth},
title = {Meta-Learning With Latent Space Clustering in Generative Adversarial Network for Speaker Diarization},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3061885},
doi = {10.1109/TASLP.2021.3061885},
abstract = {The performance of most speaker diarization systems with x-vector embeddings is both vulnerable to noisy environments and lacks domain robustness. Earlier work on speaker diarization using generative adversarial network (GAN) with an encoder network (ClusterGAN) to project input x-vectors into a latent space has shown promising performance on meeting data. In this paper, we extend the ClusterGAN network to improve diarization robustness and enable rapid generalization across various challenging domains. To this end, we fetch the pre-trained encoder from the ClusterGAN and fine tune it by using prototypical loss (meta-ClusterGAN or MCGAN) under the meta-learning paradigm. Experiments are conducted on CALLHOME telephonic conversations, AMI meeting data, DIHARD-II (dev set) which includes challenging multi-domain corpus, and two child-clinician interaction corpora (ADOS, BOSCC) related to the autism spectrum disorder domain. Extensive analyses of the experimental data are done to investigate the effectiveness of the proposed ClusterGAN and MCGAN embeddings over x-vectors. The results show that the proposed embeddings with normalized maximum eigengap spectral clustering (NME-SC) back-end consistently outperform the Kaldi state-of-the-art x-vector diarization system. Finally, we employ embedding fusion with x-vectors to provide further improvement in diarization performance. We achieve a relative diarization error rate (DER) improvement of 6.67% to 53.93% on the aforementioned datasets using the proposed fused embeddings over x-vectors. Besides, the MCGAN embeddings provide better performance in the number of speakers estimation and short speech segment diarization compared to x-vectors and ClusterGAN on telephonic conversations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1204–1219},
numpages = {16}
}

@article{10.1109/TASLP.2019.2938863,
author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, Aaron},
title = {Unsupervised Speech Representation Learning Using WaveNet Autoencoders},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2938863},
doi = {10.1109/TASLP.2019.2938863},
abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder VAE, and a discrete Vector Quantized VAE VQ-VAE. We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2041–2053},
numpages = {13}
}

@inproceedings{10.1145/3490035.3490286,
author = {Mazumder, Seshadri and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C. V.},
title = {Translating Sign Language Videos to Talking Faces},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490286},
doi = {10.1145/3490035.3490286},
abstract = {Communication with the deaf community relies profoundly on the interpretation of sign languages performed by the signers. In light of the recent breakthroughs in sign language translations, we propose a pipeline that we term "Translating Sign Language Videos to Talking Faces". In this context, we improve the existing sign language translation systems by using POS tags to improve language modeling. We further extend the challenge to develop a system that can interpret a video from a signer to an avatar speaking in spoken languages. We focus on the translation systems that attempt to translate sign languages to text without glosses, an expensive annotation form. We critically analyze two state-of-the-art architectures, and based on their limitations, we improvise the systems. We propose a two-stage approach to translate sign language into intermediate text followed by a language model to get the final predictions. Quantitative evaluations on the challenging benchmarks on RWTH-PHOENIX-Weather 2014 T show that the translation accuracy of the texts generated by our translation model improves the state-of-the-art models by approximately 3 points. We then build a working text to talking face generation pipeline by bringing together multiple existing modules. The overall pipeline is capable of generating talking face videos with speech from sign language poses. Additional materials about this project including the codes and a demo video can be found in https://seshadri-c.github.io/SLV2TF/},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {27},
numpages = {10},
keywords = {sign language, POS tagging, sign language translation, sign language to text, sign language recognition},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.1145/3324884.3416546,
author = {Gros, David and Sezhiyan, Hariharan and Devanbu, Prem and Yu, Zhou},
title = {Code to Comment "Translation": Data, Metrics, Baselining &amp; Evaluation},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416546},
doi = {10.1145/3324884.3416546},
abstract = {The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task---specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using "affinity pairs" of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {746–757},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3395035.3425253,
author = {Boateng, George and Sels, Laura and Kuppens, Peter and Hilpert, Peter and Kowatsch, Tobias},
title = {Speech Emotion Recognition among Couples Using the Peak-End Rule and Transfer Learning},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425253},
doi = {10.1145/3395035.3425253},
abstract = {Extensive couples? literature shows that how couples feel after a conflict is predicted by certain emotional aspects of that conversation. Understanding the emotions of couples leads to a better understanding of partners? mental well-being and consequently their relationships. Hence, automatic emotion recognition among couples could potentially guide interventions to help couples improve their emotional well-being and their relationships. It has been shown that people's global emotional judgment after an experience is strongly influenced by the emotional extremes and ending of that experience, known as the peak-end rule. In this work, we leveraged this theory and used machine learning to investigate, which audio segments can be used to best predict the end-of-conversation emotions of couples. We used speech data collected from 101 Dutch-speaking couples in Belgium who engaged in 10-minute long conversations in the lab. We extracted acoustic features from (1) the audio segments with the most extreme positive and negative ratings, and (2) the ending of the audio. We used transfer learning in which we extracted these acoustic features with a pre-trained convolutional neural network (YAMNet). We then used these features to train machine learning models - support vector machines - to predict the end-of-conversation valence ratings (positive vs negative) of each partner. The results of this work could inform how to best recognize the emotions of couples after conversation-sessions and eventually, lead to a better understanding of couples? relationships either in therapy or in everyday life.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {17–21},
numpages = {5},
keywords = {convolutional neural network, transfer learning, couples, affective computing, speech processing, peak-end rule, support vector machine, speech emotion recognition},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3267935.3267948,
author = {Zhao, Ziping and Zhao, Yiqin and Bao, Zhongtian and Wang, Haishuai and Zhang, Zixing and Li, Chao},
title = {Deep Spectrum Feature Representations for Speech Emotion Recognition},
year = {2018},
isbn = {9781450359856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267935.3267948},
doi = {10.1145/3267935.3267948},
abstract = {Automatically detecting emotional state in human speech, which plays an effective role in areas of human machine interactions, has been a difficult task for machine learning algorithms. Previous work for emotion recognition have mostly focused on the extraction of carefully hand-crafted and tailored features. Recently, spectrogram representations of emotion speech have achieved competitive performance for automatic speech emotion recognition. In this work we propose a method to tackle the problem of deep features, herein denoted as deep spectrum features, extraction from the spectrogram by leveraging Attention-based Bidirectional Long Short-Term Memory Recurrent Neural Networks with fully convolutional networks. The learned deep spectrum features are then fed into a deep neural network (DNN) to predict the final emotion. The proposed model is then evaluated on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset to validate its effectiveness. Promising results indicate that our deep spectrum representations extracted from the proposed model perform the best, 65.2% for weighted accuracy and 68.0% for unweighted accuracy when compared to other existing methods. We then compare the performance of our deep spectrum features with two standard acoustic feature representations for speech-based emotion recognition. When combined with a support vector classifier, the performance of the deep feature representations extracted are comparable with the conventional features. Moreover, we also investigate the impact of different frequency resolutions of the input spectrogram on the performance of the system.},
booktitle = {Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data},
pages = {27–33},
numpages = {7},
keywords = {spectrogram representation, fully convolutional networks, attention mechanism, bidirectional long short-term memory, speech emotion recognition},
location = {Seoul, Republic of Korea},
series = {ASMMC-MMAC'18}
}

@article{10.1145/3576913,
author = {Ghosal, Sayani and Jain, Amita},
title = {HateCircle and Unsupervised Hate Speech Detection Incorporating Emotion and Contextual Semantics},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3576913},
doi = {10.1145/3576913},
abstract = {The explosive growth of social media has fueled an extensive increase in online freedom of speech. The worldwide platform of human voice creates possibilities to assail other users without facing any consequences, and flout social etiquettes, resulting in an inevitable increase of hate speech. Nowadays, English hate speech detection is a popular research area, but the prevalence of implicit hate content in regional languages desire effective language-independent models. The proposed research is the first unsupervised Hindi and Bengali hate content detection framework consisting of three significant concepts: HateCircle, hate tweet classification, and code-switch data preparation algorithms. The novel HateCircle method is proposed to detect hate orientation for each term by co-occurrence patterns of words, contextual semantics, and emotion analysis. The efficient multiclass hate tweet classification algorithm is proposed with parts of speech tagging, Euclidean distance, and the Geometric median methods. The detection of hate content is more efficient in the native script compared to the Roman script, so the transliteration algorithm is also proposed for code-switch data preparation. The experimentation evaluates the combination of various lexicons with our enriched hate lexicon that achieves a maximum of 0.74 F1-score for the Hindi and 0.88 F1-score for the Bengali datasets. The novel HateCircle and hate tweet detection framework evaluates with our proposed parts of speech tagging and Geometric median detection methods. Results reveal that HateCircle and hate tweet detection framework also achieves a maximum of 0.73 accuracy for the Hindi and 0.78 accuracy for the Bengali dataset. The experiment results signify that contextual semantic hate speech detection research with a language-independency feature offsets the growth of implicit abusive text in social media.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {108},
numpages = {28},
keywords = {hate speech detection, Indian languages, Low-resource languages, parts-of-speech tagging, social media, code-switch script, emotion analysis, contextual semantics}
}

@inproceedings{10.1145/3591569.3591610,
author = {Pham, Nhat Truong and Dang, Duc Ngoc Minh and Pham, Bich Ngoc Hong and Nguyen, Sy Dzung},
title = {SERVER: Multi-Modal Speech Emotion Recognition Using Transformer-Based and Vision-Based Embeddings},
year = {2023},
isbn = {9781450399616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591569.3591610},
doi = {10.1145/3591569.3591610},
abstract = {This paper proposes a multi-modal approach for speech emotion recognition (SER) using both text and audio inputs. The audio embedding is extracted by using a vision-based architecture, namely VGGish, while the text embedding is extracted by using a transformer-based architecture, namely BERT. Then, these embeddings are fused using concatenation to recognize emotional states. To evaluate the effectiveness of the proposed method, the benchmark dataset, namely IEMOCAP, is employed in this study. Experimental results indicate that the proposed method is very competitive and better than most of the latest and state-of-the-art methods using multi-modal analysis for SER. The proposed method achieves 63.00% unweighted accuracy (UA) and 63.10% weighted accuracy (WA) on the IEMOCAP dataset. In the future, an extension of multi-task learning and multi-lingual approaches will be investigated to improve the performance and robustness of multi-modal SER. For reproducibility purposes, our code is publicly available.},
booktitle = {Proceedings of the 2023 8th International Conference on Intelligent Information Technology},
pages = {234–238},
numpages = {5},
keywords = {BERT, speech emotion recognition, VGGish, multi-modal emotion recognition},
location = {Da Nang, Vietnam},
series = {ICIIT '23}
}

@article{10.1109/TASLP.2021.3101617,
author = {Byun, Jaeuk and Shin, Jong Won},
title = {Monaural Speech Separation Using Speaker Embedding From Preliminary Separation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3101617},
doi = {10.1109/TASLP.2021.3101617},
abstract = {In speech separation, the identities of the speakers may be an important cue to discriminate speeches in the mixture and separate them better. A few recent researches used the speaker embedding as an additional information, but they often require prior information about the target speaker or used noisy speaker embedding extracted from the mixture signal. In this article, we propose monaural speech separation that utilizes the speaker embedding in the later separator blocks, which is extracted from the intermediate separated results obtained by the early stages of the separator network. The later blocks in the separator networks consisting of repeated blocks such as the fully-convolutional time-domain audio separation network (Conv-TasNet) or the successive downsampling and resampling of multi-resolution features (SuDoRM-RF) are modified to take the speaker information as a form of affine transformation or addition to the original input tensor. The experimental results showed that the proposed methods significantly improved the performances of existing separation systems with a moderate number of additional parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {aug},
pages = {2753–2763},
numpages = {11}
}

@inproceedings{10.1145/3462244.3479914,
author = {Wang, Siyang and Alexanderson, Simon and Gustafson, Joakim and Beskow, Jonas and Henter, Gustav Eje and Sz\'{e}kely, \'{E}va},
title = {Integrated Speech and Gesture Synthesis},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479914},
doi = {10.1145/3462244.3479914},
abstract = {Text-to-speech and co-speech gesture synthesis have until now been treated as separate areas by two different research communities, and applications merely stack the two technologies using a simple system-level pipeline. This can lead to modeling inefficiencies and may introduce inconsistencies that limit the achievable naturalness. We propose to instead synthesize the two modalities in a single model, a new problem we call integrated speech and gesture synthesis (ISG). We also propose a set of models modified from state-of-the-art neural speech-synthesis engines to achieve this goal. We evaluate the models in three carefully-designed user studies, two of which evaluate the synthesized speech and gesture in isolation, plus a combined study that evaluates the models like they will be used in real-world applications – speech and gesture presented together. The results show that participants rate one of the proposed integrated synthesis models as being as good as the state-of-the-art pipeline system we compare against, in all three tests. The model is able to achieve this with faster synthesis time and greatly reduced parameter count compared to the pipeline system, illustrating some of the potential benefits of treating speech and gesture synthesis together as a single, unified problem.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {177–185},
numpages = {9},
keywords = {gesture generation, neural networks, speech synthesis},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3405755.3406125,
author = {Zepf, Sebastian and Gupta, Arijit and Kr\"{a}mer, Jan-Peter and Minker, Wolfgang},
title = {EmpathicSDS: Investigating Lexical and Acoustic Mimicry to Improve Perceived Empathy in Speech Dialogue Systems},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406125},
doi = {10.1145/3405755.3406125},
abstract = {In human-to-human conversations, showing empathy and thus understanding for the situation of the opposite party is crucial for a natural conversation. Thereby, emotional mimicry, i.e. imitating expressions of the person whom we are interacting with, is one of the basic mechanisms contributing to empathy. State-of-the-art speech dialogue systems still lack the ability of showing empathy, which limits naturalness. Thus, we present EmpathicSDS, a prototype to investigate the potential of lexical and acoustic mimicry for improving empathy in conversational interfaces. Our prototype comprises three different modes: 1.) neutral, where the system's response to a user query is static, 2.) lexical mimicry, where the wording of the user is reappraised by the system, and 3.) lexical and acoustic mimicry, which applies both lexical mimicry and a matching of the system's voice emotion to the user's emotional state. We conducted a user study with 33 participants to evaluate the effect of the mimicry approaches on user perception and to explore the role of user emotions. Our results show that lexical mimicry significantly improves perceived empathy and personalization without affecting efficiency. Acoustic mimicry can further improve naturalness in the condition of positive emotion while impairing efficiency in the negative condition.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {2},
numpages = {9},
keywords = {empathic speech dialogue systems, affective computing, mimicry, EmpathicSDS, empathy},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3486713.3486733,
author = {Liu, Serena and Chan, Jonathan},
title = {Testing the Effectiveness of CNN and GNN and Exploring the Influence of Different Channels on Decoding Covert Speech from EEG Signals: CNN and GNN on Decoding Covert Speech from EEG Signals},
year = {2021},
isbn = {9781450385107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486713.3486733},
doi = {10.1145/3486713.3486733},
abstract = {In this paper, the effectiveness of two deep learning models was tested and the significance of 62 different electroencephalogram (EEG) channels were explored on covert speech classification tasks using time series EEG signals. Experiments were done on the classification between the words “in” and “cooperate” from the ASU dataset and the classification between 11 different prompts from the KaraOne dataset. The types of deep learning models used are the 1D convolutional neural network (CNN) and the graphical neural network (GNN). Overall, the CNN model showed decent performance with an accuracy of around 80% on the classification between “in” and “cooperate”, while the GNN seemed to be unsuitable for time series data. By examining the accuracy of the CNN model trained on different EEG channels, the prefrontal and frontal regions appeared to be the most relevant to the performance of the model. Although this finding is noticeably different from various previous works, it could provide possible insights into the cortical activities behind covert speech.},
booktitle = {The 12th International Conference on Computational Systems-Biology and Bioinformatics},
pages = {17–26},
numpages = {10},
keywords = {brain-computer interface, convolutional neural networks (CNN), graphical neural networks (GNN), imagined speech, covert speech, electroencephalography (EEG)},
location = {Virtual (GMT+7 Bangkok Time), Thailand},
series = {CSBio2021}
}

@inproceedings{10.1145/3472634.3474080,
author = {Wang, Yumin and Ye, Jingyu and Wu, Hanzhou},
title = {Generating Watermarked Speech Adversarial Examples},
year = {2021},
isbn = {9781450385671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472634.3474080},
doi = {10.1145/3472634.3474080},
abstract = {Increasing methods use deep learning for auto-generating speech adversarial examples that can easily lead speech recognition systems to make incorrect predictions. It is necessary to prevent the speech adversarial example generation model from being abused by any unauthorized users. It implies that, both the model and the adversarial examples generated by the model should be protected and monitored. One may build a complex access protocol to avoid the model leakage, which, however, has limited control when the model was obtained by authorized users. To deal with this problem, in this paper, we propose a method to mark the speech adversarial example generation model by optimizing a combined loss function allowing a watermark to be embedded into the generated adversarial examples. Accordingly, the resultant model not only generates speech adversarial examples that can fool the target model, but also allows us to identify the ownership by detecting watermarks from outputs. Moreover, by retrieving the watermark from an unknown speech signal, we can judge whether the speech signal is an adversarial example generated by a specific model. We have proved through experiments that the speech adversarial example generation model optimized with the proposed method can effectively deceive the state-of-the-art speech classification network and trace the source of the generated adversarial examples.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China},
pages = {254–260},
numpages = {7},
keywords = {copyright protection., information hiding, watermarking, Speech adversarial examples, deep learning},
location = {Hefei, China},
series = {ACM TURC '21}
}

@inproceedings{10.1145/3325480.3326550,
author = {Arteaga, Nicole and Drew, Glenda},
title = {The Origin of Anti-Hispanic Rhetoric - The Spanish Black Legend},
year = {2019},
isbn = {9781450359177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325480.3326550},
doi = {10.1145/3325480.3326550},
abstract = {This demonstration is meant to educate the community about a topic called The Spanish Black Legend. The demonstration will feature interactive components meant to engage the general public and pique their interest in learning more about the source and history of Anti-Hispanic Rhetoric. The demonstration will include interactive features through a timeline-based strategy where the user will visualize the events of the Spanish Black Legend as they move through their story. The timeline-based strategy will be used to clearly demonstrate the history of Anti-Hispanicism as it began and has manifested into the way society presently speaks about and treats Chicanx and Latinx peoples. This demonstration will be displayed on a tablet with the hopes that conference attendees interact with the story from beginning to end and walk away with a deeper understanding of a topic that is very pertinent in today's political climate. The goal of this platform is to educate about this topic while providing an opportunity for users to share their thoughts and experiences with each other both during and after the demonstration.},
booktitle = {Proceedings of the 2019 Conference on Creativity and Cognition},
pages = {536–540},
numpages = {5},
keywords = {discrimination, learning, anti-hispanic rhetoric, education, the spanish black legend},
location = {San Diego, CA, USA},
series = {C&amp;C '19}
}

@article{10.1109/TASLP.2021.3129363,
author = {Wang, Zhong-Qiu and Wichern, Gordon and Roux, Jonathan Le},
title = {Convolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3129363},
doi = {10.1109/TASLP.2021.3129363},
abstract = {A promising approach for speech dereverberation is based on supervised learning, where a deep neural network (DNN) is trained to predict the direct sound from noisy-reverberant speech. This data-driven approach is based on leveraging prior knowledge of clean speech patterns, and seldom explicitly exploits the linear-filter structure in reverberation, i.e., that reverberation results from a linear convolution between a room impulse response (RIR) and a dry source signal. In this work, we propose to exploit this linear-filter structure within a deep learning based monaural speech dereverberation framework. The key idea is to first estimate the direct-path signal of the target speaker using a DNN and then identify signals that are decayed and delayed copies of the estimated direct-path signal, as these can be reliably considered as reverberation. They can be either directly removed for dereverberation, or used as extra features for another DNN to perform better dereverberation. To identify the copies, we estimate the underlying filter (or RIR) by efficiently solving a linear regression problem per frequency in the time-frequency domain. We then modify the proposed algorithm for speaker separation in reverberant and noisy-reverberant conditions. State-of-the-art speech dereverberation and speaker separation results are obtained on the REVERB, SMS-WSJ, and WHAMR! datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3476–3490},
numpages = {15}
}

@inproceedings{10.1145/3539597.3572721,
author = {Saha, Punyajoy and Das, Mithun and Mathew, Binny and Mukherjee, Animesh},
title = {Hate Speech: Detection, Mitigation and Beyond},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3572721},
doi = {10.1145/3539597.3572721},
abstract = {Social media sites such as Twitter and Facebook have connected billions of people and given the opportunity to the users to share their ideas and opinions instantly. That being said, there are several negative consequences as well such as online harassment, trolling, cyber-bullying, fake news, and hate speech. Out of these, hate speech presents a unique challenge as it is deeply engraved into our society and is often linked with offline violence. Social media platforms rely on human moderators to identify hate speech and take necessary action. However, with the increase in online hate speech, these platforms are turning toward automated hate speech detection and mitigation systems. This shift brings several challenges to the plate, and hence, is an important avenue to explore for the computation social science community.In this tutorial, we present an exposition of hate speech detection and mitigation in three steps. First, we describe the current state of research in the hate speech domain, focusing on different hate speech detection and mitigation systems that have developed over time. Next, we highlight the challenges that these systems might carry like bias and the lack of transparency. The final section concretizes the path ahead, providing clear guidelines for the community working in hate speech and related domains. We also outline the open challenges and research directions for interested researchers.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1232–1235},
numpages = {4},
keywords = {social media, counter speech, mitigation, hate speech, detection},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3569192.3569216,
author = {Bou Zeidan, Dona Elisa and Noun, Abir and Nassereddine, Mohamad and Charara, Jamal and Chkeir, Aly},
title = {Speech Recognition for Functional Decline Assessment in Older Adults},
year = {2023},
isbn = {9781450396868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569192.3569216},
doi = {10.1145/3569192.3569216},
abstract = {Functional decline is one of the serious syndromes experienced among older adults. Its early assessment is critical to preventing its symptoms. Some Comprehensive Geriatric Assessment CGA questionnaires, chosen amongst others, can be performed as in-home self-assessments by older adults using QuestIO, a device based on automatic speech recognition ASR. This paper investigates the performance of the ASR on English Isolated words while using different features; Mel Frequency Cepstral Coefficient (MFCC), Relative spectra-perceptual linear prediction (RASTA-PLP), Perceptual linear prediction (PLP), Linear Prediction Cepstral Coefficients (LPCCs) or a combination of these, and the random forest classifier, to select the features that give the best performance. The performance was obtained based on the word recognition rate WRR and the real-time factor RTF. As a result, we selected the MFCC and RASTA-PLP cepstral coefficients. The WRR reached for these features is 96.57% with an RTF of 11\texttimes{}10-4.},
booktitle = {Proceedings of the 9th International Conference on Bioinformatics Research and Applications},
pages = {149–153},
numpages = {5},
keywords = {Perceptual linear prediction (PLP), Functional Decline, Elderly, Speech Recognition, Mel Frequency Cepstral Coefficient (MFCC), Relative spectra-perceptual linear prediction (RASTA-PLP), Linear Prediction Cepstral Coefficients (LPCCs), Random forest},
location = {Berlin, Germany},
series = {ICBRA '22}
}

@inproceedings{10.1145/3578741.3578801,
author = {Wang, Chao and Wen, Yao and Lhamo, Phurba and Tashi, Nyima},
title = {A Streaming End-to-End Speech Recognition Approach Based on WeNet for Tibetan Amdo Dialect},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578801},
doi = {10.1145/3578741.3578801},
abstract = {Speech recognition is a technique to transcribe acoustic features into text sequences. However, traditional speech recognition model cannot get an effective performance, when dealed with Tibetan Amdo dialect dataset which requires a large amount of linguistic knowledge. In order to solve this issue, we propose an end-to-end speech streaming recognition model which can not only realize the transcription of the Tibetan Amdo dialect but also solve the Tibetan Amdo dialect streaming recognition problem. In the model, we choose Tibetan syllables as the modeling unit and MFCC as the acoustic features. Furthermore, extensive experimental results show good results on our self-built thousand-hour level dataset. Finally, the Character Error Rate(CER) of speech streaming recognition on ourself dataset is 10.73%, which is a relative improvement is 17.65% compared to the baseline model. The CER of speech transcription is 10.23%, which is a relative improvement is 16.01% compared to the baseline model.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {317–322},
numpages = {6},
keywords = {WeNet, amdo dialect, end-to-end, tibetan speech recognition},
location = {Sanya, China},
series = {MLNLP '22}
}

@inproceedings{10.1145/3587103.3594201,
author = {Geissler, Markus and Servin, Christian and Tang, Cara},
title = {Broadening Effective Assessment with Bloom's for Computing: A Call to Translate Computing-Related Verbs into World Languages},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594201},
doi = {10.1145/3587103.3594201},
abstract = {The ACM Education Board recently endorsed the ACM CCECC's Bloom's for Computing publication [2] which enhances the traditional verb list of Bloom's Revised Taxonomy [3] with computing-related verbs to facilitate the development of learning outcomes and competencies which enable more effective assessment of student achievement in the computing disciplines. The specific goals of this poster are a) to inform the global computing community about Bloom's for Computing and various opportunities it provides, and b) to invite members to contribute to the community by translating the Bloom's for Computing verbs into their native language and publicizing them throughout their language area.While terminology originating from the English language dominates many areas of study, including the computing disciplines, creating detailed learning outcomes consisting of native language elements can facilitate more effective communication between faculty, students, and industry partners about academic expectations and achievement. Several learning taxonomies have been translated into different languages to broaden their impact [1,5].By creating Bloom's for Computing, the ACM CCECC worked to remove the layer of abstraction between cross-disciplinary Bloom's verbs and more effective learning outcomes which implement verbs used in various computing disciplines. Having the computing-related verbs available in different languages would make them more easily accessible for non-native English speakers.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {656},
numpages = {1},
keywords = {competencies, Bloom's for computing, translation, learning outcomes, computing-related verbs, computing disciplines},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@article{10.1109/TASLP.2020.2993152,
author = {Bhattacharjee, Mrinmoy and Prasanna, S. R. Mahadeva and Guha, Prithwijit},
title = {Speech/Music Classification Using Features From Spectral Peaks},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2993152},
doi = {10.1109/TASLP.2020.2993152},
abstract = {Spectrograms of speech and music contain distinct striation patterns. Traditional features represent various properties of the audio signal but do not necessarily capture such patterns. This work proposes to model such spectrogram patterns using a novel Spectral Peak Tracking (SPT) approach. Two novel time-frequency features for speech vs. music classification are proposed. The proposed features are extracted in two stages. First, SPT is performed to track a preset number of highest amplitude spectral peaks in an audio interval. In the second stage, the location and amplitudes of these peak traces are used to compute the proposed feature sets. The first feature involves the computation of mean and standard deviation of peak traces. The second feature is obtained as averaged component posterior probability vectors of Gaussian mixture models learned on the peak traces. Speech vs. music classification is performed by training various binary classifiers on these proposed features. Three standard datasets are used to evaluate the efficiency of the proposed features for speech/music classification. The proposed features are benchmarked against five baseline approaches. Finally, the best-proposed feature is combined with two contemporary deep-learning based features to show that such combinations can lead to more robust speech vs. music classification systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1549–1559},
numpages = {11}
}

@inproceedings{10.1145/3503161.3548081,
author = {Hegde, Sindhu B. and Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548081},
doi = {10.1145/3503161.3548081},
abstract = {In this work, we address the problem of generating speech from silent lip videos for any speaker in the wild. In stark contrast to previous works, our method (i) is not restricted to a fixed number of speakers, (ii) does not explicitly impose constraints on the domain or the vocabulary and (iii) deals with videos that are recorded in the wild as opposed to within laboratory settings. The task presents a host of challenges, with the key one being that many features of the desired target speech, like voice, pitch and linguistic content, cannot be entirely inferred from the silent face video. In order to handle these stochastic variations, we propose a new VAE-GAN architecture that learns to associate the lip and speech sequences amidst the variations. With the help of multiple powerful discriminators that guide the training process, our generator learns to synthesize speech sequences in any voice for the lip movements of any person. Extensive experiments on multiple datasets show that we outperform all baselines by a large margin. Further, our network can be fine-tuned on videos of specific identities to achieve a performance comparable to single-speaker models that are trained on $4times$ more data. We conduct numerous ablation studies to analyze the effect of different modules of our architecture. We also provide a demo video that demonstrates several qualitative results along with the code and trained models on our website http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6250–6258},
numpages = {9},
keywords = {lip-to-speech, talking-face videos, speech synthesis, hybrid vae-gan},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2020.2995273,
author = {Zhao, Yan and Wang, DeLiang and Xu, Buye and Zhang, Tao},
title = {Monaural Speech Dereverberation Using Temporal Convolutional Networks With Self Attention},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2995273},
doi = {10.1109/TASLP.2020.2995273},
abstract = {In daily listening environments, human speech is often degraded by room reverberation, especially under highly reverberant conditions. Such degradation poses a challenge for many speech processing systems, where the performance becomes much worse than in anechoic environments. To combat the effect of reverberation, we propose a monaural (single-channel) speech dereverberation algorithm using temporal convolutional networks with self attention. Specifically, the proposed system includes a self-attention module to produce dynamic representations given input features, a temporal convolutional network to learn a nonlinear mapping from such representations to the magnitude spectrum of anechoic speech, and a one-dimensional (1-D) convolution module to smooth the enhanced magnitude among adjacent frames. Systematic evaluations demonstrate that the proposed algorithm improves objective metrics of speech quality in a wide range of reverberant conditions. In addition, it generalizes well to untrained reverberation times, room sizes, measured room impulse responses, real-world recorded noisy-reverberant speech, and different speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {1598–1607},
numpages = {10}
}

@inproceedings{10.1145/3453800.3453826,
author = {Thanh Diep, Hai and Thi My Nguyen, Thanh and Ngoc Le, Bich and Xuan Dao, Quy},
title = {Evaluation of Vietnamese Speech Recognition Platforms},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453826},
doi = {10.1145/3453800.3453826},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {141–146},
numpages = {6},
keywords = {natural langue processing, word error rate, speech recognition, API},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@article{10.1109/TASLP.2022.3225649,
author = {Zhang, Qiquan and Qian, Xinyuan and Ni, Zhaoheng and Nicolson, Aaron and Ambikairajah, Eliathamby and Li, Haizhou},
title = {A Time-Frequency Attention Module for Neural Speech Enhancement},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3225649},
doi = {10.1109/TASLP.2022.3225649},
abstract = {Speech enhancement plays an essential role in a wide range of speech processing applications. Recent studies on speech enhancement tend to investigate how to effectively capture the long-term contextual dependencies of speech signals to boost performance. However, these studies generally neglect the time-frequency (T-F) distribution information of speech spectral components, which is equally important for speech enhancement. In this paper, we propose a simple yet very effective network module, which we term the T-F attention (TFA) module, that uses two parallel attention branches, i.e., time-frame attention and frequency-channel attention, to explicitly exploit position information to generate a 2-D attention map to characterise the salient T-F speech distribution. We validate our TFA module as part of two widely used backbone networks (residual temporal convolution network and Transformer) and conduct speech enhancement with four most popular training objectives. Our extensive experiments demonstrate that our proposed TFA module consistently leads to substantial enhancement performance improvements in terms of the five most widely used objective metrics, with negligible parameter overheads. In addition, we further evaluate the efficacy of speech enhancement as a front-end for a downstream speech recognition task. Our evaluation results show that the TFA module significantly improves the robustness of the system to noisy conditions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {462–475},
numpages = {14}
}

@inproceedings{10.1145/3469213.3471318,
author = {Liu, Zelin},
title = {Research on Subtitle Translation from the Perspective of Computer-Aided Translation},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3471318},
doi = {10.1145/3469213.3471318},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {290},
numpages = {4},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3510858.3511426,
author = {Sun, Kang},
title = {Design of Intelligent Recognition English Translation Model Based on Association Rule Mining},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511426},
doi = {10.1145/3510858.3511426},
abstract = {Due to the rapid development of globalization, the information flow between different countries shows high speed, and English has become the main language of international communication. At present, the application value of intelligent recognition technology in different fields is increasing. The English machine translation model based on modern intelligent recognition technology can improve the efficiency and accuracy of English machine translation and realize barrier free communication. However, the traditional English machine translation method based on syntactic analysis can not solve the problem of partial structural ambiguity in the massive English language in intelligent recognition technology, which has the problem of low accuracy of machine translation. With the development of modern intelligent recognition technology, there are many intelligent machine translation tools. The current machine translation results of online machine translation still have some defects, especially after the server is used to carry out comparative learning on data in different languages in the full text range, it can obtain the grammar and text correlation laws between languages, which has the disadvantages of low efficiency and low accuracy of machine translation. Therefore, the recognizable technology of association rule mining should be used to realize accurate machine translation of English.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {930–934},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@article{10.1109/TASLP.2019.2942140,
author = {Prathosh, A. P. and Srivastava, Varun and Mishra, Mayank},
title = {Adversarial Approximate Inference for Speech to Electroglottograph Conversion},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942140},
doi = {10.1109/TASLP.2019.2942140},
abstract = {Speech produced by human vocal apparatus conveys substantial non-semantic information including the gender of the speaker, voice quality, affective state, abnormalities in the vocal apparatus etc. Such information is attributed to the properties of the voice source signal, which is usually estimated from the speech signal. However, most of the source estimation techniques depend heavily on the goodness of the model assumptions and are prone to noise. A popular alternative is to indirectly obtain the source information through the Electroglottographic (EGG) signal that measures the electrical admittance around the vocal folds using dedicated hardware. In this paper, we address the problem of estimating the EGG signal directly from the speech signal, devoid of any hardware. Sampling from the intractable conditional distribution of the EGG signal given the speech signal is accomplished through optimization of an evidence lower bound. This is constructed via minimization of the KL-divergence between the true and the approximated posteriors of a latent variable learned using a deep neural auto-encoder that serves an informative prior. We demonstrate the efficacy of the method at generating the EGG signal by conducting several experiments on datasets comprising multiple speakers, voice qualities, noise settings and speech pathologies. The proposed method is evaluated on many benchmark metrics and is found to agree with the gold standard while proving better than the state-of-the-art algorithms on a few tasks such as epoch extraction.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2183–2196},
numpages = {14}
}

@inproceedings{10.1145/3383652.3423882,
author = {Ferstl, Ylva and Neff, Michael and McDonnell, Rachel},
title = {Understanding the Predictability of Gesture Parameters from Speech and Their Perceptual Importance},
year = {2020},
isbn = {9781450375863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383652.3423882},
doi = {10.1145/3383652.3423882},
abstract = {Gesture behavior is a natural part of human conversation. Much work has focused on removing the need for tedious hand-animation to create embodied conversational agents by designing speech-driven gesture generators. However, these generators often work in a black-box manner, assuming a general relationship between input speech and output motion. As their success remains limited, we investigate in more detail how speech may relate to different aspects of gesture motion. We determine a number of parameters characterizing gesture, such as speed and gesture size, and explore their relationship to the speech signal in a two-fold manner. First, we train multiple recurrent networks to predict the gesture parameters from speech to understand how well gesture attributes can be modeled from speech alone. We find that gesture parameters can be partially predicted from speech, and some parameters, such as path length, being predicted more accurately than others, like velocity. Second, we design a perceptual study to assess the importance of each gesture parameter for producing motion that people perceive as appropriate for the speech. Results show that a degradation in any parameter was viewed negatively, but some changes, such as hand shape, are more impactful than others. A video summarization can be found at https://youtu.be/aw6-_5kmLjY.},
booktitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
articleno = {19},
numpages = {8},
keywords = {perception, machine learning, gesture modelling, speech gestures},
location = {Virtual Event, Scotland, UK},
series = {IVA '20}
}

@article{10.1109/TASLP.2022.3153265,
author = {Chuang, Shang-Yi and Wang, Hsin-Min and Tsao, Yu},
title = {Improved Lite Audio-Visual Speech Enhancement},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153265},
doi = {10.1109/TASLP.2022.3153265},
abstract = {Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm for a car-driving scenario. Compared to conventional AVSE systems, LAVSE requires less online computation and to some extent solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the additional cost of processing visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1345–1359},
numpages = {15}
}

@article{10.1145/3358185,
author = {Park, Sunghyun and Wu, Youfeng and Lee, Janghaeng and Aupov, Amir and Mahlke, Scott},
title = {Multi-Objective Exploration for Practical Optimization Decisions in Binary Translation},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358185},
doi = {10.1145/3358185},
abstract = {In the design of mobile systems, hardware/software (HW/SW) co-design has important advantages by creating specialized hardware for the performance or power optimizations. Dynamic binary translation (DBT) is a key component in co-design. During the translation, a dynamic optimizer in the DBT system applies various software optimizations to improve the quality of the translated code. With dynamic optimization, optimization time is an exposed run-time overhead and useful analyses are often restricted due to their high costs. Thus, a dynamic optimizer needs to make smart decisions with limited analysis information, which complicates the design of optimization decision models and often causes failures in human-made heuristics. In mobile systems, this problem is even more challenging because of strict constraints on computing capabilities and memory size.To overcome the challenge, we investigate an opportunity to build practical optimization decision models for DBT by using machine learning techniques. As the first step, loop unrolling is chosen as the representative optimization. We base our approach on the industrial strength DBT infrastructure and conduct evaluation with 17,116 unrollable loops collected from 200 benchmarks and real-life programs across various domains. By utilizing all available features that are potentially important for loop unrolling decision, we identify the best classification algorithm for our infrastructure with consideration for both prediction accuracy and cost. The greedy feature selection algorithm is then applied to the classification algorithm to distinguish its significant features and cut down the feature space. By maintaining significant features only, the best affordable classifier, which satisfies the budgets allocated to the decision process, shows 74.5% of prediction accuracy for the optimal unroll factor and realizes an average 20.9% reduction in dynamic instruction count during the steady-state translated code execution. For comparison, the best baseline heuristic achieves 46.0% prediction accuracy with an average 13.6% instruction count reduction. Given that the infrastructure is already highly optimized and the ideal upper bound for instruction reduction is observed at 23.8%, we believe this result is noteworthy.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {57},
numpages = {19},
keywords = {Loop unrolling<!--?clr?-->}
}

@inproceedings{10.1145/3592572.3592844,
author = {Salvi, Davide and Bestagini, Paolo and Tubaro, Stefano},
title = {Synthetic Speech Detection through Audio Folding},
year = {2023},
isbn = {9798400701870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592572.3592844},
doi = {10.1145/3592572.3592844},
abstract = {In the field of synthetic speech generation, recent advancements in deep learning and speech synthesis methods have enabled the possibility of creating highly realistic fake speech tracks that are difficult to distinguish from real ones. Since the malicious use of these data can lead to dangerous consequences, the audio forensics community has focused on developing synthetic speech detectors to determine the authenticity of speech tracks. In this work we focus on the wide class of detectors that analyze audio streams on a frame-by-frame basis. We propose a technique to reduce the inference time of these detectors by relying on the fact that it is possible to mix multiple audio frames in a single one (i.e., in the same way a mono track is obtained from a stereo one). We test the proposed audio folding technique on speech tracks obtained from the ASVspoof 2019 dataset. The technique proves effective with both entirely and partially fake speech tracks and shows remarkable results, reducing processing time down to 25%.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Multimedia AI against Disinformation},
pages = {3–9},
numpages = {7},
keywords = {Synthetic Speech, Audio Forensics, Audio Folding, Digital signal processing},
location = {Thessaloniki, Greece},
series = {MAD '23}
}

@inproceedings{10.1145/3532213.3532246,
author = {Wu, Delin and Deng, Baoqing and Zhuang, Xiutian},
title = {Design of Speech Recognition Robot},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532246},
doi = {10.1145/3532213.3532246},
abstract = {Nowadays, with the continuous development of science and technology, chip processing performance continues to improve, speech recognition technology has made great progress. There are a number of products on the market that support speech recognition. The speech recognition and control system with high reliability and high recognition rate is very important for the popularization and promotion of speech recognition technology. According to the working principle of speech recognition, using LD3320 speech recognition chip in the advantages of speech recognition technology, design a robot can be controlled by speech. The program of the robot is through C language and assembly language to complete the software programming, the final effect can be achieved, through speech recognition, the robot can automatically complete the left turn, right turn, backward, forward, stop and other functions.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {221–227},
numpages = {7},
keywords = {LD3320, Speech recognition, Robot},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3352411.3352432,
author = {Pratiwi, Nur Indah and Budi, Indra and Jiwanggi, Meganingrum Arista},
title = {Hate Speech Identification Using the Hate Codes for Indonesian Tweets},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352432},
doi = {10.1145/3352411.3352432},
abstract = {The hate speech has become the major source of negativity spread in all over the social media. As the social media becomes aware of this issue, they gradually build several new regulations to handle the spread of hate speech e.g. by automatically blocking or suspending the accounts or posts containing hate speech. However, the social media users have become more creative in expressing the hate speech. To avoid the social media regulations regarding the hate speech, users usually use some special codes to interact with each other. This study aims to utilize the hate codes to identify the hate speech on the social media data. We used the Indonesian tweets as the dataset. We utilized Logistic Regression, Support Vector Machine, Na\"{\i}ve Bayes, and Random Forest Decision Tree as the classifiers. The highest F-Measure score for the hate speech identification was 80.71% by using the hate code feature combined with Logistic Regression as the classifier.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {128–133},
numpages = {6},
keywords = {Twitter, Hate speech, Classification, Hate code},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@inproceedings{10.1145/3452446.3452566,
author = {Wang, Ying and Yu, Jing},
title = {A Study on Translation Teaching and Learning from Bourdieu's Sociological Perspective},
year = {2021},
isbn = {9781450389815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452446.3452566},
doi = {10.1145/3452446.3452566},
booktitle = {2021 2nd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {478–481},
numpages = {4},
keywords = {translation and introduction, Huang Di's Classic of Internal Medicine, communication, Bourdieu sociology},
location = {Dalian, China},
series = {IPEC2021}
}

@article{10.1109/TASLP.2020.3000593,
author = {Sadeghi, Mostafa and Leglaive, Simon and Alameda-Pineda, Xavier and Girin, Laurent and Horaud, Radu},
title = {Audio-Visual Speech Enhancement Using Conditional Variational Auto-Encoders},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3000593},
doi = {10.1109/TASLP.2020.3000593},
abstract = {Variational auto-encoders (VAEs) are deep generative latent variable models that can be used for learning the distribution of complex data. VAEs have been successfully used to learn a probabilistic prior over speech signals, which is then used to perform speech enhancement. One advantage of this generative approach is that it does not require pairs of clean and noisy speech signals at training. In this article, we propose audio-visual variants of VAEs for single-channel and speaker-independent speech enhancement. We develop a conditional VAE (CVAE) where the audio speech generative process is conditioned on visual information of the lip region. At test time, the audio-visual speech generative model is combined with a noise model based on nonnegative matrix factorization, and speech enhancement relies on a Monte Carlo expectation-maximization algorithm. Experiments are conducted with the recently published NTCD-TIMIT dataset as well as the GRID corpus. The results confirm that the proposed audio-visual CVAE effectively fuses audio and visual information, and it improves the speech enhancement performance compared with the audio-only VAE model, especially when the speech signal is highly corrupted by noise. We also show that the proposed unsupervised audio-visual speech enhancement approach outperforms a state-of-the-art supervised deep learning method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1788–1800},
numpages = {13}
}

@article{10.1145/3469089,
author = {Bertini, Flavio and Allevi, Davide and Lutero, Gianluca and Montesi, Danilo and Calz\`{a}, Laura},
title = {Automatic Speech Classifier for Mild Cognitive Impairment and Early Dementia},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3469089},
doi = {10.1145/3469089},
abstract = {The World Health Organization estimates that 50 million people are currently living with dementia worldwide and this figure will almost triple by 2050. Current pharmacological treatments are only symptomatic, and drugs or other therapies are ineffective in slowing down or curing the neurodegenerative process at the basis of dementia. Therefore, early detection of cognitive decline is of the utmost importance to respond significantly and deliver preventive interventions. Recently, the researchers showed that speech alterations might be one of the earliest signs of cognitive defect, observable well in advance before other cognitive deficits become manifest. In this article, we propose a full automated method able to classify the audio file of the subjects according to the progress level of the pathology. In particular, we trained a specific type of artificial neural network, called autoencoder, using the visual representation of the audio signal of the subjects, that is, the spectrogram. Moreover, we used a data augmentation approach to overcome the problem of the large amount of annotated data usually required during the training phase, which represents one of the most major obstacles in deep learning. We evaluated the proposed method using a dataset of 288 audio files from 96 subjects: 48 healthy controls and 48 cognitively impaired participants. The proposed method obtained good classification results compared to the state-of-the-art neuropsychological screening tests and, with an accuracy of 90.57%, outperformed the methods based on manual transcription and annotation of speech.},
journal = {ACM Trans. Comput. Healthcare},
month = {oct},
articleno = {8},
numpages = {11},
keywords = {classification, speech data augmentation, neural networks, Dementia, mild cognitive impairment}
}

@inproceedings{10.1145/3563137.3563170,
author = {Novais, Rui and Cardoso, Pedro J.S. and Rodrigues, Jo\~{a}o M.F.},
title = {Emotion Classification from Speech by an Ensemble Strategy},
year = {2023},
isbn = {9781450398077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563137.3563170},
doi = {10.1145/3563137.3563170},
abstract = {Humans are prepared to comprehend each other's emotions through subtle body movements and speech expressions, and from those, they change the way they deliver/understand messages when communicating between them. Socially assistive robots need to empower their ability in recognizing emotions in a way to change the interaction with humans, especially with elders. This paper presents a framework for speech emotion prediction supported by an ensemble of distinct out-of-the-box methods, being the main contribution of the integration of the outputs of those methods in a single prediction consistent with the expression presented by the system's user. Results show a classification accuracy of 75.56% over the RAVDESS dataset and 86.43% in a group of datasets constituted by RAVDESS, SAVEE, and TESS.},
booktitle = {Proceedings of the 10th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {85–90},
numpages = {6},
keywords = {Ensembles, Machine Learning, Speech Emotion Recognition, Emotions},
location = {Lisbon, Portugal},
series = {DSAI '22}
}

@inproceedings{10.1145/3423325.3423733,
author = {Altinkaya, Mehmet and Smeulders, Arnold W.M.},
title = {A Dynamic, Self Supervised, Large Scale AudioVisual Dataset for Stuttered Speech},
year = {2020},
isbn = {9781450381567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423325.3423733},
doi = {10.1145/3423325.3423733},
abstract = {Stuttering affects at least 1% of the world population. It is caused by irregular disruptions in speech production. These interruptions occur in various forms and frequencies. Repetition of words or parts of words, prolongations, or blocks in getting the words out are the most common ones.Accurate detection and classification of stuttering would be important in the assessment of severity for speech therapy. Furthermore, real time detection might create many new possibilities to facilitate reconstruction into fluent speech. Such an interface could help people to utilize voice-based interfaces like Apple Siri and Google Assistant, or to make (video) phone calls more fluent by delayed delivery.In this paper we present the first expandable audio-visual database of stuttered speech. We explore an end-to-end, real-time, multi-modal model for detection and classification of stuttered blocks in unbound speech. We also make use of video signals since acoustic signals cannot be produced immediately. We use multiple modalities as acoustic signals together with secondary characteristics exhibited in visual signals will permit an increased accuracy of detection.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal Conversational AI},
pages = {9–13},
numpages = {5},
keywords = {audiovisual dataset, stuttered speech, stammering, disfluent speech dataset, stuttered speech dataset, multi modal stuttering detection, speech disfluency},
location = {Seattle, WA, USA},
series = {MuCAI ?20}
}

@article{10.1109/TASLP.2021.3093817,
author = {Anidjar, Or Haim and Lapidot, Itshak and Hajaj, Chen and Dvir, Amit and Gilad, Issachar},
title = {Hybrid Speech and Text Analysis Methods for Speaker Change Detection},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3093817},
doi = {10.1109/TASLP.2021.3093817},
abstract = {Speaker Change Detection (SCD) is the task of segmenting an input audio-recording according to speaker interchanges. Nowadays, many applications, such as Speaker Diarization (SD) or automatic vocal transcription, depend on this segmentation task. In this paper, we focus on the essential task of the SD problem, the audio segmenting process, and suggest a solution for the SCD problem, as well as the assignment of clustered speaker labels for the extracted segments, and applying the solution over two datasets: a commercial dataset in Hebrew and the ICSI Meeting Corpus. As such, we propose a hybrid framework for the SCD problem that is learned by textual information and speech signals and the meta-data features that can be extracted from them. Moreover, we demonstrate the negative correlation between an increase in the number of speakers in the training dataset and the influence on the overall diarization system's performance, which is improved using our efficient SCD component. Finally, we show how our proposed hybrid framework remains robust compared to the ICSI Meeting Corpus, as the experimental evaluation's training and testing is based on two languages.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2324–2338},
numpages = {15}
}

@inproceedings{10.1145/3457682.3457718,
author = {Ali, Wazir and Kumar, Rajesh and Dai, Yong and Kumar, Jay and Tumrani, Saifullah},
title = {Neural Joint Model for Part-of-Speech Tagging and Entity Extraction},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457718},
doi = {10.1145/3457682.3457718},
abstract = {Part-of-speech tagging and named entity recognition (NER) are fundamental sequential labeling tasks in natural language processing (NLP), where joint learning of both tasks is an effective one-step solution. Limited efforts have been made by existing research to meet such needs for Sindhi language. As POS tagging and NER are highly correlative sequence tagging tasks, so most often, a word recognized by the NER system may be recognized as a noun by a POS tagger. Thus, in this paper, we propose a neural joint model based on a bidirectional long-short term memory (BiLSTM) network and adversarial transfer learning to incorporate syntactic information from two tasks by using task-shared information. The syntactic structure captures and provides the information of long-range dependencies among words. Moreover, the self-attention is employed to capture intra-sentence dependencies to the joint model explicitly. Empirical results on two benchmark datasets show that our proposed joint model consistently and significantly surpass the existing methods.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {239–245},
numpages = {7},
keywords = {Named Entity Recognition, Sindhi language, Adversarial transfer learning, Parts-of-speech tagging},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1145/3563395,
author = {Prietch, Soraia and S\'{a}nchez, J. Alfredo and Guerrero, Josefina},
title = {A Systematic Review of User Studies as a Basis for the Design of Systems for Automatic Sign Language Processing},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/3563395},
doi = {10.1145/3563395},
abstract = {Deaf persons, whether or not they are sign language users, make up one of various existing marginalized populations that historically have been socially and politically underrepresented. Unfortunately, this also happens in technology design. Conducting user studies in which marginalized populations are represented is a step towards guaranteeing their right to participate in choices and decisions that are made for, with, and by them. This article presents and discusses results from a Systematic Literature Review (SLR) of user studies in the design of systems for Automatic Sign Language Processing (ASLP). Following our SLR protocol, from 2,486 papers initially found, we applied inclusion and exclusion criteria to finally select 37 papers in our review. We excluded publications that were not full papers, were not related to our main topic of interest, or that reported results that had been updated by more recent papers. All the selected papers focus on user studies as a basis for the design of three major aspects of ASLP: generation (ASLG), recognition (ASLR), and translation (ASLT). With regard to our specific area of interest, we analyzed four areas related to our research questions: goals and research methods, types of user involvement in the interaction design life cycle, cultural and collaborative aspects, and other lessons learned from the primary studies under review. Salient findings from our analysis show that numerical scale questionnaires are the most frequently used research instruments, co-designing ASLP systems with sign language users is not a common practice (as potential users are included mostly in the evaluation phase), and only seldom are Deaf persons who are sign language users included as members of research teams. These findings point to the need of conducting more inclusive and qualitative research for, with and by Deaf persons who are sign language users.},
journal = {ACM Trans. Access. Comput.},
month = {nov},
articleno = {36},
numpages = {33},
keywords = {automatic generation, universal design, automatic recognition, User studies, assistive technology, systematic review, automatic translation, sign language}
}

@article{10.1145/3501397,
author = {P, Jasir M. and Balakrishnan, Kannan},
title = {Text-to-Speech Synthesis: Literature Review with an Emphasis on Malayalam Language},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3501397},
doi = {10.1145/3501397},
abstract = {Text-to-Speech Synthesis (TTS) is an active area of research to generate synthetic speech from underlying text. The identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech. It falls under the category of Natural Language Processing (NLP), which aims to bridge the gap in communication between human and machine. So far as Western languages like English are concerned, the research to produce intelligent and natural synthetic speech has advanced considerably. But in a multilingual state like India, many regional languages viz. Malayalam is underexplored when it comes to NLP. In this article, we try to amalgamate the major research works performed in the area of TTS in English and the prominent Indian languages, with a special emphasis on the South Indian language, Malayalam. This review intends to provide right direction to the research activities in the language, in the area of TTS.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {76},
numpages = {56},
keywords = {Indian language TTS, Malayalam TTS, TTS literature review, Text to speech synthesis}
}

@inproceedings{10.1145/3568294.3580129,
author = {Pramanick, Pradip and Sarkar, Chayan},
title = {Utilizing Prior Knowledge to Improve Automatic Speech Recognition in Human-Robot Interactive Scenarios},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580129},
doi = {10.1145/3568294.3580129},
abstract = {The prolificacy of human-robot interaction not only depends on a robot's ability to understand the intent and content of the human utterance but also gets impacted by the automatic speech recognition (ASR) system. Modern ASR can provide highly accurate (grammatically and syntactically) translation. Yet, the general purpose ASR often misses out on the semantics of the translation by incorrect word prediction due to open-vocabulary modeling. ASR inaccuracy can have significant repercussions as this can lead to a completely different action by the robot in the real world. Can any prior knowledge be helpful in such a scenario? In this work, we explore how prior knowledge can be utilized in ASR decoding. Using our experiments, we demonstrate how our system can significantly improve ASR translation for robotic task instruction.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {471–475},
numpages = {5},
keywords = {cognitive robot, asr, hri, robotics knowledge, embodied agent},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@article{10.1145/3486580,
author = {Ehret, Jonathan and B\"{o}nsch, Andrea and Asp\"{o}ck, Lukas and R\"{o}hr, Christine T. and Baumann, Stefan and Grice, Martine and Fels, Janina and Kuhlen, Torsten W.},
title = {Do Prosody and Embodiment Influence the Perceived Naturalness of Conversational Agents’ Speech?},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1544-3558},
url = {https://doi.org/10.1145/3486580},
doi = {10.1145/3486580},
abstract = {For conversational agents’ speech, either all possible sentences have to be prerecorded by voice actors or the required utterances can be synthesized. While synthesizing speech is more flexible and economic in production, it also potentially reduces the perceived naturalness of the agents among others due to mistakes at various linguistic levels. In our article, we are interested in the impact of adequate and inadequate prosody, here particularly in terms of accent placement, on the perceived naturalness and aliveness of the agents. We compare (1) inadequate prosody, as generated by off-the-shelf text-to-speech (TTS) engines with synthetic output; (2) the same inadequate prosody imitated by trained human speakers; and (3) adequate prosody produced by those speakers. The speech was presented either as audio-only or by embodied, anthropomorphic agents, to investigate the potential masking effect by a simultaneous visual representation of those virtual agents. To this end, we conducted an online study with 40 participants listening to four different dialogues each presented in the three Speech levels and the two Embodiment levels. Results confirmed that adequate prosody in human speech is perceived as more natural (and the agents are perceived as more alive) than inadequate prosody in both human (2) and synthetic speech (1). Thus, it is not sufficient to just use a human voice for an agents’ speech to be perceived as natural—it is decisive whether the prosodic realisation is adequate or not. Furthermore, and surprisingly, we found no masking effect by speaker embodiment, since neither a human voice with inadequate prosody nor a synthetic voice was judged as more natural, when a virtual agent was visible compared to the audio-only condition. On the contrary, the human voice was even judged as less “alive” when accompanied by a virtual agent. In sum, our results emphasize, on the one hand, the importance of adequate prosody for perceived naturalness, especially in terms of accents being placed on important words in the phrase, while showing, on the other hand, that the embodiment of virtual agents plays a minor role in the naturalness ratings of voices.},
journal = {ACM Trans. Appl. Percept.},
month = {oct},
articleno = {21},
numpages = {15},
keywords = {speech, text-to-speech, accentuation, virtual acoustics, Embodied conversational agents (ECAs), embodiment, prosody, audio}
}

@inproceedings{10.1145/3536221.3558058,
author = {Yoon, Youngwoo and Wolfert, Pieter and Kucherenko, Taras and Viegas, Carla and Nikolov, Teodor and Tsakov, Mihail and Henter, Gustav Eje},
title = {The GENEA Challenge 2022: A Large Evaluation of Data-Driven Co-Speech Gesture Generation},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3558058},
doi = {10.1145/3536221.3558058},
abstract = {This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. This year’s dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which previously was a major challenge in the field. The evaluation results are a revolution, and a revelation. Some synthetic conditions are rated as significantly more human-like than human motion capture. To the best of our knowledge, this has never been shown before on a high-fidelity avatar. On the other hand, all synthetic motion is found to be vastly less appropriate for the speech than the original motion-capture recordings.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {736–747},
numpages = {12},
keywords = {embodied conversational agents, evaluation paradigms, gesture generation},
location = {Bengaluru, India},
series = {ICMI '22}
}

@inproceedings{10.1145/3394486.3403331,
author = {Xu, Jin and Tan, Xu and Ren, Yi and Qin, Tao and Li, Jian and Zhao, Sheng and Liu, Tie-Yan},
title = {LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403331},
doi = {10.1145/3394486.3403331},
abstract = {Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than $98%$ intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2802–2812},
numpages = {11},
keywords = {dual transformation, rare language, automatic speech recognition, transfer learning, text to speech, knowledge distillation, low resource, speech synthesis},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3427228.3427289,
author = {Shirvanian, Maliheh and Mohammed, Manar and Saxena, Nitesh and Anand, S Abhishek},
title = {Voicefox: Leveraging Inbuilt Transcription to Enhance the Security of Machine-Human Speaker Verification against Voice Synthesis Attacks},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427289},
doi = {10.1145/3427228.3427289},
abstract = {In this paper, we propose Voicefox1, a defense against the threat of automated voice synthesis attacks in machine-based and human-based speaker verification applications. Voicefox is based on a hitherto undiscovered potential of speech-to-text transcription, already built into these applications. Voicefox relies on the premise that while the synthesized samples might be falsely accepted by the speaker verification systems and human listeners, they cannot be transcribed as accurately as a natural human voice by transcribers. Voicefox is not a speaker verification system, but rather an independent module that can be integrated with any speaker verification system to enhance its security against voice synthesis attacks. To test our premise and as an essential pre-requisite for building Voicefox, we ran an extensive study that measures the accuracy of off-the-shelf speech-to-text techniques when confronted with the synthesized samples generated by the state-of-the-art speech synthesis techniques. Our results show that the transcription error rate for the synthesized voices is significantly higher, on average 2-3x, than the error rate for natural voices. This study quantitatively proves our hypothesis that human voices are transcribed more accurately than synthesized voices. We further propose several post-transcription rules in designing Voicefox, including acceptance of transcribed text even if up to a certain number of words are not transcribed correctly, and ignoring the words not available in the reference dictionary. Using these rules, Voicefox can effectively reduce the false rejection rates to as low as 1.20-4.69% depending on the application and the transcriber used, and reduce the false accept rates to 0% for dictionaries with phonetically-distinct words.},
booktitle = {Annual Computer Security Applications Conference},
pages = {870–883},
numpages = {14},
keywords = {Speech-to-Text, Key Exchange Authentication, Speaker Verification},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3490099.3511164,
author = {Arakawa, Riku and Yakura, Hiromu and Goto, Masataka},
title = {BeParrot: Efficient Interface for Transcribing Unclear Speech via Respeaking},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511164},
doi = {10.1145/3490099.3511164},
abstract = {Transcribing speech from audio files to text is an important task not only for exploring the audio content in text form but also for utilizing the transcribed data as a source to train speech models, such as automated speech recognition (ASR) models. A post-correction approach has been frequently employed to reduce the time cost of transcription where users edit errors in the recognition results of ASR models. However, this approach assumes clear speech and is not designed for unclear speech (such as speech with high levels of noise or reverberation), which severely degrades the accuracy of ASR and requires many manual corrections. To construct an alternative approach to transcribe unclear speech, we introduce the idea of respeaking, which has primarily been used to create captions for television programs in real time. In respeaking, a proficient human respeaker repeats the heard speech as shadowing, and their utterances are recognized by an ASR model. While this approach can be effective for transcribing unclear speech, one problem is that respeaking is a highly cognitively demanding task and extensive training is often required to become a respeaker. We address this point with BeParrot, the first interface designed for respeaking that allows novice users to benefit from respeaking without extensive training through two key features: parameter adjustment and pronunciation feedback. Our user study involving 60 crowd workers demonstrated that they could transcribe different types of unclear speech 32.2&nbsp;% faster with BeParrot than with a conventional approach without losing the accuracy of transcriptions. In addition, comments from the workers supported the design of the adjustment and feedback features, exhibiting a willingness to continue using BeParrot for transcription tasks. Our work demonstrates how we can leverage recent advances in machine learning techniques to overcome the area that is still challenging for computers themselves with the help of a human-in-the-loop approach.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {832–840},
numpages = {9},
keywords = {speech transcription, respeak, automated speech recognition},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3404709.3404746,
author = {Samonte, Mary Jane C.},
title = {An Assistive Technology Using FSL, Speech Recognition, Gamification and Online Handwritten Character Recognition in Learning Statistics for Students with Hearing and Speech Impairment},
year = {2020},
isbn = {9781450375337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404709.3404746},
doi = {10.1145/3404709.3404746},
abstract = {The application of modern technology improves the education system and also provides a better quality of life, especially for those with physical disabilities and with challenging learning environments. This study focus on the development of an e-tutor system that introduces innovative way to assist teachers in presenting lessons and assessment through multi-approach system. Students with or without disability can have access to learning modules and assessment activities even in the comfort of their homes, as part of giving a disruptive supplementary tool for learning. Statistics is the foundation in learning the advance topics of big data and data analytics applied in business intelligence. Nowadays, it is important to give early learning interventions to prepare our young generation. This research is about the use of assistive technology applied in designing and developing a website to teach Filipino students with Filipino Sign Language (FSL) and basic Statistics. This study utilizes speech-to-text, gamified learning, and handwritten character recognition in order to present topics in Statistics in an interactive and animated environment. The study is composed of four (4) modules: Filipino sign language (for lessons in videos and/or dictionary), speech-to-text and speech-to visual approach (for reading), gamified learning (for more interactive environment) and handwritten character recognition (for writing) in teaching and learning Statistics.},
booktitle = {Proceedings of the 6th International Conference on Frontiers of Educational Technologies},
pages = {92–97},
numpages = {6},
keywords = {Assistive Technology, Handwritten Character Recognition, Filipino Sign Language, Gamified Learning, Speech Recognition},
location = {Tokyo, Japan},
series = {ICFET '20}
}

@inproceedings{10.1145/3348445.3348485,
author = {Wattanacheep, Bhattarabhorn and Chitsobhuk, Orachat},
title = {Prediction of 3D Rotation and Translation from 2D Images},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348485},
doi = {10.1145/3348445.3348485},
abstract = {The prediction of three-dimensional (3D) rotation and translation can be retrieved from two-dimensional (2D) images to build 3D models from large collections of images. In this paper, the process starts by extracting the features of images via transfer learning approach from Deep Neural Network model called VGG19. Even though the features extracted from VGG19 are usually adopted in image recognition application; in this research, we apply these features to the prediction model to obtain rotation and translation parameters. Due to the large size of the feature dimensions, it is necessary to perform dimensional reduction technique called latent semantic analysis (LSA) to decrease the feature dimensions and remain only the important ones. Then, the regression estimation technique based on the idea of Support Vector Machine (SVM) is used to predict the rotation and translation parameters. The accuracy is estimated by comparing the prediction results with the corresponding ground truth set. The average errors of rotation and translation of 3D prediction from 2D images are approximately 0.2419 degrees and 1.35 meters respectively.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {49–52},
numpages = {4},
keywords = {Deep Learning, Image Processing, Robotics, 3D Reconstruction},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@article{10.1109/TASLP.2020.2997197,
author = {Janbakhshi, Parvaneh and Kodrasi, Ina and Bourlard, Herv\'{e}},
title = {Automatic Pathological Speech Intelligibility Assessment Exploiting Subspace-Based Analyses},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2997197},
doi = {10.1109/TASLP.2020.2997197},
abstract = {Competitive state-of-the-art automatic pathological speech intelligibility measures typically rely on regression training on a large number of features, require a large amount of healthy speech training data, or are applicable only to phonetically balanced scenarios where healthy and pathological speakers utter the same utterances. As a result, their performance in unseen data is unsatisfactory, and they cannot be used in low-resource languages or in phonetically unbalanced scenarios. To overcome these drawbacks, we propose a subspace-based intelligibility (SBI) measure. The SBI measure operates based on the hypothesis that dominant spectral patterns of pathological speech differ from intelligible speech (where the pathological and intelligible speech signals do not need to match in phonetic content), with the difference increasing as pathological speech intelligibility decreases. The SBI measure uses a minimal number of speech recordings to compute dominant spectral basis vectors spanning intelligible and pathological speech. The subspaces spanned by the intelligible and pathological spectral basis vectors are compared to each other through a subspace distance measure, which is directly used (i.e., without any training) as the pathological speech intelligibility estimate. Exploiting psychoacoustic evidence on the importance of spectral modulation cues to the perceived speech intelligibility and clinical evidence on the degradation of these cues in pathological speech, we show that the power of the proposed SBI measure lies in capturing the effect of spectral modulation degradation. To be able to additionally track possible degradations in the temporal structure of the pathological speech signal, we also propose two extensions of the SBI measure by incorporating short-time temporal information. Experimental results for different languages and speech pathologies show that the proposed intelligibility measures yield high and significant correlations with subjective intelligibility ratings, while not requiring any regression training or a large number of healthy speech recordings and being applicable to phonetically unbalanced scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {1717–1728},
numpages = {12}
}

@inproceedings{10.1145/3514094.3539519,
author = {Reyero Lobo, Paula},
title = {Bias in Hate Speech and Toxicity Detection},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3539519},
doi = {10.1145/3514094.3539519},
abstract = {Many Artificial Intelligence (AI) systems rely on finding patterns in large datasets, which are prone to bias and exacerbate existing segregation and inequalities of marginalised communities. Due to their socio-technical impact, bias in AI has become a pressing issue. In this work, we investigate discrimination prevention methods on the assumption that disparities of specific populations in the training samples are reproduced or even amplified in the AI system outcomes. We aim to identify the information from vulnerable groups in the training data, uncover potential inequalities in how data capture these groups and provide additional information about them to alleviate inequalities, e.g., stereotypical and generalised views that lead to learning discriminatory associations. We develop data preprocessing techniques in automated moderation (AI systems to flag or filter online abuse) due to its substantial social implications and existing challenges common to many AI applications.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {910},
numpages = {1},
keywords = {semantic web, toxic speech, bias, artificial intelligence},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3465631.3465808,
author = {Yuan, Weimin and Peng, Jiming},
title = {Translation Engineering: Translation in the Age of Information Technology},
year = {2021},
isbn = {9781450385015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465631.3465808},
doi = {10.1145/3465631.3465808},
abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.},
booktitle = {Retracted on September 15, 2021The Sixth International Conference on Information Management and Technology},
articleno = {138},
numpages = {6},
location = {Jakarta, Indonesia},
series = {ICIMTECH 21}
}

@inproceedings{10.1145/3357160.3357671,
author = {Wan, Youhui and Don, Lanqing and Da, Weihui},
title = {A Supra-Modal Decoding Mechanism: Evidence from Chinese Speakers Learning English},
year = {2019},
isbn = {9781450369725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357160.3357671},
doi = {10.1145/3357160.3357671},
abstract = {Multimodal information interaction is a common way of communication involved in language teaching. The study on the neural mechanism in the process of language information processing will be helpful to make better use of the above way. This research investigated the supra-modal network when Chinese subjects processed English interpretation, and found that the recruited subjects capitalized on the occipital lobe and frontal lobe to achieve semantic processing for both visual and auditory modules. For these Chinese subjects, reading activation tended to be occipital-lobe focused, while in previous models (Models of Wernicke-Geschwind), subjects (native/ nearly native with recruited languages) heavily relied on the temporal lobe. This research indicated that the recruited late bilingual subjects adopted a different network to decode language signals, opposed to models in the previous literature. Moreover, the occipital lobe remained crucial for the recruited Chinese bilinguals to process materials with high levels of difficulty. For learners of similar backgrounds, visualization shall be emphasized through multimodal information interactions during future language instructions, as well as in interpretation teaching.},
booktitle = {NeuroManagement and Intelligent Computing Method on Multimodal Interaction},
articleno = {5},
numpages = {6},
keywords = {Supra-modal network, Chinese speaker learning English, Natural Language, Neural mechanism, Language translation},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3503162.3503176,
author = {Modha, Sandip and Mandl, Thomas and Shahi, Gautam Kishore and Madhu, Hiren and Satapara, Shrey and Ranasinghe, Tharindu and Zampieri, Marcos},
title = {Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages and Conversational Hate Speech},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503176},
doi = {10.1145/3503162.3503176},
abstract = {The HASOC track is dedicated to the evaluation of technology for finding Offensive Language and Hate Speech. HASOC is creating a multilingual data corpus mainly for English and under-resourced languages(Hindi and Marathi). This paper presents one HASOC subtrack with two tasks. In 2021, we organized the classification task for English, Hindi, and Marathi. The first task consists of two classification tasks; Subtask 1A consists of a binary and fine-grained classification into offensive and non-offensive tweets. Subtask 1B asks to classify the tweets into Hate, Profane and offensive. Task 2 consists of identifying tweets given additional context in the form of the preceding conversion. During the shared task, 65 teams have submitted 652 runs. This overview paper briefly presents the task descriptions, the data and the results obtained from the participant’s submission.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {1–3},
numpages = {3},
keywords = {Under-resourced language, hate speech, Multilingual Datasets, social media},
location = {Virtual Event, India},
series = {FIRE '21}
}

@inproceedings{10.1145/3426020.3426094,
author = {Ko, Debbie Honghee and Ul Muhammad, Ammar Ul Hassan and Majeed, Saima and Choi, Jaeyoung},
title = {Font2Fonts: A Modified Image-to-Image Translation Framework for Font Generation},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426094},
doi = {10.1145/3426020.3426094},
abstract = {Generating a font from scratch requires font domain knowledge additionally, it's a labor intensive and time-consuming task. With the remarkable success of deep learning methods for image synthesis, many researchers are focusing on generating fonts by utilizing these methods. In order to utilize these deep learning methods for font generation, language specific font image datasets are manually prepared which is a cumbersome and time-consuming task. Additionally, existing supervised image-to-image translation methods like pix2pix are able to do only one-to-one domain translation therefore they cannot be applied to font generation task which is multi-domain. In this paper, we propose a model, Font2Fonts, a conditional generative adversarial network (GAN) for font synthesis in a supervised setting. Unlike pix2pix which can only translate from one font domain to the other, Font2Fonts is a multi-domain translation model. The proposed method can synthesize high quality diverse font images using a single end-to-end network. By our qualitative and quantitative experiments, we verify the effectiveness of our proposed model. Moreover, we also propose a Unicode-based module for automatically generating font image dataset. Our proposed Unicode-based method can be easily applied for preparing font dataset of various language characters.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {288–294},
numpages = {7},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@article{10.1145/3355089.3356487,
author = {Wang, Yujia and Wang, Wenguan and Liang, Wei and Yu, Lap-Fai},
title = {Comic-Guided Speech Synthesis},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356487},
doi = {10.1145/3355089.3356487},
abstract = {We introduce a novel approach for synthesizing realistic speeches for comics. Using a comic page as input, our approach synthesizes speeches for each comic character following the reading flow. It adopts a cascading strategy to synthesize speeches in two stages: Comic Visual Analysis and Comic Speech Synthesis. In the first stage, the input comic page is analyzed to identify the gender and age of the characters, as well as texts each character speaks and corresponding emotion. Guided by this analysis, in the second stage, our approach synthesizes realistic speeches for each character, which are consistent with the visual observations. Our experiments show that the proposed approach can synthesize realistic and lively speeches for different types of comics. Perceptual studies performed on the synthesis results of multiple sample comics validate the efficacy of our approach.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {187},
numpages = {14},
keywords = {comics, speech synthesis, deep learning}
}

@inproceedings{10.1145/3395035.3425255,
author = {Boateng, George and Kowatsch, Tobias},
title = {Speech Emotion Recognition among Elderly Individuals Using Multimodal Fusion and Transfer Learning},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425255},
doi = {10.1145/3395035.3425255},
abstract = {Recognizing the emotions of the elderly is important as it could give an insight into their mental health. Emotion recognition systems that work well on the elderly could be used to assess their emotions in places such as nursing homes and could inform the development of various activities and interventions to improve their mental health. However, several emotion recognition systems are developed using data from younger adults. In this work, we train machine learning models to recognize the emotions of elderly individuals via performing a 3-class classification of valence and arousal as part of the INTERSPEECH 2020 Computational Paralinguistics Challenge (COMPARE). We used speech data from 87 participants who gave spontaneous personal narratives. We leveraged a transfer learning approach in which we used pretrained CNN and BERT models to extract acoustic and linguistic features respectively and fed them into separate machine learning models. Also, we fused these two modalities in a multimodal approach. Our best model used a linguistic approach and outperformed the official competition of unweighted average recall (UAR) baseline for valence by 8.8% and the mean of valence and arousal by 3.2%. We also showed that feature engineering is not necessary as transfer learning without fine-tuning performs as well or better and could be leveraged for the task of recognizing the emotions of elderly individuals. This work is a step towards better recognition of the emotions of the elderly which could eventually inform the development of interventions to manage their mental health.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {12–16},
numpages = {5},
keywords = {deep learning, computational paralinguistics, multimodal fusion, transfer learning, speech emotion recognition, LSTM, elderly individuals, support vector machine, BERT, CNN, affective computing, SBERT},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1109/TCBB.2020.3024228,
author = {Lin, Bo and Deng, Shuiguang and Gao, Honghao and Yin, Jianwei},
title = {A Multi-Scale Activity Transition Network for Data Translation in EEG Signals Decoding},
year = {2020},
issue_date = {Sept.-Oct. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3024228},
doi = {10.1109/TCBB.2020.3024228},
abstract = {Electroencephalogram (EEG) is a non-invasive collection method for brain signals. It has broad prospects in brain-computer interface (BCI) applications. Recent advances have shown the effectiveness of the widely used convolutional neural network (CNN) in EEG decoding. However, some studies reveal that a slight disturbance to the inputs, e.g., data translation, can change CNN’s outputs. Such instability is dangerous for EEG-based BCI applications because signals in practice are different from training data. In this study, we propose a multi-scale activity transition network (MSATNet) to alleviate the influence of the translation problem in convolution-based models. MSATNet provides an activity state pyramid consisting of multi-scale recurrent neural networks to capture the relationship between brain activities, which is a translation-invariant feature. In the experiment, Kullback–Leibler divergence is applied to measure the degree of translation. The comprehensive results demonstrate that our method surpasses the AUC of 0.0080, 0.0254, 0.0393 in 1, 5, and 10 KL divergence compared to competitors with various convolution structures.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {sep},
pages = {1699–1709},
numpages = {11}
}

@inproceedings{10.1145/3373625.3418046,
author = {Zhou, Zhenxing and Neo, Yisiang and Lui, King-Shan and Tam, Vincent W.L. and Lam, Edmund Y. and Wong, Ngai},
title = {A Portable Hong Kong Sign Language Translation Platform with Deep Learning and Jetson Nano},
year = {2020},
isbn = {9781450371032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3418046},
doi = {10.1145/3373625.3418046},
abstract = {As hearing loss is arousing more and more public concern, different researches have been conducted on translating the sign language into spoken language. However, most of these researches remain in a theoretical level and few of them investigate how to realize a real system. In this paper, we introduce an effective and portable Hong Kong sign language recognition platform which can translate the Hong Kong sign language within a few seconds. In this platform, there are mainly two parts: a mobile application and a Jetson Nano. The mobile application accounts for preprocessing the sign video and transferring the videos to Jetson Nano. Then, Jetson Nano will translate sign videos into spoken language with the pretrained deep learning model and return the results to the mobile application. With this platform, non-disabled people can easily translate and understand the sign performed by deaf people through mobile phones quickly. We believe that this platform can significantly facilitate the daily communication between deaf people and the others in Hong Kong.},
booktitle = {Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {89},
numpages = {4},
keywords = {sign language recognition, video recognition, jetson nano, deep learning},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{10.1145/3539597.3575793,
author = {Papakyriakopoulos, Orestis and Xiang, Alice},
title = {Considerations for Ethical Speech Recognition Datasets},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3575793},
doi = {10.1145/3539597.3575793},
abstract = {Speech AI Technologies are largely trained on publicly available datasets or by the massive web-crawling of speech. In both cases, data acquisition focuses on minimizing collection effort, without necessarily taking the data subjects' protection or user needs into consideration. This results to models that are not robust when used on users who deviate from the dominant demographics in the training set, discriminating individuals having different dialects, accents, speaking styles, and disfluencies. In this talk, we use automatic speech recognition as a case study and examine the properties that ethical speech datasets should possess towards responsible AI applications. We showcase diversity issues, inclusion practices, and necessary considerations that can improve trained models, while facilitating model explainability and protecting users and data subjects. We argue for the legal &amp; privacy protection of data subjects, targeted data sampling corresponding to user demographics &amp; needs, appropriate meta data that ensure explainability &amp; accountability in cases of model failure, and the sociotechnical &amp; situated model design. We hope this talk can inspire researchers &amp; practitioners to design and use more human-centric datasets in speech technologies and other domains, in ways that empower and respect users, while improving machine learning models' robustness and utility.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1287–1288},
numpages = {2},
keywords = {human-centric ai, data collection, automated speech recognition, algorithmic fairness},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3366423.3380067,
author = {Zhang, Yin and He, Yun and Wang, Jianling and Caverlee, James},
title = {Adaptive Hierarchical Translation-Based Sequential Recommendation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380067},
doi = {10.1145/3366423.3380067},
abstract = {We propose an adaptive hierarchical translation-based sequential recommendation called HierTrans that first extends traditional item-level relations to the category-level, to help capture dynamic sequence patterns that can generalize across users and time. Then unlike item-level based methods, we build a novel hierarchical temporal graph that contains item multi-relations at the category-level and user dynamic sequences at the item-level. Based on the graph, HierTrans adaptively aggregates the high-order multi-relations among items and dynamic user preferences to capture the dynamic joint influence for next-item recommendation. Specifically, the user translation vector in HierTrans can adaptively change based on both a user’s previous interacted items and the item relations inside the user’s sequences, as well as the user’s personal dynamic preference. Experiments on public datasets demonstrate the proposed model HierTrans consistently outperforms state-of-the-art sequential recommendation methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2984–2990},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3335595.3335602,
author = {Lim, Hyewon and Hwang, Hyesun and Kim, Kee Ok and Yang, Yeon Ji},
title = {How Should "AI Speakers" Touch Consumer Hearts? Text Mining of Mass Media about AI Speakers},
year = {2019},
isbn = {9781450371766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335595.3335602},
doi = {10.1145/3335595.3335602},
abstract = {This study investigates consumer interest and concerns about Artificial Intelligence (AI) speakers. An AI speaker, otherwise called a smart speaker, is a wireless speaker and voice command device with an integrated interactive virtual assistant. A general understanding of social issues about and interest in AI technology is acquired through an analysis of keywords on mass media in discussions where AI technology is applied in real life.},
booktitle = {Proceedings of the XX International Conference on Human Computer Interaction},
articleno = {55},
numpages = {2},
keywords = {AI speaker, Smart speaker, Artificial intelligence, Text mining},
location = {Donostia, Gipuzkoa, Spain},
series = {Interacci\'{o}n '19}
}

@inproceedings{10.1145/3447548.3467126,
author = {Mishra, Shaunak and Kuznetsov, Mikhail and Srivastava, Gaurav and Sviridenko, Maxim},
title = {VisualTextRank: Unsupervised Graph-Based Content Extraction for Automating Ad Text to Image Search},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467126},
doi = {10.1145/3447548.3467126},
abstract = {Numerous online stock image libraries offer high quality yet copyright free images for use in marketing campaigns. To assist advertisers in navigating such third party libraries, we study the problem of automatically fetching relevant ad images given the ad text (via a short textual query for images). Motivated by our observations in logged data on ad image search queries (given ad text), we formulate a keyword extraction problem, where a keyword extracted from the ad text (or its augmented version) serves as the ad image query. In this context, we propose VisualTextRank: an unsupervised method to (i) augment input ad text using semantically similar ads, and (ii) extract the image query from the augmented ad text. VisualTextRank builds on prior work on graph based context extraction (biased TextRank in particular) by leveraging both the text and image of similar ads for better keyword extraction, and using advertiser category specific biasing with sentence-BERT embeddings. Using data collected from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search feature for onboarding advertisers, we demonstrate the superiority of VisualTextRank compared to competitive keyword extraction baselines (including an 11% accuracy lift over biased TextRank). For the case when the stock image library is restricted to English queries, we show the effectiveness of VisualTextRank on multilingual ads (translated to English) while leveraging semantically similar English ads. Online tests with a simplified version of VisualTextRank led to a 28.7% increase in the usage of stock image search, and a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native ad platform.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3404–3413},
numpages = {10},
keywords = {content extraction, online advertising, multilingual, image search},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1109/TASLP.2018.2876171,
author = {Tan, Ke and Chen, Jitong and Wang, DeLiang},
title = {Gated Residual Networks With Dilated Convolutions for Monaural Speech Enhancement},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Press},
volume = {27},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2876171},
doi = {10.1109/TASLP.2018.2876171},
abstract = {For supervised speech enhancement, contextual information is important for accurate mask estimation or spectral mapping. However, commonly used deep neural networks DNNs are limited in capturing temporal contexts. To leverage long-term contexts for tracking a target speaker, we treat speech enhancement as a sequence-to-sequence mapping, and present a novel convolutional neural network CNN architecture for monaural speech enhancement. The key idea is to systematically aggregate contexts through dilated convolutions, which significantly expand receptive fields. The CNN model additionally incorporates gating mechanisms and residual learning. Our experimental results suggest that the proposed model generalizes well to untrained noises and untrained speakers. It consistently outperforms a DNN, a unidirectional long short-term memory LSTM model, and a bidirectional LSTM model in terms of objective speech intelligibility and quality metrics. Moreover, the proposed model has far fewer parameters than DNN and LSTM models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {189–198},
numpages = {10}
}

@inproceedings{10.1145/3536220.3558075,
author = {Richter, Vanessa and Neumann, Michael and Kothare, Hardik and Roesler, Oliver and Liscombe, Jackson and Suendermann-Oeft, David and Prokop, Sebastian and Khan, Anzalee and Yavorsky, Christian and Lindenmayer, Jean-Pierre and Ramanarayanan, Vikram},
title = {Towards Multimodal Dialog-Based Speech &amp; Facial Biomarkers of Schizophrenia},
year = {2022},
isbn = {9781450393898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536220.3558075},
doi = {10.1145/3536220.3558075},
abstract = {We present a scalable multimodal dialog platform for the remote digital assessment and monitoring of schizophrenia. Patients diagnosed with schizophrenia and healthy controls interacted with Tina, a virtual conversational agent, as she guided them through a brief set of structured tasks, while their speech and facial video was streamed in real-time to a back-end analytics module. Patients were concurrently assessed by trained raters on validated clinical scales. We find that multiple speech and facial biomarkers extracted from these data streams show significant differences (as measured by effect sizes) between patients and controls, and furthermore, machine learning models built on such features can classify patients and controls with high sensitivity and specificity. We further investigate, using correlation analysis between the extracted metrics and standardized clinical scales for the assessment of schizophrenia symptoms, how such speech and facial biomarkers can provide further insight into schizophrenia symptomatology.},
booktitle = {Companion Publication of the 2022 International Conference on Multimodal Interaction},
pages = {171–176},
numpages = {6},
keywords = {biomarkers, facial metrics, multimodal dialog system, computer vision, schizophrenia, facial landmarks, speech metrics, mediapipe},
location = {Bengaluru, India},
series = {ICMI '22 Companion}
}

@article{10.1109/TASLP.2021.3125143,
author = {Lin, Ju and van Wijngaarden, Adriaan J. de Lind and Wang, Kuang-Ching and Smith, Melissa C.},
title = {Speech Enhancement Using Multi-Stage Self-Attentive Temporal Convolutional Networks},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3125143},
doi = {10.1109/TASLP.2021.3125143},
abstract = {Multi-stage learning is an effective technique for invoking multiple deep-learning modules sequentially. This paper applies multi-stage learning to speech enhancement by using a multi-stage structure, where each stage comprises a self-attention (SA) block followed by stacks of temporal convolutional network (TCN) blocks with doubling dilation factors. Each stage generates a prediction that is refined in a subsequent stage. A feature fusion block is inserted at the input of later stages to re-inject original information. The resulting multi-stage speech enhancement system, multi-stage SA-TCN, is compared with state-of-the-art deep-learning speech enhancement methods using the LibriSpeech and VCTK datasets. The multi-stage SA-TCN system’s hyperparameters are fine-tuned, and the impact of the SA block, the feature fusion block, and the number of stages are determined. The use of a multi-stage SA-TCN system as a front-end for automatic speech recognition systems is also investigated. It is shown that the multi-stage SA-TCN systems perform well relative to other state-of-the-art systems in terms of speech enhancement and speech recognition scores.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3440–3450},
numpages = {11}
}

@article{10.1145/3460656,
author = {Bringmann, Karl and K\"{u}Nnemann, Marvin and Nusser, Andr\'{e}},
title = {Discrete Fr\'{e}chet Distance under Translation: Conditional Hardness and an Improved Algorithm},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/3460656},
doi = {10.1145/3460656},
abstract = {The discrete Fr\'{e}chet distance is a popular measure for comparing polygonal curves. An important variant is the discrete Fr\'{e}chet distance under translation, which enables detection of similar movement patterns in different spatial domains. For polygonal curves of length n in the plane, the fastest known algorithm runs in time \~{O}(n5) [12]. This is achieved by constructing an arrangement of disks of size \~{O}(n4), and then traversing its faces while updating reachability in a directed grid graph of size N := \~{O}(n5), which can be done in time \~{O}(√ N) per update [27]. The contribution of this article is two-fold. First, although it is an open problem to solve dynamic reachability in directed grid graphs faster than \~{O}(√ N), we improve this part of the algorithm: We observe that an offline variant of dynamic s-t-reachability in directed grid graphs suffices, and we solve this variant in amortized time \~{O}(N1/3) per update, resulting in an improved running time of \~{O}(N4.66) for the discrete Fr\'{e}chet distance under translation. Second, we provide evidence that constructing the arrangement of size \~{O}(N4) is necessary in the worst case by proving a conditional lower bound of n4 - o(1) on the running time for the discrete Fr\'{e}chet distance under translation, assuming the Strong Exponential Time Hypothesis.},
journal = {ACM Trans. Algorithms},
month = {jul},
articleno = {25},
numpages = {42},
keywords = {Fr\'{e}chet distance, conditional lower bounds}
}

@article{10.1145/3273956,
author = {Chang, Kyungwook and Kadetotad, Deepak and Cao, Yu and Seo, Jae-Sun and Lim, Sung Kyu},
title = {Power, Performance, and Area Benefit of Monolithic 3D ICs for On-Chip Deep Neural Networks Targeting Speech Recognition},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3273956},
doi = {10.1145/3273956},
abstract = {In recent years, deep learning has become widespread for various real-world recognition tasks. In addition to recognition accuracy, energy efficiency and speed (i.e., performance) are other grand challenges to enable local intelligence in edge devices. In this article, we investigate the adoption of monolithic three-dimensional (3D) IC (M3D) technology for deep learning hardware design, using speech recognition as a test vehicle. M3D has recently proven to be one of the leading contenders to address the power, performance, and area (PPA) scaling challenges in advanced technology nodes. Our study encompasses the influence of key parameters in DNN hardware implementations towards their performance and energy efficiency, including DNN architectural choices, underlying workloads, and tier partitioning choices in M3D designs. Our post-layout M3D designs, together with hardware-efficient sparse algorithms, produce power savings and performance improvement beyond what can be achieved using conventional 2D ICs. Experimental results show that M3D offers 22.3% iso-performance power saving and 6.2% performance improvement, convincingly demonstrating its entitlement as a solution for DNN ASICs. We further present architectural and physical design guidelines for M3D DNNs to maximize the benefits.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {nov},
articleno = {42},
numpages = {19},
keywords = {low power design, high performance design, on-chip deep neural networks, Monolithic 3D IC, speech recognition}
}

@article{10.1145/3427330,
author = {Williams, Adam S. and Ortega, Francisco R.},
title = {Understanding Gesture and Speech Multimodal Interactions for Manipulation Tasks in Augmented Reality Using Unconstrained Elicitation},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ISS},
url = {https://doi.org/10.1145/3427330},
doi = {10.1145/3427330},
abstract = {This research establishes a better understanding of the syntax choices in speech interactions and of how speech, gesture, and multimodal gesture and speech interactions are produced by users in unconstrained object manipulation environments using augmented reality. The work presents a multimodal elicitation study conducted with 24 participants. The canonical referents for translation, rotation, and scale were used along with some abstract referents (create, destroy, and select). In this study time windows for gesture and speech multimodal interactions are developed using the start and stop times of gestures and speech as well as the stoke times for gestures. While gestures commonly precede speech by 81 ms we find that the stroke of the gesture is commonly within 10 ms of the start of speech. Indicating that the information content of a gesture and its co-occurring speech are well aligned to each other. Lastly, the trends across the most common proposals for each modality are examined. Showing that the disagreement between proposals is often caused by a variation of hand posture or syntax. Allowing us to present aliasing recommendations to increase the percentage of users' natural interactions captured by future multimodal interactive systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {202},
numpages = {21},
keywords = {gesture and speech interaction, elicitation, interaction, multimodal, augmented reality}
}

@article{10.1109/TASLP.2021.3126925,
author = {Hong, Joanna and Kim, Minsu and Park, Se Jin and Ro, Yong Man},
title = {Speech Reconstruction With Reminiscent Sound Via Visual Voice Memory},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126925},
doi = {10.1109/TASLP.2021.3126925},
abstract = {The goal of this work is to reconstruct speech from silent video, in both speaker dependent and independent ways. Unlike previous works that have been mostly restricted to a speaker dependent setting, we propose Visual Voice memory to restore essential auditory information to generate proper speech from different speakers and even unseen speakers. The proposed memory takes additional auditory information that corresponds to the input face movements and stores the auditory contexts that can be recalled by the given input visual features. Specifically, the Visual Voice memory contains value and key memory slots, where value memory slots are for saving the audio features, and key memory slots are for storing the visual features in the same location of the saved audio features. Guiding each memory to properly save each feature, the model can adequately produce the speech. Hence, our method employs both video and audio information during training time but does not require any additional auditory input during inference. Our key contributions are: (1) proposing the Visual Voice memory that brings rich information of audio that complements the visual features, thus producing high-quality speech from silent video, and (2) enabling multi-speaker and unseen speaker training by memorizing auditory features and the corresponding visual features. We validate the proposed framework on GRID and Lip2Wav datasets and show that our method surpasses the performance of previous works on both multi-speaker and speaker independent settings. We also demonstrate that the Visual Voice memory contains meaningful information to reconstruct speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {3654–3667},
numpages = {14}
}

@inproceedings{10.1145/3526114.3558715,
author = {Hiraki, Hirotaka and Rekimoto, Jun},
title = {SilentWhisper: Faint Whisper Speech Using Wearable Microphone},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558715},
doi = {10.1145/3526114.3558715},
abstract = {Voice interaction is a fundamental human capacity, and we can use voice user interfaces just speaking. However, in public spaces, we are hesitant to use them because of consideration for their surroundings and low privacy. Silent speech, a method that recognizes the movement of speech in silence, has been proposed as a solution to this problem, and it allows us to maintain our privacy when speaking. However, existing silent speech interfaces are burdensome because the sensor must be kept in contact with the face and mouth, and commands must be prepared for each user. In this study, we propose a method to input whispered speech at a quiet volume that cannot be heard by others using a pin microphone. Experimental results show that a recognition rate was 13.9% WER and 6.4% CER for 210 phrases. We showed that privacy-preserving vocal input is possible by whispering voices which are not comprehensible to others.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {27},
numpages = {3},
keywords = {silent speech, whisper, speech, murmur, voice user interface},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@inproceedings{10.1145/3474085.3475223,
author = {Bhattacharya, Uttaran and Childs, Elizabeth and Rewkowski, Nicholas and Manocha, Dinesh},
title = {Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475223},
doi = {10.1145/3474085.3475223},
abstract = {We present a generative adversarial network to synthesize 3D pose sequences of co-speech upper-body gestures with appropriate affective expressions. Our network consists of two components: a generator to synthesize gestures from a joint embedding space of features encoded from the input speech and the seed poses, and a discriminator to distinguish between the synthesized pose sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral coefficients and the text transcript computed from the input speech in separate encoders in our generator to learn the desired sentiments and the associated affective cues. We design an affective encoder using multi-scale spatial-temporal graph convolutions to transform 3D pose sequences into latent, pose-based affective features. We use our affective encoder in both our generator, where it learns affective features from the seed poses to guide the gesture synthesis, and our discriminator, where it enforces the synthesized gestures to contain the appropriate affective expressions. We perform extensive evaluations on two benchmark datasets for gesture synthesis from the speech, the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the best baselines, we improve the mean absolute joint error by 10-33%, the mean acceleration difference by 8-58%, and the Fr\'{e}chet Gesture Distance by 21-34%. We also conduct a user study and observe that compared to the best current baselines, around 15.28% of participants indicated our synthesized gestures appear more plausible, and around 16.32% of participants felt the gestures had more appropriate affective expressions aligned with the speech.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2027–2036},
numpages = {10},
keywords = {gesture synthesis, machine learning, co-speech gestures, mel-frequency cepstral coefficients, generative adversarial network, intelligent agents, affective computing, affective expressions},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1109/TASLP.2021.3065202,
author = {Chen, Xianhong and Bao, Changchun},
title = {Phoneme-Unit-Specific Time-Delay Neural Network for Speaker Verification},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3065202},
doi = {10.1109/TASLP.2021.3065202},
abstract = {Variations of speech content increase the difficulty of speaker verification. In this paper, to alleviate the negative effect of the variations, phoneme-unit-specific time-delay neural network (PUSTDNN) is proposed and applied to the state-of-the-art x-vector system. It models each phoneme unit with an individual time-delay neural network (TDNN). That is to say, each TDNN mainly deals with a phoneme unit. Compared with handling all phoneme units together, when handling a phoneme unit, a TDNN can extract more discriminative speaker information, thus improving the system performance. Two realizations of the PUSTDNN are proposed. The first one can retain speech temporal information. The second one further combines all the TDNNs in a PUSTDNN into a larger TDNN to reduce computational complexity. To avoid model overfitting, the phoneme units are obtained by clustering phonemes based on the phonetic knowledge and phonetic sparsity degree. The PUSTDNN is also compared with two other techniques, i.e., phonetic vector and multitask. Experiments on the Fisher, NIST SRE10, and VoxCeleb datasets show that the phonetic vector technique is most robust to the phoneme unit recognition accuracy. When the accuracy is high enough, the multitask performs better than the phonetic vector, and the PUSTDNN performs best and can achieve over 10% relative improvement compared with the x-vector baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1243–1255},
numpages = {13}
}

@inproceedings{10.1145/3395035.3425183,
author = {Lyakso, Elena E. and Frolova, Olga V.},
title = {Early Development Indicators Predict Speech Features of Autistic Children},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425183},
doi = {10.1145/3395035.3425183},
abstract = {The goal of the study is to reveal the correlation between speech peculiarities and different aspects of development of children with autism spectrum disorders. The participants in the study were 28 children with autism spectrum disorders (ASD) aged 4-11 years and 64 adults - listening to children's speech samples. Children with ASD were divided into two groups: ASD-1 - ASD is the leading symptom (F84, n=17); children assigned to ASD-2 (n=11) had other disorders accompanied by ASD symptomatology (F83 + F84). Recording of children's speech and behavior was carried out in the most similar situations: a dialogue with the experimenter, viewing pictures and retelling a story about them or answers to questions, book reading. The child's psychophysiological characteristics were estimated according to the method which includes determining the leading hemisphere by speech (dichotic listening test - DLT), phonemic hearing, and the profile of lateral functional asymmetry (PLFA). All tasks and the time of the study were adapted to the child's capacities. The study analyzed the level of speech formation in 4-11 year-old children with ASD, identified direct and indirect relationships between the features of early development, its psychophysiological indicators, and the speech development level at the time of the study. The ability of adults to recognize the psychoneurological state of children via their speech is determined. The results of the study support the need to increase focus on and understanding of the language strengths and weaknesses in children with ASD and an individual approach to teaching children.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {514–521},
numpages = {8},
keywords = {typical development, atypical development, Russian language, speech ontogenesis, child speech acoustics},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@article{10.1109/TASLP.2019.2942439,
author = {Arnela, Marc and Dabbaghchian, Saeed and Guasch, Oriol and Engwall, Olov},
title = {MRI-Based Vocal Tract Representations for the Three-Dimensional Finite Element Synthesis of Diphthongs},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2942439},
doi = {10.1109/TASLP.2019.2942439},
abstract = {The synthesis of diphthongs in three-dimensions 3D involves the simulation of acoustic waves propagating through a complex 3D vocal tract geometry that deforms over time. Accurate 3D vocal tract geometries can be extracted from Magnetic Resonance Imaging MRI, but due to long acquisition times, only static sounds can be currently studied with an adequate spatial resolution. In this work, 3D dynamic vocal tract representations are built to generate diphthongs, based on a set of cross-sections extracted from MRI-based vocal tract geometries of static vowel sounds. A diphthong can then be easily generated by interpolating the location, orientation and shape of these cross-sections, thus avoiding the interpolation of full 3D geometries. Two options are explored to extract the cross-sections. The first one is based on an adaptive grid AG, which extracts the cross-sections perpendicular to the vocal tract midline, whereas the second one resorts to a semi-polar grid SPG strategy, which fixes the cross-section orientations. The finite element method FEM has been used to solve the mixed wave equation and synthesize diphthongs [${alpha i}$] and [${alpha u}$] in the dynamic 3D vocal tracts. The outputs from a 1D acoustic model based on the Transfer Matrix Method have also been included for comparison. The results show that the SPG and AG provide very close solutions in 3D, whereas significant differences are observed when using them in 1D. The SPG dynamic vocal tract representation is recommended for 3D simulations because it helps to prevent the collision of adjacent cross-sections.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {2173–2182},
numpages = {10}
}

@inproceedings{10.1145/3447548.3467176,
author = {Hao, Xiaobo and Liu, Yudan and Xie, Ruobing and Ge, Kaikai and Tang, Linyao and Zhang, Xu and Lin, Leyu},
title = {Adversarial Feature Translation for Multi-Domain Recommendation},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467176},
doi = {10.1145/3447548.3467176},
abstract = {Real-world super platforms such as Google and WeChat usually have different recommendation scenarios to provide heterogeneous items for users' diverse demands. Multi-domain recommendation (MDR) is proposed to improve all recommendation domains simultaneously, where the key point is to capture informative domain-specific features from all domains. To address this problem, we propose a novel Adversarial feature translation (AFT) model for MDR, which learns the feature translations between different domains under a generative adversarial network framework. Precisely, in the multi-domain generator, we propose a domain-specific masked encoder to highlight inter-domain feature interactions, and then aggregate these features via a transformer and a domain-specific attention. In the multi-domain discriminator, we explicitly model the relationships between item, domain and users' general/domain-specific representations with a two-step feature translation inspired by the knowledge representation learning. In experiments, we evaluate AFT on a public and an industrial MDR datasets and achieve significant improvements. We also conduct an online evaluation on a real-world MDR system. We further give detailed ablation tests and model analyses to verify the effectiveness of different components. Currently, we have deployed AFT on WeChat Top Stories. The source code is in https://github.com/xiaobocser/AFT.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2964–2973},
numpages = {10},
keywords = {cross-domain recommendation, multi-domain recommendation, recommender system, GAN, feature transfer},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3383652.3423874,
author = {Alexanderson, Simon and Sz\'{e}kely, \'{E}va and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
title = {Generating Coherent Spontaneous Speech and Gesture from Text},
year = {2020},
isbn = {9781450375863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383652.3423874},
doi = {10.1145/3383652.3423874},
abstract = {Embodied human communication encompasses both verbal (speech) and non-verbal information (e.g., gesture and head movements). Recent advances in machine learning have substantially improved the technologies for generating synthetic versions of both of these types of data: On the speech side, text-to-speech systems are now able to generate highly convincing, spontaneous-sounding speech using unscripted speech audio as the source material. On the motion side, probabilistic motion-generation methods can now synthesise vivid and lifelike speech-driven 3D gesticulation. In this paper, we put these two state-of-the-art technologies together in a coherent fashion for the first time. Concretely, we demonstrate a proof-of-concept system trained on a single-speaker audio and motion-capture dataset, that is able to generate both speech and full-body gestures together from text input. In contrast to previous approaches for joint speech-and-gesture generation, we generate full-body gestures from speech synthesis trained on recordings of spontaneous speech from the same person as the motion-capture data. We illustrate our results by visualising gesture spaces and textspeech-gesture alignments, and through a demonstration video.},
booktitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
articleno = {1},
numpages = {3},
keywords = {Gesture synthesis, text-to-speech, neural networks},
location = {Virtual Event, Scotland, UK},
series = {IVA '20}
}

@inproceedings{10.1145/3341162.3343841,
author = {Kim, Auk and Choi, Woohyeok and Park, Jungmi and Kim, Kyeyoon and Lee, Uichin},
title = {Predicting Opportune Moments for In-Vehicle Proactive Speech Services},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3343841},
doi = {10.1145/3341162.3343841},
abstract = {Auditory-verbal or speech interactions with in-vehicle information systems have became increasingly popular. This opens up a whole new realm of possibilities for serving drivers with proactive speech services such as contextualized recommendations and interactive decision-making. However, prior studies have warned that such interactions may consume considerable attentional resources, thus degrade driving performance. This work aims to develop a machine learning model that can find opportune moments for the driver to engage in proactive speech interaction by using the vehicle and environment sensor data. Our machine learning analysis shows that opportune moments for interruption can be conservatively inferred with an accuracy of 0.74.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {101–104},
numpages = {4},
keywords = {interruptibility, speech interface, human-vehicle interaction, in-vehicle information system, speech-based interaction, auditory-verbal interface},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3368926.3369662,
author = {Asakura, Takuya and Akama, Shunsuke and Shimokawara, Eri and Yamaguchi, Toru and Yamamoto, Shoji},
title = {Emotional Speech Generator by Using Generative Adversarial Networks},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369662},
doi = {10.1145/3368926.3369662},
abstract = {In this paper, we propose an affective voice conversion method that can generate an emotional phonation from neutral speech by using cycle-consistent generative adversarial networks (CycleGAN). Our method uses the Mel-cepstral coefficients (MCEPs), which are extracted from speech signal as an input. Next, we apply the modified network model which is comprised two components with a generator and discriminator. In this generator network, the pairing structure with an encoder and decoder is used for an accurate and fast calculation in the learning process. Furthermore, we construct two types of encoder; the one equips a content encoder for the linguistic-information, and the other equips a domain encoder for the emotional-information. This separation is enable to reproduce the smooth speech with emotional information. Finally, we evaluate the emotion expression and sound quality of speeches by using the subjective evaluation for an accuracy of emotional change. As the result, although it is necessary to improve the deterioration of sound quality, our method has accomplished to convert the emotion of the speech.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {9–14},
numpages = {6},
keywords = {Mean Opinion Score, Deep Learning, Emotional speech, Cycle-GAN},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@inproceedings{10.1145/3584954.3584996,
author = {Pedersen, Jens Egholm and Singhal, Raghav and Conradt, Jorg},
title = {Translation and Scale Invariance for Event-Based Object Tracking},
year = {2023},
isbn = {9781450399470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584954.3584996},
doi = {10.1145/3584954.3584996},
abstract = {Without temporal averaging, such as rate codes, it remains challenging to train spiking neural networks for temporal regression tasks. In this work, we present a novel method to accurately predict spatial coordinates from event data with a fully spiking convolutional neural network (SCNN) without temporal averaging. Our method performs on-par with artificial neural networks (ANN) of similar complexity. Additionally, we demonstrate faster convergence in half the time using translation- and scale-invariant receptive fields. To permit comparison with conventional frame-based ANNs, we base our results on a simulated event-based dataset with an unrealistic high density. Therefore, we hypothesize that our method significantly outperform ANNs in settings with lower event density, as seen in real-life event-based data. Our model is fully spiking and can be ported directly to neuromorphic hardware.},
booktitle = {Proceedings of the 2023 Annual Neuro-Inspired Computational Elements Conference},
pages = {79–85},
numpages = {7},
keywords = {coordinate regression, event-based vision, spiking neural networks, object tracking},
location = {San Antonio, TX, USA},
series = {NICE '23}
}

@inproceedings{10.1145/3234695.3236354,
author = {Ballati, Fabio and Corno, Fulvio and De Russis, Luigi},
title = {Assessing Virtual Assistant Capabilities with Italian Dysarthric Speech},
year = {2018},
isbn = {9781450356503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3236354},
doi = {10.1145/3234695.3236354},
abstract = {The usage of smartphone-based virtual assistants (e.g., Siri or Google Assistant) is growing, and their spread has generally a positive impact on device accessibility, e.g., for people with disabilities. However, people with dysarthria or other speech impairments may be unable to use these virtual assistants with proficiency. This paper investigates to which extent people with ALS-induced dysarthria can be understood and get consistent answers by three widely used smartphone-based assistants, namely Siri, Google Assistant, and Cortana. We focus on the recognition of Italian dysarthric speech, to study the behavior of the virtual assistants with this specific population for which no relevant studies are available. We collected and recorded suitable speech samples from people with dysarthria in a dedicated center of the Molinette hospital, in Turin, Italy. Starting from those recordings, the differences between such assistants, in terms of speech recognition and consistency in answer, are investigated and discussed. Results highlight different performance among the virtual assistants. For speech recognition, Google Assistant is the most promising, with around 25% of word error rate per sentence. Consistency in answer, instead, sees Siri and Google Assistant provide coherent answers around 60% of times.},
booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {93–101},
numpages = {9},
keywords = {accessibility, automatic speech recognition, dysarthria, conversational assistant, speech impairment},
location = {Galway, Ireland},
series = {ASSETS '18}
}

@inproceedings{10.1145/3450618.3469176,
author = {Pandey, Laxmi and Sabbir Arif, Ahmed},
title = {Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in Real-Time MRI},
year = {2021},
isbn = {9781450383714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450618.3469176},
doi = {10.1145/3450618.3469176},
abstract = {We propose a novel deep neural network-based learning framework that understands acoustic information in the variable-length sequence of vocal tract shaping during speech production, captured by real-time magnetic resonance imaging (rtMRI), and translate it into text. In an experiment, it achieved a 40.6% PER at sentence-level, much better compared to the existing models. We also performed an analysis of variations in the geometry of articulation in each sub-regions of the vocal tract with respect to different emotions and genders. Results suggest that each sub-regions distortion is affected by both emotion and gender.},
booktitle = {ACM SIGGRAPH 2021 Posters},
articleno = {27},
numpages = {2},
keywords = {neural network, vocal tract, silent speech, speech, MRI, accessibility},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3495018.3495381,
author = {Zhai, Lian},
title = {Application of Artificial Intelligence Technology in Tourism English Translation},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495381},
doi = {10.1145/3495018.3495381},
abstract = {Tourism, especially foreign tourism, is a typical cross-cultural communication activity. Tourism English translation involves all aspects of cultural differences between China and the West. Based on the analysis of the four main factors affecting tourism English translation, as well as the differences between Chinese and Western culture and Tourism English rhetoric, this paper puts forward some constructive suggestions to further improve the accuracy of tourism information translation, so that tourists from all over the world can better understand China. This paper briefly introduces the importance and practical significance of improving the quality of tourism English translation from a cross-cultural perspective. This paper puts forward several common types of tourism English translation, including transliteration, free translation, translation. Based on this premise, this paper discusses the specific strategies to improve the quality of tourism English translation from a cross-cultural perspective. From the perspective of personnel training in Colleges and universities, this paper innovates the learning mode and optimizes the learning mechanism, aiming to provide reference for related work. The results show that the maximum F value of C algorithm is 39.58%, a algorithm is 44.92%, D algorithm is 54.58%.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1274–1278},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1109/TASLP.2021.3110146,
author = {Kourkounakis, Tedd and Hajavi, Amirhossein and Etemad, Ali},
title = {FluentNet: End-to-End Detection of Stuttered Speech Disfluencies With Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3110146},
doi = {10.1109/TASLP.2021.3110146},
abstract = {Millions of people are affected by stuttering and other speech disfluencies, with the majority of the world having experienced mild stutters while communicating under stressful conditions. While there has been much research in the field of automatic speech recognition and language models, stutter detection and recognition has not received as much attention. To this end, we propose an end-to-end deep neural network, FluentNet, capable of detecting a number of different stutter types. FluentNet consists of a Squeeze-and-Excitation Residual convolutional neural network which facilitate the learning of strong spectral frame-level representations, followed by a set of bidirectional long short-term memory layers that aid in learning effective temporal relationships. Lastly, FluentNet uses an attention mechanism to focus on the important parts of speech to obtain a better performance. We perform a number of different experiments, comparisons, and ablation studies to evaluate our model. Our model achieves state-of-the-art results by outperforming other solutions in the field on the publicly available UCLASS dataset. Additionally, we present LibriStutter: a stuttered speech dataset based on the public LibriSpeech dataset with synthesized stutters. We also evaluate FluentNet on this dataset, showing the strong performance of our model versus a number of baseline and state-of-the-art techniques.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2986–2999},
numpages = {14}
}

@inproceedings{10.1145/3462244.3481274,
author = {Li, Yuanchao},
title = {Semi-Supervised Learning for Multimodal Speech and Emotion Recognition},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3481274},
doi = {10.1145/3462244.3481274},
abstract = {Speech Emotion Recognition (SER) is becoming necessary for interactive spoken dialogue systems as users are expecting empathy from computers. Recent work has shown the importance of approaching this problem from a multimodal perspective, with models that combine visual, acoustic, and lexical features performing better than models based on single modalities. However, current SER models are not robust to out of domain data, partly due to the fact that emotion labeled corpora are generally small. This paper outlines my PhD research plan that aims to improve the SER model by proposing to jointly train with an Automatic Speech Recognition (ASR) model using a novel cross-task semi-supervised learning approach on unlabeled data. The ASR model would be benefit from the training approach and serve as the lexical features provider. This joint ASR-SER model is expected to alleviate the lack of data problem and to be applied in real-life applications such as human-computer interaction and digital health.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {817–821},
numpages = {5},
keywords = {semi-supervised learning, audio-visual speech recognition, multimodal fusion, speech emotion recognition},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@article{10.1109/TASLP.2021.3133196,
author = {Hou, Mixiao and Zhang, Zheng and Cao, Qi and Zhang, David and Lu, Guangming},
title = {Multi-View Speech Emotion Recognition Via Collective Relation Construction},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3133196},
doi = {10.1109/TASLP.2021.3133196},
abstract = {Automatic emotion recognition from speech plays a fundamental role towards advanced emotional intelligence in human-machine interaction systems. The discriminative knowledge from speech for effective emotion recognition may come from multiple physical properties such as energy spectrum, frequency, prosody, which could be collected as multi-view representations. However, the current works fail to fully explore the underlying interactive relations among multiple speech representations for emotion recognition. In this paper, we propose a novel Collective Multi-view Relation Network (CMRN) to exploit the intrinsic characteristics of multi-view speech representations for discriminative speech emotion recognition. Generally, the proposed CMRN consists of three sub-networks, <italic>i.e.,</italic> view-specific attention network, multi-view shared attention network and collective relation network. Specifically, the view-specific attention network is designed to excavate the distinguishable view-specific features deduced from the original speech. By contrast, the multi-view shared attention network is conceived to capture the collaborative knowledge from multiple views. Moreover, a well-designed collective relation network is explicitly constructed to characterize the shared-specific correlations, which could reflect the underlying physical interaction capabilities. As such, the decision phase can comprehensively leverage the shared and view-specific information of multiple representations, such that the final privileged deciding principle can aggregate the heterogeneous information of multi-view features to make accurate emotion recognition. Extensive experiments on two benchmark datasets demonstrate the superb performance of the proposed method in comparison with some state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {218–229},
numpages = {12}
}

@inproceedings{10.1145/3319502.3374801,
author = {Velner, Ella and Boersma, Paul P.G. and de Graaf, Maartje M.A.},
title = {Intonation in Robot Speech: Does It Work the Same as with People?},
year = {2020},
isbn = {9781450367462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319502.3374801},
doi = {10.1145/3319502.3374801},
abstract = {Human-robot interaction (HRI) research aims to design natural interactions between humans and robots. Intonation, a social signaling function in human speech investigated thoroughly in linguistics, has not yet been studied in HRI. This study investigates the effect of robot speech intonation in four conditions (no intonation, focus intonation, end-of-utterance intonation, or combined intonation) on conversational naturalness, social engagement, and people's humanlike perception of the robot collecting objective and subjective data of participant conversations (n = 120). Our results showed that humanlike intonation partially improved subjective naturalness but not observed fluency, and that intonation partially improved social engagement but did not affect humanlike perceptions of the robot. Given that our results mainly differed from our hypotheses based on human speech intonation, we discuss the implications and provide suggestions for future research to further investigate conversational naturalness in robot speech intonation.},
booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {569–578},
numpages = {10},
keywords = {turn taking, conversation analysis, linguistics, speech intonation, human-robot interaction},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@article{10.1109/TASLP.2019.2898816,
author = {Lotfian, Reza and Busso, Carlos},
title = {Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2898816},
doi = {10.1109/TASLP.2019.2898816},
abstract = {This study introduces a method to design a curriculum for machine-learning to maximize the efficiency during the training process of deep neural networks DNNs for speech emotion recognition. Previous studies in other machine-learning problems have shown the benefits of training a classifier following a curriculum where samples are gradually presented in increasing level of difficulty. For speech emotion recognition, the challenge is to establish a natural order of difficulty in the training set to create the curriculum. We address this problem by assuming that, ambiguous samples for humans are also ambiguous for computers. Speech samples are often annotated by multiple evaluators to account for differences in emotion perception across individuals. While some sentences with clear emotional content are consistently annotated, sentences with more ambiguous emotional content present important disagreement between individual evaluations. We propose to use the disagreement between evaluators as a measure of difficulty for the classification task. We propose metrics that quantify the inter-evaluation agreement to define the curriculum for regression problems and binary and multi-class classification problems. The experimental results consistently show that relying on a curriculum based on agreement between human judgments leads to statistically significant improvements over baselines trained without a curriculum.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {815–826},
numpages = {12}
}

@inproceedings{10.1145/3587828.3587872,
author = {Rakib, Mohammed and Hossain, Md. Ismail and Mohammed, Nabeel and Rahman, Fuad},
title = {Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-Gram Language Models},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587872},
doi = {10.1145/3587828.3587872},
abstract = {Although over 300M around the world speak Bangla, scant work has been done in improving Bangla voice-to-text transcription due to Bangla being a low-resource language. However, with the introduction of the Bengali Common Voice 9.0 speech dataset, Automatic Speech Recognition (ASR) models can now be significantly improved. With 399hrs of speech recordings, Bengali Common Voice is the largest and most diversified open-source Bengali speech corpus in the world. In this paper, we outperform the State-of-the-Art (SOTA) pretrained Bengali ASR models by finetuning a pretrained wav2vec2 model on the common voice dataset. We also demonstrate how to significantly improve the performance of an ASR model by adding an n-gram language model as a post-processor. Finally, we do some experiments and hyperparameter tuning to generate a robust Bangla ASR model that is better than the existing ASR models.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {297–301},
numpages = {5},
keywords = {Automatic Speech Recognition, Deep Neural Networks, Speech Signal Processing, Bangla Speech Recognition, Speech-to-Text},
location = {Kuantan, Malaysia},
series = {ICSCA '23}
}

@inproceedings{10.1145/3482632.3482714,
author = {Qiu, Jing},
title = {Development and Application of English Translation Software Based on Computer Corpus},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482714},
doi = {10.1145/3482632.3482714},
abstract = {On the practical level, bilingual dictionary writing, translation teaching, interpreter training and translation assistance can be carried out through parallel corpora. In English teaching, we should make effective use of corpora, take students as the teaching subject, and teachers should be problem-oriented, and spread students' thinking, so that students can discuss corpora by observing corpora, forming a cooperative relationship with others, knowing the background of problems, and then solving corpus problems. With the continuous development of modern computer software technology, the development and use of English translation software has attracted people's attention. In order to effectively improve the accuracy of modern electronic dictionary translation, this paper analyzes the modern English teaching strategies and the design ideas of English translation software based on computer corpus.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {385–389},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1145/3568673,
author = {Chopra, Abhishek and Sharma, Deepak Kumar and Jha, Aashna and Ghosh, Uttam},
title = {A Framework for Online Hate Speech Detection on Code-Mixed Hindi-English Text and Hindi Text in Devanagari},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3568673},
doi = {10.1145/3568673},
abstract = {Social Media has been growing and has provided the world with a platform to opine, debate, display, and discuss like never before. It has a major influence in research areas that analyze human behavior and social groups, and the phenomenon of social interactions is even being used in areas such as Internet of Things. This constant stream of data connecting individuals and organizations across the globe has had a tremendous impact on the functioning of society and even has the power to sway elections. Despite having numerous benefits, social media has certain issues such as the prevalence of fake news, which has also led to the rise of the hate speech phenomenon. Due to lax security throughout these social media platforms, these issues continue to exist without any repercussions. This leads to cyberbullying, defamation, and presents grave security concerns. Even though some work has been done independently on native scripts, hate speech detection, and code-mixed data, there exists a lack of academic work and research in the area of detecting hate speech in transliterated code-mixed data and in-text containing native language scripts. Research in this field is inhibited greatly due to the multiple variations in grammar and spelling and in general a lack of availability of annotated datasets, especially when it comes to native languages. This article comes up with a method to automate hate speech detection in code-mixed and native language text. The article presents an architecture containing a Tabnet classifier-based model trained on features extracted using MuRIL from transliterated code-mixed textual data. The article also shows that the same model works well on features extracted from text in Devanagari despite being trained on transliterated data.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {134},
numpages = {21},
keywords = {Hindi text classification, Cyber security, hate speech detection, cyber security systems, natural language processing, TabNet, code-mixing}
}

@inproceedings{10.1145/3548636.3548648,
author = {Zeng, Rui and Xiong, Shengwu},
title = {Lip to Speech Synthesis Based on Speaker Characteristics Feature Fusion},
year = {2022},
isbn = {9781450396820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548636.3548648},
doi = {10.1145/3548636.3548648},
abstract = {Lip to speech synthesis (Lip2Speech) is a technology that reconstructs speech from the silent talking face video. With the development of deep learning, achievements have been made in this field. Due to the silent talking face video does not contain the speaker characteristics information, reconstructing speech directly from the silent talking video will lose the characteristic information of the speaker, thus reducing the quality of the reconstructed speech. In this paper we proposed a new framework using the pre-trained speaker encoder network which extract the speaker characteristics information. More specially: (1) The pretrained speaker encoder network generates a fixed-dimensional embedding vector from a few seconds of given speaker's speech, which contains the speaker characteristics information, (2) The content encoder uses a stack of 3D convolutions to extracts the content information of the video, (3) a sequence-to-sequence synthesis network based on Tacotron2 that generates Mel-spectrogram from silent video, conditioned on the speaker's identity embedding. Experimental results show that, using the pretrained speaker encoder can improved the speech reconstruction quality.},
booktitle = {Proceedings of the 4th International Conference on Information Technology and Computer Communications},
pages = {78–83},
numpages = {6},
keywords = {multi-speaker speech synthesis, feature fusion, speaker characteristic information, lip to speech synthesis},
location = {Guangzhou, China},
series = {ITCC '22}
}

@article{10.1109/TASLP.2020.3040523,
author = {Liu, Rui and Sisman, Berrak and Bao, Feilong and Yang, Jichen and Gao, Guanglai and Li, Haizhou},
title = {Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for Mongolian Speech Synthesis},
year = {2020},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3040523},
doi = {10.1109/TASLP.2020.3040523},
abstract = {Prosodic phrasing is an important factor that affects naturalness and intelligibility in text-to-speech synthesis. Studies show that deep learning techniques improve prosodic phrasing when large text and speech corpus are available. However, for low-resource languages, such as Mongolian, prosodic phrasing remains a challenge for various reasons. First, the database suitable for system training is limited. Second, word composition knowledge that is prosody-informing has not been used in prosodic phrase modeling. To address these problems, in this article, we propose a feature augmentation method in conjunction with a self-attention neural classifier. We augment input text with morphological and phonological decompositions of words to enhance the text encoder. We study the use of self-attention classifier, that makes use of global context of a sentence, as a decoder for phrase break prediction. Both objective and subjective evaluations validate the effectiveness of the proposed phrase break prediction framework, that consistently improves voice quality in a Mongolian text-to-speech synthesis system.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {274–285},
numpages = {12}
}

@inproceedings{10.1145/3534678.3539268,
author = {Qu, Xinghua and Wei, Pengfei and Gao, Mingyong and Sun, Zhu and Ong, Yew Soon and Ma, Zejun},
title = {Synthesising Audio Adversarial Examples for Automatic Speech Recognition},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539268},
doi = {10.1145/3534678.3539268},
abstract = {Adversarial examples in automatic speech recognition (ASR) are naturally sounded by humans yet capable of fooling well trained ASR models to transcribe incorrectly. Existing audio adversarial examples are typically constructed by adding constrained perturbations on benign audio inputs. Such attacks are therefore generated with an audio dependent assumption. For the first time, we propose the Speech Synthesising based Attack (SSA), a novel threat model that constructs audio adversarial examples entirely from scratch, i.e., without depending on any existing audio to fool cutting-edge ASR models. To this end, we introduce a conditional variational auto-encoder (CVAE) as the speech synthesiser. Meanwhile, an adaptive sign gradient descent algorithm is proposed to solve the adversarial audio synthesis task. Experiments on three datasets (i.e., Audio Mnist, Common Voice, and Librispeech) show that our method could synthesise naturally sounded audio adversarial examples to mislead the start-of-the-art ASR models. Our web-page containing generated audio demos is at https://sites.google.com/view/ssa-asr/home.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1430–1440},
numpages = {11},
keywords = {adversarial attack, automatic speech recognition, speech synthesis},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.5555/3310435.3310615,
author = {Bringmann, Karl and K\"{u}nnemann, Marvin and Nusser, Andr\'{e}},
title = {Fr\'{e}chet Distance under Translation: Conditional Hardness and an Algorithm via Offline Dynamic Grid Reachability},
year = {2019},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The discrete Fr\'{e}chet distance is a popular measure for comparing polygonal curves. An important variant is the discrete Fr\'{e}chet distance under translation, which enables detection of similar movement patterns in different spatial domains. For polygonal curves of length n in the plane, the fastest known algorithm runs in time \~{O}(n5) [Ben Avraham, Kaplan, Sharir '15]. This is achieved by constructing an arrangement of disks of size O(n4), and then traversing its faces while updating reachability in a directed grid graph of size N = O(n2), which can be done in time [MATH HERE] per update [Diks, Sankowski '07]. The contribution of this paper is two-fold.First, although it is an open problem to solve dynamic reachability in directed grid graphs faster than [MATH HERE], we improve this part of the algorithm: We observe that an offline variant of dynamic s−t-reachability in directed grid graphs suffices, and we solve this variant in amortized time \~{O}(N1/3) per update, resulting in an improved running time of \~{O}(n4.66. . .) for the discrete Fr\'{e}chet distance under translation. Second, we provide evidence that constructing the arrangement of size O(n4) is necessary in the worst case, by proving a conditional lower bound of n4−−o(1) on the running time for the discrete Fr\'{e}chet distance under translation, assuming the Strong Exponential Time Hypothesis.},
booktitle = {Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {2902–2921},
numpages = {20},
location = {San Diego, California},
series = {SODA '19}
}

@article{10.1109/TASLP.2020.3030494,
author = {Boulianne, Gilles},
title = {A Study of Inductive Biases for Unsupervised Speech Representation Learning},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3030494},
doi = {10.1109/TASLP.2020.3030494},
abstract = {Distributed representations, or embeddings, are commonly learned without supervision on very large unannotated corpora for natural language processing. In speech processing, deep network-based representations such as bottlenecks and x-vectors have had some success,but are limited to supervised or partly supervised settings where annotations are available and are not optimized to separate underlying factors. Here, we propose a generative model with deep encoders and decoders that can learn interpretable speech representations without supervision. Our inductive biases operate as prior distributions in a variational autoencoder model and allow us to separate several latent variables along a continuous range of time-scale properties, as opposed to binary oppositions or hierarchical factorization that have been previously proposed. On simulated data, we confirm that these biases enable the model to accurately recover phonetic and speaker underlying factors. On TIMIT and LibriSpeech, they yield representations that separate phonetic and speaker information, as evidenced by unsupervised results on downstream phoneme and speaker classification tasks using a simple k-means classifier.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2781–2795},
numpages = {15}
}

@article{10.1109/TASLP.2022.3161153,
author = {Ragni, Anton and Gales, Mark J. F. and Rose, Oliver and Knill, Katherine M. and Kastanos, Alexandros and Li, Qiujia and Ness, Preben M.},
title = {Increasing Context for Estimating Confidence Scores in Automatic Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3161153},
doi = {10.1109/TASLP.2022.3161153},
abstract = {Accurate confidence measures for predictions from machine learning techniques play a critical role in the deployment and training of many speech and language processing applications. For example, confidence scores are important when making use of automatically generated transcriptions in training automatic speech recognition (ASR) systems, as well as down-stream applications, such as information retrieval and conversational assistants. Previous work on improving confidence scores for these systems has focused on two main directions: designing features correlated with improved confidence prediction; and employing sequence models to account for the importance of contextual information. Few studies, however, have explored incorporating contextual information more broadly, such as from the future, in addition to the past, or making use of alternative multiple hypotheses in addition to the most likely one. This article introduces two general approaches for encapsulating contextual information from lattices. Experimental results illustrating the importance of increasing contextual information for estimating confidence scores are presented on a range of limited resource languages where word error rates range between 30% and 60%. The results show that the novel approaches provide significant gains in the accuracy of confidence estimation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1319–1329},
numpages = {11}
}

@inproceedings{10.1145/3474085.3475692,
author = {Khan, Zaid and Fu, Yun},
title = {Exploiting BERT for Multimodal Target Sentiment Classification through Input Space Translation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475692},
doi = {10.1145/3474085.3475692},
abstract = {Multimodal target/aspect sentiment classification combines multimodal sentiment analysis and aspect/target sentiment classification. The goal of the task is to combine vision and language to understand the sentiment towards a target entity in a sentence. Twitter is an ideal setting for the task because it is inherently multimodal, highly emotional, and affects real world events. However, multimodal tweets are short and accompanied by complex, possibly irrelevant images. We introduce a two-stream model that translates images in input space using an object-aware transformer followed by a single-pass non-autoregressive text generation approach. We then leverage the translation to construct an auxiliary sentence that provides multimodal information to a language model. Our approach increases the amount of text available to the language model and distills the object-level information in complex images. We achieve state-of-the-art performance on two multimodal Twitter datasets without modifying the internals of the language model to accept multimodal data, demonstrating the effectiveness of our translation. In addition, we explain a failure mode of a popular approach for aspect sentiment analysis when applied to tweets. Our code is available at https://github.com/codezakh/exploiting-BERT-thru-translation.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3034–3042},
numpages = {9},
keywords = {bert, twitter, vision-language, sentiment analysis, deep learning},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1109/TASLP.2019.2933146,
author = {Sun, Sining and Guo, Pengcheng and Xie, Lei and Hwang, Mei-Yuh},
title = {Adversarial Regularization for Attention Based End-to-End Robust Speech Recognition},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933146},
doi = {10.1109/TASLP.2019.2933146},
abstract = {End-to-end speech recognition, such as attention based approaches, is an emerging and attractive topic in recent years. It has achieved comparable performance with the traditional speech recognition framework. Because end-to-end approaches integrate acoustic and linguistic information into one model, the perturbation in the acoustic level such as acoustic noise, could be easily propagated to the linguistic level. Thus improving model robustness in real application environments for these end-to-end systems is crucial. In this paper, in order to make the attention based end-to-end model more robust against noises, we formulate regulation of the objective function with adversarial training examples. Particularly two adversarial regularization techniques, the fast gradient-sign method and the local distributional smoothness method, are explored to improve noise robustness. Experiments on two publicly available Chinese Mandarin corpora, AISHELL-1 and AISHELL-2, show that adversarial regularization is an effective approach to improve robustness against noises for our attention-based models. Specifically, we obtained 18.4% relative character error rate CER reduction on the AISHELL-1 noisy test set. Even on the clean test set, we showed 16.7% relative improvement. As the training set increases and covers more environmental varieties, our proposed methods remain effective despite that the improvement shrinks. Training on the large AISHELL-2 training corpus and testing on the various AISHELL-2 test sets, we achieved 7.0%–12.2% relative error rate reduction. To our knowledge, this is the first successful application of adversarial regularization to sequence-to-sequence speech recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {1826–1838},
numpages = {13}
}

@inproceedings{10.1145/3482632.3483055,
author = {Han, Fang},
title = {Analysis of Spanish Teaching Mode under Internet},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483055},
doi = {10.1145/3482632.3483055},
abstract = {In recent years, Spanish majors have been popularized in higher vocational colleges and have become an important part of Spanish professional education in higher education in China. Spanish higher vocational education can provide more Spanish talents for enterprises and industries to meet the needs of the market. Therefore, strengthening the research on the teaching management of Spanish majors in higher vocational colleges plays an important role in theory and practice. According to the survey, there are more than 30 native speakers in the world, and nearly 500 million people in the world use Spanish, while only more than 40 institutions of higher learning offer Spanish. With the increasing economic and trade exchanges between China and Spain and Latin America, the demand for Spanish talents will increase. Although the scale of Spanish teaching in China has made great progress, it is far from meeting the needs of Spanish talents in the market. Nowadays, Spanish teaching in China is in a period of rapid development, but the age distribution of teachers is uneven, young teachers are the majority, and lack of teaching experience. In addition, because Spanish teaching in colleges and universities is relatively later than other foreign languages, such as English and Japanese.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {938–942},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3573942.3573975,
author = {Ke, Dengfeng and Xie, Yanlu and Zhang, Jinsong and Huang, Liangjie},
title = {Solving Size and Performance Dilemma by Reversible and Invertible Recurrent Network for Speech Enhancement: Solving Size and Performance Dilemma by Reversible and Invertible Recurrent Network for Speech Enhancement},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573975},
doi = {10.1145/3573942.3573975},
abstract = {Reducing parameter numbers and improving system performance is considered a dilemma problem. As is known to all, reducing parameter numbers will lead to performance degradation, while improving performance often lead to parameter numbers increasing. To solve the above dilemma, we propose a reversible and invertible recurrent (RAIR) network in this paper: Firstly, we construct a reversible dual-path architecture to avoid information loss for two arbitrary functions, F and G. That is to say, no matter what kinds of F and G are and no matter how small the model is, feature maps go through the network without any information loss. Secondly, we adopt an invertible 1x1 convolution to improve channel information remixing. Lastly, we employ a dual-path recurrences (DPR) block that operates in the frequency and the time dimensions separately for the F function and a 3x3 convolution for the G function in the above reversible architecture, which reduces parameter numbers dramatically. Although the model is tiny, experiments on Voice Bank + DEMAND show that our reversible and invertible recurrent architecture improves all the performance metrics: COVL from 3.57 to 3.78, wideband PESQ from 2.94 to 3.15, and STOI from 0.947 to 0.951. The proposed model achieves state-of-the-art results with only 190K parameters. To the best of our knowledge, it is the state-of-the-art model with the smallest size.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {956–962},
numpages = {7},
keywords = {Invertible Network, Speech Enhancement, Noise Reduction, Reversible Network},
location = {Xiamen, China},
series = {AIPR '22}
}

@article{10.1109/TASLP.2022.3224285,
author = {Wang, Zhong-Qiu and Wichern, Gordon and Watanabe, Shinji and Le Roux, Jonathan},
title = {STFT-Domain Neural Speech Enhancement With Very Low Algorithmic Latency},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3224285},
doi = {10.1109/TASLP.2022.3224285},
abstract = {Deep learning based speech enhancement in the short-time Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window can lead to higher frequency resolution and potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed using the same window size. To reduce this inherent latency, we adapt a conventional dual-window-size approach, where a regular input window size is used for STFT but a shorter output window is used for overlap-add, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT-iSTFT configuration, we employ complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the DNN-predicted RI components to conduct frame-online beamforming, the results of which are used as extra features for a second DNN to perform frame-online postfiltering. The frequency-domain beamformer can be easily integrated with our DNNs and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation on noisy-reverberant speech enhancement shows the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {397–410},
numpages = {14}
}

@article{10.1109/TASLP.2022.3169634,
author = {Kumar, Neeraj and Narang, Ankur and Lall, Brejesh},
title = {Zero-Shot Normalization Driven Multi-Speaker Text to Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3169634},
doi = {10.1109/TASLP.2022.3169634},
abstract = {Text-to-speech (TTS) systems are designed to synthesize natural and expressive speech, adapt to an unseen voice, and capture the speaking style of an unseen speaker by converting text into speech. The introduction of an unseen speaker’s speaking style into a TTS system offers a wide range of application scenarios, including personal assistant, news broadcast, and audio navigation, among others. The style of the speech varies from person to person and every person exhibits his or her style of speaking that is determined by the language, demography, culture and other factors. Style is best captured by the prosody of a signal. It is an ongoing research area with numerous real-world applications that produces high-quality multi-speaker voice synthesis while taking into account prosody and in a zero-shot manner. Despite the fact that several efforts have been made in this area, it continues to be an interesting and difficult topic to solve. In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person’s style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We generate the 256 dimensional speaker embedding using a speaker encoder based on wav2vec2.0 based architecture. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK [1] and LibriTTS [2] datasets, using visualization of hessian of proposed model, multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1679–1693},
numpages = {15}
}

@article{10.1109/TASLP.2019.2933698,
author = {Elshamy, Samy and Fingscheidt, Tim},
title = {DNN-Based Cepstral Excitation Manipulation for Speech Enhancement},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2933698},
doi = {10.1109/TASLP.2019.2933698},
abstract = {This contribution aims at speech model-based speech enhancement by exploiting the source-filter model of human speech production. The proposed method enhances the excitation signal in the cepstral domain by making use of a deep neural network DNN. We investigate two types of target representations along with the significant effects of their normalization. The new approach exceeds the performance of a formerly introduced classical signal processing-based cepstral excitation manipulation CEM method in terms of noise attenuation by about 1.5&nbsp;dB. We show that this gain also holds true when comparing serial combinations of envelope and excitation enhancement. In the important low-SNR conditions, no significant trade-off for speech component quality or speech intelligibility is induced, while allowing for substantially higher noise attenuation. In total, a traditional purely statistical state-of-the-art speech enhancement system is outperformed by more than 3&nbsp;dB noise attenuation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {1803–1814},
numpages = {12}
}

@inproceedings{10.1145/3323503.3360619,
author = {Nascimento, Gabriel and Carvalho, Flavio and Cunha, Alexandre Martins da and Viana, Carlos Roberto and Guedes, Gustavo Paiva},
title = {Hate Speech Detection Using Brazilian Imageboards},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360619},
doi = {10.1145/3323503.3360619},
abstract = {With the changes in human interaction prompted by the development of communications platforms over the internet, hate speech and offensive language emerged as a contemporary problem. Social networks allow users with different opinions and backgrounds to interact without direct eye-to-eye contact. It brings a sense of safety to promote hate speech, which is even more significant in anonymous environments. There are sites called imageboards, composed of different boards aggregating different topics. On some boards, anonymous users widely promote hate speech. However, only a few works in literature have focused on hate speech in imageboards content. This work aims to classify Brazilian Portuguese texts to detect hate speech, using data from the Brazilian 55chan imageboard to build a dataset with hate speech content. Three classifiers were trained to hate speech binary classification. The Linear Support Vector Classifier achieved the best result with 0.955 of F1-score.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {325–328},
numpages = {4},
keywords = {imageboards, text mining, hate speech detection},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@article{10.1109/TASLP.2019.2925934,
author = {Xie, Yue and Liang, Ruiyu and Liang, Zhenlin and Huang, Chengwei and Zou, Cairong and Schuller, Bjorn},
title = {Speech Emotion Classification Using Attention-Based LSTM},
year = {2019},
issue_date = {November 2019},
publisher = {IEEE Press},
volume = {27},
number = {11},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2925934},
doi = {10.1109/TASLP.2019.2925934},
abstract = {Automatic speech emotion recognition has been a research hotspot in the field of human–computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory LSTM recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {nov},
pages = {1675–1685},
numpages = {11}
}

@inproceedings{10.1145/3482632.3482749,
author = {Cui, Gaili},
title = {Design of Intelligent Recognition English Translation Model Based on Feature Extraction Algorithm},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482749},
doi = {10.1145/3482632.3482749},
abstract = {In recent years, with the deepening of globalization, international cooperation is becoming more and more extensive, and the importance of English is increasing. Aiming at the problems that the semantic context of English is not obvious in the process of English translation in traditional English translation system, the optimal translation solution is not reached in the process of selecting the optimal feature semantics, and the translation accuracy is low, an intelligent recognition English translation model based on feature extraction algorithm is designed. Search module is used to complete the search of basic meaning and subject content of vocabulary to be proofread, grasp the user's behavior data through behavior log and optimize the system; In the method based on the maximum entropy principle, the whole task of clause recognition is divided into three parts: sentence head recognition, sentence tail recognition and complete clause recognition. Experimental results show that the proposed algorithm has higher recognition rate.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {553–557},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3510858.3510982,
author = {Tu, Xiangying},
title = {Design of Intelligent Recognition English Translation Model Based on Improved Apriori Algorithm},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510982},
doi = {10.1145/3510858.3510982},
abstract = {Natural language processing is a method in computer science to obtain and analyze meaning from human language and interact with human in an intelligent way. The translation recognition results of Apriori algorithm model have data points overlapping, and the accuracy can not be effectively guaranteed. In order to identify phrases accurately, an intelligent recognition model of machine translation based on improved Apriori algorithm is proposed for the low accuracy of conventional algorithm model. This algorithm creates a corpus for marking phrases, so that phrases can be searched automatically. In addition, a machine translation intelligent recognition model is created to plan the intelligent recognition model based on data collection, processing and output. The information is collected and processed in a planned way, and the feature parameters are extracted to realize the intelligent recognition of machine translation. The designed machine translation intelligent recognition model is experimentally analyzed and the experimental data are recorded. The experimental analysis shows that the designed machine translation recognition model can finish translation work. The algorithm overcomes the disadvantages of Apriori, improves the operation speed and processing performance, is more suitable for machine translation tasks, and provides a novel idea of intelligent machine translation.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {429–432},
numpages = {4},
location = {Changsha, China},
series = {ICASIT 2021}
}

@article{10.1145/3510582,
author = {Chen, Yuxuan and Zhang, Jiangshan and Yuan, Xuejing and Zhang, Shengzhi and Chen, Kai and Wang, Xiaofeng and Guo, Shanqing},
title = {SoK: A Modularized Approach to Study the Security of Automatic Speech Recognition Systems},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {2471-2566},
url = {https://doi.org/10.1145/3510582},
doi = {10.1145/3510582},
abstract = {With the wide use of Automatic Speech Recognition (ASR) in applications such as human machine interaction, simultaneous interpretation, audio transcription, and so on, its security protection becomes increasingly important. Although recent studies have brought to light the weaknesses of popular ASR systems that enable out-of-band signal attack, adversarial attack, and so on, and further proposed various remedies (signal smoothing, adversarial training, etc.), a systematic understanding of ASR security (both attacks and defenses) is still missing, especially on how realistic such threats are and how general existing protection could be. In this article, we present our systematization of knowledge for ASR security and provide a comprehensive taxonomy for existing work based on a modularized workflow. More importantly, we align the research in this domain with that on security in Image Recognition System (IRS), which has been extensively studied, using the domain knowledge in the latter to help understand where we stand in the former. Generally, both IRS and ASR are perceptual systems. Their similarities allow us to systematically study existing literature in ASR security based on the spectrum of attacks and defense solutions proposed for IRS, and pinpoint the directions of more advanced attacks and the directions potentially leading to more effective protection in ASR. In contrast, their differences, especially the complexity of ASR compared with IRS, help us learn unique challenges and opportunities in ASR security. Particularly, our experimental study shows that transfer attacks across ASR models are feasible, even in the absence of knowledge about models (even their types) and training data.},
journal = {ACM Trans. Priv. Secur.},
month = {may},
articleno = {17},
numpages = {31},
keywords = {Adversarial attacks, speech system security, machine learning security}
}

@article{10.1109/TASLP.2020.2980991,
author = {Cai, Weicheng and Chen, Jinkun and Zhang, Jun and Li, Ming},
title = {On-the-Fly Data Loader and Utterance-Level Aggregation for Speaker and Language Recognition},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980991},
doi = {10.1109/TASLP.2020.2980991},
abstract = {In this article, our recent efforts on directly modeling utterance-level aggregation for speaker and language recognition is summarized. First, an on-the-fly data loader for efficient network training is proposed. The data loader acts as a bridge between the full-length utterances and the network. It generates mini-batch samples on the fly, which allows batch-wise variable-length training and online data augmentation. Second, the traditional dictionary learning and Baum-Welch statistical accumulation mechanisms are applied to the network structure, and a learnable dictionary encoding&nbsp;(LDE) layer is introduced. The former accumulates discriminative statistics from the variable-length input sequence and outputs a single fixed-dimensional utterance-level representation. Experiments were conducted on four different datasets, namely NIST LRE&nbsp;2007, AP17-OLR, SITW, and NIST SRE&nbsp;2016. Experimental results show the effectiveness of the proposed batch-wise variable-length training with online data augmentation and the LDE layer, which significantly outperforms the baseline methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1038–1051},
numpages = {14}
}

@inproceedings{10.1145/3577163.3595112,
author = {Bhagtani, Kratika and Bartusiak, Emily R. and Yadav, Amit Kumar Singh and Bestagini, Paolo and Delp, Edward J.},
title = {Synthesized Speech Attribution Using The Patchout Spectrogram Attribution Transformer},
year = {2023},
isbn = {9798400700545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577163.3595112},
doi = {10.1145/3577163.3595112},
abstract = {The malicious use of synthetic speech has increased with the recent availability of speech generation tools. It is important to determine whether a speech signal is authentic (spoken by a human) or is synthesized and to determine the generation method used to create it. Identifying the synthesis method is known as synthetic speech attribution. In this paper, we propose the use of a transformer deep learning method that analyzes mel-spectrograms for synthetic speech attribution. Our method known as Patchout Spectrogram Attribution Transformer (PSAT) can distinguish new, unseen speech generation methods from those seen during training. PSAT demonstrates high performance in attributing synthetic speech signals. Evaluation on the DARPA SemaFor Audio Attribution Dataset and the ASVSpoof2019 Dataset shows that our method achieves more than 95% accuracy in synthetic speech attribution and performs better than existing deep learning approaches.},
booktitle = {Proceedings of the 2023 ACM Workshop on Information Hiding and Multimedia Security},
pages = {157–162},
numpages = {6},
keywords = {synthetic speech, transformers, mel-spectrograms, audio forensics, deep learning},
location = {Chicago, IL, USA},
series = {IH&amp;MMSec '23}
}

@inproceedings{10.1145/3588432.3591513,
author = {Parmar, Gaurav and Kumar Singh, Krishna and Zhang, Richard and Li, Yijun and Lu, Jingwan and Zhu, Jun-Yan},
title = {Zero-Shot Image-to-Image Translation},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591513},
doi = {10.1145/3588432.3591513},
abstract = {Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse, high-quality images. However, directly applying these models for real image editing remains challenging for two reasons. First, it is hard for users to craft a perfect text prompt depicting every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we introduce pix2pix-zero, an image-to-image translation method that can preserve the original image’s content without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the content structure, we propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. Finally, to enable interactive editing, we distill the diffusion model into a fast conditional GAN. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {11},
numpages = {11},
keywords = {Image Editing, Deep Generative Models, Diffusion Models},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3482632.3484022,
author = {Yu, Xiaoling},
title = {Construction and Application on Parallel Corpus for College Japanese Translation Teaching},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484022},
doi = {10.1145/3482632.3484022},
abstract = {The bilingual parallel corpus has the closest relationship with translation teaching. It provides abundant teaching resources and convenient teaching methods, which is conducive to the formation of relatively stable translation skills in a large number of practices. In order to carry out college Japanese translation teaching based on parallel corpus, three subsystems including parallel corpus function, corpus management, query statistics and user management, as well as several functional frameworks under each sub-subsystem are designed; the inverted index file of parallel corpus is designed which is used to improve retrieval efficiency; specific application strategies are proposed: select language materials in a standardized manner, increase language input in the teaching process, cultivate students' technical awareness, apply students’ typical mistakes or translated works to teaching feedback, and make good marks of corpus work with alignment, carry out corpus-based translation collocation teaching, and cultivate students' autonomous learning ability.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1706–1710},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1145/3414524,
author = {Khan, Muhammad Moin and Shahzad, Khurram and Malik, Muhammad Kamran},
title = {Hate Speech Detection in Roman Urdu},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3414524},
doi = {10.1145/3414524},
abstract = {Hate speech is a specific type of controversial content that is widely legislated as a crime that must be identified and blocked. However, due to the sheer volume and velocity of the Twitter data stream, hate speech detection cannot be performed manually. To address this issue, several studies have been conducted for hate speech detection in European languages, whereas little attention has been paid to low-resource South Asian languages, making the social media vulnerable for millions of users. In particular, to the best of our knowledge, no study has been conducted for hate speech detection in Roman Urdu text, which is widely used in the sub-continent. In this study, we have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, we have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, we have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {9},
numpages = {19},
keywords = {South Asian Languages, Roman Urdu, Hate speech detection, Low-resource languages}
}

@article{10.1109/TASLP.2022.3178232,
author = {Lu, Cheng and Zong, Yuan and Zheng, Wenming and Li, Yang and Tang, Chuangao and Schuller, Bj\"{o}rn W.},
title = {Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3178232},
doi = {10.1109/TASLP.2022.3178232},
abstract = {In this paper, we propose a novel domain invariant feature learning (DIFL) method to deal with speaker-independent speech emotion recognition (SER). The basic idea of DIFL is to learn the speaker-invariant emotion feature by eliminating domain shifts between the training and testing data caused by different speakers from the perspective of multi-source unsupervised domain adaptation (UDA). Specifically, we embed a hierarchical alignment layer with the strong-weak distribution alignment strategy into the feature extraction block to firstly reduce the discrepancy in feature distributions of speech samples across different speakers as much as possible. Furthermore, multiple discriminators in the discriminator block are utilized to confuse the speaker information of emotion features both inside the training data and between the training and testing data. Through them, a multi-domain invariant representation of emotional speech can be gradually and adaptively achieved by updating network parameters. We conduct extensive experiments on three public datasets, i. e., Emo-DB, eNTERFACE, and CASIA, to evaluate the SER performance of the proposed method, respectively. The experimental results show that the proposed method is superior to the state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2217–2230},
numpages = {14}
}

@inproceedings{10.1145/3342827.3342840,
author = {Komatsu, Seiya and Sasayama, Manabu},
title = {Speech Error Detection Depending on Linguistic Units},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342840},
doi = {10.1145/3342827.3342840},
abstract = {In this research, we aim at the construction of a system which detects, points out and corrects speech error (slip of the tongue) of a human speech that occurs in a dialogue system (example: Pepper, Amazon Echo, Google Home) and a human dialogue. In the present dialogue system, even if human makes a speech error, the system cannot recognize it, which could lead to broken communication. So far, we have created a system to detect speech error using deep learning. In this study, we propose a method to augmented training data used for deep learning. The training data is a corpus that collects examples of speech error. At present, the number of training data is insufficient to detect with high accuracy. Therefore, it is necessary to augment the training data. Specifically, the feature of the speech error is examined from an existing speech error corpus, and extended rules are created. The data augmentation of training data is performed by generating dialogue sentence which made the speech error based on the rule. As a result of evaluation experiment, detection accuracy was improved in LSTM model by data augmentation.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {75–79},
numpages = {5},
keywords = {Data Augmentation, Speech Error, Natural Language Processing, Corpus, Deep Learning},
location = {Tokushima, Japan},
series = {NLPIR '19}
}

@article{10.1109/TASLP.2023.3235194,
author = {Chen, Weidong and Xing, Xiaofen and Xu, Xiangmin and Pang, Jianxin and Du, Lan},
title = {SpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic Speech Processing},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3235194},
doi = {10.1109/TASLP.2023.3235194},
abstract = {Paralinguistic speech processing is important in addressing many issues, such as sentiment and neurocognitive disorder analyses. Recently, Transformer has achieved remarkable success in the natural language processing field and has demonstrated its adaptation to speech. However, previous works on Transformer in the speech field have not incorporated the properties of speech, leaving the full potential of Transformer unexplored. In this paper, we consider the characteristics of speech and propose a general structure-based framework, called SpeechFormer++, for paralinguistic speech processing. More concretely, following the component relationship in the speech signal, we design a unit encoder to model the intra- and inter-unit information (<italic>i.e.</italic>, frames, phones, and words) efficiently. According to the hierarchical relationship, we utilize merging blocks to generate features at different granularities, which is consistent with the structural pattern in the speech signal. Moreover, a word encoder is introduced to integrate word-grained features into each unit encoder, which effectively balances fine-grained and coarse-grained information. SpeechFormer++ is evaluated on the speech emotion recognition (IEMOCAP &amp; MELD), depression classification (DAIC-WOZ) and Alzheimer's disease detection (Pitt) tasks. The results show that SpeechFormer++ outperforms the standard Transformer while greatly reducing the computational cost. Furthermore, it delivers superior results compared to the state-of-the-art approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jan},
pages = {775–788},
numpages = {14}
}

@inproceedings{10.1145/3487212.3487338,
author = {Werner, Marcel Christian and Schneider, Klaus},
title = {Translation of Continuous Function Charts to Imperative Synchronous Quartz Programs},
year = {2021},
isbn = {9781450391276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487212.3487338},
doi = {10.1145/3487212.3487338},
abstract = {Programmable logic controllers operating in a sequential execution scheme are widely used for various applications in industrial environments with real-time requirements. The graphical programming languages described in the third part of IEC 61131 are often intended to perform open and closed loop control tasks. Continuous Function Charts (CFCs) represent an additional language accepted in practice which can be interpreted as an extension of IEC 61131-3 Function Block Diagrams. Those charts allow more flexible positioning and interconnection of function blocks, but can quickly become difficult to manage. Furthermore, the sequential execution order forces a sequential processing of possible independent and thus possibly parallel program paths. The question arises whether a translation of existing CFCs to synchronous programs considering independent actions can lead to a more manageable software model. While current formalization approaches for CFCs primarily focus on verification, the focus of this approach is on restructuring and possible reuse in engineering. This paper introduces a possible automated translation of CFCs to imperative synchronous Quartz programs and outlines the potential for reducing the states of equivalent extended finite state machines through restructuring.},
booktitle = {Proceedings of the 19th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {104–110},
numpages = {7},
keywords = {formal languages, synchronous languages, software restructuring, software reusability, programmable logic controllers},
location = {Virtual Event, China},
series = {MEMOCODE '21}
}

@article{10.1109/TASLP.2020.3030500,
author = {Li, Zuchao and Guan, Chaoyu and Zhao, Hai and Wang, Rui and Parnow, Kevin and Zhang, Zhuosheng},
title = {Memory Network for Linguistic Structure Parsing},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3030500},
doi = {10.1109/TASLP.2020.3030500},
abstract = {Memory-based learning can be characterized as a lazy learning method in machine learning terminology because it delays the processing of input by storing the input until needed. Linguistic structure parsing, which has been in a performance improvement bottleneck since the latest series of works was presented, determines the syntactic or semantic structure of a sentence. In this article, we construct a memory component and use it to augment a linguistic structure parser which allows the parser to directly extract patterns from the known training treebank to form memory. The experimental results show that existing state-of-the-art parsers reach new heights of performance on the main benchmarks for dependency parsing and semantic role labeling with this memory network.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2743–2755},
numpages = {13}
}

@article{10.1109/TASLP.2022.3171965,
author = {Fan, Weiquan and Xu, Xiangmin and Cai, Bolun and Xing, Xiaofen},
title = {ISNet: Individual Standardization Network for Speech Emotion Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3171965},
doi = {10.1109/TASLP.2022.3171965},
abstract = {Speech emotion recognition plays an essential role in human-computer interaction. However, cross-individual representation learning and individual-agnostic systems are challenging due to the distribution deviation caused by individual differences. The existing related approaches mostly use the auxiliary task of speaker recognition to eliminate individual differences. Unfortunately, although these methods can reduce interindividual voiceprint differences, it is difficult to dissociate interindividual expression differences since each individual has its unique expression habits. In this paper, we propose an individual standardization network (ISNet) for speech emotion recognition to alleviate the problem of interindividual emotion confusion caused by individual differences. Specifically, we model individual benchmarks as representations of nonemotional neutral speech, and ISNet realizes individual standardization using the automatically generated benchmark, which improves the robustness of individual-agnostic emotion representations. In response to individual differences, we also propose more comprehensive and meaningful individual-level evaluation metrics. In addition, we continue our previous work to construct a challenging large-scale speech emotion dataset (LSSED). We propose a more reasonable division method of the training set and testing set to prevent individual information leakage. Experimental results on datasets of both large and small scales have proven the effectiveness of ISNet, and the new state-of-the-art performance is achieved under the same experimental conditions on IEMOCAP and LSSED.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {1803–1814},
numpages = {12}
}

@article{10.1145/3414685.3417838,
author = {Yoon, Youngwoo and Cha, Bok and Lee, Joo-Haeng and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
title = {Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417838},
doi = {10.1145/3414685.3417838},
abstract = {For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {222},
numpages = {16},
keywords = {co-speech gesture, nonverbal behavior, evaluation of a generative model, multimodality, neural generative model}
}

@inproceedings{10.1145/3242969.3243683,
author = {Sertolli, Benjamin and Cummins, Nicholas and Sengur, Abdulkadir and Schuller, Bjoern W.},
title = {Deep End-to-End Representation Learning for Food Type Recognition from Speech},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3243683},
doi = {10.1145/3242969.3243683},
abstract = {The use of Convolutional Neural Networks (CNN) pre-trained for a particular task, as a feature extractor for an alternate task, is a standard practice in many image classification paradigms. However, to date there have been comparatively few works exploring this technique for speech classification tasks. Herein, we utilise a pre-trained end-to-end Automatic Speech Recognition CNN as a feature extractor for the task of food-type recognition from speech. Furthermore, we also explore the benefits of Compact Bilinear Pooling for combining multiple feature representations extracted from the CNN. Key results presented indicate the suitability of this approach. When combined with a Recurrent Neural Network classifier, our strongest system achieves, for a seven-class food-type classification task an unweighted average recall of 73.3% on the test set of the iHEARu-EAT database.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {574–578},
numpages = {5},
keywords = {recurrent neural networks, eating condition, end-to-end learning, deep representation learning, compact bilinear pooling},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@article{10.1109/TASLP.2020.3030489,
author = {Agrawal, Purvi and Ganapathy, Sriram},
title = {Interpretable Representation Learning for Speech and Audio Signals Based on Relevance Weighting},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3030489},
doi = {10.1109/TASLP.2020.3030489},
abstract = {The learning of interpretable representations from raw data presents significant challenges for time series data like speech. In this work, we propose a relevance weighting scheme that allows the interpretation of the speech representations during the forward propagation of the model itself. The relevance weighting is achieved using a sub-network approach that performs the task of feature selection. A relevance sub-network, applied on the output of first layer of a convolutional neural network model operating on raw speech signals, acts as an acoustic filterbank (FB) layer with relevance weighting. A similar relevance sub-network applied on the second convolutional layer performs modulation filterbank learning with relevance weighting. The full acoustic model consisting of relevance sub-networks, convolutional layers and feed-forward layers is trained for a speech recognition task on noisy and reverberant speech in the Aurora-4, CHiME-3 and VOiCES datasets. The proposed representation learning framework is also applied for the task of sound classification in the UrbanSound8K dataset. A detailed analysis of the relevance weights learned by the model reveals that the relevance weights capture information regarding the underlying speech/audio content. In addition, speech recognition and sound classification experiments reveal that the incorporation of relevance weighting in the neural network architecture improves the performance significantly.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {2823–2836},
numpages = {14}
}

@inproceedings{10.1145/3460120.3484742,
author = {Wenger, Emily and Bronckers, Max and Cianfarani, Christian and Cryan, Jenna and Sha, Angela and Zheng, Haitao and Zhao, Ben Y.},
title = {"Hello, It's Me": Deep Learning-Based Speech Synthesis Attacks in the Real World},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484742},
doi = {10.1145/3460120.3484742},
abstract = {Advances in deep learning have introduced a new wave of voice synthesis tools, capable of producing audio that sounds as if spoken by a target speaker. If successful, such tools in the wrong hands will enable a range of powerful attacks against both humans and software systems (aka machines). This paper documents efforts and findings from a comprehensive experimental study on the impact of deep-learning based speech synthesis attacks on both human listeners and machines such as speaker recognition and voice-signin systems. We find that both humans and machines can be reliably fooled by synthetic speech, and that existing defenses against synthesized speech fall short. These findings highlight the need to raise awareness and develop new protections against synthetic speech for both humans and machines.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {235–251},
numpages = {17},
keywords = {speech synthesis, neural networks, biometric security},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@article{10.1109/TASLP.2019.2905167,
author = {Koriyama, Tomoki and Kobayashi, Takao},
title = {Statistical Parametric Speech Synthesis Using Deep Gaussian Processes},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2905167},
doi = {10.1109/TASLP.2019.2905167},
abstract = {This paper proposes a framework of speech synthesis based on deep Gaussian processes DGPs, which is a deep architecture model composed of stacked Bayesian kernel regressions. In this method, we train a statistical model of transformation from contextual features to speech parameters in a similar manner to deep neural network DNN-based speech synthesis. To apply DGPs to a statistical parametric speech synthesis framework, our framework uses an approximation method, doubly stochastic variational inference, which is suitable for an arbitrary amount of data. Since the training of DGPs is based on the marginal likelihood that takes into account not only data fitting, but also model complexity, DGPs are less vulnerable to overfitting compared with DNNs. In experimental evaluations, we investigated a performance comparison of the proposed DGP-based framework with a feedforward DNN-based one. Subjective and objective evaluation results showed that our DGP framework yielded a higher mean opinion score and lower acoustic feature distortions than the conventional framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {948–959},
numpages = {12}
}

@article{10.1109/TASLP.2022.3153258,
author = {Pan, Zexu and Tao, Ruijie and Xu, Chenglin and Li, Haizhou},
title = {Selective Listening by Synchronizing Speech With Lips},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153258},
doi = {10.1109/TASLP.2022.3153258},
abstract = {A speaker extraction algorithm seeks to extract the speech of a target speaker from a multi-talker speech mixture when given a cue that represents the target speaker, such as a pre-enrolled speech utterance, or an accompanying video track. Visual cues are particularly useful when a pre-enrolled speech is not available. In this work, we don’t rely on the target speaker’s pre-enrolled speech, but rather use the target speaker’s face track as the speaker cue, that is referred to as the auxiliary reference, to form an attractor towards the target speaker. We advocate that the temporal synchronization between the speech and its accompanying lip movements is a direct and dominant audio-visual cue. Therefore, we propose a self-supervised pre-training strategy, to exploit the speech-lip synchronization cue for target speaker extraction, which allows us to leverage abundant unlabeled in-domain data. We transfer the knowledge from the pre-trained model to the attractor encoder of the speaker extraction network. We show that the proposed speaker extraction network outperforms various competitive baselines in terms of signal quality, perceptual quality, and intelligibility, achieving state-of-the-art performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1650–1664},
numpages = {15}
}

@article{10.1109/TASLP.2022.3231700,
author = {Barhoush, Mahdi and Hallawa, Ahmed and Peine, Arne and Martin, Lukas and Schmeink, Anke},
title = {Localization-Driven Speech Enhancement in Noisy Multi-Speaker Hospital Environments Using Deep Learning and Meta Learning},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3231700},
doi = {10.1109/TASLP.2022.3231700},
abstract = {This work addresses the problem of 3D-localizing and enhancing the speech of one main speaker in noisy multi-speaker hospital environments using a multi-channel microphone array. In our model, we propose conducting speaker localization using a machine learning model based on convolutional recurrent neural networks (CRNN) followed by minimum variance distortionless response (MVDR) beamforming. In addition, to ensure that our speech enhancement module is adaptive when deployed in different environments, we trained a meta learning model. Firstly, in the localization step, an estimation of the direction of arrival (DOA) in the elevation and azimuth planes is executed. This is conducted in a 3D space with the presence of noise, reverberation, and up to two more speakers. Using estimated DOA, the MVDR beamformer then enhances the speech of the main speaker. In order to test our model, we adopted and simulated a real-world problem where the objective was to enhance the speech of a clinician in a noisy intensive care unit (ICU) with the presence of other speakers. Furthermore, in order to validate our model, we adopted a speech-to-text module to evaluate the word error rate. Moreover, we implemented our algorithm on hardware using commercially available components and tested it in a real environment. Results showed that our model outperforms other machine learning and non-machine learning algorithms. Finally, we used our trained Meta Learning model to show that our model can adapt to new environments while maintaining high performance after retraining with only a few-shot recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {670–683},
numpages = {14}
}

@article{10.1109/TASLP.2019.2913499,
author = {Lee, Chia-Hsuan and Lee, Hung-yi and Wu, Szu-Lin and Liu, Chi-Liang and Fang, Wei and Hsu, Juei-Yang and Tseng, Bo-Hsiang},
title = {Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD},
year = {2019},
issue_date = {September 2019},
publisher = {IEEE Press},
volume = {27},
number = {9},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2913499},
doi = {10.1109/TASLP.2019.2913499},
abstract = {A user can scan through a text easily, but it is not the case for spoken content, because they cannot be directly displayed on-screen. As a result, accessing large collections of spoken content is much more difficult and time-consuming than doing so for the text content. It would therefore be helpful to develop machines that understand spoken content. In this paper, we propose two new tasks for machine comprehension of spoken content. The first is a listening comprehension test for TOEFL, a challenging academic English examination for English learners who are not the native English speakers. We show that the proposed model outperforms the naive approaches and other neural network based models by exploiting the hierarchical structures of natural languages and the selective power of attention mechanism. For the second listening comprehension task&nbsp;–&nbsp;spoken SQuAD&nbsp;–&nbsp;we find that speech recognition errors severely impair machine comprehension; we propose the use of subword units to mitigate the impact of these errors.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {1469–1480},
numpages = {12}
}

@inproceedings{10.1109/MICRO.2018.00036,
author = {Shin, Seunghee and LeBeane, Michael and Solihin, Yan and Basu, Arkaprava},
title = {Neighborhood-Aware Address Translation for Irregular GPU Applications},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00036},
doi = {10.1109/MICRO.2018.00036},
abstract = {Recent studies on commercial hardware demonstrated that irregular GPU workloads could bottleneck on virtual-to-physical address translations. GPU's single-instruction-multiple-thread (SIMT) execution can generate many concurrent memory accesses, all of which require address translation before accesses can complete. Unfortunately, many of these address translation requests often miss in the TLB, generating many concurrent page table walks. In this work, we investigate how to reduce address translation overheads for such applications.We observe that many of these concurrent page walk requests, while irregular from the perspective of a single GPU wavefront, still fall on neighboring virtual page addresses. The address mappings for these neighboring pages are typically stored in the same 64-byte cache line. Since cache lines are the smallest granularity of memory access, the page table walker implicitly reads address mappings (i.e., page table entries or PTEs) of many neighboring pages during the page walk of a single virtual address (VA). However, in the conventional hardware, mappings not associated with the original request are simply discarded. In this work, we propose mechanisms to coalesce the address translation needs of all pending page table walks in the same neighborhood that happen to have their address mappings fall on the same cache line. This is almost free; the page table walker (PTW) already reads a full cache line containing address mappings of all pages in the same neighborhood. We find this simple scheme can reduce the number of accesses to the in-memory page table by 37% on average. This speeds up a set of GPU workloads by an average of 1.7X.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {352–363},
numpages = {12},
keywords = {GPU, computer architecture, virtual address},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{10.1145/3539490.3539601,
author = {Mohapatra, Payal and Pandey, Akash and Islam, Bashima and Zhu, Qi},
title = {Speech Disfluency Detection with Contextual Representation and Data Distillation},
year = {2022},
isbn = {9781450394031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539490.3539601},
doi = {10.1145/3539490.3539601},
abstract = {Stuttering affects almost 1% of the world's population. It has a deep sociological impact and hinders the people who stutter from taking advantage of voice-assisted services. Automatic stutter detection based on deep learning can help voice assistants to adapt themselves to atypical speech. However, disfluency data is very limited and expensive to generate. We propose a set of preprocessing techniques: (1) using data with high inter-annotator agreement, (2) balancing different classes, and (3) using contextual embeddings from a pretrained network. We then design a disfluency classification network (DisfluencyNet) for automated speech disfluency detection that takes these contextual embeddings as an input. We empirically demonstrate high performance using only a quarter of the data for training. We conduct experiments with different training data size, evaluate the model trained on the lowest amount of training data with SEP-28k baseline results, and evaluate the same model on the FluencyBank dataset baseline results. We observe that, even by using a quarter of the original size of the dataset, our F1 score is greater than 0.7 for all types of disfluencies except one,textit{ blocks}. Previous works also reported lower performance with textit{blocks} type of disfluency owing to its large diversity amongst speakers and events. Overall, with our approach using only a few minutes of data, we can train a robust network that outperforms the baseline results for all disfluencies by at least 5%. Such a result is important to stress the fact that we can now reduce the required amount of training data and are able to improve the quality of the dataset by appointing more than two annotators for labeling speech disfluency within a constrained labeling budget.},
booktitle = {Proceedings of the 1st ACM International Workshop on Intelligent Acoustic Systems and Applications},
pages = {19–24},
numpages = {6},
keywords = {Contextual Representation, Speech Disfluency, Neural Networks, Deep Learning},
location = {Portland, OR, USA},
series = {IASA '22}
}

@inproceedings{10.1145/3503161.3547728,
author = {Xu, Kele and Feng, Ming and Huang, Weiquan},
title = {Seeing Speech: Magnetic Resonance Imaging-Based Vocal Tract Deformation Visualization Using Cross-Modal Transformer},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547728},
doi = {10.1145/3503161.3547728},
abstract = {As an essential component to advance speech science, understanding of speech production can be greatly helpful to improve our understanding of motor control, dynamical systems of humans during natural speech. Different medical imaging modalities have been leveraged to visualize the dynamic process, in which Magnetic resonance imaging (MRI) provides a valuable tool for evaluating static postures. In this demo, we present our solution to visualize the vocal tract deformation, leveraging the correlation between the MRI and the acoustical signals. We first formulate the problem as a cross-modal prediction task and a novel cross-modal Transformer network is proposed. Thus, we can infer the deformation of the vocal tract by only utilizing the acoustical signals. Then, we present an interactive framework, which can be used to visualize the deformation utilizing the aforementioned network. We hope our solution can also be helpful in pronunciation training for children with sound speech disorders and second language learning.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6947–6949},
numpages = {3},
keywords = {acoustic-to-articulatory inversion, transformer, cross modality machine learning, speech production},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2021.3092838,
author = {Furnon, Nicolas and Serizel, Romain and Essid, Slim and Illina, Irina},
title = {DNN-Based Mask Estimation for Distributed Speech Enhancement in Spatially Unconstrained Microphone Arrays},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3092838},
doi = {10.1109/TASLP.2021.3092838},
abstract = {Deep neural network (DNN)-based speech enhancement algorithms in microphone arrays have now proven to be efficient solutions to speech understanding and speech recognition in noisy environments. However, in the context of ad-hoc microphone arrays, many challenges remain and raise the need for distributed processing. In this paper, we propose to extend a previously introduced distributed DNN-based time-frequency mask estimation scheme that can efficiently use spatial information in form of so-called compressed signals which are pre-filtered target estimations. We study the performance of this algorithm named Tango under realistic acoustic conditions and investigate practical aspects of its optimal application. We show that the nodes in the microphone array cooperate by taking profit of their spatial coverage in the room. We also propose to use the compressed signals not only to convey the target estimation but also the noise estimation in order to exploit the acoustic diversity recorded throughout the microphone array.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jun},
pages = {2310–2323},
numpages = {14}
}

@inproceedings{10.1145/3267935.3267950,
author = {Huang, Dong-Yan and Chandra, Ellensi and Yang, Xiangting and Zhou, Ying and Ming, Huaiping and Lin, Weisi and Dong, Minghui and Li, Haizhou},
title = {Visual Speech Emotion Conversion Using Deep Learning for 3D Talking Head},
year = {2018},
isbn = {9781450359856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267935.3267950},
doi = {10.1145/3267935.3267950},
abstract = {In this paper, we present an audio-visual emotion conversion based on deep learning for 3D talking head. The technology aims at retargeting neutral facial and speech expression into emotional ones. The challenging issues are how to control dynamics and variations of different expressions of both speech and the face. The controllability of facial expressions is achieved by training a parallel neutral and emotional marker-based facial motion capture data using a temporal restricted Boltzmann machine (TRBMs) for emotion transfer, while emotional voice conversion is to use long short term memory recurrent neural networks (LSTM-RNNs). Through the combination of 3D skinning method and 3D motion capture, we can make facial animation model close to physical reality for different expressions of 3D talking head. Results on subjective emotion recognition task show that recognition rates of the synthetic audio-visual emotion are comparable to those given the original videos of the speaker.},
booktitle = {Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data},
pages = {7–13},
numpages = {7},
keywords = {visual speech emotion conversion, tempral restricted boltzmann machine, long short term memeory recurrent neural network, motion capture, emotional voice conversion, 3d talking head, visual transfer},
location = {Seoul, Republic of Korea},
series = {ASMMC-MMAC'18}
}

@article{10.1109/TASLP.2018.2845665,
author = {Xue, Wei and Moore, Alastair. H. and Brookes, Mike and Naylor, Patrick A.},
title = {Modulation-Domain Multichannel Kalman Filtering for Speech Enhancement},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2845665},
doi = {10.1109/TASLP.2018.2845665},
abstract = {Compared with single-channel speech enhancement methods, multichannel methods can utilize spatial information to design optimal filters. Although some filters adaptively consider second-order signal statistics, the temporal evolution of the speech spectrum is usually neglected. By using linear prediction LP to model the inter-frame temporal evolution of speech, single-channel Kalman filtering KF based methods have been developed for speech enhancement. In this paper, we derive a multichannel KF MKF that jointly uses both interchannel spatial correlation and interframe temporal correlation for speech enhancement. We perform LP in the modulation domain, and by incorporating the spatial information, derive an optimal MKF gain in the short-time Fourier transform domain. We show that the proposed MKF reduces to the conventional multichannel Wiener filter if the LP information is discarded. Furthermore, we show that, under an appropriate assumption, the MKF is equivalent to a concatenation of the minimum variance distortion response beamformer and a single-channel modulation-domain KF and therefore present an alternative implementation of the MKF. Experiments conducted on a public head-related impulse response database demonstrate the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {oct},
pages = {1833–1847},
numpages = {15}
}

@inproceedings{10.1145/3386164.3389100,
author = {Musaev, Muhammadjon and Khujayorov, Ilyos and Ochilov, Mannon},
title = {Image Approach to Speech Recognition on CNN},
year = {2020},
isbn = {9781450376617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386164.3389100},
doi = {10.1145/3386164.3389100},
abstract = {In this paper has been discussed about speech recognition using spectrogram images and deep convolution neural network(CNN) of Uzbek spoken digits. Spectrogram images from speech signal were generated and it were used for deep CNN training. Presented CNN model contains 3 convolution layers and 2 fully connected layers that discriminative features can be divided and estimated of spectrogram images by those layers. In current research period, dataset of Uzbek spoken digits were made and in based on presented CNN model they were trained. Testing results shows that, proposed approach for Uzbek spoken digits classified 100% accuracy.},
booktitle = {Proceedings of the 2019 3rd International Symposium on Computer Science and Intelligent Control},
articleno = {57},
numpages = {6},
keywords = {Speech Recognition, Spectrogram Image, Speech Classification, Convolutional Neural Network},
location = {Amsterdam, Netherlands},
series = {ISCSIC 2019}
}

@article{10.1109/TASLP.2022.3156797,
author = {Lee, Moa and Lee, Junmo and Chang, Joon-Hyuk},
title = {Non-Autoregressive Fully Parallel Deep Convolutional Neural Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3156797},
doi = {10.1109/TASLP.2022.3156797},
abstract = {Deep learning-based speech synthesis evolves by employing a sequence-to-sequence (seq2seq) structure with an attention mechanism. The seq2seq speech synthesis model consists of a pair of the encoder for delivering the linguistic features and the decoder for predicting the mel-spectrogram, and learns the alignment between text and speech through the attention mechanism. The decoder predicts the mel-spectrogram by an autoregressive flow that considers the current input and what they have learned from previous inputs. This is beneficial when processing the sequential data, as in speech synthesis. However, the recursive generation of speech typically requires extensive training time, which slows the speed of synthesis. To overcome these obstacles, we propose a non-autoregressive framework for fully parallel deep convolutional neural speech synthesis. Firstly, we design a new synthesis paradigm that integrates a time-varying metatemplate (TVMT), whose length is modeled with a separate conditional distribution, to prepare the decoder input. The decoding step converts the TVMT into spectral features, which eliminates the autoregressive flow. Secondly, we propose a structure that uses multiple decoders interconnected by up-down chains with an iterative attention mechanism. The decoder chains distribute the burden of decoding, progressively infusing the information obtained from the training target example into the chains to refine the predicted spectral features at each decoding step. For each decoder, the attention mechanism is repeatedly applied to produce the elaborated alignment between the linguistic features and the TVMT, which is gradually transformed into the spectral features. The proposed architecture substantially improves the synthesis speed, and the resulting speech quality is superior to that of a conventional autoregressive model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {1150–1159},
numpages = {10}
}

@inproceedings{10.1145/3472307.3484649,
author = {Kim, Sujeong and Tamrakar, Amir},
title = {“How to Best Say It?” : Translating Directives in Machine Language into Natural Language in the Blocks World},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484649},
doi = {10.1145/3472307.3484649},
abstract = {We propose a method to generate optimal natural language for block placement directives generated by a machine’s planner during human-agent interactions in the blocks world. A non user-friendly machine directive, e.g., move(ObjId, toPos), is transformed into visually and contextually grounded referring expressions that are much easier for the user to comprehend. We describe an algorithm that progressively and generatively transforms the machine’s directive in ECI (Elementary Composable Ideas)-space, generating many alternative versions of the directive. We then define a cost function to evaluate the ease of comprehension of these alternatives and select the best option. The parameters for this cost function were derived empirically from a user study that measured utterance-to-action timings.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {289–293},
numpages = {5},
keywords = {optimization, natural language generation, perception, cost},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@article{10.1145/3373608,
author = {Song, Hyun-Je and Park, Seong-Bae},
title = {Korean Part-of-Speech Tagging Based on Morpheme Generation},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3373608},
doi = {10.1145/3373608},
abstract = {Two major problems of Korean part-of-speech (POS) tagging are that the word-spacing unit is not mapped one-to-one to a POS tag and that morphemes should be recovered during POS tagging. Therefore, this article proposes a novel two-step Korean POS tagger that solves the problems. This tagger first generates a sequence of lemmatized and recovered morphemes that can be mapped one-to-one to a POS tag using an encoder-decoder architecture derived from a POS-tagged corpus. Then, the POS tag of each morpheme in the generated sequence is finally determined by a standard sequence labeling method. Since the knowledge for segmenting and recovering morphemes is extracted automatically from a POS-tagged corpus by an encoder-decoder architecture, the POS tagger is constructed without a dictionary nor handcrafted linguistic rules. The experimental results on a standard dataset show that the proposed method outperforms existing POS taggers with its state-of-the-art performance.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {jan},
articleno = {41},
numpages = {10},
keywords = {Part-of-speech tagging, morphologically complex languages, morpheme generation}
}

@inproceedings{10.1145/3491101.3519700,
author = {Rekimoto, Jun},
title = {DualVoice: A Speech Interaction Method Using Whisper-Voice as Commands},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519700},
doi = {10.1145/3491101.3519700},
abstract = {Applications based on speech recognition have become widely used, and speech input is increasingly being utilized to create documents. However, it is still difficult to correct misrecognition by speech, which makes it necessary to re-edit documents by other means such as manual input. It is also difficult to input symbols and commands because these may be misrecognized as text letters. To deal with these problems, we propose a speech interaction method called DualVoice in which commands are input in a whispered voice and letters in a normal voice. The proposed method does not require any special hardware other than a regular microphone, thus enabling a complete hands-free interaction, and it can be used in a wide range of situations where speech recognition is already available. We designed two neural networks, one for discriminating normal speech from whispered speech, and the second for recognizing whisper speech.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {335},
numpages = {6},
keywords = {whisper voice classification, whisper voice recognition, neural networks},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3340672.3341119,
author = {Lester, Martin Mariusz},
title = {Analysis of MiniJava Programs via Translation to ML},
year = {2019},
isbn = {9781450368643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340672.3341119},
doi = {10.1145/3340672.3341119},
abstract = {MiniJava is a subset of the object-oriented programming language Java. Standard ML is the canonical representative of the ML family of functional programming languages, which includes F# and OCaml. Different program analysis and verification tools and techniques have been developed for both Java-like and ML-like languages. Naturally, the tools developed for a particular language emphasise accurate treatment of language features commonly used in that language. In Java, this means objects with mutable properties and dynamic method dispatch. In ML, this means higher order functions and algebraic datatypes with pattern matching.We propose to translate programs from one language into the other and use the target language's tools for analysis and verification. By doing so, we hope to identify areas for improvement in the target language's tools and suggest techniques, perhaps as used in the source language's tools, that may guide their improvement. More generally, we hope to develop tools for reasoning about programs that are more resilient to changes in the style of code and representation of data. We begin our programme by outlining a translation from MiniJava to ML that uses only the core features of ML; in particular, it avoids the use of ML's mutable references.},
booktitle = {Proceedings of the 21st Workshop on Formal Techniques for Java-like Programs},
articleno = {6},
numpages = {3},
keywords = {automated verification, static analysis, ML, program transformation, Java},
location = {London, United Kingdom},
series = {FTfJP '19}
}

@inproceedings{10.1145/3282866.3282872,
author = {Gao, Yuyang and Guo, Xiaojie and Zhao, Liang},
title = {Local Event Forecasting and Synthesis Using Unpaired Deep Graph Translations},
year = {2018},
isbn = {9781450360357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282866.3282872},
doi = {10.1145/3282866.3282872},
abstract = {Local rare event forecasting and synthesis on networks are highly useful for emergence management. For example, synthesizing traffic congestion and disease diffusion over the road network and disease-contact network respectively of specific geo-locations is highly important for transportation planning and disease outbreaks intervention. This task requires to learn how the events of congestion or disease "translate" the graph patterns from source mode (e.g., without event) to target mode (e.g., with event) based on historical data for some locations. Then it needs to apply such "translation" upon a source-mode graph pattern in a new location's network, in order to estimate and foresee what it will look like in target-mode in this location.Such task is called graph translation, which is an analogy and generalization to image and text translation. Similar to the situations in image and text translation, paired training data, which consists of pairs of source-mode graph and its corresponding target-mode, will usually not be available. In this work, we propose an approach for learn the translation of graphs from source-mode to target-mode such that the generated target-mode is indistinguishable from the distribution of the real target-mode using an adversarial loss. Because there is no paired training data, we also learn an inverse translation from target-mode to source-mode and couple these two translation mappings through cycle consistency loss. Extensive experiments on both synthetic and real-world application data demonstrate that the proposed approaches is capable of generating graphs close to real target graphs. Case studies on the synthesized networks have also been illustrated and analyzed to show the reasonableness of the generated target-mode graphs.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Analytics for Local Events and News},
articleno = {5},
numpages = {8},
keywords = {graph generation, graph translation, representation learning, Deep learning},
location = {Seattle, WA, USA},
series = {LENS'18}
}

@inproceedings{10.1145/3458709.3458941,
author = {Rekimoto, Jun and Nishimura, Yu},
title = {Derma: Silent Speech Interaction Using Transcutaneous Motion Sensing},
year = {2021},
isbn = {9781450384285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458709.3458941},
doi = {10.1145/3458709.3458941},
abstract = {Silent speech interaction (SSI) enables speech communication without uttering an actual voice and can have a potential to make speech interaction available in public places. However, commonly studied image-based lip-reading for SSI requires a camera in front of the face and thus it is not suitable for mobile use. Ultrasound imaging requires expensive and complex equipment. In contrast, we propose a much simpler method by skin motion sensing. Two small 6-DOF accelerometer/angular velocity sensors attached under the chin acquire 12-dimensional multidimensional information of skin motion caused by the silent utterance. With neural networks, 35 different silent commands are identified with a recognition rate of 94%. While previous lip-reading studies have normally inferred speech from images of video with vocal speech, this study also proposes a method of learning entirely from non-vocal speech only. Compared to previous studies, we consider the proposed solution is less visible, lightweight, and is not affected by lighting conditions.},
booktitle = {Proceedings of the Augmented Humans International Conference 2021},
pages = {91–100},
numpages = {10},
keywords = {silent speech interaction, neural networks, skin motion sensing},
location = {Rovaniemi, Finland},
series = {AHs '21}
}

@inproceedings{10.1145/3394171.3414444,
author = {Bu, Yaohua and Li, Weijun and Ma, Tianyi and Chen, Shengqi and Jia, Jia and Li, Kun and Lu, Xiaobo},
title = {Visual-Speech Synthesis of Exaggerated Corrective Feedback},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414444},
doi = {10.1145/3394171.3414444},
abstract = {To provide more discriminative feedback for the second language (L2) learners to better identify their mispronunciation, we propose a method for exaggerated visual-speech feedback in computer-assisted pronunciation training (CAPT). The speech exaggeration is realized by an emphatic speech generation neural network based on Tacotron, while the visual exaggeration is accomplished by ADC Viseme Blending, namely increasing Amplitude of movement, extending the phone's Duration and enhancing the color Contrast. User studies show that exaggerated feedback outperforms non-exaggerated version on helping learners with pronunciation identification and pronunciation improvement.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4521–4523},
numpages = {3},
keywords = {emphatic speech synthesis, visual-speech exaggeration, pronunciation learning, corrective feedback},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3340555.3356093,
author = {Aftab, Abdul Rafey},
title = {Multimodal Driver Interaction with Gesture, Gaze and Speech},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356093},
doi = {10.1145/3340555.3356093},
abstract = {The ever-growing research in computer vision has created new avenues for user interaction. Speech commands and gesture recognition are already being applied in various touch-based inputs. It is, therefore, foreseeable, that the use of multimodal input methods for user interaction is the next phase in development. In this paper, I propose a research plan of novel methods for the use of multimodal inputs for the semantic interpretation of human-computer interaction, specifically applied to a car driver. A fusion methodology has to be designed that adequately makes use of a recognized gesture (specifically finger pointing), eye gaze and head pose for the identification of reference objects, while using the semantics from speech for a natural interactive environment for the driver. The proposed plan includes different techniques based on artificial neural networks for the fusion of the camera-based modalities (gaze, head and gesture). It then combines features extracted from speech with the fusion algorithm to determine the intent of the driver.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {487–492},
numpages = {6},
keywords = {gesture recognition, RNN, late fusion, speech commands, CNN., eye-tracking, LSTM, head pose, Data fusion},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3240508.3241911,
author = {Kumar, Yaman and Aggarwal, Mayank and Nawal, Pratham and Satoh, Shin'ichi and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {Harnessing AI for Speech Reconstruction Using Multi-View Silent Video Feed},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241911},
doi = {10.1145/3240508.3241911},
abstract = {Speechreading or lipreading is the technique of understanding and getting phonetic features from a speaker's visual features such as movement of lips, face, teeth and tongue. It has a wide range of multimedia applications such as in surveillance, Internet telephony, and as an aid to a person with hearing impairments. However, most of the work in speechreading has been limited to text generation from silent videos. Recently, research has started venturing into generating (audio) speech from silent video sequences but there have been no developments thus far in dealing with divergent views and poses of a speaker. Thus although, we have multiple camera feeds for the speech of a user, but we have failed in using these multiple video feeds for dealing with the different poses. To this end, this paper presents the world's first ever multi-view speech reading and reconstruction system. This work encompasses the boundaries of multimedia research by putting forth a model which leverages silent video feeds from multiple cameras recording the same subject to generate intelligent speech for a speaker. Initial results confirm the usefulness of exploiting multiple camera views in building an efficient speech reading and reconstruction system. It further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Next, it lays out various innovative applications for the proposed system focusing on its potential prodigious impact in not just security arena but in many other multimedia analytics problems.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1976–1983},
numpages = {8},
keywords = {cnn-lstm models, automatic speech reconstruction, multimedia systems, lipreading, speechreading, speech reconstruction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{10.1109/TASLP.2019.2901643,
author = {Ibarrola, Francisco Javier and Spies, Ruben Daniel and Persia, Leandro Ezequiel Di},
title = {Switching Divergences for Spectral Learning in Blind Speech Dereverberation},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2901643},
doi = {10.1109/TASLP.2019.2901643},
abstract = {When recorded in an enclosed room, a sound signal will most certainly get affected by reverberation. This not only undermines audio quality, but also poses a problem for many human-machine interaction technologies that use speech as their input. In this paper, a new blind, two-stage dereverberation approach based in a generalized $beta$-divergence as a fidelity term over a non-negative representation is proposed. The first stage consists of learning the spectral structure of the signal solely from the observed spectrogram, while the second stage is devoted to model reverberation. Both steps are taken by minimizing a cost function in which the aim is put either in constructing a dictionary or a good representation by changing the divergence involved. In addition, an approach for finding an optimal fidelity parameter for dictionary learning is proposed. An algorithm for implementing the proposed method is described and tested against state-of-the-art methods. Results show improvements for both artificial reverberation and real recordings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {881–891},
numpages = {11}
}

@inproceedings{10.1145/3405755.3406118,
author = {Wu, Yunhan and Edwards, Justin and Cooney, Orla and Bleakley, Anna and Doyle, Philip R. and Clark, Leigh and Rough, Daniel and Cowan, Benjamin R.},
title = {Mental Workload and Language Production in Non-Native Speaker IPA Interaction},
year = {2020},
isbn = {9781450375443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405755.3406118},
doi = {10.1145/3405755.3406118},
abstract = {Through smartphones and smart speakers, intelligent personal assistants (IPAs) have made speech a common interaction modality. With linguistic coverage and varying functionality levels, many speakers engage with IPAs using a non-native language. This may impact mental workload and patterns of language production used by non-native speakers. We present a mixed-design experiment, where native (L1) and non-native (L2) English speakers completed tasks with IPAs via smartphones and smart speakers. We found significantly higher mental workload for L2 speakers in IPA interactions. Contrary to our hypotheses, we found no significant differences between L1 and L2 speakers in number of turns, lexical complexity, diversity, or lexical adaptation when encountering errors. These findings are discussed in relation to language production and processing load increases for L2 speakers in IPA interaction.},
booktitle = {Proceedings of the 2nd Conference on Conversational User Interfaces},
articleno = {3},
numpages = {8},
keywords = {non-native language speakers, speech interface, voice user interface, intelligent personal assistants},
location = {Bilbao, Spain},
series = {CUI '20}
}

@inproceedings{10.1145/3519939.3523710,
author = {Schuster, Philipp and Brachth\"{a}user, Jonathan Immanuel and M\"{u}ller, Marius and Ostermann, Klaus},
title = {A Typed Continuation-Passing Translation for Lexical Effect Handlers},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523710},
doi = {10.1145/3519939.3523710},
abstract = {Effect handlers are a language feature which enjoys popularity in academia and is also gaining traction in industry. Programs use abstract effect operations and handlers provide meaning to them in a delimited scope. Each effect operation is handled by the dynamically closest handler. Using an effect operation outside of a matching handler is meaningless and results in an error. A type-and-effect system prevents such errors from happening. Lexical effect handlers are a recent variant of effect handlers with a number of attractive properties. Just as with traditional effect handlers, programs use effect operations and handlers give meaning to them. But unlike with traditional effect handlers, the connection between effect operations and their handler is lexical. Consequently, they typically have different type-and-effect systems. The semantics of lexical effect handlers as well as their implementations use multi-prompt delimited control. They rely on the generation of fresh labels at runtime, which associate effect operations with their handlers. This use of labels and multi-prompt delimited control is theoretically and practically unsatisfactory. Our main result is that typed lexical effect handlers do not need the full power of multi-prompt delimited control. We present the first CPS translation for lexical effect handlers to pure System F. It preserves well-typedness and simulates the traditional operational semantics. Importantly, it does so without requiring runtime labels. The CPS translation can be used to study the semantics of lexical effect handlers as well as as an implementation technique.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {566–579},
numpages = {14},
keywords = {lexical effect handlers, continuation-passing style},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{10.1145/3404555.3404618,
author = {Liu, Chunlei and Wang, Longbiao and Dang, Jianwu},
title = {Amplitude Consistent Enhancement for Speech Dereverberation},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404618},
doi = {10.1145/3404555.3404618},
abstract = {The mapping and masking methods based on deep learning are both essential methods for speech dereverberation at present, which typically enhance the amplitude of the reverberant speech while letting the reverberant phase unprocessed. The reverberant phase and enhanced amplitude are used to synthesize the target speech. However, because the overlapping frames interfere with each other during the superposition process (overlap-and-add), the final synthesized speech signal will deviate from the ideal value. In this paper, we propose an amplitude consistent enhancement method (ACE) to solve this problem. With ACE to train the deep neural networks (DNNs), we use the difference between amplitudes of the synthesized and clean speech as the loss function. Also, we propose a method of adding an adjustment layer to improve the regression accuracy of DNN. The speech dereverberation experiments show that the proposed method has improved the PESQ and SNR by 5% and 15% compared with the traditional signal approximation method.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {412–417},
numpages = {6},
keywords = {amplitude enhancement, mapping, Speech dereverberation, deep learning, masking},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00074,
author = {Hong, Jaemin},
title = {Improving Automatic C-to-Rust Translation with Static Analysis},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00074},
doi = {10.1109/ICSE-Companion58688.2023.00074},
abstract = {While popular in system programming, C has been infamous for its poor language-level safety mechanisms, leading to critical bugs and vulnerabilities. C programs can still have memory and thread bugs despite passing type checking. To resolve this long-standing problem, Rust has been recently developed with rich safety mechanisms, including its notable ownership type system. It prevents memory and thread bugs via type checking. By rewriting legacy C programs in Rust, their developers can discover unknown bugs and avoid adding new bugs. However, the adaptation of Rust in legacy programs is still limited due to the high cost of manual C-to-Rust translation. Rust's safe features are semantically different from C's unsafe features and require programmers to precisely understand the behavior of their programs for correct rewriting. Existing C-to-Rust translators do not relieve this burden because they syntactically translate C features into unsafe Rust features, leaving further refactoring for programmers. In this paper, we propose the problem of improving the state-of-the-art C-to-Rust translation by automatically replacing unsafe features with safe features. Specifically, we identify two important unsafe features to be replaced: lock API and output parameters. We show our results on lock API and discuss plans for output parameters.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {273–277},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1109/TASLP.2019.2955252,
author = {Zhang, Weijian and Song, Peng},
title = {Transfer Sparse Discriminant Subspace Learning for Cross-Corpus Speech Emotion Recognition},
year = {2019},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2955252},
doi = {10.1109/TASLP.2019.2955252},
abstract = {Cross-corpus speech emotion recognition has attracted much attention due to the widespread existence of various emotional speech in life. It takes one corpus for training and another corpus for testing, and generally involves the following two basic problems: the corpus-invariant feature representation and relevance across different corpora. To deal with these two problems, we propose a novel transfer learning method called transfer sparse discriminant subspace learning (TSDSL) in this article. Specifically, to solve the first problem, we learn a common feature subspace of different corpora by introducing the discriminative learning and <inline-formula><tex-math notation="LaTeX">$ell _{2,1}-$</tex-math></inline-formula>norm penalty, which can learn the most discriminative features across different corpora. To address the second problem, we construct a novel nearest neighbor graph as the distance metric, in which the similarity between different corpora can be measured simultaneously. Extensive experiments are carried out on cross-corpus speech emotion recognition tasks, and the results show that our method can achieve competitive performance compared with state-of-the-art algorithms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {dec},
pages = {307–318},
numpages = {12}
}

@inproceedings{10.1145/3526114.3558640,
author = {Chiba, Mariko and Yamada, Wataru and Ochiai, Keiichi},
title = {Shadowed Speech: An Audio Feedback System Which Slows Down Speech Rate},
year = {2022},
isbn = {9781450393218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526114.3558640},
doi = {10.1145/3526114.3558640},
abstract = {In oral communication, it is important to speak at a speed appropriate for the situation. However, we need a lot of training in order to control our speech rate as intended. This paper proposes a speech rate control system which enables the user to speak at a pace closer to the target rate using Delayed Auditory Feedback (DAF). We implement a prototype and confirm that the proposed system can slow down the speech rate when the user speaks too fast without giving any instructions to the speaker on how to respond to the audio feedback.},
booktitle = {Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {66},
numpages = {3},
keywords = {Computer-Enhanced Interaction, Delayed Auditory Feedback},
location = {Bend, OR, USA},
series = {UIST '22 Adjunct}
}

@article{10.1109/TASLP.2022.3203891,
author = {Xue, Boyang and Hu, Shoukang and Xu, Junhao and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
title = {Bayesian Neural Network Language Modeling for Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3203891},
doi = {10.1109/TASLP.2022.3203891},
abstract = {State-of-the-art neural network language models (NNLMs) represented by long short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming highly complex. They are prone to overfitting and poor generalization when given limited training data. To this end, an overarching full Bayesian learning framework encompassing three methods is proposed in this paper to account for the underlying uncertainty in LSTM-RNN and Transformer LMs. The uncertainty over their model parameters, choice of neural activations and hidden output representations are modeled using Bayesian, Gaussian Process and variational LSTM-RNN or Transformer LMs respectively. Efficient inference approaches were used to automatically select the optimal network internal components to be Bayesian learned using neural architecture search. A minimal number of Monte Carlo parameter samples as low as one was also used. These allow the computational costs incurred in Bayesian NNLM training and evaluation to be minimized. Experiments are conducted on two tasks: AMI meeting transcription and Oxford-BBC LipReading Sentences 2 (LRS2) overlapped speech recognition using state-of-the-art LF-MMI trained factored TDNN systems featuring data augmentation, speaker adaptation and audio-visual multi-channel beamforming for overlapped speech. Consistent performance improvements over the baseline LSTM-RNN and Transformer LMs with point estimated model parameters and drop-out regularization were obtained across both tasks in terms of perplexity and word error rate (WER). In particular, on the LRS2 data, statistically significant WER reductions up to 1.3% and 1.2% absolute (12.1% and 11.3% relative) were obtained over the baseline LSTM-RNN and Transformer LMs respectively after model combination between Bayesian NNLMs and their respective baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {sep},
pages = {2900–2917},
numpages = {18}
}

@article{10.1109/TASLP.2021.3060805,
author = {Kamper, Herman and Matusevych, Yevgen and Goldwater, Sharon},
title = {Improved Acoustic Word Embeddings for Zero-Resource Languages Using Multilingual Transfer},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3060805},
doi = {10.1109/TASLP.2021.3060805},
abstract = {Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. Such embeddings can form the basis for speech search, indexing and discovery systems when conventional speech recognition is not possible. In <italic>zero-resource</italic> settings where unlabelled speech is the only available resource, we need a method that gives robust embeddings on an arbitrary language. Here we explore multilingual transfer: we train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. We consider three multilingual recurrent neural network (RNN) models: a classifier trained on the joint vocabularies of all training languages; a Siamese RNN trained to discriminate between same and different words from multiple languages; and a correspondence autoencoder&nbsp;(CAE) RNN trained to reconstruct word pairs. In a word discrimination task on six target languages, all of these models outperform state-of-the-art unsupervised models trained on the zero-resource languages themselves, giving relative improvements of more than 30% in average precision. When using only a few training languages, the multilingual CAE-RNN performs better, but with more training languages the other multilingual models perform similarly. Using more training languages is generally beneficial, but improvements are marginal on some languages. We present probing experiments which show that the CAE-RNN encodes more phonetic, word duration, language identity and speaker information than the other multilingual models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {feb},
pages = {1107–1118},
numpages = {12}
}

@inproceedings{10.1145/3573428.3573659,
author = {Li, Sirui and Zhang, Qinya and Li, Yunpeng and Li, Guanyu and Li, Senyan and Wang, Shaoxuan},
title = {Analyzing Speaker Information in Self-Supervised Models to Improve Unsupervised Speech Recognition},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573659},
doi = {10.1145/3573428.3573659},
abstract = {The quality of speech representation is the key to the success of unsupervised speech recognition. Self-supervised models contain a variety of audio information, and non-speech content information is not beneficial or even harmful to speech recognition, so removing speaker information can greatly improve the accuracy of unsupervised speech recognition. To effectively remove speaker information, this paper first analyzes the audio features extracted by wav2vec 2.0 and HuBERT self-supervised model qualitatively and quantitatively, and derives the mean value of each sentence as a representation of speaker information. The extracted speech representations are then speaker normalized to obtain valid features to be fed into the unsupervised speech recognition model for training. Experimental results on the TIMIT dataset show that the speaker normalization method can significantly reduce the error rate of unsupervised speech recognition.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1300–1305},
numpages = {6},
keywords = {speech recognition, speech representation learning, unsupervised learning},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1145/3503162.3503168,
author = {Dowlagar, Suman and Mamidi, Radhika},
title = {A Survey of Recent Neural Network Models on Code-Mixed Indian Hate Speech Data},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503168},
doi = {10.1145/3503162.3503168},
abstract = {In recent years, given the exponential increase in social media content also led to an increase in online hate speech. We need automatic hate speech detection methods due to the volume of data on the web. Various approaches have been proposed to address hate speech and offensive content on social media. This paper surveys how neural-based models have rapidly evolved to address hate speech on multilingual code-mixed data. We discuss the current state of the research in hate speech and offensive language detection on code-mixed Indian datasets.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {67–74},
numpages = {8},
keywords = {Code-Mixing, Neural Networks, Survey, Hate Speech Detection},
location = {Virtual Event, India},
series = {FIRE '21}
}

@article{10.1109/TASLP.2022.3164199,
author = {Bhattacharjee, Mrinmoy and Prasanna, S. R. M. and Guha, Prithwijit},
title = {Clean vs. Overlapped Speech-Music Detection Using Harmonic-Percussive Features and Multi-Task Learning},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3164199},
doi = {10.1109/TASLP.2022.3164199},
abstract = {Detection of speech and music signals in isolated and overlapped conditions is an essential preprocessing step for many audio applications. Speech signals have wavy and continuous harmonics, while music signals exhibit horizontally linear and discontinuous harmonic patterns. Music signals also contain more percussive components than speech signals, manifested as vertical striations in the spectrograms. In case of speech music overlap, it might be challenging for automatic feature learning systems to extract class-specific horizontal and vertical striations from the combined spectrogram representation. A pre-processing step of separating the harmonic and percussive components before training might aid the classifier. Thus, this work proposes the use of harmonic-percussive source separation method to generate features for better detection of speech and music signals. Additionally, this work also explores the traditional and cascaded-information multi-task learning (MTL) frameworks to design better classifiers. MTL framework aids the training of the main task by employing simultaneous learning of several related auxiliary tasks. Results have been reported both on synthetically generated speech music overlapped signals and real recordings. Four state-of-the-art approaches are used for performance comparison. Experiments show that harmonic and percussive decomposition of spectrograms perform better as features. Moreover, the MTL-framework based classifiers further improve performances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {apr},
pages = {1–10},
numpages = {10}
}

@inproceedings{10.1145/3572334.3572372,
author = {Nkemelu, Daniel and Shah, Harshil and Best, Michael and Essa, Irfan},
title = {Tackling Hate Speech in Low-Resource Languages with Context Experts},
year = {2023},
isbn = {9781450397872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572334.3572372},
doi = {10.1145/3572334.3572372},
abstract = {Given Myanmar’s historical and socio-political context, hate speech spread on social media have escalated into offline unrest and violence. This paper presents findings from our remote study on the automatic detection of hate speech online in Myanmar. We argue that effectively addressing this problem will require community-based approaches that combine the knowledge of context experts with machine learning tools that can analyze the vast amount of data produced. To this end, we develop a systematic process to facilitate this collaboration covering key aspects of data collection, annotation, and model validation strategies. We highlight challenges in this area stemming from small and imbalanced datasets, the need to balance non-glamorous data work and stakeholder priorities, and closed data-sharing practices. Stemming from these findings, we discuss avenues for further work in developing and deploying hate speech detection systems for low-resource languages.},
booktitle = {Proceedings of the 2022 International Conference on Information and Communication Technologies and Development},
articleno = {5},
numpages = {11},
keywords = {digital threats, context experts, hate speech, democracy, low-resource text classification},
location = {Seattle, WA, USA},
series = {ICTD '22}
}

@inproceedings{10.1145/3594315.3594384,
author = {Dou, Huijing and Xing, Luyang and Guo, Hongliang},
title = {Spatial Smoothing Algorithm Based on Virtual Array Extended Translation},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594384},
doi = {10.1145/3594315.3594384},
abstract = {The traditional algorithm used the spatial smoothing method to decorrelate and estimate the direction of the coherent signals. However, the spatial smoothing method inevitably brings about the problem of reducing the aperture of the array, which decreases the estimation accuracy. This paper proposed a spatial smoothing algorithm based on virtual array extended translation. The array was processed by conjugate extension and virtual translation, and then each group of translated virtual arrays was processed by the spatial smoothing algorithm. In this paper, the algorithm compensated the aperture of the array by constructing virtual arrays and using more information of signals in space. The simulation shows that, compared with traditional algorithms and the existing algorithm, the proposed algorithm can estimate more signals, and has higher accuracy and resolution.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {638–643},
numpages = {6},
location = {Tianjin, China},
series = {ICCAI '23}
}

@article{10.1145/3276321,
author = {Engstr\"{o}m, Henrik and \"{O}stblad, Per Anders},
title = {Using Text-to-Speech to Prototype Game Dialog},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
url = {https://doi.org/10.1145/3276321},
doi = {10.1145/3276321},
abstract = {Voice acting is common in computer games in many genres. The recording and processing of voice acting is a time-consuming process that involves, for instance, voice actors, directors, audio engineers, and game writers. Changes to the script of a game after the voice acting has been recorded are expensive. At the same time, playtests of games without voice acting may give different results than testing where it is present. This creates a situation where improvements identified from play testing are either ignored or leads to extensive re-recording of voice acting. This article presents a design science research project where text-to-speech (TTS) synthesis is used as a substitute for recorded voice acting in the early stages of game production. We propose a set of design principles that have been evaluated in a sharp game production. Our results indicate several benefits of using TTS as a prototyping tool: It can be a source of inspiration for game writers, it gives good estimations on timing and pacing of the game, and it allows for early tests of how the dialog will be perceived by players. The quality and characteristics of the voices provided by the TTS system play an important role in this process. The rapid development in the speech technology field opens many future possibilities.},
journal = {Comput. Entertain.},
month = {nov},
articleno = {2},
numpages = {16},
keywords = {design science research, Game development, game writing, game audio, speech technology, text-to-speech}
}

@article{10.1109/TASLP.2022.3195113,
author = {Geng, Mengzhe and Xie, Xurong and Ye, Zi and Wang, Tianzi and Li, Guinan and Hu, Shujie and Liu, Xunying and Meng, Helen},
title = {Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3195113},
doi = {10.1109/TASLP.2022.3195113},
abstract = {Despite the rapid progress of automatic speech recognition (ASR) technologies targeting normal speech in recent decades, accurate recognition of dysarthric and elderly speech remains highly challenging tasks to date. Sources of heterogeneity commonly found in normal speech including accent or gender, when further compounded with the variability over age and speech pathology severity level, create large diversity among speakers. To this end, speaker adaptation techniques play a key role in personalization of ASR systems for such users. Motivated by the spectro-temporal level differences between dysarthric, elderly and normal speech that systematically manifest in articulatory imprecision, decreased volume and clarity, slower speaking rates and increased dysfluencies, novel spectro-temporal subspace basis deep embedding features derived using SVD speech spectrum decomposition are proposed in this paper to facilitate auxiliary feature based speaker adaptation of state-of-the-art hybrid DNN/TDNN and end-to-end Conformer speech recognition systems. Experiments were conducted on four tasks: the English UASpeech and TORGO dysarthric speech corpora; the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The proposed spectro-temporal deep feature adapted systems outperformed baseline i-Vector and x-Vector adaptation by up to 2.63% absolute (8.63% relative) reduction in word error rate (WER). Consistent performance improvements were retained after model based speaker adaptation using learning hidden unit contributions (LHUC) was further applied. The best speaker adapted system using the proposed spectral basis embedding features produced the lowest published WER of 25.05% on the UASpeech test set of 16 dysarthric speakers.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {jul},
pages = {2597–2611},
numpages = {15}
}

@inproceedings{10.1145/3446132.3446424,
author = {Utitiaj, Ismael and Morillo, Paulina and Huanga, Diego Vallejo},
title = {Sentiment Analysis Tool for Spanish Tweets in the Ecuadorian Context},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446424},
doi = {10.1145/3446132.3446424},
abstract = {The huge amount of textual information that exists on social networks added by users through comments, has aroused a great interest in companies and research groups, which seek to use this information to identify trends and acceptance levels of brands, products, and services. A technique to know the level of acceptance or rejection of a particular topic, in an automated way, is the Sentiment Analysis. Some informatics tools incorporate this technique, however, there are few contributions to texts in Spanish. It is because of the difficulty of identifying different contexts, dialects, complex grammatical structures, and semantic language variances in each region. This article presents a web tool for the analysis of sentiments in texts written in Spanish that include Ecuadorian dialect or idioms. The tool was developed in R-Shiny with an approach lexicon-based. The tool allows the customization of the lexicons based on context and facilitates the automatic download of tweets according to search criteria such as the place, dates, and topic. To evaluate the effectiveness of the application, their result was compared with two commercial tools (Azure Text Analytics and IBM Watson NLU) and a manual score carried out by a group of people. The tests include the analysis of three corpora created from tweets. The results show the effectiveness of the tool to identify the sentiment polarity, especially in texts that include dialects, colloquial words, and negative expressions.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {75},
numpages = {6},
keywords = {IBM Watson NLU, R-Shiny, Lexicon, Azure Text Analytics, Twiter},
location = {Sanya, China},
series = {ACAI '20}
}

